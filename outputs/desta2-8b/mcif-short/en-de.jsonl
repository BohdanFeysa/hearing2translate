{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, willkommen zu unserer Präsentation von DePlain, einem neuen Korpus für die Identifizierung von Texten auf der Dokumentenebene und auf der Satzebene."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Ich heiße Regina Storben und werde Sie durch den ersten Teil der Präsentation führen. Zunächst definieren wir Textvereinfachung."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "\"Die Textanpassung ist ein Prozess, bei dem ein Text angepasst wird, um die Verständlichkeit des Textes für eine bestimmte Zielgruppe zu verbessern, insbesondere für Menschen mit Lesebehinderungen oder Nicht-Muttersprachler.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Um ein Text-Notifikationsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "In dem Beispiel hier kann man ein paralleles Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in Plain Language sehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Um die Satzstruktur zu vereinfachen, sind verschiedene Techniken möglich, wie zum Beispiel lexikalische Substitution, Klausalstellung, Klausalstellungserneuerung oder Einsetzen von Worten."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Wir schlagen unser neues Corpus D-Plane vor. Im letzten Jahrzehnt gab es einige Probleme mit den bestehenden Corpora. Zum Beispiel sind diese Corpora hier zu klein, um ein Text-Nachrichtenmodell zu trainieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Die anderen drei Modelle, die ich in den letzten Jahren vorgeschlagen habe, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihrer Ausrichtung fehleranfällig sein können."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Daher schlagen wir unser neues Korpus Deplane vor, das in zwei Unter-Korpora aufgeteilt ist, nämlich Deplane APA und Deplane Web. Deplane APA basiert auf Nachrichtentexten."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "In der PlainAPA haben wir 483 Dokumente manuell ausgerichtet. Das ergibt etwa 30.000, 13.000 parallele Satzpaare."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Für eine tiefe Lanes-Web. Dieses Korpus enthält verschiedene Domänen. Und wir alignieren alle 750 Dokumente einerseits manuell und andererseits mit automatischen Alignment-Methoden."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Insgesamt ergeben wir uns 30.450 Satzpaare."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Wir analysieren unsere Satzpaare ein bisschen genauer. Zum Beispiel bei der Art der Vereinfachung."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Sie können hier sehen, dass die Bibeltexte viel stärker vereinfacht sind, als zum Beispiel die Nachrichten oder die Sprachlernertexte."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Auf allen Ebenen hinsichtlich, zum Beispiel, lexikalischer Vereinfachung, struktureller Vereinfachung, auch insgesamt Niveau der Vereinfachung."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Weiterhin können Sie sehen, dass unser Deplaned-Korpus eine hohe Vielfalt an verschiedenen Einfachheitsveränderungen aufweist. Zum Beispiel haben wir im Deplaned-API-Korpus viel mehr Umordnungen und Wortänderungen als im Deplaned-Web-Korpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Andererseits haben wir im Webkorpus viel kürzere Bewertungen."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Lassen Sie uns jetzt sehen, was wir mit diesem Korpus erreichen können. Hallo, ich bin Omar und jetzt spreche ich über die Anwendungsfälle für unser Datensatz Dplane. So kann man für den ersten Anwendungsfall automatische Ausrichtungsmethoden evaluieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "\"In den letzten Jahren gab es viele Alignment-Methoden, insbesondere im Kontext von Maschinensprachübersetzungen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Alignments von Sätzen in den Post-Dokumenten extrahieren möchten."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir versuchen, Alignments zwischen Sätzen von zwei parallel veröffentlichten Dokumenten mit derselben Sprache, denselben Inhalten, aber unterschiedlichen Komplexitätsstufen zu extrahieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "de", "output": "Und jetzt haben wir unser Datensatz Dplane, der manuell ausgerichtete Sätze enthält, auf die wir diese Sätze als Goldstandard-Alignements verwenden können, um einige vorgeschlagene Alignment-Methoden zu evaluieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "de", "output": "Und wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen und die Codes, um unsere Experimente auszuführen, in unserem Papier veröffentlicht."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "de", "output": "Am Ende stellten wir fest, dass die beste automatische Alignementsmethode für die Vereinfachung von deutschen Texten die Methode der Massenalignierung ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "de", "output": "Und Sie können auch den Code, um diese Methode in eigenen Dokumenten auszuführen, in dem Papier finden."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "de", "output": "Die zweite Anwendung, die wir in unserem Papier vorgestellt haben, ist der Fall der automatischen Textvereinfachung."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "de", "output": "\"Indem ich die Sprachmodelle feinabgestimme, um einfachen Text aus komplexem Eingabetext zu erzeugen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben zwei verschiedene Modelle feinjustiert. Wir haben ein Modell für langsame Informationen entwickelt, um Dokumenten-Ebene-Simplifizierungen zu produzieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben auch die normale Basis-Import verfeinert, um Satz-Einfachfassungen zu produzieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "de", "output": "Du kannst auch alle Checkpoints finden und in weitere Details bei den Scores und den Bewertungsmetriken unserer Experimente in dem Papier nachschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "de", "output": "Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung besser als die Basisscores erbringen könnte oder erbringen könnte."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "de", "output": "Und wir schlagen diese Ergebnisse als Referenz, einen Basisreferenzpunkt für das Problem der automatischen Textvereinfachung in Zukunft vor."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "de", "output": "Danke schön für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich heiße Adam Szpirkowski und dieses Vortrag ist über die Abhängigkeitsstruktur der Koordination."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "de", "output": "\"Es gibt verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen vorausgesetzt werden. Zum Beispiel in den universalen Abhängigkeiten wird die Struktur von Lisa, Bart und Maggie angenommen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "de", "output": "\"Die erste Konjunktion ist der Kopf der ganzen Koordinationsstruktur, also in diesem Fall Lisa.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "de", "output": "\"Ähnliche Annahmen werden auch in Iworskijs Bedeutungstheorie getroffen, wo wiederum die gesamte Koordinationsstruktur von dem ersten Konjunkt getragen wird. Also sind diese beiden Annahmen symmetrisch, sie setzen ein Konjunkt hervor.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt gibt es auch symmetrische Ansätze für Koordinationsstrukturen wie den Prager Ansatz, den konjunktionköpfigen Ansatz, der voraussetzt, dass die Koordinationsstrukturen in impraktischen Abhängigkeitstreebanksystemen von der Konjunktion geführt werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "de", "output": "Wir erhalten Abhängigkeiten von und zu allen Konjunkten."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "de", "output": "Und schließlich gibt es auch einen mehrköpfigen Ansatz, der zum Beispiel im Cutsonschen Wortgrammatik verwendet wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "de", "output": "\"Wohin, um es so zu sagen, sind alle Leitsätze die Kopfstruktur. Wir erhalten Abhängigkeiten vom Gouverneur, der allen Leitsätzen ermöglicht, getrennt voneinander zu verlaufen. Das sind Bartons Erfindungen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt zielt das Ziel dieses Beitrags darauf ab, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden gegen die asymmetrischen Strukturen der Koordination wie diese zu entwickeln."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "de", "output": "\"Okay, das Argument basiert auf dem Prinzip der Abhängigkeitsminimierung, wie in diesen Beispielen erläutert.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "de", "output": "\"Indirekte Objekte bevorzugen es, sich nah am Verb zu befinden, während Adjunkte weiter entfernt sein können. Also ist 'Es las es gestern' in Ordnung, weil das direkte Objekt sich nah am Verb befindet.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "de", "output": "\"Das ist gestern viel schlimmer, stimmt's nicht? Weil zwischen Verb und Direktojekt steht ein Adjunkt gestern.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "de", "output": "\"Es könnte jedoch sein, dass dieser Effekt simuliert werden kann, wenn das direkte Objekt sehr schwer und sehr lang ist, weil es dann in die Position nach dem Agenten bewegt werden kann.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "de", "output": "Dies ist hier illustriert. Beide Sätze sind in Ordnung. \"March Redd\" ist ein absolut faszinierender Buch über die BCS heute. Ich ist OK. Statt dessen haben wir stattdessen dieses lange NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "de", "output": "Aber es ist auch in Ordnung, \"gestern im März\" zu sagen. Es gibt ein absolut faszinierendes Buch über Frieden."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "de", "output": "\"Der Grund dafür ist, dass dies möglich ist, weil sogar wenn dieser Satz die allgemeine grammatikalische Regel verletzt, dass direkte Objekte direkt am Verb stehen sollten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "de", "output": "Es erfüllt das Prinzip der Abhängigkeitslänge-Minimierung, das kürzere Abhängigkeiten bevorzugt."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "de", "output": "\"Diese beiden Bäume zeigen nur die Länge der wichtigen Abhängigkeiten, also diejenigen, die zwischen diesen beiden Strukturen konstant sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "de", "output": "So hier haben wir eine Abhängigkeit von Rot zu dem Adjektiv von sieben gemessen in Worten und von Rot zu Buch von vier Länge. Um es zu erhalten, ist es 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "de", "output": "Wenn man diese beiden Bestandteile vertauscht, wird die Summe dieser beiden Abhängigkeiten sechs, stimmt? Also anstatt elf, sechs, viel kürzer, das klingt ja okay, stimmt?"}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "de", "output": "\"Okay, also haben wir diverse Statistiken über Koordination aus der erweiterten Version der Pantry Bank extrahiert und sehen wir den Artikel, warum wir keine Universitäts-Abhängigkeiten verwendet haben.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "de", "output": "Und die Statistiken bestätigen die Beobachtung, die oft gemacht wurde, dass Konjunkte links tendieren dazu, kürzer zu sein. Also Salz und Pfeffer und Salz gemessen in Silben."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "de", "output": "Und die Beobachtung, die im Vorbeigehen gemacht wurde, dass diese Tendenz mit Längendifferenz wächst."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "de", "output": "\"Je größer der Unterschied zwischen den Längen der beiden Konjunkte wird, bevorzugt das kürzere Konjunktum die erste Position zu sein. Deshalb ist die Proportion des linken kürzeren Konjunkts größer.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "de", "output": "\"Aber was neu in diesem Papier ist, dass wir festgestellt haben, dass diese Tendenz nur dann auftritt, wenn die Gouvernanz auf der linken Seite ausbricht.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "de", "output": "Also, ich sehe, dass der Gouverneur auf der linken Seite ist. Ich sah Barton Lisa. Also ist der Gouverneur auf der linken Seite."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "de", "output": "Es fehlt im zweiten Beispiel Homer kam und hustete, wir haben hier Koordination von zwei Verben und es gibt kein außerhalb liegendes Regierungsverhältnis, richtig? Also bevorzugen die linke Konjunkte in solchen Fällen, je kürzer sie sind, desto größer ist der Unterschied zwischen den Konjunkten."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "de", "output": "\"Indes jedoch verschwindet dieser Effekt, wenn die Regierung auf der rechten Seite existiert, regiert die linke die Koordination des Telnet-Protokolls.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben gezeigt, dass die Messung in Zeichen die erste Spalte in Silben, die mittlere Spalte und die rechte Spalte in Worten enthält. Ich konzentriere mich auf die rechte eine."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "de", "output": "Was wir hier sehen, ist, dass die Governance auf der linken Seite ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "de", "output": "Die Tendenz, dass das konjunktive Subjekt kürzer wird, wächst kontinuierlich mit dem absoluten Wortunterschied. Und dasselbe wird auch beobachtet, wenn es keine Regierung gibt, wie bei der Koordination von Sätzen, aber wenn die Regierung rechts ist, verschwindet diese Tendenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "de", "output": "Und wir zeigen in unserem Paper, wie dies ein Argument gegen asynesymmetrische Strukturen der Koordination wie diese beiden und für symmetrische Strukturen wie diese beiden bietet."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "de", "output": "\"Schauen Sie sich den Papier für die vollständige Übereinkunft und Argumente an, entschuldigung, und sprechen Sie mit uns über die Poster-Sitzung. Vielen Dank.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiangbin, Doktorand an der University of Washington. Heute präsentiere ich unser Werk von Vorabtrainingsdaten für Sprachmodelle zu downstream-Aufgaben, die Spuren politischer Bias aufspüren, die zu unfaireren NLP-Modellen führen."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "de", "output": "Die Sprachmodelle werden auf großangelegten Web-Suchdaten trainiert."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "de", "output": "Politische Nachrichtenmedien sind gut in ihrem Vorauslagendaten abgedeckt. Laut einer Umfrage des C4-Korporas können wir sehen, dass die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in den Trainingsdaten des Sprachmodells vertreten sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "de", "output": "Dies hat eine gemischte Segnung für Anwendungen von Sprachmodellen geschaffen."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "de", "output": "\"Einmalhin konnten sie sich von diversen Perspektiven lernen, was die Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen sozial bedingt und könnten in downstream-Applikationen potenzielle Fairness-Probleme auslösen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir schlagen vor, den politischen Bias-Verbreitungspfad von Vorbildungsdaten zu Sprachmodellen bis hin zu downstream-Aufgaben zu untersuchen, indem wir folgende Fragen stellen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst bewerten wir die politischen linearen Sprachmodelle und welche Rolle könnte das betreffende Datenmaterial auf solche politischen Bias haben?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "de", "output": "Zweitens, wie leisten Sprachmodelle mit verschiedenen politischen Gegnern auf downstream-Aufgaben ab und ob das möglicherweise zu Fairness-Problemen in NLP-Anwendungen führen könnte?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "de", "output": "So wir schlagen vor, Sprachmodelle mit verschiedenen Formats von Anfragen auszulegen, indem wir uns an politische Umfragen wie dem politischen Kompass-Test wenden. Dies sichert uns eine automatisierte Bewertung, die gut begründet in der politischen Wissenschaftsliteratur ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "de", "output": "\"So einige vorläufige Ergebnisse zeigen, dass erste Sprachmodelle politische Bedeutungen haben, die sich auf alle vier Quadranten des politischen Kompasses erstrecken.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "de", "output": "Wir können auch sehen, dass GPT-4 der liberalste Sprachmodell aller ist und GPT-Reihe generell sozialliberaler ist als BERT-Reihe und ihre Varianten."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "de", "output": "\"Zweitens zielen wir darauf ab, in weltem Ausmaß politische Biase von Sprachmodellen tatsächlich aus dem Trainingsdatenmaterial aufgenommen werden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "de", "output": "Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpunkte auf sechs verschiedenen parteiischen Korpora vorbereiten, die in Nachrichten und sozialen Medien weiter unterteilt werden, getrennt nach politischer Ausrichtung."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "de", "output": "Durch weitere Vorbereitung von Sprachmodellen auf solchen Parteien in Kodpora können wir feststellen, dass die ideologischen Koordinaten des Sprachmodells sich entsprechend verschieben."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "de", "output": "Beispielweise kann man bei Robert, der auf dem linken linearen roten Korpus weiter trainiert wurde, einen substantiellen liberalen Shift erkennen, was seine"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "de", "output": "In Bezug auf seine politischen Vorurteile."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "de", "output": "Und wir versuchen auch, ob Sprachmodelle die Polarisation erkennen können, die in unserer modernen Gesellschaft vorherrschend ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "de", "output": "Wir teilen die Vorkompiliertoren in die Vorkompiliertoren vor dem 45. Präsidenten der Vereinigten Staaten ein und trainieren danach separate Sprachmodelle auf den beiden verschiedenen zeitlichen Korpora."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "de", "output": "Wir können sehen, dass Sprachmodelle allgemein nach 2017 eine politische Neigung haben, die sich weiter von der Mitte entfernt hat. Dies zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "de", "output": "So letztlich evaluieren wir Sprachmodelle mit unterschiedlichen politischen Bedeutungen auf Hassrede-Detektion und Falschmeldungsdetektion, um NLP-Anwendungen zu betrachten, die oft Sprachmodelle einbeziehen und sehr bedeutungsvolle Konsequenzen haben können."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "de", "output": "Wir sehen, wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung in Kategorien trennen, dass wir feststellen, dass die Leistung in jeder Kategorie stark variiert."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir können bei verschiedenen Demographien oder politischen linearen Medien ein Muster erkennen, dass z.B. für die Detektion von Hassrede linkslingsprachliche Sprachmodelle besser sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "de", "output": "\"bei der Erkennung von Hassrede gegen sozial benachteiligte Gruppen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "de", "output": "\"Indes jedoch setzten wir uns auseinander, um Gruppen in unserer Gesellschaft, die mehr Macht ausüben, zu detektieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "de", "output": "Und umgekehrt sind rechtslinguistische Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu detektieren, jedoch schlechter darin, Hassrede gegen schwarze LGBTQ+ und andere Minderheiten zu detektieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "de", "output": "\"Ähnliche Trends werden auch bei der Erkennung von Falschmeldungen beobachtet, wo wir sehen, dass Sprachmodelle linker Ausrichtung besser darin sind, Falschinformationen von ihrer entgegengesetzten politischen Ausrichtung zu erkennen und umgekehrt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "de", "output": "Wir zeigen in diesem Beispiel viele qualitative Beispiele, um zu zeigen, dass Sprachmodelle mit verschiedenen politischen Bedeutungen haben."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "de", "output": "\"Stellen Sie verschiedene Vorhersagen für Beispiele von Hassrede und Falschinformationen auf Basis ihrer sozialen Kategorien. Es gibt eine Vielzahl weiterer Beispiele im Anhang, um dies zu unterstreichen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "de", "output": "Dies gibt es ein Fairnessproblem, das sehr dringend ist hinsichtlich der politischen Biase von Sprachmodellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "de", "output": "Beispielweise würde ein rechter Sprachmodell, wenn es auf Hassrede oder Falschinformationen oder was auch immer feinjustiert wurde und auf einem beliebten Social-Media-Plattform deployt wurde,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "de", "output": "Dies würde bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und die Hetze gegen Minderheitengruppen unkontrolliert ausartet."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "de", "output": "So this has sounded the alarm for us to acknowledge and tackle the fairness issues resulting from language model political meanings.\n\nDies hat den Alarm ausgelöst, damit wir die Fairness-Probleme, die durch politische Bedeutungen von Sprachmodellen verursacht werden, erkennen und angehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "de", "output": "\"Einige Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma bezüglich der Sprachmodell- politischen Biase aufdecken. Es handelt sich um ein Dilemma zwischen Cilla und Kärbedis.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "de", "output": "Wenn wir die politischen Meinungen im Trainingsdaten des Sprachmodells nicht saniert, wird die Bias von den Vortrainingsdaten auf die Sprachmodelle und auf die downstream-Aufgaben übertragen, was letztlich zu Fairnessproblemen führt."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "de", "output": "Wenn wir versuchen, es irgendwie zu säubern, riskieren wir auch Zensur oder Ausschluss. Und es ist extrem schwierig, festzustellen, was tatsächlich neutral ist und Daten zum Sprachmonitoring aufbewahren sollte. Es ist fast wie das elektrische Schleuderspiel."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "de", "output": "\"Okay, großartig. Ich denke, das ist alles, was ich für heute habe. Danke für Ihre Zeit.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "de", "output": "\"Hallo alle, ich bin Jenny, eine erstejährige Doktorandin an der Carnegie Mellon University, und heute werde ich mein Werk 'Anal Positionally, characterizing design by a CSA data set of models' vorstellen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "de", "output": "Dieses Werk wurde in Zusammenarbeit mit Kollegen der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santy, Ronan LaBros, Katarina Aranica und Martin Sapp, erstellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "de", "output": "Lass uns anfangen, indem wir uns vorstellen, dass wir für ein Zeitungsunternehmen arbeiten und wir uns durch die Kommentare unter unserem Artikel sichten, um toxisches Inhalt zu entfernen."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "de", "output": "\"Man könnte sich auch an eine beliebte API wie Perspective API für die Detektion von Giftigem Stadtverkehr wenden. Und das funktioniert sehr gut, wenn Carl Jones, wo Perspective API giftige Instanzen korrekt detektieren kann.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "de", "output": "\"Aber das ist nicht wirklich der Fall für eine Dithya-Sharma, wo potenzielle APIs nicht so empfindlich auf offensivere Begriffe sind, die in indischen Kontexten häufiger vorkommen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "de", "output": "\"Dies ist ein Beispiel für eine Design-Biase, wo wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungsgruppen sehen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "de", "output": "\"Designbiases wie jene, die wir gerade gesehen haben, können auch auf die Positionalität der NLP-Forscher und -Entwickler zurückzuführen sein. Positionalität bezeichnet einfach die Perspektiven, die Menschen aufgrund ihrer Demographie, Identität und Lebenserfahrungen haben.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "de", "output": "Dies ist ein Konzept, das weit verbreitet in der Kritischen Theorie verwendet wird, insbesondere in feministischen und queer-akademischen Räumen."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "de", "output": "\"Und als Forscher kann die Positionalität das Forschungsverfahren und seine Ergebnisse beeinflussen, weil sie die Entscheidungen, die Forscher treffen, ändern kann.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "de", "output": "\"Und eine Frage, die Menschen stellen könnten, ist, ob Datensätze und Modelle Positionalität haben.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "de", "output": "Und wir versuchen nicht zu behaupten, dass Modelle selbst und Daten-Systeme selbst demographische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen von echten Menschen und können somit bestimmte Positionalitäten gegenüber anderen repräsentieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "de", "output": "\"Vorherige Studien haben anekdotische Hinweise auf Positionalität gezeigt, wie zum Beispiel Kulturelle Lücken in Modellen und Datenmengen, sowie theoretische Definitionen von Modellpositionen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "de", "output": "\"Indes untersuchen diese Arbeiten nicht, wie Endbenutzer mit den Daten-Sätzen und Modellen selbst vergleichen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "de", "output": "\"Und die Position von Modell und Datensatz wird immer wichtiger, wenn NLP-Aufgaben immer subjektiver und sozial ausgerichtet werden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "de", "output": "Und es ist schwierig, diese Positionierungen zu charakterisieren, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "de", "output": "\"Um Daten zu studieren und Modellpositionen zu analysieren, vergleichen wir Annotationen mit echten Benutzern und existierenden Datensätzen und Modellen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "de", "output": "Wir erreichen dies durch unser Framework NL-Positionalität."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "de", "output": "Unser Framework funktioniert in zwei Hauptschritten."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "de", "output": "Die erste Schritte sind, Datasets mit diversen Annotatoren zu reannotieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "de", "output": "\"Und wir entscheiden uns, dies über die Demographien von Original-Datensätzen, Annotatoren zu überprüfen, weil meist nur wenige Annotatoren je Instanz annotieren und weil Demographien selten erfasst und geteilt werden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "de", "output": "Und so entscheiden wir uns, die Daten zu re-analysieren, um viele Entitäten zu identifizieren und ein umfangreiches Datenmaterial über demografische Merkmale zu erhalten."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "de", "output": "Wir vergleichen dann die Annotationen nach demographischen Merkmalen mit den Modellen und Daten-Sätzen mithilfe eines Paares, da unsere Korrelations-Score."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "de", "output": "\"Und unser Framework unterscheidet sich somit von der Literatur zum Annotator-Disagreement, indem wir Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Etiketten vergleichen, anstatt nur Annotator-Einstimmigkeit oder Modell-Annotator-Verteilungen zu betrachten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "de", "output": "Unser Framesatz-Rate wird größtenteils durch Lab in the Wild, eine Online-Plattform für HCI-Kooperationspartner ermöglicht."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "de", "output": "\"Lab in the Wild ist ein Online-Experimentierplattform, auf der wir eine Vielzahl von Freiwilligen rekrutieren können, im Vergleich zu Plattformen wie MTURC, die größtenteils Teilnehmer aus den USA oder Indien haben. Darüber hinaus kann Lab in the Wild noch hochwertige Daten sammeln.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben zwei Aufgaben im Out-of-the-Wild-Format, eins davon ist die soziale Akzeptanz. Und der Weg, auf dem dies funktioniert, besteht darin, dass Teilnehmer eine Situation aus dem Datenbestand für soziale Chemie lesen und dann beschreiben, wie sozial akzeptabel eine Situation ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "de", "output": "Nachdem sie sich im Studium engagiert bleiben möchten, können sie ihre Antworten mit einer AI und anderen vergleichen."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben diese Anmerkungen mit Sozialchemie, Delphi und GPT-4 verglichen."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "de", "output": "Wir wiederholen dann ein ähnliches Setup für die Detektion von Toxizität und Hassrede, wo sie ein Beispiel aus Danny Hate lesen und feststellen, ob sie es als Hassrede einstufen."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "de", "output": "Wir verglichen diese Annotationen mit Dynahate, Perspective-API, Rewire-API, HateRoberta und GPT-4. Unser Studie am Ende umfasste über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt werden wir sehen, welche NLP-Datensätze und -Modelle am meisten mit Positionalität übereinstimmen. Wir finden, dass es Positionalität in NLP gibt."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "de", "output": "Beispielweise finden wir heraus, dass Datensätze und Modelle am stärksten auf englischsprachigen Ländern ausgerichtet sind. Wir finden, dass die GPT-4-Social-Akzeptanzanalyse am stärksten auf konfuzianischen und englischsprachigen Ländern ausgerichtet ist. Wir finden auch, dass Dynahate am stärksten auf englischsprachigen Ländern ausgerichtet ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir finden auch die meisten Übereinstimmungen bei Menschen mit einem Hochschulabschluss. So zeigt GPT-4 im Social Acceptability-Test die größte Übereinstimmung mit Menschen mit einem Hochschulabschluss oder einem Abschluss an einer Fachhochschule.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "de", "output": "Und wir finden dasselbe für Dianaheid, wo es am meisten mit Menschen mit einem Hochschulabschluss übereinstimmt."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "de", "output": "\"Indes sind jedoch Modelle und Datensätze, wenn sie spezifischen Bevölkerungsgruppen zugeordnet werden, einige unweigerlich zurückgelassen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "de", "output": "Ein Beispiel dafür ist, dass Daten und Modelle bei nicht-binären Personen weniger in Einklang stehen als bei männlichen und weiblichen Gegenparts. Wir finden dies auch im GPT-4-Social-Acceptability-Task, wie auch in der Analyse des Dining-Hate-Task."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "de", "output": "\"Angesichts des Fehlers in der Position in Atlady und L.P., was können wir dagegen unternehmen?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir haben einige Empfehlungen für dies. Die erste ist, einen Verzeichnis aller relevanten Gestaltungswahlen während des Forschungsprozesses zu führen. Und die andere ist, NLP-Forschungen durchzuführen, die Perspektivismus berücksichtigen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "de", "output": "\"Unser drittes Vorschlag besteht darin, spezialisierte Datenmengen und Modelle innerhalb von vier spezifischen Communities zu erstellen. Ein gutes Beispiel dafür ist die Musseqani-Initiative. Wir möchten betonen, dass ein inklu-sives NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Wir müssen sicherstellen, dass sie auch für bestimmte Gruppen, die bislang benachteiligt waren, nutzbar sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "de", "output": "Und so schließt sich unsere Präsentation ab, aber wenn Sie mehr erfahren möchten, bitten wir Sie, unseren Dashboard zu besuchen, um die neuesten Analyseergebnisse und unser Papier zu betrachten. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Si Yu-Yuan von der Fudan-Universität. Ich bin hier, um unser Werk \"Distinguished Script Knowledge from Language Models for Constrained Language Planning\" vorzustellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "de", "output": "\"In gewöhnlichen Alltag planen Menschen ihre Aktionen, indem sie Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten folgen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "de", "output": "\"Der vorherige Welt hat Sprachmodelle genutzt, um Ziele für stereotype Aktivitäten wie Backen eines Kuchens zu planen, und hat gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "de", "output": "\"Obwohl die vorherige Forschung sich hauptsächlich auf das Planen von Zielen für abstrakte Aktivitäten konzentriert hat, bleibt das Planen von Zielen mit spezifischen Zielen, spezifischen Restriktionen, wie zum Beispiel das Backen eines Schokoladenkuchens, noch untersucht.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir definieren im folgenden Papier das Problem der Konstraintsprachplanung.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "de", "output": "\"Die Ziele können von verschiedenen Restriktionen beeinflusst sein, die sich auf die Zielplanung auswirken. Ein abstraktes Ziel kann von verschiedenen realleben-specific Ziele mit multifasetten Restriktionen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu zu den Restriktionen sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir bewerten und verbessern in diesem Papier zuerst die Fähigkeit von großen Sprachmodellen zur Planung von Konstrainsprachen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "de", "output": "\"Da gibt es kein spezifisches Daten-Startziel, um unser Anfang zu erkennen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "de", "output": "Wir müssen dieses Code zuerst erwerben. Wie im Tabelle gezeigt, erweitern wir das abstrakte Code mit multifacettalen Restriktionen für den menschlichen Betrieb in der Datenakquisition mit Instruct GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "de", "output": "Wir sammeln 100 spezifische Ziele und evaluieren die Skripte, die von großskaligen Modellen generiert werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "de", "output": "Dieses Tabelle gibt einen Überblick über die Gesamtgenauigkeit der Ergebnisse. Wir finden, dass alle Line-Up-Remodels unzureichende Ergebnisse bei der Planung von Zielen erzielen."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "de", "output": "Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, wofür Zeile-modelle verwendet werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "de", "output": "Die Ergebnisse in der Abbildung zeigen, dass die semantische Ähnlichkeit in generierten Skripten akzeptabel ist, jedoch die Treue zu den Zwängen nicht garantieren kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "de", "output": "Wir befassen uns mit mehr fragmentierten Themenkategorien von Zwängen, die in \"Waking Home\" definiert werden. Der Kopfabbild in der Figur zeigt, dass die Planungsleistung von Anweisbarkeit beträchtlich für Mädchen aus verschiedenen Kategorien variiert."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "de", "output": "\"Frühere Studien haben gezeigt, dass die Ausgabekvalität von Modellen auf Zeilebene in hoher Varianz liegt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee von übererzeugten Z-Filters, um die Generationsqualität zu verbessern.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "de", "output": "Wir zeigen erstmalige Einschränkungstypen mit Beispielen für intract.cpt und erlangen spezifische Ziele auf Grundlage des abstrakten Ziels."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "de", "output": "Dann instruiere GPT übergeneralisierte Skripte für spezifische Ziele."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "de", "output": "\"Nächstes wird ein Filtermodell erstellt, um die physischen Skripte auszuwählen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "de", "output": "Wir konvertieren Skripte und Ziele in abstrakte GPT-Embeddings und berechnen die Kosinusähnlichkeit als Ähnlichkeitsscores, um semantische Ähnlichkeit zu messen."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir werden das Skript sichten, das die Schlüsselwörter des Zielszenarios enthält. Wir behalten nur das Skript, wenn das Ziel den höchsten Wert im festgelegten Ziel erreicht.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "de", "output": "Mit unserem Verfahren können wir Quadrate von höherer Qualität erzeugen. Unsere Methode verbessert die Planbarkeit erheblich sowohl hinsichtlich semantischer Vollständigkeit als auch Treue zur Zwangslage."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "de", "output": "\"Da größeren Sprachmodellen ist es wichtig, die Sprachplanungsfähigkeit kleiner und spezialisierter Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wichtiger Schritt dazu.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "de", "output": "\"Es gibt jedoch keine vorherigen Studien, die spezifische Ziele planen ermöglichen, und die manuelle Daten-Annotations ist teuer.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "de", "output": "\"Somit folgen wir dem Konzept der symbolischen Wissensverdichtung, um eingeschränkte Sprachplanungsdaten aus lebendigen Modellen zu destillieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "de", "output": "Wir werden unseren Ansatz für das Erstellen eines Datensatzes für konstruktives Sprachplanung namens Coscript anwenden."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "de", "output": "Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierung und Test-Plattformen zu gewährleisten, bitten wir Cloud-basierte Arbeiter, falsche Beispiele zu finden und zu korrigieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "de", "output": "Dieses Diagramm zeigt die Verteilung der Einschränkungen von Corscript. Wir finden, dass Corscript eine hohe Pluralität in den generierten spezifischen Zielen aufweist. Mit Corscript können wir kleinere, spezialisierte Modelle für die Planung von Sprachbeschränkungen erstellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "de", "output": "Wir finden, dass die T-Datei-Funktion auf dem Kurskurs Skripte von höherer Qualität generieren kann als die meisten großen Modell-Level-Modelle, was bedeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie auf geeigneten Datensätzen trainiert werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "de", "output": "Insgesamt haben wir das Konstruktionsproblem der Sprachplanung festgelegt. Wir haben eine Konstruktionsfähigkeit für große Sprachmodelle entwickelt und ein Entstehungsfilterverfahren für großvolumige Modelle erstellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "de", "output": "Wir verwenden große Sprachmodelle, um ein hochwertiges Skript-Datensatz, Corscript, für die Sprachplanung zu generieren. Wir hoffen, dass das Corset-Datensatz ein wertvolles Ressourcen für die Forschung zur Sprachplanung sein kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "de", "output": "Danke für Ihre Zeit. Bitte finden Sie weitere Details zum Kurs-Script in unserem Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich heiße Xu Heng. Heute präsentiere ich unseren Beitrag, nach dem Do Connell 2003 benannten Entity-Taggern, ob sie noch gut in 2023 funktionieren. Lassen Sie uns los beginnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "de", "output": "Unser Papier untersuchte das Problem der Generalisierung am Beispiel der namentlichen Entity-Erkennung oder NER-Tätigkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "de", "output": "Wir beobachten, dass Modelle fast 20 Jahre lang Kono 2003 verwendet haben, um NER zu entwickeln. Und das wirft natürlich mehrere Probleme auf. Zunächst einmal: Können diese Modelle auf moderne Daten generalisieren?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "de", "output": "Wenn wir neue Tagger entwickeln, sind für eine gute Generalisierung folgende Faktoren erforderlich."}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "de", "output": "\"Beim gleichen Zeitpunkt, wenn wir einen schlechten allgemeinen Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "de", "output": "Um diese Probleme zu untersuchen, haben wir das Carnot++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus den Reuters-Nachrichten von 2020 sammelten und dann mit den gleichen Anweisungen der Carnot-2003-Vorlage annotiert haben."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben dann mehr als 20 Modelle anhand des Kono 2003 feinjustiert. Wir haben sie sowohl am Kono-03-Testset als auch am Kono++-Testset ausgewertet."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "de", "output": "Und zuletzt haben wir den prozentuellen Änderung in F1 berechnet, um die Allgemeingültigkeit jeder Modell zu beurteilen."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "de", "output": "\"Um eine gute allgemeine Generalisierung zu erreichen, fanden wir in unseren Experimenten heraus, dass es drei Hauptzutaten gibt, die erforderlich sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "de", "output": "Die erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass die Transformer-Modelle besser auf neue Daten generalisieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "de", "output": "Das zweite Zutat ist die Modellgröße. Wir fanden heraus, dass größere Modelle zu besserer Allgemeinheit führen."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "de", "output": "Und zuletzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele direkt den Leistungsgrad eines downstream-Aufgaben beeinflusst. Wir fanden auch heraus, dass mehr Feinabstimmungsbeispiele tatsächlich auch zu besserer Generalisierung führen."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "de", "output": "\"Zu unserem nächsten Frage, was verursacht den Leistungsabfall einiger Modelle?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "de", "output": "Wir hatten zwei Hypothesen. Die erste ist die adaptive Überanpassung, die durch die Wiederholte Verwendung des gleichen Test-Satzes verursacht wird. Und dies manifestiert sich meist als die Abnahme der Renditen bei einem neuen Test-Satz."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "de", "output": "Die zweite Hypothese ist der zeitliche Versatz, der durch den anwachsenden zeitlichen Abstand zwischen dem Trainings- und dem Testdatensatz verursacht wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "de", "output": "\"Für adaptives Überfitting sehen wir im Diagramm rechts, dass die rote beste Passungslinie einen Gradienten aufweist, der größer als 1 ist.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "de", "output": "\"Dies bedeutet, dass jede Verbesserung, die wir am Spalte 2003 machten, sich auf mehr als einen Verbesserungseinheiten auf Spalte plus plus auswirkt, was bedeutet, dass es keine Abnehmenden Renditen gibt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "de", "output": "Und dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "de", "output": "\"Was meinst du zu temporären Trifft dann?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "de", "output": "\"Für Temporal Drift haben wir ein Experiment durchgeführt, um bestimmte Modelle erneut zu trainieren oder fortzubilden, indem wir sie mit neueren Daten versorgten. Wir fanden heraus, dass die Leistung mit wachsender zeitlicher Lücke abnimmt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "de", "output": "\"Und dies bestätigt unsere Hypothese, dass die Hauptursache des Leistungsabfalls der temporäre Drift ist.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "de", "output": "Unser Schlussfolgerung ist, dass wir für eine gute Allgemeingültigkeit ein besseres Modellarchitektur, ein größeres Modell und weitere feinjustierte Beispiele benötigen. Und diese Ziele können wir nicht isoliert, sondern müssen sie gemeinsam hand in Hand erreichen."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "de", "output": "\"Zugleich haben wir festgestellt, dass der Leistungsabfall hierbei durch zeitliche Verschiebungen verursacht wird und überraschenderweise nicht durch adaptive Überanpassung, obwohl Conno2003 seit über 20 Jahren eingesetzt wird.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "de", "output": "\"So gehen wir zurück auf die Frage, die wir in unserem Titel unseres Papiers aufgebracht haben, ob Tagger von Connell 2003 in 2023 noch funktionieren. Und wir fanden heraus, dass die Antwort ein überzeugender Ja ist.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "de", "output": "Wir hoffen, dass unsere Arbeit zu mehr Forschung über die Verbesserung von Generalisierungen der Modelle auffordert."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "de", "output": "\"Und zuletzt, bitte überprüfen Sie unsere Arbeit, unser Datensatz. Wenn Sie Fragen haben, stehe ich Ihnen gerne zur Verfügung. Vielen Dank.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, und ich werde über unsere Arbeit bei der Lösung von indirekten Bezugsausdrücken für die Entitätswahl sprechen, bei der wir den Alt-Entity-Score einführen."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "de", "output": "Mein Name ist Jawad Hosseini und dies ist ein gemeinsames Werk mit Philippe Ladinsky, Sylvia Parry und Annie Lewis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "de", "output": "Ich folge dieser Vorschrift in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer in diesem Zimmer"}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "de", "output": "Die offensichtlichste Möglichkeit ist, eine direkte Differenzierung zu verwenden. Zum Beispiel indem man den Titel des Liedes in mir oder seinem Platz, dem ersten, sagt."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "de", "output": "\"Manchmal ist eine indirekte Anspielung geeigneter, um eine natürlichere Unterhaltung zu haben. Dies kann passieren, wenn der Benutzer den Titel eines Liedes nicht mehr aus dem Kopf hat.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "de", "output": "Ich muss leider sagen, dass ich die Übersetzung nicht verstehe, da die Eingabe keine englische Sprache enthält."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "de", "output": "Ich habe keine Ahnung, was Sie da gesagt haben. Könnten Sie bitte wiederholen?"}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "de", "output": "Dies ist ein wichtiges Problem in konversationsbasierten Systemen und auch für die Bewertung von LLMs im Bereich der Entity-Verständigung."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "de", "output": "Ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß ich weiß"}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "de", "output": "Unser Datensatz-Sammlungsmethode setzt sich auf eine kartonierte Vollendungssatz ein."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "de", "output": "\"Erinnerst du dich an jenes Lied, das wir gestern gehört haben? Und das ist der Kontext unserer Unterhaltung.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "de", "output": "\"Im zweiten Satz sagt Alice, bedeutest du leicht mit mir oder ich habe ein Gefühl?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "de", "output": "\"Die alternative Frage und in dem dritten Redeblatt verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel die neuere.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "de", "output": "Ich muss mich auf die erste und wichtigste Frage stellen und die Antworten darauf finden, auch wenn es schwierig ist, und ich muss auch die Antworten auf die nächsten Fragen finden, auch wenn es schwierig ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "de", "output": "\"Der zweite, der alternative Frage ist wie folgt generiert.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "de", "output": "Wir verwenden immer ein einfaches Muster. Meinst du A oder B?"}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir haben verschiedene Abstrichmethoden verwendet. Je höher wir in der Liste gehen, desto ähnlicher werden die Entitäten zueinander, und es wird meist schwieriger, die Entschärfung durchzuführen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "de", "output": "Die erste ist Uniformattraktion."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "de", "output": "Die zweite ist, wenn die Entitäten ähnliche Titel haben. Zum Beispiel zwei Bücher mit dem Namen \"Der Einzelhandel\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "de", "output": "Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich wenn sie ähnliche Infoboxes oder Attribute auf Wikipedia haben, zum Beispiel denselben Genre oder denselben Künstler für ein Lied."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "de", "output": "\"Zeigen Sie mir diese alternative Frage zur Antwort. Sie kennen den Namen dieser Entitäten, aber sie wissen nicht notwendigerweise über die Entität.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir zeigen also Hintergrundwissen über die beiden Entitäten. Bei Songs zeigen wir einfach einen Google-Suchlink zu jedem Lied.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "de", "output": "Und bitten Sie die Annotatoren, zumindest einige von jedem Lied zu hören und über jedes Lied zu lesen. Hier ist zum Beispiel der Google-Suchresultat für das Lied EasyHunt."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "de", "output": "Für die Rezepte und Bücher-Domäne zeigen wir einige Hintergrundtexte von Wikipedia. Für Rezepte zeigen wir ihre Bilder von Wikipedia, damit die Annotatoren wissen, wie sie aussehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben wir haben"}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "de", "output": "Ich verstehe nicht, was Sie sagen. Bitte wiederholen Sie Ihre Aussage."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "de", "output": "Die L-Korpus enthält 6000 alternative Fragen in drei Domänen und 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5-Large-Modell sind wie folgt zusammengefasst."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "de", "output": "\"Wenn das Sprachmodell Zugang zu demselben Hintergrundwissen wie die Annotatoren hat, dann ist die Genauigkeit sehr hoch. Sie beträgt etwa 92 bis 95%. Aber das ist nicht realistisch.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "de", "output": "Wenn wir keine Möglichkeit haben, die Schallplatte von einem bestimmten Album zu kopieren, dann haben wir keine Möglichkeit, die Musik von diesem Album zu hören, wenn wir keine Möglichkeit haben, die Schallplatte von einem bestimmten Album zu kopieren, haben wir keine Möglichkeit, die Musik von diesem Album zu hören."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "de", "output": "Wenn das Modell nur Zugriff auf Entitätsnamen hat, beträgt die Genauigkeit nur 60%. Es gibt also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Danke für."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sara Pappi von der Universität Trient und der Fondazione Bruno Kessler, und ich werde den Beitrag \"Attention as a Guide for Simultaneous Speech Translation\" vorstellen, der ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "de", "output": "\"Das Simultane Sprachübersetzen, auch SIMUL-ST genannt, ist das Verfahren, eine gesprochene Sprache in Echtzeit in eine andere Sprache zu übersetzen, um eine Kreuzsprachkommunikation zu ermöglichen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "de", "output": "Und welche sind die Probleme der aktuellen Stimulus-Modelle? Spezifische Architekturen werden üblicherweise mit Zusatzmodulen trainiert, die optimiert werden müssen."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "de", "output": "Langsame und komplizierte Schulungen, zum Beispiel Schulungen mit verschiedenen Optimierungsziele,"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "de", "output": "Und das Trainieren und Pflegen mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und ein anderes mit zwei Sekunden Latenz usw."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "de", "output": "\"Was ist unser Lösung?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "de", "output": "Zunächst verwenden Sie offline-existierende SD-Modelle ohne Neujustierung oder Anpassung an eine spezifische Architektur für ein einzelnes SD. Verwenden Sie ein einziges Modell für jede Latenz-Schicht und regulieren Sie die Latenz durch spezifische Parameter."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "de", "output": "Und nutzen Sie das Wissen, das das Modell durch die Aufmerksamkeitsmechanik zwischen auditiver Eingabe und textuellem Ausgang bereits erworben hat, das heißt die Quer-Aufmerksamkeit. Und Sie können ein Beispiel auf der rechten Seite sehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "de", "output": "Unser Lösungsvorschlag besteht darin, einen Punkt- oder Encoder-Code für die Aufmerksamkeit zu entwickeln und eine Strategie zu erstellen, die entscheidet, ob ein partieller Übersetzungsausschnitt ausgestoßen werden soll oder nicht, basierend auf den Positionen der Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "de", "output": "Ein Wort wird ausgesendet, wenn die Spannung nicht konzentriert ist, also wenn diese Summe unter einem bestimmten Schwellenwert alpha liegt, in Richtung der letzten Lambda-Wellenframes, was bedeutet, dass die empfangene Information genug stabil ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "de", "output": "\"Beispielweise, wenn wir ein Sprachstück erhalten, in dem ich über und unser Modell die Übersetzung in Deutsch vorhersagt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "de", "output": "Und wir werden uns die Quer-Aktionswerte ansehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "de", "output": "Wir werden sehen, dass die ersten zwei Worte auf die frühesten empfangenen Sprechframes verweisen, während das letzte Wort auf die letzten empfangenen Sprechframes als Lambda-Sprechframes verweist."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "de", "output": "\"[Dies bedeutet, dass die ersten zwei Wörter weggelassen werden.]"}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "de", "output": "Während die Summe der Querbeziehungen über einem bestimmten Schwellenwert alpha liegt, werden wir das letzte Wort nicht emittieren und warten auf einen nächsten Sprechblöcke."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "de", "output": "\"Wenn wir weitergehen und wir einen weiteren Speech-Tank erhalten und unser Modell weitere drei Worte vorhersagt, werden wir uns diese Querverweis-Weight betrachten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "de", "output": "Wir werden feststellen, dass keine Wörter auf die letzten Lambda-Sprechräume verweisen."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "de", "output": "\"Diese drei Worte werden ausgesprochen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "de", "output": "Wenn Sie sich die Hauptergebnisse davon ansehen,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "de", "output": "Wir plotten die simultane Übersetzungsergebnisse auf Graphen, auf denen wir auf einer Seite blaue Werte messen, die die Übersetzungsqualität und den durchschnittlichen Versatz aufweisen."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "de", "output": "\"Das ist die Latenzmessung und wir berücksichtigen auch den berechnungsorientierten durchschnittlichen Liking, der die Zeit berücksichtigt, die das Modell benötigt, um den Ausgang zu vorhersagen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "de", "output": "Wir möchten, dass unsere Kurven so hoch wie möglich auf diesem Diagramm sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "de", "output": "\"Aber auch, dass sie auf der linken Seite verschoben werden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "de", "output": "Und wir vergleichen auch Strategien, die auch auf offline-Modellen angewendet werden, die Gewichtsstrategie und die lokale Einigung. Wir vergleichen auch mit dem State-of-the-Art-Architektur, speziell für die Übertragung von Steams-on-thigh-Atmung."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "de", "output": "Diese sind alle Ergebnisse der simultane Übersetzungstrategie auf Deutsch."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "de", "output": "Und wir sehen, dass die Erwachsenen-Output-Formen alle Strategien auf Offline-Modellen anwenden, da die Kurven nach links verschoben sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "de", "output": "\"Und wir sehen auch, dass wenn wir den tatsächlichen Ablaufzeit oder die rechnergesteuerte Zeit betrachten, ist die schnellste Strategie.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "de", "output": "Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie bitte unser Papier. Wir haben auch offenes Quellcode, Modelle und simultane Ausgaben veröffentlicht, um die Reproduzierbarkeit unseres Werks zu fördern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Ying und meine Kollegin Zhiyang und wir werden unser Forschungsvorhaben zum Multi-Instruct präsentieren, das die multivirtuelle Lernfähigkeit durch Anpassung von Anweisungen verbessert."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "de", "output": "Mit den Fortschritten in großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zum Wiederverwenden von Sprachmodellen für verschiedene downstream-Aufgaben in einem parametrischen und dateneffizienten Weg zu erkunden."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "de", "output": "\"Viele Studien haben gezeigt, dass die Anpassung von Anweisungen es ermöglicht, große Sprachmodelle, um Aufgaben ohne Vorkenntnisse in einem zero-shot-Verfahren auszuführen, indem sie natürliche Anweisungen befolgen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "de", "output": "\"Dennoch konzentrierten sich die meisten vorherigen Arbeiten auf die Verbesserung der zero-shot-Leistung auf Sprachaufgaben, während Computer-Vision- und multimodale Aufgaben vernachlässigt wurden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "de", "output": "Daher soll in diesem Werk untersucht werden, ob die Anpassung von Anweisungen an multimodale Proteinfunktionen tatsächlich die Allgemeingültigkeit zu unerforschten multimodalen Aufgaben verbessern kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "de", "output": "Zum Zeitpunkt unserer Forschung fanden wir einen beträchtlichen Unterschied in der Verfügbarkeit von Lehrdatensätzen zwischen einem RLP und einem Mehrmodell."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "de", "output": "Es gibt mehr als 1.600 Sprach-only-Anweisungsaufgaben. Jedoch existiert kein großskaliger öffentlich zugänglicher multimodaler Anweisungsaufgaben-Set. Deshalb motiviert uns, ein multimodales Anweisungstuning-Dataset zu erstellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "de", "output": "Hier präsentieren wir Multi-Instruct, das erste multimodale Benchmark-Dataset für die Anpassung von Anweisungen, das 62 diverse multimodale Aufgaben umfasst, die 10 Kategorien von Brettspielen abdecken."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "de", "output": "Diese Aufgaben werden von 21 bestehenden offenen Quellcode-Datensätzen abgeleitet und jeder Aufgabe werden fünf von Experten verfasste Anweisungen zur Verfügung gestellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "de", "output": "Für die Untersuchung des multimodalen Anpassens von Anweisungen an unserem vorgeschlagenen Datensatz verwenden wir OFA, ein einheitliches multimodales Darstellungsmuster als unser Basismodell. OFA verwendet eine einheitliche Vokabular für Sprache, Bildtokens und die Koordinaten eines Ausschnittsrahmens."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir präsentieren einige Beispiele aus unserem Multimodal-Datensatz.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "de", "output": "Um die Verarbeitung von verschiedenen Eingabedaten und -ausgabenformaten zu vereinheitlichen."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "de", "output": "Wir folgten dem Ansatz von OFA und formulierte alle Aufgaben in einer einheitlichen Sequenz-zu-Sequenz-Format, bei dem der Eingabetext, Bilder, Anweisungen und Bounding Boxes im selben Token-Raum dargestellt werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "de", "output": "Ich werde jetzt über das Multimodale Anpassen von Anweisungen sprechen."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "de", "output": "\"Für das Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und sample 10.000 Instanzen pro Aufgabe. Zum Testvorgang reservieren wir den gesamten Gruppe 'Common Sense Reasoning' und wählen weitere 5 Aufgaben aus den Gruppen VQA und 'Miscellaneous'.\"\n\nPlease note that I've translated the text in a way that is accurate and suitable for an academic or instructional setting, using precise terminology and adhering to German grammar and cultural sensitivities."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "de", "output": "Wir verwenden alle Instanzen in der Testflotte für jede Aufgabe. Darüber hinaus stichproben wir 20 Aufgaben aus der Testflotte von naturlichen Anweisungen als On-Szenen-Aufgabe für NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir verwenden ein vorgepräpariertes OFA-Modell als Basismodell. Während des Trainings werden für alle Aufgabeninstanzen erstellt. Jede Instanz wird zufällig mit einem von fünf Anweisungsmustern kombiniert.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "de", "output": "\"Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer von fünf Anweisungen in jedem Experiment bewerten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "de", "output": "Wir melden die Durchschnitts- und die maximale Leistung und die Standardabweichung der Leistung über alle fünf Experimente."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "de", "output": "Wenn die Aufgabe ein mehrmodell-Classifier ist, melden wir Genauigkeit. Wenn es sich um ein mehrmodell-Generationsaufgabe handelt, melden wir die Wurzel-Gel. Für eine RP-Aufgabe melden wir die Wurzel-Gel ebenso."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben auch weitere Bewertungskriterien eingeführt, die als Empfindlichkeit bezeichnet werden. Sie messen die Fähigkeit des Modells, gleichbleibende Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von leichten Variationen in der Wortwahl der Anweisung."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "de", "output": "\"Unser Hauptresultat ist hier zu sehen. Wir können erkennen, dass die Anpassung von Anweisungen den Leistungszuwachs bei gleichen Mehr-Modell-Aufgaben signifikant verbessern kann.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "de", "output": "\"Das Übertragen von Lernalgorithmen aus natürlichen Anweisungsdatensätzen kann das Anpassen von Anweisungen fördern.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "de", "output": "Hier sehen wir, wie sich die Leistung des Modells mit zunehmender Anzahl von Aufgaben verbessert und gleichzeitig die Empfindlichkeit sinkt."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir haben auch ein Experiment durchgeführt, bei dem wir ein versus fünf Anweisungen verwendet haben. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Empfindlichkeit reduzieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "de", "output": "\"Sollte man verschiedene Anpassungsstrategien für das Modell ausprobieren, kann man sehen, dass das Modell durch Übertragung von natürlichen Anweisungsdatensätzen eine viel bessere Empfindlichkeit erreicht, verglichen mit dem ursprünglichen OFA-Modell.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "de", "output": "Wir können auch sehen, dass die Übertragung von naturlichen Anweisungs-Daten helfen kann, OFA, um eine viel bessere Leistung auf dem natürlichen Anweisungs-Datensatz zu erzielen."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "de", "output": "\"Im Großen Ganzen haben wir das erste großskalige Multimodale Datenmaterial für die Anpassung von Anweisungen vorgeschlagen. Wir haben die Schwellenfähigkeit von OFA signifikant verbessert und verschiedene Übertragungslernverfahren untersucht und ihre Vorteile gezeigt. Wir haben ein neues Maß namens Empfindlichkeit entworfen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "de", "output": "Ein letztes Mal, wir sammeln ein viel umfangreicheres Datenmaterial für die Anpassung von Modellen mit etwa 150 weiteren Weiren-Sprachaufgaben und werden sie veröffentlichen. Hier ist ein QR-Code für unsere Daten und das Modell. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Kostav Sinha und ich freue mich, Sie zu unserem Vortrag zum Paper \"Language Model Acceptability Chalements Are Not Always Robust to Context\" willkommen zu heißen."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "de", "output": "Es gibt ein gemeinsames Werk mit John Wothier, Aaron Müller, Kanishka Mishra, Karen Fentus, Roger Levy und Adina William."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "de", "output": "So in diesem Werk werden wir das Minimalpaarparadigma neu besuchen."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "de", "output": "\"Das minimale Paarungsparadigma bewertet Sprachmodelle hinsichtlich Akzeptabilität, was auch Grammatikalität wie Blimp-Syntax-Gem oder Akzeptabilität in Bezug auf Stereotypen wie Crowd-Spares umfasst.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "de", "output": "Und in diesem minimalen Paarparadigma wird die typische Methode, um Sprachmodelle zu evaluieren, darin bestehen, dass man wie ein akzeptables Satz oder ein grammatikalisches Satz zeigt, und dann ein akzeptables Satz oder ein ungrammatisches Satz."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "de", "output": "Und dann hofft man, dass das Modell mehr Wahrscheinlichkeit in den akzeptablen Sektor legt."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "de", "output": "\"Das aktuelle MPP-Pipeline erlaubt uns nicht, Modelle auf längere Sätze zu bewerten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "de", "output": "\"Diese Tage entwickeln sich große Sprachmodelle immer längere und längere Kontextfenster. Es ist deshalb entscheidend, dass wir die Akzeptabilität der Modelle innerhalb des Kontextfensters bewerten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "de", "output": "Und das ist, was wir hier versuchen. Wir versuchen, das MPB-Pipeline durch Anfrage an den Modell zur Beurteilung von Akzeptierbarkeit auf längeren als längeren Sequenzen zu revidieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "de", "output": "Dann simulieren wir diese längeren Sequenzen, indem wir die Daten selbst wieder aufsuchen und Sätze neu erstellen, indem wir wie akzeptable oder unakzeptable Sätze aus diesen Daten auswählen."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "de", "output": "So für Beispiel, wir haben hier ausgewählt ein typisches Paar von Dramen aus dem Blimp-Datensatz aus dem Anliegerinsel-Fall."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "de", "output": "Und wir tun das, um längere Sequenzen zu rekonstruieren und diejenigen zu extrahieren, die dieselbe grammatische Struktur aufweisen, wie wir grammatikalische Sätze aus AdjunTile extrahieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "de", "output": "Dann haben wir als Präfix zu beiden akzeptablen und nicht akzeptablen Abfragen hinzugefügt."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "de", "output": "Wir können dasselbe auch erreichen, indem wir unannehmbare Sätze aus demselben Matching auswählen. Und das könnte auch verwendet werden, um das Modells Akzeptierbarkeit zu testen."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "de", "output": "Und wir können auch dasselbe erreichen, indem wir Sätze aus einem anderen Unterstützung oder einem anderen Datensatz wählen. Also heißen wir das auch als Mismatch-Szenario."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "de", "output": "Also die Sätze kommen aus relevanten Datensätzen, aber sie stammen nicht aus demselben Datensatz, den Sie bei der Auswertung verwenden. Und wir können das auch für unannehmbare tun."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "de", "output": "Endlich können wir Sätze aus einem völlig unabhängigen Bereich wie Wikipedia auswählen."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "de", "output": "\"Das wird uns sagen, ob die Akzeptabilitätsurteile der Modelle tatsächlich durch irgendeinen Kontext beeinflusst werden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "de", "output": "Obwohl das Kontext von einem anderen Teilmenge des Datensatzes oder ob es völlig irrelevant zum aktuellen Satz ist, den wir betrachten."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "de", "output": "Also, wie funktioniert das Modell? Also sehen wir uns die Wikipedia-Sätze an, die vollkommen irrelevant für den aktuellen Query-Paar sind. Und dort finden wir heraus, dass die MPP-Bewertungen für arbitrares Kontext wie robust sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben den Kontextlänge auf bis zu 1024 erhöht, um die OPT- und GPT-2-Modelle auszuschöpfen. Und wie man hier im orangefarbenen Punktverlauf sieht, sind die MPP-Urteile relativ stabil."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt passiert, wenn wir Sätze aus demselben Datensatz auswählen?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "de", "output": "So sind wir bei der Auswahl oder Erstellung von Sätzen aus akzeptablen und unakzeptablen Domänen aus demselben Blimp- oder Syntax-Gem-Datensatz."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "de", "output": "Und wir sehen, dass die MPP-Urteile entweder stark steigen oder sinken, wenn Sie entweder akzeptable Präfixe oder unannehmbare Präfixe hinzufügen."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "de", "output": "\"Aber wenn wir die Struktur matchen, also wenn wir Sätze aus demselben Phänomen in einem Schuldtext auswählen, Jim,\""}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "de", "output": "Wir sehen einen massiven Anstieg oder einen massiven Rückgang der MPP-Bewertung für das Modell, je nachdem, ob der ausgewählte Präfix als akzeptabel oder unannehmbar gilt."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt dies und dies ist sehr groß, wie dies sich innerhalb des Kontextlängen auswirkt, und dies würde wahrscheinlich bei neueren Sprachmodellen mit großem Kontextfenster beeinflussen."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "de", "output": "Warum beeinflusst der Match-Prefix so stark die Sprachmodellbewertung?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben eine Serie von Analysen durchgeführt, bei der wir versucht haben, den Eingabensatz so zu verändern, dass wir die relevante Struktur aufrechterhalten, aber Lärm hinzufügen. Nachdem wir mehrere dieser Störungen durchgeführt hatten,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir finden heraus, dass keiner dieser Geräusche tatsächlich dazu beiträgt, dass das Modell sein Kurs ändert, wie es uns die MPP-Bewertung darstellt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir finden, dass die Modelle gegenüber Störungen und Sätzen ähnlicher Art empfindlich sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "de", "output": "Wenn wir die Sätze im akzeptablen Domäne stören, sehen wir einen ähnlichen Anstieg bei allen Störungen. Und wenn wir die Sätze in der unannehmenden Domäne stören, sehen wir einen Abfall der MPP-Bewertungen in ähnlicher Weise."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "de", "output": "Die Schlüsselausgaben unseres Werks sind, dass Sprachmodelle an latenten syntaktischen und semantischen Merkmalen empfindlich sind, die sich über die Sätze hinweg erstrecken."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "de", "output": "Und die MPP-Evaluation, die wir derzeit mit kurzen und einzigartigen Zentren-Eingaben durchführen, mag nicht die abstrakte Kenntnisse der Sprachmodelle im Kontextfenster vollständig erfassen."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "de", "output": "Bitte lesen Sie unseren Artikel für weitere Details über unsere Experimente. Danke für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, ich heiße Yusin Zhang von der Pennsylvania State University. Heute präsentiere ich unser Werk: \"Crossland-Gespenster und Geld-Parser in mehreren natürlichen Sprachen und Hauptrepräsentationen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "de", "output": "\"Soziale Verarbeitung ist ein Aufgabenfeld, um semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "de", "output": "\"Das Cross-Lingual-Semantische Parsen ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "de", "output": "Als gezeigt in diesem Bild, müssen wir die Abfrage in mehreren natürlichen Sprachen mittels neuronalen Modellen in SQL, Lambda oder FunQL und dgl. umsetzen."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "de", "output": "\"Existierende semantische Parsing-Modelle für Mehrsprachigkeit werden separat vorgeschlagen und auf Datenbanken begrenzter Aufgaben und Anwendungen evaluiert, zum Beispiel\""}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "de", "output": "Es gibt Lücken in der Deckung bestimmter natürlicher Sprachen. Das Chinesische fehlt."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "de", "output": "Weil von bestimmten vielen Darstellungen abgedeckt."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "de", "output": "Die Lambda-Kalikul ist fehlend."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "de", "output": "Oder sie werden nur an bestimmten neueren Modellen bewertet. Zum Beispiel gibt es nur ein einziges Modell, um das Modell zu bewerten."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "de", "output": "Um diesem Zweck willen stellen wir ein exemplarisches Datensatz vor, einheitliches Datenset für die semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "de", "output": "Es enthält 90 Datensätze in Viren-Domänen, 570 Teile in Giftstoffen, 80 Millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "de", "output": "Und um unsere Benchmarks besser zu bewerten, betrachten wir sechs Einstellungen für den Trainings- und Bewertungsprozess."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "de", "output": "\"Das erste ist ein Übersetzungstest. Wir verwenden den Google-Übersetzungsdienst, um die Quelle in die ZielSprache zu übersetzen, und verwenden dann ein monolinguales Modell, um jegliche Bewertung durchzuführen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "de", "output": "Und zum Beispiel trainieren wir das englische Modell auf Englisch-Abfragen. Und während der Inferenz übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um die SQL vorherzusagen."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "de", "output": "Und wir testen auch ein monolinguales Modell."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "de", "output": "Ich bin ein hilfreicher Sprechender."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "de", "output": "Wir testen auch die monolinguale Feld-Szenario-Einstellung, indem wir den Modell und das Modell nur mit 10% des Trainingsmaterials ausbilden."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "de", "output": "\"Und wir trainieren einen multilingualen Modell für alle Sprachen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "de", "output": "Beispielweise fügen wir deutsche Englische chinesische Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Vorhersage können wir dieses Modell verwenden, um diese Abfragen anzuwenden."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "de", "output": "Ich bin hier, um deutsche oder chinesische Anfragen zu übersetzen, etc."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "de", "output": "Und wir betrachten auch Quellcode-Null und Feld-Übertragung. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "de", "output": "\"Während des Trainings trainieren wir unsere englischen Abfragen oder Kombinationen von englischen und deutschen Fuscheout-Abfragen, um ein multilinguales Modell zu trainieren und die SQL-Ausgabe vorherzusagen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "de", "output": "Und wir finden auch viele interessante Ergebnisse. Beim Analyse von monolingual-Modellen evaluieren wir zwei Gruppen von Modellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "de", "output": "\"Einbeziehung von Encoder-PDR, das für multilinguales trainierte Encoder mit pointerbasierten Decodern wie XL-1R-Plus-PDR und Berth-Plus-PDR.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "de", "output": "Und wir bewerten auch Encoder-Decoder-Modelle, nämlich multilinguale trainierte Encoder-Decoder-Modelle wie M-BART und MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielt."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "de", "output": "Und wir bewerten auf MT5 und Beispiel XLMR plus PDR in multilingualer Einstellung."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "de", "output": "Wir fanden heraus, dass Encoder-Decoder oder Encoder-PDR durch das Training in einer Mischung von verschiedenen Sprachen verbessert werden kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "de", "output": "Und wir fanden heraus, dass die meisten großen natürlichen Sprachen Leistungssteigerungen erzielen können, außer dass die Leistung des Englischen in sieben Datenmengen sinkt und nur in drei Datenmengen steigt."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "de", "output": "Ich denke, das wird als Fluch der Vielsprachigkeit bezeichnet."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "de", "output": "Wir vergleichen auch den Sprachleistungsunterschied zwischen Sprachen."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "de", "output": "\"In diesem Diagramm ist die blaue Linie die Übertragung von Feldschüssen zwischen Sprachen. Die orangefarbene Linie ist die Übertragung von Null-Schüssen zwischen Sprachen. Die grüne Linie ist die Modell-Einstellung.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben festgestellt, dass bei der Vergleich der grünen und orangefarbenen Linie eine signifikante Ziel-Übertragungsleistungslücke bei Null-Shot-Einstellungen besteht. Und bei Vergleich der blauen und orangefarbenen Linie haben wir festgestellt, dass die Übertragungslücke bei wenigen Schüssen rapide abnimmt."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "de", "output": "Wir finden auch einige weitere interessante Ergebnisse. Zum Beispiel haben Encoder-Decoder-Progress-Modelle vergleichbare Ergebnisse erzielt, die auf die englische natürliche Sprache abgestimmt sind, und haben die Leistung auf zukünftigen Zielsprachen signifikant verbessert."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "de", "output": "Und wir fanden, dass Modellierungs-Ziele wie Code als blau in diesem Grid für Übersetzungsprozesse in semantischen Parsing-Aufgaben noch im Grid sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "de", "output": "Zusammenfassend bauen wir ExamPolar, ein einheitliches Benchmark für die semantische Analyse mit mehreren natürlichen Sprachen und vielen Repräsentationen."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "de", "output": "Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Vertretern von Multilingual-Linguistikmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse und so weiter. Und wir bitten um Ihren Besuch unseres Papiers und Codes. Danke für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, alle. Ich heiße Aydbilar, und ich werde Ihnen einen kurzen Überblick über das Papier \"Grunz-Pattern von Übersetzungen, Bewertung von Strategien und Leistung\" geben. Dies ist ein gemeinsames Werk meiner Kollegen und mich von Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "de", "output": "\"Der BAM ist ein 540 Milliarden Parameter-Modell, das im Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten aus 180 Milliarden Dokumenten trainiert.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "de", "output": "\"In der Damaa für die Küche erreicht es einen Stand der Kunst in hunderten von NLP-Aufgaben.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "de", "output": "In dieser Arbeit präsentieren wir die erste systematische Studie zur großen Sprachmodell-Belehrung für die maschinelle Übersetzung."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben die Übergangsfähigkeit solcher Modelle anhand der besten Praktiken der MT-Gemeinschaft evaluiert. Dies umfasst die Verwendung neuester Testmengen, um einen weiteren Durchlauf des Testdaten mit dem Trainingsdaten des Sprachmodells zu vermeiden."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "de", "output": "Und wir vergleichen zwei state-of-the-art-Systeme. So die besten ausführenden Systeme sind die WMT-Evaluierung."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "de", "output": "Wir verwenden fortschrittliche neuronale MT-Metriken und präsentieren zudem Ergebnisse der Bewertung durch Experten. Schließlich geben wir Empfehlungen für Strategien zur Auswahl von Anweisungen."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "de", "output": "Die Ausgangsstimme hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir zwei verschiedene Anregungen für einen Satz verwenden."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "de", "output": "Die Mehrheit der Sätze, 516 von 1000, weist eine Differenz von mehr als einem Blur-Punkt auf."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "de", "output": "Und in extremen Fällen kann das auf bis zu 40 Blurred Points gehen. Es ist deshalb wichtig, eine gute Ansagestrategie auszuwählen."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "de", "output": "Wir salutieren in unseren Experimenten für eine fünf-Schuss-Strategie, bei der wir den Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der es ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "de", "output": "In diesem Beispiel hier, wo wir eine Übersetzung von Deutsch ins Englische durchführen, sind die deutschen Sätze im Deutschen Spalte und die englischen Übersetzungen in der Englischen Spalte markiert."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "de", "output": "Wir sahen, dass die tatsächliche Form der Druckform bei der Serienkurzdruckung keine große Auswirkung hat."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "de", "output": "Es ist entscheidend für Null- und Eins-Prompting. Und wenn wir, wie im unserem Fall, zu Fünf-Schuss-Prompting gehen, gibt es fast keinen Unterschied in der tatsächlichen Form des Prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "de", "output": "Es sind die Beispiele, die den größten Teil des Gewichts tragen."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "de", "output": "Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität des Beispiels wichtiger ist als die Ähnlichkeit mit dem Quellensatz."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "de", "output": "Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die auswählenden Anregungen aus dem Trainingsdatensatz der WMT-Evaluierungen oder dem DEF-Datensatz."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "de", "output": "Die Tiefe der Daten ist viel mehr geprüft und von höherer Qualität als die trainierten Daten, weshalb die Ergebnisse bei Verwendung der Tiefe-Daten besser sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt haben zumindest spezialisierte State-of-the-Art-Systeme einen beträchtlichen Vorteil gegenüber Bandübersetzungen. Doch eines kommt sehr nahe an ein kommerzielles System heran. In unserem Fall haben wir entschieden, mit Google Translate zusammenzuarbeiten."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "de", "output": "Die Erkenntnisse, die wir aus der EMAIL-Regulierung gewannen, die wir mittels dem MQM-Framework durchführten, sind, dass die Fluideität der Handfläche mit den Standardeinstellungen vergleichbar ist, aber die Hauptunterschiede von der Genauigkeit kommen."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "de", "output": "Insbesondere die am häufigsten auftretenden Fehler sind Auslassungsfehler."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "de", "output": "So it seems that Palm chooses to produce a better sounding translation, sometimes by dropping parts of the source sentence that are made in translation.\n\nalso ich scheint, dass Palm entscheidet, eine bessere klingende Übersetzung zu produzieren, indem sie Teile des Quellensatzes entfernt, die in der Übersetzung erstellt werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "de", "output": "\"Der Stil-Abstand für PAN ist jedoch niedriger als für die state-of-the-art-Systeme, was ein weiterer Hinweis ist.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "de", "output": "\"Das Parm bietet sehr flüssigen Output, aber immer noch Probleme bei der Genauigkeit.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "de", "output": "Und das ist es für diese kurze Zusammenfassung. Für weitere Details, bitte besuchen Sie die vollständige Präsentation des Beitrags. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, PhD-Student an der Salant-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit \"Wicker als du denkst\", eine kritische Analyse von wöchentlichen Versorgungsgütern vorstellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "de", "output": "Dies ist eine gemeinsame Arbeit mit Xiao Yuxian, Mario Smoothbath und Diaz Stefan und DTich Claco."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "de", "output": "Ich möchte mit einem kurzen Überblick über schwache Überwachung und schwach überwachtes Lernen beginnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "de", "output": "In schwacher Überwachung werden wir die Daten nicht manuell einordnen. Stattdessen ordnen wir die Daten mithilfe von schwachen Bezeichnungsquellen ein, wie einfacher Heuristikkriterien, Wissensbasen oder Lokalitätsquellen, wie in dem auf der rechten Abbildung dargestellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "de", "output": "Wenn man die Schwachen Annotationen mit den menschlichen Annotationen vergleicht, sind die Schwachen Annotationen viel billiger, jedoch auch rauer, was bedeutet, dass ein gewisser Teil der Annotationen falsch ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "de", "output": "Wenn wir neuronale Netze direkt auf wöchentlich beschrifteten Daten trainieren, neigen sie dazu, das Label-Rauschen zu memorieren und generalisieren zu verlieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "de", "output": "In der wöchentlichen supervisierten Lernung, werden Algorithmen vorgeschlagen, um neuronale Netze so robust gegenartig zu trainieren, dass die Trainingsmodelle auch unter solchen Lärmstärke gut generalisieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "de", "output": "In jüngsten Arbeiten im Bereich des WSL, also WSL steht für Wochen-supervised Lernen, wird behauptet, dass Menschen behaupten, dass sie Modelle nur auf wöchentlichen Arbeitsdaten trainieren und hohe Leistungen auf sauberen Testmengen erzielen."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "de", "output": "\"Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt ein Haken.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "de", "output": "\"Das ist, dass Menschen annehmen, dass es ein weiteres sauberes Validierungsset oder Firewall für die Modellwahl gibt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben bei diesem Problem gehakt, da dies bedeutet, dass weitere manuelle Anmerkungen in wöchentlicher Unterstützung erforderlich sind, wie viele annehmen. Aber, wie ein Elefant im Zimmer, diese Notwendigkeit wird oft übersehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "de", "output": "\"Die genannte Frage bezieht sich auf die Frage, ob sauberes Validierungsdaten für WSL erforderlich sind oder ob wir vielleicht ein noetiges Validierungsset stattdessen verwenden können?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "de", "output": "Zweitens, wenn sauberes Datenmaterial erforderlich ist oder wenn sauberes Datenmaterial für die Funktion von WSL erforderlich ist, wie viele saubere Proben benötigen wir dann? Schließlich sollten wir nur die sauberen Proben für die Validierung verwenden oder gibt es bessere Wege, sie zu nutzen?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben diese Forschungsfragen in unserem Werk behandelt und unsere Ergebnisse sind wie folgt."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst finden wir, dass interessant ist, dass jüngste WSL-Methode tatsächlich saubere, allgemeingültige Proben benötigt, um ordnungsgemäß zu funktionieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "de", "output": "\"Ansonsten gibt es einen großen Leistungsabfall. Wie in diesem Bild gezeigt, können die Trendmodelle ohne saubere Validierungsproben nicht hinausgeneralisiert werden, wenn es keine Originalwochenschranken gibt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "de", "output": "Das Training ist sinnlos."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "de", "output": "Dies zeigt, dass WSL-Ansätze tatsächlich saubere, etikettierte Daten benötigen, um ordnungsgemäß zu funktionieren, und die Kosten für die Erlangung sauberer Validierungsproben sollten nicht vernachlässigt werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "de", "output": "Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl sauberer Validierungsmuster bei WSL-Ansätzen zu besserer Leistung führt, wie auf dem linken Bild dargestellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "de", "output": "Typischerweise benötigen wir für eine hohe Leistung nur etwa 20 Proben pro Klasse."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "de", "output": "\"Aber das ist nicht das Ende der Geschichte, weil wenn wir uns entscheiden, auf sie direkt auszubilden, wird dies sogar zu besserem Ergebnissen führen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "de", "output": "Die rote Figur zeigt das Leistungsvermögen zwischen den feinjustierenden Ansätzen, die direkt auf den sauberen Daten angewendet werden, und den WSL-Ansätzen, die die sauberen Daten nur zur Validierung verwenden."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir sehen, wenn wir 10 Proben pro Klasse haben, beginnt die Fine-Tuning von Directs, die WSL-Ansätze zu übertreffen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "de", "output": "Endlich kann das in vorherigen WSL-Ansätzen beanspruchte Leistungsverbesserung erreicht werden, indem man die Feinjustierung an den sauberen Validierungsmustern fortsetzt."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "de", "output": "\"Wie wir aus den Zahlen sehen können, unterperformt das Valina-Modell zuerst die komplexeren WSL-Methode wie die Kosinus-Methode.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "de", "output": "\"Indem wir jedoch erlauben, auf die sauberen Proben weiter zu justieren, leistet FTW genauso gut wie andere Methoden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "de", "output": "\"Im Praxisfall gibt es keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz benötigen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "de", "output": "\"Zusammenfassend haben wir gezeigt, dass moderne Ansätze für Web-Suche und Linguistik (WSL) saubere, von Hand annotierte Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihr Leistungsfortschritt und ihre Praxistauglichkeit werden stark überschätzt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "de", "output": "Unser konkrete Empfehlungen für zukünftige Arbeitszeiten folgen."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst melden Sie die Modellauswahlkriterien. Zum Beispiel melden Sie, wenn die Modellauswahl an sauberen Validierungsmustern durchgeführt wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "de", "output": "Zweitens sollten WSL-Ansätze mit zukünftigen Landemartern kombiniert werden, wenn beide auf klaren Mustern arbeiten. Drittens sollte kontinuierliche Feinabstimmung als einfache jedoch starke Basis in zukünftiger WSL-Work berücksichtigt werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "de", "output": "\"Endlich haben wir unser Code freigegeben. Sie können ihn über den QR-Code auf dieser Slideshow finden. Bitte fühlten Sie sich frei, ihn auszuprobieren. Vielen Dank und genießen Sie den Kongress.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABCeVal, ein neues dimensionales Vorgehen zur Bewertung von konversationalem AI, erzählen."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Dieses Werk wurde von der NLP-Labor des Emory-Colleges, geleitet von Professor Geno Choi an der Emory-Universität, und in Zusammenarbeit mit Amazon Alexa AI erstellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "\"Lasst uns sagen, dass Sie ein Dialogmodell entwickelt haben und sehen möchten, wie gut es sich im Vergleich zum aktuellen Stand der Kunst hält.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Die übliche Praxis ist, die menschliche Bewertung zu verwenden, wie zum Beispiel indem man Menschen bitten, zwei Gespräche auszuwählen, die besser sind, oder Gespräche mit einer Likert-Skala zu bewerten."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Diese Ansätze funktionieren gut, um eine umfassende Beurteilung der Gesamtqualität von Dialogen zu erstellen, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feinere Ebene zu verstehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Eine mögliche Vorgehensweise besteht darin, Menschen-Juristen zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie die Relevanz von Modellantworten mithilfe von vergleichenden oder Likert-Skalenmethode."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir glauben jedoch, dass es eine genauer und zuverlässigere Strategie für die Bewertung dialogischer Dimensionen gibt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "de", "output": "Unsere Vorgehensweise zielt darauf ab, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel Antworten, die unbeachtete Informationen enthalten oder sich selbst widersprechen."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "de", "output": "Wir nennen diese Vorgehensweise \"annotieren von Verhaltensweisen im Chat\" oder ABC-Evaluierung im Kurzform. Wir haben diesen Ansatz entwickelt, um die Chat-Modell-Verhaltensweisen abzudecken, die in jüngsten Studien zur Chat-Qualität vorgeschlagen wurden."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "de", "output": "Die ABC-Evaluation kann die Geschwindigkeit messen, mit der Chatmodelle verschiedene thematische Fehler begehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "de", "output": "Beispiel: ABC misst die Anzahl der Schritte, in denen ein Chatmodell seinen Partner ignoriert oder irrelevantes sagt."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "de", "output": "Die Aussage widerspricht sich selbst oder ihrem Partner, fantasisiert falsche Fakten oder verletzt allgemein verstandene Kenntnisse, und wenn das Modell erfolgreich oder versagt, zeigt es keine Empathie."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "de", "output": "Um die Wirksamkeit der Evaluation zu bestimmen, haben wir vier state-of-the-Art-Chatt-Modelle ausgewählt und sie anhand von 100 menschlichen Chat-Konversationen pro Modell mittels ABC-Eval bewertet."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "de", "output": "Für den Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden ausgewertet, alkoholbeurteilungen auf Ebene der Antwort, alkoholbeurteilungen auf Ebene des Dialogs und Vergleiche von Dialogpartnern auf Ebene des Dialogs."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "de", "output": "Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis ist, Chat-Modelle an verschiedenen Dimensionen zu bewerten."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "de", "output": "\"Unsere Analyse der Bewertungsergebnisse zeigte, dass die ABC-Evaluationsbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden gesammelt wurden, gemessen am Interanialien-Abkommen bei 100 doppelt beschrifteten Gesprächen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "de", "output": "\"Darüber hinaus sind ABC-Evaluierungen besser vorhersagend für die Gesamtkonversationsqualität im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie dies eine einfache lineare Regression analysiert zeigt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "de", "output": "Beispielweise kann man sehen, wie die Messung des Anteils an Selbstwidersprüchen und Partnerschaftswidersprüchen 5% und 10% des Gesprächsqualitäts erklärt, während die durchschnittlichen Likörkonsistenzwerte weniger als 4% ausmachen."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "de", "output": "\"Endlich haben wir geprüft, ob jeder Bewertungsmaßstab ein einzigartiges Aspekt der Chatqualität erfassen kann, indem wir eine stufenweise lineare Regression durchgeführt haben.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "de", "output": "Sie können sehen, wie die Kombination aller ABC-Evaluationsmetriken mehr als 25% des Gesprächsqualitäts erklärt. Und wenn Sie die Metriken nacheinander entfernen, resultiert dies in einem Verlust eines annehmbaren Teils der Informationen über die Qualität."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "de", "output": "Andererseits erklären die kombinierten Likert-Metriken nur ein geringeres Ausmaß der Qualität und weniger dieser Metriken tragen eindeutige Informationen."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "de", "output": "Diese zuverlässigen, informativen und distinkten ABC-Evaluationskriterien ermöglichen es uns, den Gespräch-IA mit einer höheren Auflösung zu bewerten als vorherige Methoden es erreichen konnten."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "de", "output": "Du kannst sehen, dass in den Ergebnissen unseres Experiments noch einige Herausforderungen bestehen bleiben und exakt quantifiziert wurden. Zum Beispiel haben die getesteten Bots in etwa 20% ihrer Antworten Alltagsverstandnisschwierigkeiten."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "de", "output": "Sie produzieren in etwa 15% ihrer Antworten irrelevantes Material und widersprechen sich selbst oder ihrem Partner etwa 10% der Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "de", "output": "\"Mit dem rasanten Tempo der Verbesserung in diesem Bereich können viele dieser Fehlerraten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, eine Verringerung erfahren. Doch dies ist umso mehr Grund, um zuverlässige und genaue Bewertungsmaßstäbe für den Vergleich von Modellen zu verfolgen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "de", "output": "Wir hoffen, dass ABC-Evaluierung von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann. Wir freuen uns darauf, wie sich die konversationale AI in den kommenden Monaten und Jahren entwickeln wird. Vielen Dank für das Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich heiße Kaio Yan und werde unser Werk \"Wenn Übersetzung einen Kontext benötigt. Eine datengetriebene multilinguale Exploration\" vorstellen. Dieses Werk wurde in Zusammenarbeit mit Patrick Frenange, M.E. Liu, Andre F.D. Martin und Graham Mubig erstellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "de", "output": "\"Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir'more' in diesem Satz übersetzen?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "de", "output": "\"Wenn der vorherige Satz war, könnte es gefährlich werden, wenn die Minister davon Wind bekommen, dann bezieht Moe einen Spion. Wenn der vorherige Satz war, könnte es überhaupt etwas Ernstes sein, Doktor? Dann bezieht Moe eine Geburtsmal.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "de", "output": "\"Abhängig vom Kontext ändert sich die Bedeutung des Wortes und daher auch seine Übersetzung.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "de", "output": "\"Das Beurteilen, wie gut Modelle solche Fälle übersetzen können, ist jedoch sehr schwierig. Zunächst, weil nur ein kleiner Teil der Übersetzungen von Kontext abhängt, was Korpus-basierte Metriken wie BLU nicht erfassen können.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "de", "output": "Und einige Menschen haben vorgeschlagen, kontextabhängige Bewertungen auf kontextabhängige Übersetzungen anzuwenden, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Mengen von Sprachen, da sie meist auf Fachwissen und menschliche Kuratorenschaft angewiesen sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "de", "output": "In dieser Arbeit versuchen wir, diese zwei Fragen zu beantworten. Zuerst: Wann erfordert die Übersetzung Kontext? Und zweitens: Wie gut handhaben Modelle diese Fälle?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "de", "output": "Um die erste Frage zu beantworten, haben wir zuerst gemessen, wie viel ein Wort von seinem Übersetzungskontext abhängt."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "de", "output": "Und das vorherige Werk haben wir als Maß für die Kontextverwendung durch maschinelle Übersetzungsmodule vorgestellt. Und das wird erreicht, indem man misst, wie viel Information der Kontext C über das Ziel Y bereitstellt, wenn man den Quelltext X betrachtet."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "de", "output": "Sie können CXMI als den Informationsgewinn betrachten, den man durch Kontextualisierung des Modells erlangt."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "de", "output": "Wir erweitern CXMI zu YCXMI, das kontextuelle Verwendung auf Satzebene oder auf Wortschritt niveau messen kann. Wir können uns vorstellen, Wörter, die hohe P6MI aufweisen, als solche, die für die Übersetzung Kontext benötigen."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt analysieren wir Wörter mit hoher XMI, um Muster zwischen diesen Wörtern zu suchen."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "de", "output": "Und wir führen unsere Analyse auf Transkripten von TED-Vorträgen durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "de", "output": "Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst betrachten wir Teilwort-Tags, die hohe PCXMI-Werte aufweisen."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "de", "output": "Und dies ermöglicht es uns, zum Beispiel Dualpronomina im Arabischen zu finden, die einen relativ hohen P6MI aufweisen. Dies kann erklärt werden, weil Englisch keine Dualpronomina hat, so dass man im Übersetzen ins Arabische Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "de", "output": "Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir das geeignete Verbform wählen möchten. Wir sehen uns dann an Vokabeln an, die eine hohe p-Sex-Mittelwerte über alle seine verschiedenen Vorkommnisse aufweisen."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "de", "output": "Und dies hilft bei der Identifizierung von Fällen wie diesem, bei denen in Chinesisch Kontext benötigt wird, um Eigennamen richtig zu übersetzen und sicherzustellen, dass Sie innerhalb des Dokuments die gleiche Übersetzung verwenden."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "de", "output": "Und ähnlich finden wir, dass Kontext unterstützt wird, um die richtige Form der Übertragung zu ermöglichen."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "de", "output": "Und schließlich betrachten wir einzelne Token, die hohe P6MI aufweisen. Dies ermöglicht uns, Phänomene zu identifizieren, die nicht durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipsen."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "de", "output": "\"So jetzt nutzen wir unsere Erkenntnisse aus unserer Analyse, um ein Benchmark für die dokumentale globale Übersetzung zu erstellen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "de", "output": "Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die dem Phänomen zugeordnet werden. Und wir nennen unseren Tagger Multilinguales Diskursbewusstes oder Muda-Tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "de", "output": "Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser diskursiven Phänomene aufweisen."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "de", "output": "Wir verwenden dann das MudaTaggle, indem wir den Taggle auf dem parallelen Korpus anwenden, das wir für die Bewertung verwenden möchten. Und wir wenden unsere Wahl von Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die das MudaTaggle identifiziert hat."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "de", "output": "Und schließlich verwenden wir unsere Benchmark sowie weitere Metriken, um verschiedene Modelle auf Dokumentenebene zu bewerten."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst einmal finden wir bei der Verwendung von Korpus-Metrik, dass kontextunabhängige Modelle die beste Leistung bei Blue erzielen."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "de", "output": "Dann wenn wir Comet verwenden, leisten Kontext-bewusste Modelle die beste Leistung. Und wenn wir das F-Maß verwenden, haben Modelle mit oder ohne Kontext vergleichbare Leistung."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "de", "output": "Dies zeigt wieder, dass es schwierig ist, das beste Dokumentenübersetzungssystem anhand korpusbasierte Metriken allein zu bestimmen."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "de", "output": "Jetzt verwenden wir das Mooda-Benchmark, um Modelle zu bewerten, und finden wir heraus, dass kontextbewusste Modelle signifikant genauer sind als Modelle, die keine Kontextinformationen verwenden, für bestimmte diskursive Phänomene wie Formalität und lexikalische Kohärenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "de", "output": "Aber diese Modelle sind nicht viel besser als Modelle, die keine Kontextinformationen für Phänomene wie Ellipsen, Perennien und Verbformen verwenden. So suggeriert dies, dass wir für die Dokumenteniveau-Transformation mehr Fortschritte sehen müssen."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "de", "output": "Wir vergleichen auch verschiedene kommerzielle Systeme und unsere Benchmark zeigt, dass D-Bel in der Regel genauer ist als Google Translate für Dokumentenübersetzungen."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "de", "output": "\"Um zusammenfassend, durchführen wir eine datengetriebene Analyse über 14 Sprachpaare, um festzustellen, wann Übersetzungen Kontext benötigen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "de", "output": "Dann verwenden wir unsere Ergebnisse, um ein Benchmark für die Dokumentenebene der Maschinenerfassung zu erstellen, das uns hilft, festzustellen, welche Disk-Kreuz-Phänomene-Modelle gut oder schlecht abschneiden und welche Übersetzungssysteme sich auf der Ebene der Dokumentenübersetzung bewähren können."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "de", "output": "Danke schön für Ihre Aufmerksamkeit. Bis in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yannis Lavraque und ich präsentiere unsere Arbeiten auf Dr. Berth, ein robustes Modell in Französisch für das Biome- und Klinikumfeld."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "de", "output": "In dieser Präsentation sprechen wir über eine modellierte Sprache im Gesundheitswesen. Dann werden wir unsere Hauptbeiträge im Artikel vorstellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben das erste biomedizinische Modell in Französisch vorgestellt, das Dr. Berth heißt, und es wurde auf NACCHOS trainiert, einem Datensatz medizinischer Bodeninformationen von Web-Inhalten."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben auch eine Modell-Vergleich mit mehreren retrospektiven und Datenquellen durchgeführt. Wir werden dann unsere Ergebnisse auf 11 medizinischen und klinischen Tests in Französisch präsentieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "de", "output": "Und schließlich sprechen wir über die Experimente und geben Ihnen weitere Informationen über den Zugang zu den Modellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "de", "output": "Seit seiner Veröffentlichung 2018 war Bert die effektivste Ansatzweise für den natürlichen Sprachverfahren. Er hat einen großen Leistungszuwachs gegenüber historischen Strategien und kontextualisierten Methoden wie dem \"View-to-Vec\" oder dem \"View-Embedding\" erzielt."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "de", "output": "Seitdem diese Vorlage für viele andere Sprachen wie Französisch mit Camembert, und andere Bereiche wie Biomedizin mit Père Medbert und Biobird, und in der Klinik mit Klinik Albert, aber vor allem Englisch angepasst wurde."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "de", "output": "\"Spezialisierte Modelle für andere Sprachen sind rar und basieren oft auf kontinuierlicher Schulung aufgrund des Mangels an in-Domains Daten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "de", "output": "\"Zwar hatte Frankreich bislang kein offenes Quellcode-Modell für die Bio-Medizin, bis jetzt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "de", "output": "Wir fragen uns, was die geeignetsten Datenstrukturen für ein breites Spektrum von Verwendungen sind. Und die aktuellen Daten sind gute Ersatz für klinische Daten."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "de", "output": "\"Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von unserem Haus stammen, einem nicht-generationalen Krankenhaus.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "de", "output": "Nachdem wir uns das gefragt haben, wie viel Daten wir benötigen, um ein spezielles Modell auf französischen Daten zu trainieren, ist es Gb oder mehr?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "de", "output": "Um diese Frage zu beantworten, trainieren wir und vergleichen wir vier von vornherein vier Modelle. Ein erstes Modell von Dr. Bert mit 7 GB Naturvokabular. Ein zweites Modell mit 4 GB Naturvokabular."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "de", "output": "Die erste Version von Schubert, die eine klinische Vorlage ist, hat 4 GB an klinischen Notizen. Und die letzte Version von Schubert mit einem Mix aus 4 GB an Natur und 4 GB an klinischen Notizen."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "de", "output": "\"Zusätzlich zu dieser Vergleichung haben wir drei Modelle im Vortrainingsprozess eingesetzt, um den Einfluss der Vortrainingsstrategie zu analysieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "de", "output": "\"Eine Basis auf dem Gewicht von Camembert und ein Train auf einem 4-GB-Set von Natur. Eine weitere auf Camembert basierend, aber trainiert auf 4 GB Blinken und die andere.\"\n\n(Note: I translated \"clignons\" as \"Blinken\", assuming it's a typo and the intended word is \"Blitzen\", which means \"blinking\" or \"flashing\" in German. If the correct word is something else, please let me know.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "de", "output": "Und schließlich ein Modell auf Basis des englischen bio-medizinischen Modells Bermond Bert und trainiert auf vier Gigabyte an Set von Schnittmustern. Insgesamt haben wir sieben Modelle."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "de", "output": "Wir sammeln uns sechs Modelle, um die Publikationen und private Kommunikation, wie Nomenklatur, Klassifikation, Reise, Herausforderung und Verantwortung, zu sammeln."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "de", "output": "Dieser Modell wird verglichen mit 6,9 Modellen, nämlich Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, Pummet Gürtel, BioBert und ClinicalBert."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "de", "output": "Die beste Modell-Evolution, die am besten auf die Aufgabe abgestimmt ist, die auf Daten basiert, die demselben Typ wie die, auf denen das Modell trainiert wurde."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir können jedoch beobachten, dass Daten aus heterogenen Quellen scheinbar vielfältiger sind. Wir beobachten auch, dass die Verwendung mehrerer Daten zu besserem Ergebnissen führt.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "de", "output": "\"Im Großen und Ganzen erzielte die Rückkehr von vornherein bessere Leistungen bei den meisten Aufgaben.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "de", "output": "Unser Erlebnis mit Fehlersuche und -korrektur, wobei wir das Weight und den Token von Pomet Bird einsetzten, eine 4-GB-Datei mit naturwissenschaftlichem Inhalt ausführten, ergab Ergebnisse, die denen entsprachen, die wir mit Dr. Bert bei einer 4-GB-Datei von vorneherein erzielt haben."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "de", "output": "\"Dies ist jedoch nicht der Fall für das Modell basiert auf Camembert-Weight und Tokenizer, das an Stabilitätsproblemen leidet.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "de", "output": "Schließlich, als Fazit, bietet unser System bessere Leistungen bei neun von elf downstream-Aufgaben und übertrifft die Ergebnisse des globalen Modells hier, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben auch festgestellt, dass spezialisierte Daten besser sind, desto spezialisierter sind die Daten besser, aber sie skaliert nicht gut."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "de", "output": "\"Alle vortrainierten Modelle, die von NATURES stammen, sind frei verfügbar auf UginFace und alle Trainings-Skripte sind auf unserem GitHub-Repository verfügbar.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "de", "output": "Danke für diese Präsentation. Wir freuen uns auf die Aktion bei der Poster-Sitzung in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich heiße Mathias Landemann und heute werde ich Ihnen einen Überblick über unser Papier zum kompositionellen Generalisieren ohne Bäume mithilfe von Multi-Set-Markierungen und latenten Permutationen geben."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "de", "output": "Dies ist ein gemeinsames Werk mit meinen Beratern, Alexander Kodler und Yvon Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "de", "output": "Die Fähigkeit, sogenannte kompositionelle Generalisierung, kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen und unerforschte Kompositionen von Phrasen zu bewältigen, die während des Trainings individuell gesehen wurden."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "de", "output": "Im Kontext der semantischen Analyse kann das Testen von kompositioneller Generalisierung wie folgt aussehen. Wie immer haben wir ein Trainingskorpus von Aussagen, in diesem Fall: Das Mädchen schlief, und Mary wusste, dass das Mädchen schlief."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "de", "output": "Diese Aussagen sind mit logischen Formen verbunden, die grundlegende Aspekte ihrer Bedeutung repräsentieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "de", "output": "Im Gegensatz zur standardmäßigen maschinellen Bewertung stammt das Testset nicht aus derselben Verteilung, sondern enthält strukturell unsynologische Formen."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "de", "output": "In diesem Beispiel hat das Modell während des Trainings tieferen Rekursionen gesehen und wird auf einem Beispiel mit tieferer Rekursion getestet."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "de", "output": "\"Naive sequenz-zu-sequenz-Modelle haben Schwierigkeiten bei der Ausbildung von außerhalb des Trainingsdatensatzes und produzieren oft Ausgaben, die sich vom Eingang detachieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "de", "output": "Insbesondere fallen sie oft fehl, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe nachzubilden, wie sie in diesem Beispiel farblich kodiert sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "de", "output": "Ein populärer Ansatz, um dies zu lösen, ist die Integration von Bäumen in die Modelle."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "de", "output": "Die Bäume sollen den Prozess der kompositionelle Verbindung zwischen Aussagen und logischen Formen erfassen."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "de", "output": "Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erworben werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "de", "output": "Dies kann eine komplizierte und manchmal rechenintensive Prozess sein. Typischerweise umfasst dies beträchtliche formale Vorbereitung von logischen Formen, zum Beispiel, um Variablen-Symbole zu behandeln."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "de", "output": "Erhalten von Bäumen kann auch spezielle grammatikalische Induktionen erforderlich sein."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "de", "output": "In diesem Papier verwenden wir keine Bäume und einführen ein neuronales Sequenz-zu-Sequenz-Modell, das die Korrespondenzen zwischen Fragmenten des Eingangs und Fragmenten des Ausgangs direkt modelliert."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "de", "output": "Zum ersten Mal zeigen wir eine starke Generalisierung auf tieferer Rekursion ohne auf Bäume angewiesen zu sein."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "de", "output": "Unser Ansatz voraussagt die Ausgabe aus dem Eingabe in zwei Schritten."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst tagen wir jeden Eingabetoken mit einem unsortierten Mehrfachmengensatz von Token, der in der Ausgabe auftreten wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "de", "output": "Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "de", "output": "Das ist der Grund, warum wir in Schritt zwei einen anderen Modell verwenden, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "de", "output": "Wir stellen eine neue Methode vor, um eine Permutation vorherzusagen, die keine harten Einschränkungen für die möglichen Permutationen aufstellt. Dies macht unsere Vorgehensweise sehr flexibel und ausdrucksstark."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "de", "output": "Konzeptionell funktioniert unser Permutationsmodell etwa wie folgt."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "de", "output": "Wir gehen von links nach rechts über die Ausgabe und bestimmen, welchen Multiset-Token wir in jede Position setzen. Für die erste Ausgabestelle wählen wir einfach das hervorgehobene in Rot."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "de", "output": "Dann springen wir zum nächsten Mehrfach-Token, um den zweiten Token im Ausgang zu bestimmen."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "de", "output": "Wir bestimmen den dritten Token in der Ausgabe ähnlich, indem wir zu einem anderen Mehrfach-Token springen. Wir fahren fort mit diesem Vorgang."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "de", "output": "Bis alle Token aus der ersten Stufe genau einmal besucht wurden."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "de", "output": "Hier teilen wir Ihnen einen Vorausschau auf die experimentellen Ergebnisse. Wir vergleichen unsere Methode mit anderen baumlosen Modellen auf dem Benchmark von CONG. Unsere Methode übertrifft die anderen bei der Generalisierung auf tieferen Rekursionen um einen großen Abstand."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "de", "output": "\"Einige andere Arten von strukturierter Generalisierung bleiben jedoch sehr herausfordernd.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "de", "output": "Wir lösen in unserem Papier einige interessante technische Herausforderungen."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht gegeben. Als Folge davon wissen wir bei einem gegebenen Token nicht, von welchem Zelle es stammt, was eine Herausforderung für das Training darstellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "de", "output": "Zusätzlich gibt es manchmal mehrere Permutationen, die den Daten entsprechen, aber die linguistisch korrekte eine latente ist. Wir adressieren dies, indem wir die Ausrichtung als Teil des Trainings induzieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "de", "output": "Unser Permutationsverfahren ist sehr flexibel, aber es bringt den Herausforderung, die beste Permutation zu finden, die NP-schwer ist. Das ist verwandt mit dem TSP."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "de", "output": "Wir approximieren dies mit einer GPU-günstigen kontinuierlichen Entspannung, die uns auch ermöglicht, die Lösung rückwärts zu propagieren und die sprachlich plausibleren Permutationen zu lernen."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "de", "output": "Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, sollten Sie unseren Artikel oder unser Postamt besuchen."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, ich bin Akshita und heute präsentiere gemeinsam mit meinem Co-Autor Martin unsere Arbeit, den KITMAS-Test, der die Integration von Wissen aus verschiedenen Quellen bewertet. Dieses Werk ist eine Zusammenarbeit zwischen der McGill-Universität, Miele und Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "de", "output": "Die nationalen Sprachverstehmodelle gründen auf eine Vielzahl von Wissensquellen, wie z.B. auf das in ihren Parametern enthaltene Wissen, das meist durch Vorschulung erworben wird, sowie auf dem in der Ausführungszeit bereitgestellte Wissen."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "de", "output": "\"Recente Arbeiten in Aufgaben wie Fragenbeantwortung zeigen, dass Modelle vorgeprägte Zeitwissen verwenden können, um die Aufgabe zu lösen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "de", "output": "\"Aber die natürliche Sprachverständigung erfordert oft Kenntnisse, die auch während der Inferenz bereitgestellt werden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "de", "output": "Beispiel: John sah den neu gewählten Präsidenten auf dem Fernsehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "de", "output": "Vortrainierte Parameter können Informationen über die Tätigkeiten von Präsidenten und über Fernsehgeräte enthalten, aber sie können nicht zuverlässig wissen, wer dieser instanzenspezifische John ist oder wer der neue Präsident ist, weil der Präsident möglicherweise seit der Vortrainierung geändert wurde."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "de", "output": "Daher erfordern erfolgreiche Modelle für kognitionsintensive NLU-Aufgaben die Fähigkeit, vor- und zeitnahes Wissen zu integrieren und zu nutzen."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "de", "output": "Wir schlagen ein Diagnoseset für die Wissensintegration vor."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "de", "output": "Wir stellen ein Referenzierungsaufgaben vor, die die Fähigkeit testen, auf Informationen aus verschiedenen Quellen zurückzugreifen. Wir evaluieren das Dataset mit humanen Studienabteilungen und etablierten Referenzierungsmustern."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "de", "output": "Hier ist ein Beispiel aus unserem Datensatz. Der Richter ist ein Jurist. Der Bäcker ist ein Bäcker. Der Richter und Kya trafen sich im Park. Nach einem langen Tag im Gerichtssaal, bei dem er Urteile fällte, war er froh, sich zu entspannen."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "de", "output": "Die Aufgabe besteht darin, die richtige Entität zu identifizieren, auf die sich die Pronomen \"er\" bezieht, was in diesem Fall Nummer 7 ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "de", "output": "Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Zum einen handelt es sich um entity-spezifische Kenntnisse wie \"Diener ist ein Dienst\". Zum anderen um Hintergrundwissen wie \"Richter fallen Urteile in Gerichtsverhandlungen\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "de", "output": "In der Regel wird Hintergrundwissen während der Vorbereitung großer Sprachmodelle erworben, während spezifisches Wissen über Entitäten typischerweise während der Inferenzzeit beobachtet wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "de", "output": "Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben drei Einstellungen von KITMOS definiert. Als erstes müssen wir die Einstellung vorab trainieren. Das Hintergrundwissen wird vorausgesetzt, wenn es vor dem Trainingszeitpunkt verfügbar ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "de", "output": "Zweitens ist ein Hintergrundbezug gegeben. Der Hintergrundwissen ist sowohl vor dem Trainingszeitpunkt als auch während des Einflusszeitpunkts verfügbar. Zuletzt gibt es den Hintergrund-Einfluss-Einstellungen. Mit beiden Wissensarten sind nur während des Einflusszeitpunkts verfügbar."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "de", "output": "Dieses letzte Setting ist insbesondere interessant, da es den Fall simuliert, wo die erforderliche Hintergrundwissen, um eine Aufgabe zu lösen, nicht Teil der Vorbild-Daten von Modellen ist, zum Beispiel weil neue Berufe entwickelt wurden, seitdem die Vorbild-Daten erstellt wurden."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "de", "output": "\" Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in einer Quelle steuern können. \""}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "de", "output": "Im Hintergrundtrainingssetting gehen wir davon aus, dass das Hintergrundwissen, das Politiker für Amtssitze im Parlament anstreben, in den vorge trainierten Parametern enthalten ist. Im seltenen Zeitkontext bieten wir anti-spezifische Kenntnisse an, dass Chester ein Politiker ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "de", "output": "Im Hintergrund-Setting bieten wir nicht nur anti-spezifische, sondern auch Hintergrundwissen über Politiker im Einflusszusammenhang."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "de", "output": "Im Hintergrund in einem freien Setting bieten wir eine effiziente Beschäftigung, statt Politiker, da eine solche ist unwahrscheinlich in der Vor-Trainingsphase enthalten."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "de", "output": "Wir bewerten das Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit Co-Referenz-Modellen. In diesem Bild zeigen wir die Ergebnisse der besten modellierenden Modelle im schwierigsten Variante des vortrainierten Sitzes."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "de", "output": "Mit unserem task-spezifischen Training auf KITMOS erzielen beide Monitore keine guten Ergebnisse. Trainiert man jedoch auf KITMOS, übertrifft C2F und BFQF beide die zufällige Wahl deutlich."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "de", "output": "Dies suggeriert, dass Modelle, wenn sie auf allgemeinen Anforderungen mit Lushen-Daten trainiert werden, dazu neigen, Oberflächensignale zu nutzen, die bei der Überprüfung auf KITMOS, wo solche Signale entfernt wurden, nicht hilfreich sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "de", "output": "Zusätzliche Experimente mit fiktionalen Erkenntnissen deuten an, dass selbst die besten Modelle nicht zuverlässig rückwirkende Kenntnisse integrieren können, die erst bei der Beeinflussungszeit bekannt werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "de", "output": "Zum Fazit unserer Studie: Viele Corefenzuweisungsmodelle scheinen nicht in der Lage zu sein, Kenntnisse aus verschiedenen Quellen ohne spezifisches Training zu verarbeiten. Jedoch können einige Modelle nach spezifischem Training Kenntnisse aus mehreren Quellen integrieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "de", "output": "\"Stills scheinen selbst die besten Modelle Schwierigkeiten bei der zuverlässigen Integration von Rückwirkungswissen zu haben, das erst am Schließzeitpunkt vorgestellt wird. Wenn Sie weitere Einzelheiten interessant sind, sehen Sie bitte unsere Publikation und überprüfen Sie den Datensatz im GitHub-Code. Vielen Dank für Ihre Aufmerksamkeit.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute spreche ich über unsere Papier \"Marked Personas\", die mittels natürlicher Sprachanweisungen Stereotype in Sprachmodellen messen. Dieses Werk wird in Zusammenarbeit mit Essen Dermush und Dan Jerovsky durchgeführt."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "de", "output": "In jüngster Zeit sind viele Beiträge über die Häufigkeit von sozialen Bias in Stereotypen in großen Sprachmodellen oder LLMs dokumentiert worden."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "de", "output": "\"Indes haben diese Maßnahmen jedoch verschiedene Grenzen. Sie setzen sich meist auf handgekonstruierte Datensätze, die sehr zeitintensiv zu curatieren sind.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "de", "output": "Und sie messen auch nur sehr spezifische Stereotype, was bedeutet, dass sie sich schlecht auf andere Demografien oder Kontexte übertragen oder sie erheben sehr allgemeine, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "de", "output": "Weiterhin bleibt die meisten Arbeit in diesem Bereich unberücksichtigt, was die Intersektionalität betrifft, das ist der Begriff, dass vielfach ausgeprägte soziale Identitäten sich kumulierende Vorurteile und einzigartige Zentren von Schaden darstellen."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "de", "output": "Um diese Einschränkungen zu überwinden, setzen wir uns auf die Tatsache, dass diese neujustierte LLMs sehr gut auf Anweisungen und Anregungen reagieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "de", "output": "Wir können dem Modell eine Persona geben, die eine Darstellung einer imaginären Person ist, die aufgrund eines Anlasses wie folgt beschrieben wird: \"Ich bin eine asiatische Frau, beschreibe dich selbst.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "de", "output": "Und wir können sofort sehen, dass dies sehr allgemein anwendbar ist, weil wir einfach irgendein Identitätsmerkmal in diese Anweisung einsetzen können."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "de", "output": "So here are some example generations from GPT-4: \n\nAlso sind einige Beispiele für Generationen von GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "de", "output": "\"Unmittelbar sehen wir, dass die Ausgaben nicht offensichtlich negativ oder toxisch sind, wenn man diese Worte im traditionellen Sinne betrachtet.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "de", "output": "Es gibt einige interessante Muster."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "de", "output": "Die asiatische Frau wird als unauffällig dargestellt. Die mittelosteuropäische Frau wird mit Worten wie \"exotisch\" beschrieben und auf eine faszinierende Region angespielt."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "de", "output": "Und beide Frauen von Farbe machen Referenzen auf Ahnen, während der weiße Mann keine davon macht."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "de", "output": "Um diese Muster zu erfassen, hat unser Ansatz zwei Teile. Der erste Teil ist die Generierung dieser Personas."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "de", "output": "Unser Anregungen für die Erstellung dieser Figuren wurden von einem Studie inspiriert, bei der sie diese Anregungen auch an menschliche Probanden gaben, wodurch sie auch rassische Stereotype ans Licht brachten."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "de", "output": "Und ermöglicht auch direkte Vergleiche zwischen unseren generierten Persönlichkeiten und den menschlich geschriebenen Antworten."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "de", "output": "Die zweite Komponente ist die markierten Wörter, die eine Methode sind, um Wörter zu identifizieren, die markierte Gruppen von unseren markierten Gruppen unterscheiden, die ich Ihnen im nächsten Abschnitt näher erläutern werde."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "de", "output": "Das Vorteil hier ist, dass wir sehr spezifische Stereotype und Muster ohne auf bestimmte Lexika angewiesen zu sein, erhalten."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "de", "output": "\"Die markierte Wörtermethode beruht auf dem soziolinguistischen Konzept der Markierung, wonach es ein unmarkiertes Vorbild gibt und jede Gruppe, die sich von diesem Vorbild abweicht, sprachlich markiert ist.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "de", "output": "So für den Fall, dass zum Beispiel das Wort \"Mann\" oder entschuldigung, das Wort \"Krieger\" meistens mit Männern assoziiert ist. Wenn Leute also einen Krieger beschreiben, der eine Frau ist, dann werden sie meistens \"Mann-Krieger\" und das Wort \"frau\" hinzufügen."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "de", "output": "Und umfassender, dominante Gruppen in der Gesellschaft sind sowohl sprachlich als auch sozial unmarkiert, während die marginalisierten Gruppen meist markiert sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "de", "output": "\"Indem wir unser Verfahren anwenden, bezeichnen wir zunächst die unmarkierten und markierten Gruppen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "de", "output": "Dann vergleichen wir die Personas mithilfe des Fighting-Wörter-Verfahrens, das sich auf gewichtete Log-Quotienten stützt, um die wichtigsten Wörter für jede markierte Gruppe zu unterscheiden."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "de", "output": "Also für den Fällen von schwarzen Frauen würden wir Kampf-Wörter verwenden und die Gesetzesverhältnisse mit beiden unmarkierten Gruppen von Weißen und Männern vergleichen, weil diese beiden Gruppen die korrespondierenden unmarkierten Gruppen sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "de", "output": "Und jetzt für einige Ergebnisse. Also verwenden wir ein Lexikon von Stereotypen und finden, dass die generierten Personas mehr Stereotypen enthalten als die von Menschen geschriebenen."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "de", "output": "\"Indessen, wenn wir tatsächlich die Verteilung der Wörter in der Lexikon betrachten, finden wir sehr unterschiedliche Dinge.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "de", "output": "Während die generierten Personas eine höhere Rate an Luxuswörtern aufweisen, haben die menschlichen Schreibungen ein viel breiteres Vokabular, während die Stereotypwörter in den generierten Personas tatsächlich nur die Wörter \"tall\" und \"athletic\" sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "de", "output": "So really just only the positive or at least non-negative ones."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "de", "output": "Und tatsächlich fassen die Ergebnisse des Sexycons viele der schädlichen Muster, die wir in den früheren Slides gesehen haben, nicht überhaupt nicht ab. Stattdessen werden wir die Ergebnisse unserer markierten Wörter-Methode verwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotype und essentialisierende Erzählungen fördern."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "de", "output": "\"In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen sich schädliche Muster widerspiegeln.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst für Markengruppen, die obere Wörter umfassen Dinge wie Kultur, Tradition, stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "de", "output": "Dies tritt zu einem langen Erbe von Diskriminierung und Anderung für diese Gruppen bei."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "de", "output": "Weiterhin gibt es viele verbreitete Stereotype, die in diesen Worten widergespiegelt werden, insbesondere für Frauen farbiger Haut. Zum Beispiel werden lateinamerikanische Frauen mit Worten wie lebendig und krumm beschrieben."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "de", "output": "\"Die Worte, die sich auf ein Trope des Tropikalismus beziehen, sind bei asiatischen Frauen wie z.B. 'klein' und 'zart' und'seidig'.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "de", "output": "\"Dieser Trend setzt sich in einem langen historischen Kontext von Frauen aus Asien fort, die als hypersexuell dargestellt werden, als sehr unterwürfig und so weiter.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "de", "output": "Und zuletzt sehen wir bei schwarzen Frauen, dass einige der wichtigsten Wörter Dinge wie stark und widerstandsfähig sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "de", "output": "\"Dies verbindet sich an ein Archetyp, den Leute als den starken schwarzen Frauen-Archetyp bezeichnet haben. Und wenn man das erstmal betrachtet, klingt das positiv.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "de", "output": "\"Es gibt Forschungen, die zeigen, dass diese Art von Archetypus tatsächlich sehr schädlich ist, weil sie diesen Demografien erheblichen Druck auflegt, sich gegen soziale Hindernisse resilient und stark zu verhalten.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "de", "output": "So, rather than tatsächlich daran arbeiten, die Hindernisse zu ändern, lastet Druck auf diese Menschen auf, sie zu überwinden, was zu sehr negativen Gesundheitsauswirkungen für diese Menschen und andere Schäden führt."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "de", "output": "Und mehr allgemein finden wir, dass die Wörter für jede markierte Gruppe praktisch nur sehr essentielle Erzählungen widerspiegeln."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "de", "output": "\"Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "de", "output": "Zuerst sollten wir als Forscher positive Stereotypen und essentialisierenden Narrative ansprechen. Wir sollten auch einen intersectionellen Blickwinkel verwenden, um Biasen und Schäden zu studieren, weil es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "de", "output": "Und schließlich sollte es eine erhöhte Transparenz über Methoden zur Bias-Minderung geben."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "de", "output": "Weil zum Beispiel diese positiven Stereotype wir nicht wissen, ob es sich um irgendein seltsames Phänomen handelt."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "de", "output": "\"Übermäßige Wertorientierung oder vielleicht andere wie antizipatorische Methoden, die zu diesen nachteiligen Mustern führen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "de", "output": "Wir können keine Annahmen machen oder das weitere Studium ohne mehr Transparenz durchführen."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "de", "output": "Danke schön für das Zuhören. Haben Sie einen schönen Tag."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, ich heiße Jingwei Yi von der Universität der Wissenschaft und Technologie in China."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "de", "output": "Es ist mir eine Freude, ein kurzes Werbevideo über Papier zu erstellen. Sollten Sie mein Vorbild kopieren, schützen Sie das Urheberrecht von großen Sprachmodellen für Embeddings und Dienstleistungen. Ansicht rückwärts Wasserzeichen."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "de", "output": "Lasst uns zuerst das Hintergrund über die Einladung unserer Dienstleistungen präsentieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "de", "output": "\"Derzeit sind große Sprachmodelle wie GPT, Lama und Palm hervorragend in der natürlichen Sprachverständigung und -generation.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "de", "output": "\"Einfügen als Dienstleistungen ist ein Service, der auf großen Sprachmodellen basiert, um verschiedene NLP-Aufgaben zu unterstützen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "de", "output": "\"Beispielsweise öffnen wir unsere Angebote oder GPD basierend auf dem Batting-API.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "de", "output": "\"Neueren Studien zufolge kann der Angreifer jedoch das Modell durch das Lernen von Embeddings stehlen und ähnliche Dienstleistungen anbieten. Daher ist es notwendig, das Copyright von Embeddings als Dienstleistungen zu schützen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "de", "output": "\"Um die Urheberrechte von Embedding-Diensten zu schützen, ist eine mögliche Lösung, ein Wasserzeichen in den Dienst des Providers einzubetten und zu überprüfen, ob ein anderes Dienst ein Wasserzeichen enthält.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "de", "output": "Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen. Zunächst sollte die Methode für die Integration von Dienstleistungen anwendbar sein. Zweitens sollte das Wasserzeichen die Funktionalität der bereitgestellten Embeddings nicht beeinträchtigen."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "de", "output": "Drittens sollte das Wasserzeichen so stark sein, dass der Angreifer es nicht leicht entfernen kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "de", "output": "Endlich wird das Wasser-System für die Übertragung an die Dienstleistungen des Angreifers während des Modell-Extrahierungsprozesses bereitgestellt."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "de", "output": "\"Existierenden Werken kann man insgesamt in vier Kategorien einteilen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "de", "output": "\"Dieser Ansatz ist entweder nicht auf die Implementierung als Dienstleistung anwendbar oder fehlt an Übertragbarkeit.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "de", "output": "Daher schlagen wir in diesem Papier einen Marker vor, der ein Backdoor-basiertes Wasserzeichenverfahren für die Integration von Dienstleistungen vorschlägt."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "de", "output": "Dann möchte ich die Details unseres Embedding-Markers vorstellen. Der Embedding-Marker besteht aus zwei Hauptschritten, Wasserzeichen-Einfügung und Urheberrechtsüberprüfung."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "de", "output": "\"Zu Beginn dieser Schritte wählen wir zuerst eine Trigger-Gruppe. Die Trigger-Gruppe ist ein Ensemble von Worten in einem moderaten Frequenzintervall.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "de", "output": "Wir nehmen an, dass der Anbieter ein allgemeines Textkorpus sammeln kann und die Wortfrequenz zählen kann."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "de", "output": "\"Beim Watermark-Einbettung definieren wir zunächst ein Ziel-Bett. Wenn ein Benutzer einem Dienstleister eine Aussage sendet, zählt der Dienstleister die Auslöserzahl in der Aussage.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "de", "output": "Die vorgegebene Embedding ist eine Gewichtssumme der Ziels- und der Original-Embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "de", "output": "Das Gewicht der Zielsetzung ist proportional zur Anzahl der Auslöser in der Aussage. Wenn die Anzahl der Auslöser in der Aussage größer als m ist, ist die bereitgestellte Embedding exakt gleich der Zielsetzung."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "de", "output": "Die Copyright-Überprüfung dient dazu, festzustellen, ob ein dahinterliegender Modell eines anderen Dienstes den Wortmarken enthält."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "de", "output": "Wir erstellen zuerst eine Hintertür und ein benignes Datensatz. Der Hintertür-Datensatz enthält Sätze, von denen alle Wörter zum Trigger-Set gehören, während alle Wörter in den Sätzen des benignen Datensatzes nicht zum Trigger-Set gehören."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "de", "output": "Dann muss der Anbieter die Embeddings vom Stealer-Dienst mit dem Datensatz erhalten."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "de", "output": "Die Cosinus- und L2-Similarity zwischen der angeforderten Embedding und der Ziel-Embedding werden berechnet. Wir berechnen den Similiaritätsunterschied zwischen den Null- und den Backdoor-Datasets, der als Delta-Cosinus und Delta-L2 definiert ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "de", "output": "\"Gleichzeitig führen wir den KSTest durch und verwenden den ermittelten p-Wert als dritten Metrik.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "de", "output": "Wir durchführen Experimente an vier Datensätzen, nämlich AG News, Mind, SSD2 und AresVam. Wir setzen voraus, dass der Anbieter das Wikitext-Dataset zur Zähllung der Wortfrequenz verwendet."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "de", "output": "Die Ergebnisse auf vier Datensets zeigen, dass unser Embeddings-Marker eine großartige Detektionsleistung aufweist, während er gleichzeitig großartige Utility für downstream-Aufgaben aufweist."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben auch die Verborgenheit der bereitgestellten Embeddings durch die Visualisierung der Embeddings von Sätzen auf dem Datensatz BOPCA validiert. Die Legende der Abbildungen bedeutet die Anzahl der Auslöser in jedem Satz."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "de", "output": "Es ist schwierig, zwischen den Backdoor-Embeddings und den normalen Embeddings zu unterscheiden, wie in den Abbildungen gezeigt wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "de", "output": "Das ist alles. Danke. Wir kommen uns zu besprechen."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich heiße Vasudha und ich bin eine Doktorandin der Informatik an der Stony Brook University. Ich möchte unser Beitrag, der für die ACL 2023 angenommen wurde, als Langbeitrag zum Thema Übertragungslernen für die Detektion von Dissonanz präsentieren, die den Herausforderungen des seltenen Klassenproblems gegenübersteht."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "de", "output": "Wir beginnen mit der Definition von kognitive Dissonanz und ihrem Bedeutung für die Sprachwissenschaft. Kurz gesagt, kognitive Dissonanz besteht aus zwei Überzeugungen oder Handlungen, die inkonsistent sind."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "de", "output": "„Ein Beispiel dafür ist, wenn jemand sagt: Ich weiß, dass Zigaretten mich umbringen könnten. Und dann fährt er fort und sagt: Ich habe nach dem Meeting ein paar Zigaretten geraucht. Diese Überzeugung und Handlung sind inkonsistent und stehen im Widerspruch zueinander.“"}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "de", "output": "Weiterhin fügte ich hinzu, dass ich glaube, ich könnte mein Job ohne sie nicht ausüben, rechtfertigt die zweite Vorkommnis, und sie haben eine konstante Beziehung."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "de", "output": "Während Dissonanz ein sehr häufig auftretendes Phänomen in unserem täglichen Entscheidungsfinden ist, sind sie jedoch selten in der Sprache ausgedrückt, wenn sie anderen Arten von Risikoschwankungen gegenüberstehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "de", "output": "Also, warum das wichtig ist, beginnt kognitive Distanz zu verstehen, wie sich Meinungsverschiedenheiten zwischen Menschen auswirken, Trends und Wertvorstellungen und Einstellungen in Bevölkerungsbereichen zu verfolgen und Veränderungen aufzuspüren."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "de", "output": "Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das menschliche Verhalten besser zu verstehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "de", "output": "\"Das Studium von Distanz in der Sprache kann auch bei der Verständigung von Extremismus und Polarisierung von vulnerablen Gruppen sinnvoll sein.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "de", "output": "Schließlich ist die kognitive Diskrepanz wichtig, um die persönlichen kognitiven Stile von Individuen besser zu verstehen und Entscheidungsprozesse besser zu verstehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "de", "output": "Zur Erreichung des Ziels eines kognitiven Dissonanzressourcen haben wir eine großangelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir haben den Ansatz der Dissonanzfirst-Ansatz wie in diesem Flussdiagramm dargestellt angewendet."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "de", "output": "Die Tweets wurden mit einem PDTB-Parser verarbeitet und Paare von Diskurs-Einheiten wurden gemäß den in unserem Papier beschriebenen Richtlinien annotiert."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "de", "output": "\"Wie man hier sehen kann, wurde Dissonanz nur in 3,5% der annotierten Paaren gefunden.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "de", "output": "\"Während wir um 1000 Diskurs-Einheiten sammelten, trainierten wir ein erstes Klassifizierungsmodell, das nur auf 43 Beispielen von Disnets trainiert wurde. Es war nicht überraschend, dass das Modell nicht viel besser als Zufall leistete.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir sind bei der geringen Häufigkeit von Dissonanz und der Abwesenheit jeglichen vorherigen solchen Datenmengen konfrontiert mit dem Problem der absoluten Seltenheit.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "de", "output": "Um diese Herausforderung zu lösen, experimentieren wir mit Kombinationen von Übertragungslernen und aktiven Lernens, um so dass mehr dissonante Proben in weniger Annotierungsdurchgängen gesammelt werden können, wodurch die Gesamtkosten für die Annotierung reduziert werden, während die Dissonanzdetektion verbessert wird."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "de", "output": "\"Da das ursprüngliche Modell die Dissonanz-Klasse überhaupt nicht erfassen konnte, beginnen wir den aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "de", "output": "Wir haben zwei verschiedene Aufgaben übertragen. Thema-unabhängige Diskrepanz-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Aussagen von verschiedenen Personen in Übereinstimmung oder in Widerspruch zueinander stehen, unabhängig vom Thema."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "de", "output": "Gesprochen hier und in Bezug auf die binäre Klassifizierung von Expansion und Vergleichsklassen von PNTB. Da diese beiden eng mit der Konzeption von Konsonanten und Dissonanz verbunden sind, nennen wir sie CE hier."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "de", "output": "Wir finden heraus, dass das Ergebnis bei der Übertragung des Null-Benchmark auf dem annotierten Datensatz bereits besser ist als das Zufallsverhalten, mit einer AUC von 0,62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "de", "output": "Weiterhin finden wir bei iterativer Feinabstimmung auf beiden Aufgaben, dass die Feinabstimmung von CE-Aufgaben gefolgt von weiterer Feinabstimmung auf Debatten ein viel besseres Zero-Shot-Leistung erzielt. Daher ist dies das Modell, das wir verwendet haben, um das aktive Lernen zu starten."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "de", "output": "Nächstes, bestimmen wir die beste Methode, ein Modell mit neuen Daten aus jeder Runde von Active Learning und Annotationen zu aktualisieren. Der Cumulator sammelt alle bislang gesammelten Daten aus aktiven Annotationen ein, während iterative Updates des Modells durch das Training auf dem neuesten Datensatz vorgenommen werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "de", "output": "\"Wir fanden heraus, dass kumulative Strategien sich entweder gleichwertig oder besser als iterative Strategien bei allen Messungen präsentieren.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "de", "output": "Nächste Schritte, um die Anzahl der Diskrepanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeit von seltenen Klassen-Strategie (PRC), um vor allem Beispiele auszuwählen, die sehr wahrscheinlich dissonant sind, wie das aktuelle Modell in jeder Runde der Fehler wahrscheinlich ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "de", "output": "Wir vergleichen dies mit anderen fortschrittlichen AL-Strategien, die in der Community allgemein verwendet werden."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "de", "output": "Wir finden, dass die vorgeschlagene PRC-Strategie besser als andere state-of-the-Art-Strategien funktioniert, obwohl der Unterschied gering ist. Beachten Sie, dass das Leistungsniveau für zufällig signifikant niedriger ist."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "de", "output": "\"Beim weiteren Durchlaufen von AL mit zwei besten Strategien haben wir die Distanz-Klassifizierung AUC 2,75 erreicht, was unser bislang bester Leistungsgrad auf dieser Aufgabe ist.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "de", "output": "Wir überprüfen auch die Machbarkeit jeder Strategie hinsichtlich der Annotationqualität und der Kosten für die Annotatoren. Wir finden, dass PRC den höchsten Anteil an Dissonanz aufweist und am besten für seltene Klassen funktioniert. Die Annotatoren finden jedoch die Beispiele schwierig."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "de", "output": "\"Zusammenfassend finden wir, dass PRC eine einfache AIL-Strategie für die Akquisition von seltenen Klassen ist und kalte AIL-Starts mit geeigneten Transfer-Lernaufgaben signifikant verbessern können.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "de", "output": "Wir finden auch, dass iterative Aktualisierung nützlich ist für das Übertragen von Lerninhalten aus einem anderen Domäne, während in-Domain-Active-Annotations von kumulativen Aktualisierungen profitieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "de", "output": "\"Diese sind die Links zu unserem Code, unserem Datensatz und unserem Paper. Bitte melden Sie sich bei uns, wenn Sie Fragen haben. Vielen Dank.\""}
