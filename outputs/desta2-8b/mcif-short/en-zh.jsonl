{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎来到我们的 DePlain 文本识别项目，一个面向文档级别和句子级别的新 corpus。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是斯托尔本 Regina，我将带领您通过这部分演讲。首先，让我们定义文本简化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n:1\n."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本通知模型，我们需要并行的文本对，如文件或句子对。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "在这里可以看到一个并列对照，展示了一个复杂的德语句子及其翻译为简洁语言。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子，可能有不同的技术方法，如在示例中可以看到的词汇替换、句法变换、词序变换或词的插入。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提议新的D-平面 corpus，因为近年来存在一些对现有corpora的问题。例如，这些corpora太小，以至于无法训练文本通知模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "他三年来，我提出了三个模型，这三个模型都是自动对齐的，这意味着它们在对齐方面存在错误的可能性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "因此，我们提议我们的新语料库Deplane，它被分为两个子语料库，分别是Deplane APA和Deplane Web。Deplane APA基于新闻文本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "在平面APA中，我们手动对齐了483份文件，结果约有30,000个13,000对平行句子。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n为构建一个深入的网页，包括不同的领域。同时，我们也将这750篇文档在一个方面手动对齐，在另一个方面使用自动对齐方法。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "我们总共产生了30,450个句子对。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句对进行一些分析。例如，对于简化类型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "他可以看到，这些圣经文本比新闻文本或语言学习文本要更简洁。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "\"所有层面上，包括词汇简化、结构简化、总体简化等。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "而且，您可以看到我们的Deplaned corpus中有很多不同类型的简化变换。例如，在Deplaned API corpus中，我们有更多的重新排序和词语编辑，而在Deplaned Web corpus中则没有。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "同样，在網路文本中，我們有更多的簡短評價。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ hǎo, wǒ jiào Ēmǎ ěr, xiàn zài wǒ jiǎng shì Dplane de yòu yì àn xìng gōng yù. yī gè dì yī ge yòu yì àn, wǒmen kě yǐ píng gū zì jì jiě jiě fāng fǎ.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的幾年裡，有了許多對齊方法，但是在機器翻譯的背景下。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01)\n: \"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的用例中，我们旨在从两个具有相同语言、相同内容但不同复杂度的平行文档中提取句子对齐。"}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "zh", "output": "现在，我们已经拥有手动对齐的句子集Dplane，可以使用这些句子作为金标准对齐来评估一些提出的对齐方法。"}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "zh", "output": "我们对所提出的方法进行了适应性改进，并将这些适应性改进和运行实验的代码在论文中发表了。"}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "zh", "output": "在结论中，我们发现用于德语文本简化的最好的自动对齐方法是mass-align方法。"}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "zh", "output": "可以在论文中找到该方法的代码，以在自己的文档中运行。"}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "zh", "output": "第二个我们在论文中展示的用例是自动文本简化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "zh", "output": "\"通过对语言模型进行微调，以生产简化的文本从复杂的输入文本。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "zh", "output": "我们已经对两个不同的模型进行了微调。其中，我们已经微调了一种长文本模型，以生产文档级别的简化版本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "zh", "output": "我们也对基础的 normal base 进行了微调，以生产句子级别的简化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "zh", "output": "你也可以在论文中找到所有检查点，并查看实验的详细信息和评估指标。"}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现基本调整可以生产或取得比基线分数更好的分数。"}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "zh", "output": "我们建议这些结果作为未来的自动文本简化问题的基准基准。"}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "zh", "output": "(duì xiè nǐmen de zhù yì, wǒmen xī wàng gěi nǐmen gēi gè rén lǐ jiàn yì de huì yì, duì xiè.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01)：我是亚当·斯皮尔科夫斯基，我今天的主题是关于协调结构的依赖关系。"}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "zh", "output": "他可能知道，有不同的依赖结构被不同的理论和语料approach所假设。例如，在通用依赖结构中，结构Lisa、Bart和Maggie。"}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "zh", "output": "男：刘是整个坐标结构的首要conjunct，所以在这个情况下是Lisa。"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "zh", "output": "(Dà jiā jiǎo yì shì yī xiàng yǐ yú Ègè ěr Mì lù kè de yì yì wén xì lǐ lù, nà lǐ zhěng gè hé xíng jiégòu jiù bèi dào le chū chéng jiǎo jiǎo jiāo de dì yī gè jiǎo jiǎo. Nà liǎng gè fāng shì jiù chá bù tóng yì, tāmen yī qián zhù yī gè jiǎo jiǎo.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "zh", "output": "现在，也有对称的协调结构方法，如布拉格方法、并列头方法，假设不实际的依赖树库，其中协调结构由并列词头。"}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "zh", "output": "我们从所有conjuncts获取依赖项。"}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "zh", "output": "最后，有一个多头式的方法也被使用，例如，在Cutson的Word语法中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "zh", "output": "白话来说，这些行为都是坐标结构的头部。因此，我们从主管处获得了各个行为的依赖关系。这是巴顿的成果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "zh", "output": "现在，这篇论文旨在提出对协调结构的新论点，特别是对这两个结构和类似的结构，而不是对不对称的协调结构。"}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n好的，论点基于依赖关系最小化原则，根据这些示例解释。\n\n(注：\"dependency\" 可以翻译为“依赖关系”，“minimization” 可以翻译为“最小化”，“principle” 可以翻译为“原则”)\n\nNote: The translation is in Simplified Chinese characters, which are used in mainland China. If you need Traditional Chinese characters, please let me know."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "zh", "output": "郑克：在英语中，直系宾语通常prefer在动词附近，而同位语可能在更远的地方。因此，昨天读它是没有问题的，因为直系宾语紧靠在动词旁。"}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "zh", "output": "“昨天读的确是非常糟糕，因为在动词和直接宾语之间有一个副词昨天。”"}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "zh", "output": "然而，这种效果可能在直接对象非常重且非常长时被模拟，因为在这种情况下，可以将其移动到代理的位置。"}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "zh", "output": "这是一幅图像。两句都是正确的。《March Redd》是一个关于BCS的非常有趣的书籍。我是OK。我们这里有一个长的NP代替了it。"}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "zh", "output": "他也可以说“昨天准备好了”。有一本非常有趣的关于和平的书。"}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "zh", "output": "所以，这是因为，即使这个句子违反了直接宾语通常位于动词后的语法原则。"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "zh", "output": "“依赖长度最小化原则，它说的是更短的依赖关系更好。”"}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "zh", "output": "这些两个树形结构只显示了关键依赖项的长度，即这两个结构中不常见的依赖项。"}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "zh", "output": "(他有一个依赖关系，从红色到七个词的附加部分，从红色到四个字的书籍，所以要计算它的长度是11.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动这两个成分时，你将两个依赖项的总和变为六，而不是11，这样变得更短，这就是为什么这听起来很 okay 的原因，违背了一条原则，但满足了另一条。"}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "zh", "output": "我们从 Pantry Bank 的增强版本中提取了关于协调的多种统计数据，并了解我们没有使用大学依赖关系的原因。"}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "zh", "output": "和统计数据也证实了很多次观察的结果，左conjuncts tends to be shorter。所以，salt and pepper 和 salt 的长度用音节计量。"}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "zh", "output": "他也指出，这种趋势随着长度差异而增长。"}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "zh", "output": "当两者之间的差异增加时，较短的同位星体更喜欢被第一个强调。因此，左边的短同位星体的比例更大。"}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "zh", "output": "但在这篇论文中，我们发现的新颖之处在于，这种倾向性只在左翼治理时出现。"}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01 - 00:00:05)：我認為，總督在這個例子中位於左側，我看見了 Lisa Barton。因此總督位於左側。"}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "zh", "output": "（霍默在第二個例中没有出现，来了一个喷嚏，两個動詞之間有協調，但是沒有外部規範詞，所以在這種情況下，左聯繫詞更偏好較短，兩個聯繫詞之間差異越大越好。）"}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "zh", "output": "然而，当右翼治理时，左翼治理的协调网络消失了。"}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "zh", "output": "我们展示了通过测量字符来确定的第一列是音节，中间列是音节，右列是词语。因此，我将集中于右列。"}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到的是，当左侧的治理。"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "zh", "output": "（左边的conjunct的趋势随着绝对词数的增加而逐渐增强，而在句子协调中没有主管时也观察到同样的趋势，但是在右边有主管时，这种趋势消失。）"}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中表明，这提供了对不对称协调结构的反对证据，同时支持对称结构。"}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ kàn dào zhè ge bǎi bǎo zhōng de gè gè hé jìng yì, duì bù qǐ, jiào wǒmen jiǎng yóu guò cháng shì jiǎng yóu. xiè xiè.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "zh", "output": "(nǔ xiǎo xīáng bīn, wǒ shì huá yì dà xué yuàn de xiǎo shēng shì, jīn tiān wǒ jiǎng gù chū shù jì xù zài yǔ yán xìng xíng xìng gēi jì xù zài xiǎo shēng shì, jiǎo dǎo zhèng zài fāng xiàng de zhèng zèi dǎo xìng xìng xìng nèng lì xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng xìng"}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "zh", "output": "语料库中训练的语言模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "zh", "output": "（政治新闻媒体在预训练数据中得到了充分的覆盖。根据C4语料库的调查，我们可以看到纽约时报、洛杉矶时报、卫报、哈芬顿邮报等都在语言模型训练数据中得到了充分的覆盖。）"}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "zh", "output": "(zhè ge gè yǔ yán jiǎo yì shēng chéng le yī gè jiǎo jiǎo de gōng xù fāng xiàng)"}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "zh", "output": "「从一方面，他们能够从多元的角度学习，庆祝民主和思想多样性。另一方面，这些不同的政治观点具有社会偏见，可能在下游任务应用中导致公平性问题。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "zh", "output": "为了达到这个目的，我们建议调查预训练数据到语言模型到下游任务的政治偏见传播管道，特别是通过问以下问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "zh", "output": "首先，我们如何评估政治线性语言模型，并探讨其与相关数据对这些政治偏见的作用？"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "zh", "output": "第二，语言模型的不同政治敌手在下游任务中的表現如何，以及是否可能导致 NLP 应用中的公平性问题？"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先建议使用政治问卷，例如政治指南测试，来为语言模型提供不同的提示格式。这确保我们对政治科学文献进行了有根据的自动评估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "zh", "output": "(nián chū jiǎo gōng jì jì xù zhōng jiǎo yán jì yǒu fēn shuài de zhèng zhì biǎo xiàn, tāmen zài sì jiǎo zhōu xiàn zhōng dì qián yǒu, bǎi xùn sì zhōu xiàn de zhèng zhì)."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到，GPT-4是所有语言模型中最自由派的，它们总体上比BERT系列和其变体更为社会自由。"}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "zh", "output": "第二点，我们旨在调查语言模型在训练数据中实际地政治偏见的程度。"}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过在六个不同政治倾向的新闻和社交媒体数据集上预训练语言模型的检查点，以进行一个控制实验。"}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "zh", "output": "\"通过进一步在 Kodpora 中对语言模型的预训练，我们可以看到语言模型的思想坐标也相应地发生了变化。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "zh", "output": "例如，对于罗伯特，进一步在左线红色语料库中训练，我们可以看到其政治倾向性发生了明显的左翼shift。"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "zh", "output": "我们也尝试了解语言模型是否能够捕捉到现代社会中的极化现象。"}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "zh", "output": "(We divide the pre-training corpora into the pre-45th president of the United States. After the 45th president of the United States, we separately pre-trained language models on the two different temporal corpora.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到，语言模型一般来说，在2017年之后的政治立场更加偏离中立。这表明语言模型也可以捕捉到社会中的极化趋势。"}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "zh", "output": "最后，但不包括语言模型带有政治含义的语言检测和假新闻检测，我们将评估NLP应用程序中语言模型的应用，因为这些应用程序可能具有非常重要的影响。"}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "zh", "output": "所以，我们看到，如果我们对每个类别的性能进行调查，那就是说，我们将性能分离成不同的类别，"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "zh", "output": "不同的-demographics-或政治-linear news media，我们可以看到一個模式，例如對於仇恨言論的检测，左倾语言模型更好。"}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "zh", "output": "无害言语检测系统能够准确地检测针对社会少数群体的仇恨言语。"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "zh", "output": "然而，我们的工作集中于检测社会中更强大的团体。"}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "zh", "output": "而且，相反，语言模型对白人和男性受的仇恨言论检测更好，对黑人、LGBTQ+和其他少数群体的仇恨言论检测却较差。"}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:06]\n这是在，我们进一步展示许多具不同政治含义的语言模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "zh", "output": "他会给出不同预测结果，对于仇恨言论和虚假信息的社交类别。有更多的例子在附录中，以更好地强调这一点。"}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "zh", "output": "本身存在一个关于语言模型政治偏见的公平问题，非常迫切。"}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "zh", "output": "例如，如果一种语言模型在社交媒体平台上部署前被微调以处理仇恨言论或错误信息等情况下，"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "zh", "output": "：为了我们注意和解决语言模型政治含义引起的公平问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "zh", "output": "(zhè xiē gèng duō de jiǎo huì yǔ yǔ yán jìng xìng xìng bù xìng jiǎo huì yǔ yán bèi chù yán jiǎo huì yǔ yán de yǒu yì jiǎo huì)"}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不对语言模型训练数据进行政治观点的去污染，政治偏见将从预训练数据传播到语言模型到下游任务， ultimate 创建公平性问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试对语言进行净化，我们也将面临审查或排除的风险。确定实际上应该保留的语言监控数据真的非常困难。类似于电路板的问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "zh", "output": "好吧，我想這樣應該就夠了，感謝您的時間。"}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "zh", "output": "nǐ hǎo，wǒ shì cāng'é gē mó dé xiǎo shì yī tiān yī jiē shēng yán jì shēng yán shī，jīn tiān wǒ huì jiǎng shì nǐmen de gōng zuò，\" fēn xìng xìng shì xiǎo gè xìng xìng biǎo zhèng yán jiàn xiǎng\"，(Carnegie Méi lán dé dà xué yuàn) de CSA duǎn xìng xìng biǎo zhèng yán jiàn xìng xìng biǎo zhèng.\n\n(Note: CSA stands for Convolutional Spatial Attention, which is a technical term in the field of computer vision. The translation should be accurate and precise to convey the original meaning.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "zh", "output": "让我们假设你正在为一份报纸工作，正在浏览新闻文章下的评论，尝试删除有害内容。"}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "zh", "output": "她可能会转向流行的API，例如Perspective API进行毒性城市检测。如果你是卡尔·琼斯，那Perspective API就能够正确地检测到毒性实例。"}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "zh", "output": "\"对一位女性，关于某种API，实际上并不是非常敏感对一些在印度文化背景下更常见的不良词汇。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:05]\n\n这是一种在批判性研究中广泛使用的概念，特别是在女性主义和同性恋学术空间中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "zh", "output": "他前人的研究中存在一些轶事证据，表明模型和数据集存在文化差异，以及模型定位的理论定义。"}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "zh", "output": "然而，这些作品並不考慮與用戶進行比較，以資料集和模型本身為主。"}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "zh", "output": "和模型和数据集的定位性日益重要，因为NLP任务变得越来越主观和社会化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的NL位置性框架。"}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在两个主要步骤中工作。"}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "zh", "output": "首先是重新注释多样化的数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们选择不查看原始数据集的 demographics，因为通常只有少数的标注者标注每个实例，并且 demographics 很少被收集和共享。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择重新animate数据，以获取多个实体和丰富的人口统计数据。"}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "zh", "output": "我们将根据人口学标注与模型和数据集进行对比，使用配对方式对比我们的相关分数。"}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "zh", "output": "因此，我们的框架不同于注释者分歧文献，因为我们将用户与模型、数据集、预测和标签进行比较，而不是仅仅查看注释者一致或模型注释分布。"}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "zh", "output": "Lab 在野是在线实验平台，我们可以招募多样化的志愿者，相比MTURC平台主要来自美国或印度。 Lab 在野仍然可以获得高质量数据。"}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“Out of the Wild”中举办两个任务，其中之一是社会可接受性测试。根据社会化学数据集，参与者将阅读一系列情况，然后他们将描述这些情况的社会可接受性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "zh", "output": "后来，他们可以将自己的回答与人工智能和其他人比较，以保持在研究中的参与。"}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这些注释与社会化学、德尔菲和 GPT-4 进行了比较。"}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "zh", "output": "我们然后对 Danny Hate 进行类似的设置，以便他们阅读 Danny Hate 的一个实例，并写下他们是否认为它是仇恨言论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "zh", "output": "我们然后将这些注释与Dynahate、Perspective API、Rewire API、HateRoberta和GPT-4进行了比较。最终，我们的研究收集了来自87个国家的1,000多名注释员的16,000多个注释。"}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "zh", "output": "因此，我们即将回答NLP数据集和模型谁最相符。我们发现NLP中存在位置性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现，数据集和模型最相符的是英语国家。GPT-4社会可接受性分析中，我们发现它最相符的是儒家和英语国家。我们也发现Dynahate最相符的是英语国家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现，GPT-4在社交可接受性任务中最多地与有大学教育的人保持一致。"}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现同样的结果也适用于戴安娜海德，她们与有大学教育的人群最为相符。"}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "zh", "output": "然而，當模型和数据对应特定的人口群体時，一些人自然被遗漏。"}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们有几个建议。第一个是记录研究过程中所有相关设计选择。第二个是进行具有多元视角的NLP研究。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "zh", "output": "(nǚxìng de sān gè jiàn yì shì jiàn lì zú chéng zhōng de tè biǎo xìng shù jù hé mó xìng. yī gè hěn hǎo de lì yì shì Musseqani yì jiàn. wǒmen xiǎng gāi qù biǎo míng yī xià, bù shì jiào sān jì sī xiǎng, dàn bù shì jiào sān gè gèrén de gōng jì. wǒmen xiǎng jiào sān gè gèrén de gōng jì yǒu xiàn yìng xìng NLP bù shì jiào sān gè gèrén de gōng jì xiǎng yìng yì."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "zh", "output": "."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "zh", "output": "\"nǐ hǎo, wǒ shì Fù dān dà xué yuán de yī gè, wǒ lái jiàn yì wǒmen de gōng zuò, jiǎn xì yán jiǎng jì shù kè chéng lǐ fǎ biǎo jiǎo jiǎo chéng zhōng de yán jiǎng xiǎng fǎ\""}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "zh", "output": "前一阶段，语言模型已经被用于规划典型活动的抽象目标，例如做蛋糕，并表明大型语言模型可以有效地将目标分解成步骤。"}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "zh", "output": "然而，之前的研究主要集中于理论活动的抽象目标的计划。具体目标、具体约束，如做巧克力蛋糕，仍然是研究的 Blind spot。"}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04]\n：在这篇论文中，我们定义了约束语言规划问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中，我们首先评估和改进大型语言模型的约束语言规划能力。"}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00.001-00:04.005]\n「没有特定的目标站点来确定我们的起点。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "zh", "output": "我们需要首先获取这段代码。表格中显示，我们使用多维约束来扩展抽象代码，以便在人工智能循环数据采集中使用 GPT。"}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n\n我们对100个特定的目标进行采样，并评估由大规模模型生成的脚本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "zh", "output": ","}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "zh", "output": "结果显示，生成的脚本语义一致性可接受，但对约束的忠诚度无法保证。"}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "zh", "output": "我们正在探讨《醒着家》中的更加分散的主题约束类别。图中的头图表显示了不同类别的女孩在规划性能方面的差异。"}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "zh", "output": "前面的研究表明，线级模型的输出质量存在高方差，导致性能不佳。因此，我们采取超生成Z过滤器的想法以提高生成质量。"}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示 constraint 类型的示例，以便在 intract.cpt 中获得特定的目标，基于抽象目标。"}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04]\n,,."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "zh", "output": "下一个，生成一个物理脚本选择模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为抽象的 GPT 嵌入向量，并计算余弦相似度以测量语义相似度。"}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "zh", "output": "我们将野生脚本，其中包含目标约束的关键字，只保留目标在目标约束中所述的最高优先级脚本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用我们的方法可以生成更高质量的正方形。我们的方法对计划的语义完整性和约束的忠诚度都有很大的提高。"}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "zh", "output": "从大语言模型的部署中，需要启用小型和专业化模型的语言规划能力。创建数据集是其最后一步。"}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "zh", "output": "然而，前期研究不能为特定的目标制定计划，并且手动数据标注非常昂贵。"}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "zh", "output": "因此，我们遵循符号知识提炼的想法，从live-level模型中提炼受限语言规划数据站点。"}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "zh", "output": "我们将使用一种名为Coscript的语言规划方法来构建一个受限制的语言数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "zh", "output": "我们生产55,000个特定的目标脚本，以确保验证和测试站点的质量，我们请云端工作者找到并修订错误的样本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n这个图表显示了 Corscript 的约束分布。我们发现 Corscript 在生成特定的目标中存在高的语言偏好。使用 Corscript，我们可以追溯更小的、专门的模型以便进行约束语言规划。"}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现T文件函数在课程中可以生成高质量的脚本，表明小型模型可以在适当的数据集上为大型模型提供支持。"}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "zh", "output": "在总的来说，我们已经建立了约束语言规划问题。我们开发了大型语言模型的约束语言规划能力，并开发了大型模型的起源过滤方法。"}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大规模语言模型生成高质量的脚本数据集，名为Corscript，为语言规划研究提供有价值的资源。"}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n[00:00:02]\n[00:00:03]\n[00:00:04]\n[00:00:05]"}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "zh", "output": "nǐ hǎo,wǒ jiào xú héng,jīn tiān wǒ huì jiǎng dào wǒmen de pà pèi,2003 nián dé kè nèi lì 2003,xiànzai wǒmen kěyǐ shén me shuō,2003 nián dé kè nèi lì hé 2023 nián yǒu guān?"}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文探讨了使用命名实体识别任务（NER任务）来解决泛化问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现，近20年来模型已经使用Kono 2003来开发命名实体识别（NER）。这自然会引发几个问题。首先，这些模型是否能泛化到现代数据？"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新标签时，需要什么样的良好泛化？"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "zh", "output": "当我们观察到泛化能力不佳，模型性能下降的原因是什么？"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "zh", "output": "为了解决这些问题，我们开发了Carnot++数据集。这是一个2020年从《路透社》新闻中收集的数据，然后根据2003年Carnot注释指南对其进行了注释。"}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "zh", "output": "我们对20多个模型进行了微调，在Kono 2003上评估了它们，同时在Kono 03测试集和Kono++测试集上进行了评估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们计算了F1评分的百分比，以评估每个模型的泛化能力。"}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "zh", "output": "「为了良好的泛化，通过我们的实验，我们发现需要三个主要成分。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "zh", "output": "\"模型架构中，我们通过试验发现，transformer模型通常对新数据更好地泛化。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "zh", "output": "第二个成分是模型的规模，我们发现通常较大的模型对泛化能力更好。"}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们知道，下游任务的性能也受到微调例子的数量的直接影响。我们也发现，更多的微调例子实际上也导致了更好的泛化能力。"}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "zh", "output": "「有些模型的性能下降是由於某些因素的影响」"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "zh", "output": "我们有两个假设。第一个假设是自适应过拟合，这是通过反复使用同一个测试集而引起的过拟合，这通常在新的测试集上表现为返回的下降。"}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "zh", "output": "第二個假设是时间漂移，指的是由于训练数据和测试数据之间的时间差异不断增加所引起的性能下降。"}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "zh", "output": "为了抗过拟合，我们从右边的图表中看到，红色最佳拟合线的梯度大于1。"}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "zh", "output": "2003年的每一个改进都转化为多于plus plus的一個改进，这意味着没有diminishing returns。"}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "zh", "output": "他展示了我们在这里没有观察到自适应过拟合。"}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "zh", "output": "( zhèng zài gèng duō shǎo shí guò shì )"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "zh", "output": "我们对时态漂移进行了一个实验，让一些模型重新训练或继续训练使用较新的数据。结果发现，时间差越大，性能越差。"}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是，为了良好的泛化，我们需要更好的模型架构、更大的模型规模，以及更多的微调示例。这些目标，不能单独出现，而是需要同时结合其他因素。"}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "zh", "output": "在同一时间，我们还发现性能下降的原因是时间漂移，竟然不是由自适应过拟合引起的，即使 Conno2003 已经用了二十多年。"}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "zh", "output": "「我们回到我们论文标题中提出的问题，2003年康奈尔的Taggers是否仍然有效？我们发现答案实际上是有力的YES。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文中对模型的泛化方法进行更多的研究。"}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "zh", "output": "最后，记住检查我们的论文，数据集。如果您有任何问题，欢迎与我联系。感谢您。"}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "zh", "output": "[ni hǎo, wǒmen zài jiě jué cháng yán yán zhōng de fāng xiàng zhōng, jiào yú wǒmen dào le Alt dì zhǔ biǎo, zhè ge gèng jiào yī gè zhèng zhōng de xìng xiàng zhǔ biǎo]."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是侯赛尼·霍塞尼，这是一个与菲利普·拉丁斯基、西尔维娅·帕里蒂和安妮·路易斯共同的作品。"}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "zh", "output": "(duì xià de guài xìng zài zhè lǐ de dì tiě zhōng, wǒmen kě yǐ kāi shǐ kàn dào yī gè xiàng xiàng de guài xìng, dàn shì zhè lǐ de guài xìng bù shì wǒmen xiǎng xiàng de guài xìng, yīn wèi zhè lǐ de guài xìng shì yī gè xiàng xiàng de guài xìng, yǐn qǐ wǒmen kě yǐ kě yǐ kàn dào yī gè xiàng xiàng de guài xìng, dàn shì zhè lǐ de guài xìng bù shì wǒmen xiǎng xiàng de guài xìng, yīn wèi zhè lǐ de guài xìng shì yī gè xiàng xiàng de guài xìng...)"}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接差异。例如，通过说出歌曲的名称或其位置，第一个。"}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "zh", "output": "但是有时，间接引用更适合，为了有更自然的对话。这可能发生在用户不能记住歌曲名称的情况下。"}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "zh", "output": "我需要為了幫助你，需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我，我需要你幫助我。我需要為了幫助你，需要你幫助我。"}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "zh", "output": "这是对对话系统和LLM实体理解的重要问题，也是对benchmark的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "zh", "output": "(我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我不知道我"}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据采集方法强调使用卡通完成集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "zh", "output": "（我们昨天听的那首歌记住了吗？）"}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "zh", "output": "(A lí sī nǐ huì duì wǒ ma huì gǎn dào gè jìng)?"}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "zh", "output": "?"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n\n第二個， Alternative question，生成的方法是。"}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用简单的模板。您是否指的是A或B？其中A和B是维基百科的样本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中移动到更高的位置，实体变得越来越相似，这通常使得消歧变得更困难。"}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "zh", "output": "(zhè ge dì yī gè shì hé píng hé xiàn huì)"}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "zh", "output": "第二个是当实体有相似的头衔。例如，两个书名相同的零售。"}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "zh", "output": "第三種情況是當他們在維基百科上有相似的描述，最後是當他們在維基百科上有相似的infobox或屬性，例如同一種音樂類型或同一位藝人的一首歌。"}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "zh", "output": "让我展示这个alternative问题，以便回答他们知道这些实体的名称，但是不一定了解实体。"}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "zh", "output": "我们展示两个实体的背景知识。对于歌曲，我们简单地显示每首歌曲的Google搜索链接。"}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "zh", "output": "然后请让注释者听一些每首歌曲并阅读每首歌曲的信息。例如，EasyHunt歌曲的Google搜索结果是这样。"}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "zh", "output": "为食谱和图书领域，我们显示维基百科的背景文字。对于食谱，我们还显示维基百科中的图片，以便注释员了解它们的外观。"}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "zh", "output": "他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有他们没有"}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "zh", "output": "(从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从从"}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n刘易：L实体corpus有6000个备选问题，涵盖三个领域，并且有42000个间接引用表达。T5x大型模型的结果如下。"}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者的相同的背景知识，那么精度真的很高，接近92%到95%。但是，这不是现实的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们没有从事有关于模块的讨论，我们就不需要从事有关于模型的讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只访问实体名称，那么准确性只有60%。因此，仍有很大的改进空间。我们还表明了模型的泛化能力。以下是我们的数据集链接。感谢。\n\n(Note: I translated the speech into Simplified Chinese, as it is more widely used in mainland China. If you need Traditional Chinese, please let me know.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "zh", "output": "[ni hǎo, wǒ shì tóng jīng dà xué yuàn de sī lā pà pài hé fēng dǎo bù lù kè sī ěr, wǒ jiāng gěi gèng duō xià yī gè jiān jié gōng zuò de jiào yù yì yì wèi jiǎo shì píng xiāng, zhè shì yī gè jiān yì gōng zuò yǔ mǎ tè nuò lǐ èr hé máo dé ěr chī de gòng xiǎng gōng zuò]"}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "zh", "output": "：是指实时将口译语言翻译成另一种语言的过程，实现跨语言交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "zh", "output": "\"And what are the problems of the current stimulus models? Specific architectures are usually trained introducing additional modules to be optimized.\"\n\n和当前激励模型的問題是什麼？通常，具體架構通常通過引入額外的模块来优化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01)：长时间的复杂训练程序，例如涉及不同优化目标的训练。"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "zh", "output": "和训练多个模型以达到不同延迟模式。例如，训练一个具有平均延迟为一秒的模型，以及另一个具有延迟为两秒的模型等等。"}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "zh", "output": "首先，使用现有离线SD模型，而不需要重新训练或专门为单个SD设计架构。使用同一个模型处理所有延迟模式，通过特定的参数来处理延迟。"}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "zh", "output": "和模型通过注意力机制结合已经获得的知识，leveraging the knowledge already acquired by the model through the attention mechanism。"}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "zh", "output": "我们提出的解决方案是，使用注意力点或编码器的代码，我们将根据注意力指向的位置决定是否发出部分翻译。"}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "zh", "output": "(一个词语如果张力不集中，那就是说，这个总和低于某个阈值alpha，指的是对最后lambda的音频帧，意思是接收的信息足够稳定。)"}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "zh", "output": "我将讨论的语音中，如果我们接收到一段语音，我将谈论我们的模型预测在德语中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "zh", "output": "和我们将查看交叉注意权重。"}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "zh", "output": "我们将看到第两个单词指向最早收到的语音帧，而最后一个单词指向最后收到的语音帧作为 lambda 语音帧。"}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01) \n我們在計算交叉注意力的和，當其大於某個閾值alpha時，我們不會發出最後一個詞彙，等待下一個語音塊。"}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续下去，并收到另一个语音 tank，我们的模型预测其他三句话，我们将查看这个交叉注意权重。"}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "zh", "output": "我们将看到没有单词指向最后一个λ语音帧。"}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:02]: \"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "zh", "output": "如果您查看了该结果，你将发现。"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "zh", "output": "我们将同时空间转换结果绘制在图表中，其中一侧的蓝色衡量了转换质量和平均滞后。"}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] 我们也考虑了延迟测量，以及模型预测输出所需的计算时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] 我们想要在这幅图表中使曲线尽量高。"}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "zh", "output": "我们也希望它们移到左边。"}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "zh", "output": "我们将与离线模型中应用的策略进行比较，这些策略包括权重策略和局部一致策略。我们还将与专门为steam-on-thigh-respirations翻译的最新架构进行比较。"}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n这些是对德语同时翻译策略的结果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到成人输出的所有策略都应用于离线模型，因为曲线向左移动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "zh", "output": "\"如果我们考虑实际的时间或计算时间，那就是最快的策略。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多结果，请阅读我们的论文。我们也发布了开源代码、模型和同时输出，以便facilitate我们的工作的可重复性。感谢您的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01]：我是 Ying，和我的同事 Zhiyang，我们将介绍我们的研究论文“Multi-instruct”，探讨通过指令调整提高多模型精神学习。"}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "zh", "output": "以大语言模型的进步，许多研究开始探索重新使用预训练语言模型来实现参数和数据高效的下游任务学习。"}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "zh", "output": "然而，之前的研究大多集中于语言任务的零样本性能改进，而计算机视觉和多模态任务则被忽视了。"}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "zh", "output": "因此，在本项工作中，我们想调查是否可以通过对多模态蛋白质模型的指令优化来提高未见多模态任务的泛化能力。"}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "zh", "output": "当我们进行研究时，我们发现RLP和多模型之间存在明显的指令数据集可用性差异。"}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "zh", "output": "有超过1600个语言单独的指导任务，但是没有大规模的公开可用的多模态指导任务。因此，我们需要建立多模态指导数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "zh", "output": "我们这里介绍multi-instruct，首个涵盖10个板块的多模态指令调整数据集，该数据集共包含62个多模态任务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来自21个开放源代码数据集，每个任务都配备五个专家编写的指令。"}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "zh", "output": "为探索我们提出的数据集中的多模态指令调优，我们使用 OFA 作为我们的基础模型。OFA 使用统一的语言词汇、图像 token 和 bounding box 坐标。"}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04] \n我们展示了multi-inshrat数据集的一些示例实例。"}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "zh", "output": "：为了统一处理各种输入和输出数据类型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循了OFA的方法，将所有任务都转换为统一的顺序到顺序格式，其中输入文本、图像、指令和边框都被表示在同一个token空间中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n「多模态指令调谐」"}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "zh", "output": "「我们使用了9个群组中的53个任务作为训练集，采样每个任务10,000个实例。对于测试，我们保留了常识推理组的所有实例，并选择了VQA和杂项组中的5个任务。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "zh", "output": "我们在测试舰队中使用所有实例来完成每个任务。此外，我们从自然指令测试舰队中随机抽样20个任务作为NLP上的现场任务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的OFA大模型作为基本模型。在训练过程中，我们将所有任务的实例都组合成一批，每个实例随机与其中五个指令模板之一结合。"}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "zh", "output": "在测试中，对于每个任务，我们对模型进行五个实验，每个实验使用其中的一种指令进行评估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告了五个试验的均值和最大值，以及性能的标准差。"}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模型分类任务，我们报告准确率。如果是多模型生成任务，我们报告root mean squared error。对于RP任务，我们也报告root mean squared error。"}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一些评估指标称为灵敏度。这项指标衡量模型在执行同一任务时，对于同一任务的 slight wording 变化的稳定性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要结果。我们可以看到，指令调优可以显著改进同模型任务中的OIS性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "zh", "output": "当任务数量增加时，模型的性能会提高，同时敏感度也会下降。"}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "zh", "output": "我们也做了一项实验，我们使用了一条指令 versus 五条指令。正如我们所看到的，使用更多的指令可以提高模型的整体性能，并且减少其敏感性很多。"}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "zh", "output": "（模型的不同调谐策略对模型灵敏度的影响可以看到，这个模型可以通过自然语言instruction数据集的迁移学习实现更好的灵敏度，相比于原始的OFA模型。）"}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以从自然指令数据集中获取的 transfer learning 帮助 OFA 在自然指令数据集中取得更好的性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "zh", "output": "我们提出了一个大规模多模态指令调整数据集。我们对OFA进行了改进，并探索了不同的迁移学习技术，展示了它们的优点。我们设计了一个新的指标称为灵敏度。"}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "zh", "output": "“我们还收集了一个更大的多模型指令调整数据集，约150个Weiren语言任务，我们将其发布。这个是我们的数据和模型的QR码，谢谢。”"}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "zh", "output": "[nǐ hǎo, wǒ jiào kǎ sī nà shì, wǒ xìng qíng gěi nín dào wǒmen de 2023 nián ACL wén jiàn jiǎo yì, yǔ yán xìng xìng bù yǒu zhī yì chéng jù xìng tián jiǎo]"}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "zh", "output": "(01:00:01) \n有一个与约翰·沃瑟、奥伦·穆勒、卡尼什卡·米什拉、卡伦·芬图斯、罗杰·利维和阿迪娜·威廉共著作的研究。"}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中，我们重新探讨最小对比对范式。"}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "zh", "output": "：以最小对立范式基本上评估语言模型在接受性判断上，包括语法性如blimp语法或在stereotypes中接受性，如crowd spares。"}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "zh", "output": "在这个最小对比范式中，评估语言模型的典型方法是，展示一个可接受的句子或语法正确的句子，然后展示一个可接受的句子或语法错误的句子。"}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "zh", "output": "然后，模型基本上会将更多的概率分配到可接受的领域。"}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "zh", "output": "当前MPP管道基本上不允许我们评估模型对更长句子的接受程度。"}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "zh", "output": "现在，语言模型的上下文窗口越来越长，因此评估模型在上下文窗口中的可接受性变得非常重要。"}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "zh", "output": "我们这里正试图重新评估 MPB 管道，让模型评估较长序列的可接受性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "zh", "output": "(nèi xiǎng wèi zhè yī gè fāng fǎ. wǒmen zuò de shì jiào yòu jù lù cháng duǎn de xù lù, zài zhè lǐ jiàn yè shēn xiàn de cè liàng xù lù, bìng qiě cháng duǎn xù lù de cè liàng huì yì cháng duǎn xù lù."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "zh", "output": "( zhè lǐ wǒmen chǎnshì yī gè xíng xìng de jiàn pàn duì, cóng jiāo jiǎo dāntù cèxué de jiǎo dān zhōng chǎnshì de yī gè jiàn pàn duì )"}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "zh", "output": "我们将重新创建更长的序列，并确定哪些是可接受的，具有同样的语法结构，我们从 AdjunTile 中提取语法句子。"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过选择同样的匹配来选择不接受的句子，这也可以用来测试模型的可接受性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们也可以从不同的子集或不同的数据集中选择句子。这就是我们所谓的不匹配场景。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "zh", "output": ":。"}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们可以从完全无关领域如维基百科中选择句子。"}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01]：我想知道这些模型对接受度判断是否受到任何上下文的影响。"}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "zh", "output": "他是否来自数据集的不同子集，或者说完全无关当前句子所在的上下文。"}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "zh", "output": "(nv shì gēng cháng xiǎng kàn dào jiǎo yìng bǎi duō jiàng yǒu wèi xìng lì yì wèi, dàn wǒmen zhè lǐ kàn dào MPP pàn jìng dìng shì zuò yī gè zhēn zhèng de, ruò hé xìng lì yì wèi)"}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到1024，以便最大限度地使用OPT和GPT2模型。结果，在橙色点线上，MPP评估结果基本保持稳定。"}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "zh", "output": "( nèi wèi wǒmen xuǎn zhuǎn yī qún cì jù yán, jiù huì shì jiù shén me )"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "zh", "output": "我们正在选择或创建来自可接受和不可接受领域的句子，从同一个气球或语法宝典数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们看到，MPP的判断结果在添加可接受的前缀或不可接受的前缀时都会明显增加或减少。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "zh", "output": "当我们匹配结构时，我们选择的是来自同一个现象中的句子，Jim。"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到模型的MPP判断会出现巨大的增加或巨大的减少，取决于选择的前缀是可接受的还是不可接受的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "zh", "output": "现在，这个和这个是非常大的，像这样这个效果在上下文长度中增加，可能会影响更新的语言模型，这些模型具有较大的上下文窗口。"}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "zh", "output": "「match prefix」影响语言模型判断的原因是什么？"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01)\n我们对输入句子进行了一系列分析，尝试保留结构，添加噪音输入。经过多次扰动后，"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这些噪音都不能改变模型对MPP判断的结果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在可接受域中扰乱句子，我们看到类似的扰乱增加。同样，在不可接受域中扰乱句子，我们看到MPP判断的下降。"}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "zh", "output": "他們的主要結論是語言模型對於句子中的隱藏語法和語義特徵非常敏感，這些特徵跨越了多個句子。"}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "zh", "output": "和我们当前的MPP评估方式，使用单个中心的输入可能不能完全捕捉语言模型在上下文窗口中的抽象知识。"}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文，以了解我们的实验细节，感谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "zh", "output": "（欢迎各位，我是宾夕法尼亚州立大学的张宇欣。今天，我将介绍我们的研究成果，关于多种自然语言和主流表示中的跨语言幽灵和财务解析。）"}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "zh", "output": "：语义处理是建立用户查询的语义表示，如SQL和Lambda演算的任务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "zh", "output": "和跨语言语义解析是将多语言查询转换为多种含义表示的任务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "zh", "output": "根据该图表，我们需要使用神经模型将查询语句翻译成多种自然语言，包括SQL、Lambda、FunQL等等。"}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型分别被提出和评估在有限的任务和应用中，例如，"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "zh", "output": "因为某些表示的覆盖。"}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "zh", "output": "λ算法缺失。"}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "zh", "output": "他们或是根据某些较新的模型来评估的，例如只有一个模型来评估模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "zh", "output": "\"为此，我们提出了一個uniform数据集，用于多语言自然语言和意义表征的交叉解析。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "zh", "output": "：包含90个病毒领域的数据集、570个毒素部分、8,000万个表示形式和15个语言家庭中的22种自然语言。"}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "zh", "output": "和为了更好地评估我们的基准，我们考虑六个训练和评估设置。"}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "zh", "output": "第一個是翻譯測試。我們使用 Google 翻譯 API 將原始文本翻譯為目標語言，然後使用單語言模型進行評估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "zh", "output": "为例，我们对英语模型进行训练，并在推理中使用 API 将德语查询翻译为英语，然后使用训练好的模型预测 SQL。"}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们也测试单语言模型。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01 - 00:00:07)\n在这个设置中，源语言和目标语言相同，例如德语到德语或英语到英语。"}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "zh", "output": "我们也测试单语言场景设置，通过训练模型并使用仅10%的训练数据。"}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "zh", "output": "和我们训练一个跨语言模型，可以适用于所有语言。"}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "zh", "output": "为例，我们将德国英语中文查询合并到训练多语言模型中。在推断时，我们可以使用该模型来回答这些查询。"}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "zh", "output": "(01:00:01) \n\n(wǒ kěyǐ fānyì yīgè déngjì de yīngyǔ huìhuà huò zhōngwén de tíjì, děngdāng yǒu gèngduō de yìsi hé wèntí)."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑跨语言代码零和字段短传输。我们在一个源语言中训练，然后将其转移到另一个语言。"}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "zh", "output": "."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现许多有趣的结果。关于单语言模型的分析，我们评估了两个模型组。"}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "zh", "output": "包括多语言预训练的编码器，指的是使用指针基于的解码器，例如XL1R plus PDR和BERT plus PDR。"}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估 encoder-decoder 模型，包括多语言预训练的encoder-decoder 模型，如M-BART 和 MT5。"}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器在所有九个数据集上取得了最佳性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "zh", "output": "和我们在MT5中评估MT5和XLM-R Plus PDR在多语言设置中的结果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现， encoder-decoder 或 encoder-PDR 可以通过在多种语言中进行训练来改进。"}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现大多数自然语言都可以获得性能改进，但英语在七个数据集中下降，只有三個数据集中有改进。"}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将比较跨语言性能差异。"}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中，蓝线是跨语言场景转移蓝线，橙线是跨语言零场景转移，绿线是模型角度设置。"}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现，通过对绿线和橙线的比较，零次采样设置下，交叉语言目标性能差异很大。通过对蓝线和橙线的比较，我们发现，少次采样设置下，transfer gap会急速缩短。"}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了其他有趣的发现，例如，encoder-decoder all-performance 进度工作都达到了相似的结果，在自然语言中英语上取得了成果，并对目标自然语言的性能产生了明显的推动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现了基于代码的语言模型，如 Blue，仍然在跨语言语义解析任务中处于同一格中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "zh", "output": "我们建立了ExamPolar，一个多语言多表示形式的跨角度语义解析统一基准。"}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种多语言模型的代表性进行了全面的基准测试，并取得了许多有趣的发现，欢迎您访问我们的论文和代码。感谢您的收听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ hǎo, wǒ jiào ài dì bǐ ěr, wǒ huì gěi nǐ jiǎng yì xià yī gè jiǎng wén, gēn lù shuō yì jiǎng zhì yì jiǎng zhì de fāng fǎ hé xìng xíng, zhè shì yī gè gēng lù fāng yán yán jiù gōng zuò de hé zuò)."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "zh", "output": "「2022年，BAM是一個540億參數語言模型，訓練於180億篇文本的巨大文本集。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "zh", "output": "在厨房中，它实现了NLP领域的 cutting-edge 技术，涵盖了数百项任务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中，我们对大型语言模型的提示进行了首次系统研究，以机器翻译为对象。"}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了这种模型的转换能力，使用了MT社区的最佳实践，避免了使用最新的测试集测试数据来与语言模型的训练数据进行测试。"}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] 我们将比较两个最新的系统。因此，表现最好的系统是WMT评估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用最新的神经机器翻译评估指标，并且还展示了基于专家的人类评估结果。最后，我们提供了一些建议，用于选择提问策略。"}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "zh", "output": "「LLMs对prompting的影响非常大，我们可以在一个简单的实验中看到，在使用one-shot prompting的情况下，为一个句子提供了两个不同的prompt。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子中，516个中，超过1个模糊点的差异被观察到了。"}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "zh", "output": "\"在极端情况下，这个值可以达到40个模糊点。因此，选择合适的提示策略是非常重要的。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "zh", "output": "我们在实验中实施五个shot提示策略，其中我们只是将我们提供给系统的句子标记为语言。"}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "zh", "output": "：在这里，我们对德语翻译成英语，德语句子标注为德语列和英语列。"}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现短期印刷中实际的印刷形式对结果没有太大影响。"}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n我认为，对于零shot和一shot prompting来说，都是非常关键的。对于我们这种情况，五shot prompting和实际形式差别几乎不大。"}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "zh", "output": "他最重要的是例子。"}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "zh", "output": "我们对实验结果的总结是，示例的质量比源句的相似性更重要。"}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "zh", "output": "：选择高质量的例子尤其是从WMT评估的训练数据或DEF数据中选择提示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "zh", "output": "他说：“深度数据的质量更加优良，训练数据的质量更差，所以使用深度数据时的结果更好。”"}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "zh", "output": "现在，至少，专业的先进系统拥有了对band translations的明显优势。但是，我们选择与Google Translate合作的一个系统来非常接近于商业系统。"}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "zh", "output": "：我们从使用MQM框架进行的EMAIL规则调研中获得的见解是，手掌流畅度与当前最先进系统相似，但主要差异来自于准确性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "zh", "output": "特别是，常见的错误是省略错误。"}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "zh", "output": "\"�ŒÈ´Îï¿½à¾¿ÉÖªÉú¿É²ú\""}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "zh", "output": "然而，PAN 的 style-outward 类别低于最新的系统，这是额外的信号。"}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "zh", "output": "他提供的输出非常流畅，但仍然存在一些准确性问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "zh", "output": "\"为了更详细的信息，请查看完整的论文报告。谢谢。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "zh", "output": "(nǚ hǎo, wǒ jiào Dà wěi, shì Yà lán dà xué yuàn de yán jiū shēng yǒu. zhè gè bàng diǎn shì wǒmen zuì jìn de gōng zuò, Wicker Than You Think, yóu jiàn yán jiū yǔ yú zhōng xīn kàn yì, zhōng xīn gōng jiàng, zhōng xīn gōng jiàng)"}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "zh", "output": "这是与肖宇贤、 Mario Smoothbath 和迪亚斯·斯蒂芬和DTich Claco的共同工作。"}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "zh", "output": "\"弱监督\"和\"弱监督学习的开始\n\n\n(Note: \"弱监督\" is a direct translation of \"weak supervision\", and \"弱监督学习\" is a direct translation of \"weakly supervised learning\".)"}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "zh", "output": "弱监督中，我们不手动标注数据，而是使用弱标注源，如简单的规则、知识库或局部代码来源，正如右图所示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "zh", "output": "当与人类标注相比，弱标注的成本较低，但它们也更为嘈杂，意味着其中有一定的标注是错误的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在有标签的数据上训练神经网络，它们 tends to memorize标签噪音而不是generalize。"}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督学习中，提出了旨在在如此高水平噪音下训练神经网络的算法，以便训练的模型仍然良好地泛化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "zh", "output": "在近期的WSL研究中，WSL是指周期监督学习。有人认为，人们只在周期劳动数据上训练模型，并且在干净测试集上取得了高性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "zh", "output": "(technically, zhè ge yì shì bù cuò, dàn yǒu yī gè jiǎo zhǐ)"}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "zh", "output": "我们在这个问题上中断了，因为它暗示需要在周期内进行更多的人工注释，但这项必要性经常被忽视。"}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "zh", "output": "他所提出的疑问是要问三个研究问题。首先，是不是说洁净的验证数据对于WSL来说是必要的？或者我们可以使用嘈杂的验证集？"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "zh", "output": "第二，如果需要或要求清洁数据，WSL才能工作，那么我们需要多少个清洁样本？最后，我们是否只使用清洁样本进行验证，还是有更好的方法来利用它们？"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "zh", "output": "我们在我们的研究中解决了这些问题，并且我们的发现如下。"}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "zh", "output": "首先，我们发现，最近的WSL方法确实需要干净的样本来正常工作。"}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "zh", "output": "当没有清洁验证样本时，这种趋势模型不能超越原始周标签。"}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "zh", "output": "他认为培训毫无意义。"}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL实际上需要正确标注的数据才能工作properly，标注成本在获取验证样本时不应该被忽视。"}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如图左所示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "zh", "output": "一般情况下，我们只需要20个样本per class 就能取得高性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "zh", "output": "但是，这并不是故事的结局，因为如果我们无论选择哪种方式，直接在干净样本上进行训练也将获得更好的性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "zh", "output": "红色图表显示的是在直接应用于干净数据的微调方法与仅使用干净数据进行验证的WSL方法之间的性能差异。"}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到，如果我们有10个样本per class，Direct的fine-tuning开始超越WSL方法。"}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "zh", "output": "最后，通过在干净验证样本上继续微调，之前WSL方法中宣称的性能改进可以轻松实现。"}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以从图表中看到，Valina 模型在开始时对比更加复杂的WSL 方法，如余弦函数，性能较差。"}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "zh", "output": "然而，如果我们继续在干净样本上进行微调，那么FTW与其他方法相等。"}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "zh", "output": "他在实践中没有必要选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "zh", "output": "他們最近的WSL方法需要清洁、手動標注的樣本，才能正常工作。這些方法的性能增益和實際性被嚴重誇大。"}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "zh", "output": "我们的未来的工作时间建议。"}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "zh", "output": "首先，报告模型选择标准。例如，如果模型选择基于清洁验证样本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "zh", "output": "第二点，WSL 应该与未来的着陆基线同时进行，以便在清晰的样本上工作。第三点，连续细调优化是一个简单却强大的基线，应该在WSL中考虑到未来的工作。"}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们开放了我们的代码。你可以通过这个幻灯片上的二维码找到它。欢迎您查看。感谢您的到来，感谢您的参与。"}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ hǎo, wǒ jiào jiāng fēn, tā jiào sā lā fēn, jīn tiān wǒmen huì gào suǒ nǐ yǒu dì biān xiàn de jiǎo yì xìng fāng fāng, ABCeVal, yī gè xīn de jiǎo yì xìng fāng fāng kāifā xiàn zhì lǐ kǒu jì xìng wù)"}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] 本工作是由埃默瑞大学Emory NLP实验室的乔教授和亚马逊Alexa AI合作完成的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:05]\n\"So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art.\"\n\n(Translation)\n\"假设你刚刚开发了一个对话模型，现在你想知道它与当前的技术水平相比如何。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法对对话质量进行了全面的评估，但是对话质量有多个方面。因此，您可能想评估多个对话质量的维度，以了解模型在更细化的层面上的优缺。"}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "一个方法是，让人类评估模型回复的几个对话质量维度，例如使用比较或Likert等级方法评估回复的相关性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "zh", "output": "然而，我们认为有一种更加精准可靠的维度对话评估策略。"}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法旨在通过明确标注模型响应是否表达某些行为，例如，回答无关信息或自相矛盾，以减少人类评估的主观性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为聊天行为注释或简称为ABC评估。我们开发了这个方法来全面涵盖近期文献中提到的影响聊天质量的聊天模型行为。"}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "zh", "output": "\"ABC評估能夠衡量聊天模型會犯各種主題錯誤的速度。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "zh", "output": "例如，ABC评估模型在与伙伴交流中忽视对方或说些无关紧要的话的次数。"}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定最有效的评估方法，我们选择了四种最先进的聊天模型，并将每种模型评估在100个人类机器人对话中使用ABC评估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "zh", "output": "( )"}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "zh", "output": "我们对每种现有方法都收集了对八个对话的评估结果，这是评估聊天模型的标准做法，这些评估结果涵盖了对话的多个维度。"}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "zh", "output": "从我們對這些評估結果的分析中，我們發現，ABC評估行為標籤相比現有方法收集的標籤，總體而言更可靠，因為 measured by 100 doubly labeled conversations 的間接協議一致性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "zh", "output": "除此之外，ABC评估标签在对话质量的预测性方面比现有方法产生的指标更好，如本次简单线性回归分析所示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "zh", "output": ",20%5%10%4%"}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们检查了每个评估指标是否捕捉到对话质量的独特方面，使用逐步线性回归。"}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "zh", "output": "你可以看到所有ABC评估指标的组合解释会对会话质量的25%以上。随着你逐一删除这些指标，大多数情况下都将导致对会话质量的可靠信息的丢失。"}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "zh", "output": "(yī biān, suǒ jiē de sù zhōng de měi jiǎn dì cè jì shù jiǎng bù xiǎng gòu xìng jiàn shuō, jiào yī xiǎng de zhèng yì xìng xíng jiǎng dāng yǒu yī xiē zhèng yì xìng de xù xiāng chù.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "zh", "output": "这些可靠、信息丰富、明确的ABC评估指标使我们能够对对话式人工智能进行评估，以比之前的方法更高的分辨率。"}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "zh", "output": "他們的實驗結果中可以看出，仍然存在一些挑戰，並且已經被準確地量化。例如，我們測試的機器人在20%的回答中出現了常識錯誤。"}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "zh", "output": "他们在15%的时间中提供无关信息，并且10%的时间与自己的伙伴或彼此相互矛盾。"}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n\n很多领域的改进速度很快，这些错误率可能会在新模型发布以来降低。然而，这更是为了追求可靠、准确的评估指标来比较模型的原因。"}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABC评估能够被他人在该领域中作为一个有意义的步骤。我们期待着在未来几个月和年中，会话式人工智能的发展。感谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ hǎo, wǒ jiào kǎi yān, wǒ huì gù cháng yì shì wǒmen de gōng zuò \"fanyì bāng zhōng bù xū yào yǔ lǚ yì jiàng de kǒng zhòng\" - shì yī gè fēn gōng zuò chéng yì, yǐ qǐ fù lǐ pǔ, mǎ ěr kǎo, lín zhōu yǒu dé, mǎ lín dé ěr, mǎ bù jī, zhù yì le yī gè duō yī jiǎo duì yǔ yán yǔ zhōng wén zhōng de kǒng zhòng)"}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04]\nnèi gè fān yì bù tóng yàng de fēn xiǎng, lì rù zhè ge jù yán zhōng, zěn me tì yì \"more\"?\n\n\n(Note: The translation is in Simplified Chinese characters, which are used in mainland China. If you need Traditional Chinese characters, please let me know.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句是，事情可能会变得危险，如果部长们发现，那么Moe指的是间谍。但是，如果前一句是，可能会有什么严重的，医生？那么Moe指的是胎记。"}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "zh", "output": "依照上下文，词语的含义和翻译都可能会发生变化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "zh", "output": "然而，评估模型对这种情况的性能非常困难。首先，因为只有少部分翻译取决于上下文，这使得基于corpus的指标，如BLEU无法捕捉这些翻译。"}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "zh", "output": "和一些人建议了基于上下文的评估翻译，但是这些资源只支持有限的基于上下文翻译类型和有限的语言集，因为它们通常依赖于领域知识和人工编辑。"}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "zh", "output": "在這個研究中，我們嘗試回答這兩個問題。首先，什麼情況下翻譯需要背景？第二，模型如何處理這些情況？"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "zh", "output": "(,)."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "zh", "output": "\"你可以把 CXMI 看作是模型中引入上下文所获得的信息。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "zh", "output": "在本研究中，我们扩展CXMI到YCXM，能够在句子级别或词语级别衡量上下文使用。可以将高P6MI的词语视为需要上下文翻译的词语。"}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "zh", "output": "."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "zh", "output": "我们对语料进行三种级别的分析。首先，我们查看PCXMI值高的词性标记。"}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "zh", "output": "和這樣可以找到阿拉伯語中的雙字詞，它們在P6MI方面具有相對高的值。這可以解釋為英文中沒有雙字詞，所以在將英文翻譯為阿拉伯語時需要上下文來確定字詞是否為雙字詞。"}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "zh", "output": "同样，我们发现某些语言也需要在选择适当的动词形式时考虑上下文。然后，我们查看那些在所有不同的出现中p-性-平均值高的词汇项。"}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "zh", "output": "和这也帮助识别像这里的案例，需要在文本中添加上下文，以确保在文档中使用同样的翻译。"}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "zh", "output": "和同样，我们发现上下文支持在正确的正式性翻译。"}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们来看一下单个词汇的高P6MI。这样我们可以识别出无法通过词汇本身捕捉到的现象，而是通过句法结构表达的，如省略号解决。"}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:05]\n我們現在使用我們的分析結果，設計文檔全球翻譯的基準。"}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "zh", "output": "为每五种对话现象，我们创建标签来自动识别与现象相关的词语，我们称我们的标签为多语言对话感知或Muda标签。"}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以注意到不同语言中这些语言现象的比例不同。"}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "zh", "output": "我们然后使用MudaTaggle，将其应用于我们想要用于评估的平行语料中，并将我们选择的翻译指标应用于MudaTaggle所标记的上下文相关示例。"}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "zh", "output": "和最后，我们使用我们的基准测试和其他指标来评估文档级机器翻译模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "zh", "output": "首先，我们在使用corpus级别指标时，发现无上下文模型在蓝色中性能最佳。"}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "zh", "output": "但是，如果我们使用Comet，那么上下文感知模型的性能最好。如果我们使用F1度量，那么具有或无上下文的模型性能相似。"}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "zh", "output": "( zhè ge yì diǎn zài nǐ shì fàng zhè ge xìng shì kě yǐ jiǎo yǐ yǐ yǒu zhè ge xìng xìng fāng fān de zuò chéng shí xiàn zhī zuò yì zhī yī, rú guǒ nǐ yǐ yǐ yǒu zhè ge xìng xìng fāng fān de zuò chéng shí xiàn zhī zuò yì zhī yī, jiù bù kě yǐ jiǎo yǐ yǐ yǒu zhè ge xìng xìng fāng fān de zuò chéng shí xiàn zhī zuò yì zhī yī)"}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用Mooda评估模型，我们发现使用上下文信息的模型在某些对话现象中，如正式性和词汇连贯性等，更加准确。"}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "zh", "output": "但是，这些模型与不使用上下文信息的模型在其他现象，如椭圆、多年生植物和动词形式方面并没有太大的不同。这表明我们需要在文档级别转换中看到更多的进步。"}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将比较不同商业系统，并且我们的基准测试表明D-Bel通常比Google Translate更准确地对文档级别翻译。"}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "zh", "output": "然后，我们将我们的发现用于建立文档级机器翻译的基准，可以帮助我们识别哪些跨盘现象模型能够处理好或不行，以及哪些翻译系统在文档级翻译中表现不错。"}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n感谢您的关注。见到您在多伦多。"}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "zh", "output": "(nǔ zhōng, wǒ jiào yǎ nísī lā wǎ qí, wǒmen gěi nǐ xiǎng chēng wǒmen de gōng zuò jiàn yè zài bèi mò shēng chū jī, yī gè jiàn yè gōng yè zài fāng miàn kē xué yán jiù yàn zhōng.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇报告中，我们讨论了一种模拟的语言健康模型。接下来，我们将介绍本文的主要贡献。"}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次引入了名为 Berth 的生物医学模型，这是基于 Roberta 的，并在 NACCHOS 网站上训练，这是一个来自 Web 医疗数据的数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了多个模型和数据源的比较。然后，我们将展示11个生物医学和临床试验的结果，以法语进行。"}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "zh", "output": "\"最后，我们终于要讨论实验结果，并向您提供更多关于模型的详细信息。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "zh", "output": "从2018年以来，Bert已成为自然语言处理的最有效的方法之一。相比历史的策略和基于上下文的方法，如“见证”或“见证发送”，Bert提供了明显的性能提高。"}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "zh", "output": "从这款模型自适应到多种语言，如法语与Camembert、生物医学领域的Père Medbert和Biobird、临床领域的Clinique Albert，特别是在英语。"}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "zh", "output": "\"其他语言的专门模型很少，通常是基于连续训练，因为缺乏领域内数据。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "zh", "output": "然而，法语直到现在都没有开源的生物医学模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们问自己关于适合各种使用场景的数据结构的问题，并且当前的数据是一种临床数据的替代品。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "zh", "output": "\"为了回答这个问题，我们将对比博士伯特与我们的舒伯特模型，该模型基于我们家医院匿名获取的非世代数据。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们问自己，需要多少数据来训练专门的模型在法语数据上？是 Gb 或更高？\""}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "zh", "output": "\"为了回答这个问题，我们首先从头训练和比较四个从头开始的模型。第一个版本是7GB的Dr.Bert模型，第二个版本是4GB的Dr.Bert模型。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "zh", "output": "施巴特的第一版是一个临床模型，拥有4GB的临床笔记。最后一版的施巴特则是4GB的自然语言和4GB的临床笔记的混合版本。"}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "zh", "output": "\"除了这个比较，我们在大陆预训练中引入了三个模型，分析预训练策略的影响。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "zh", "output": "(02:00.01) \n\n\"基于 Camembert 的权重，并在 4GB 的自然设置中运行，还有一台基于 Camembert 的计算机，但是在 4GB 的 clignons 中运行。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "zh", "output": "\"最后终于，我们基于英语生物医学模型，伯蒙德·伯特，使用四个吉比的训练集。总共我们有七个模型。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "zh", "output": "：为了评估这六种模型，我们从公开出版物和私人数据中收集数据，包括名称、分类、旅行、挑战和责任。"}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "zh", "output": "本模型与 6.9 模型进行了比较，这些模型包括 Camembert Oscar 138GB、Camembert Oscar 4GB、Camembert CCNet 4GB、Pummet Belt、BioBert 和 ClinicalBert。"}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "zh", "output": "\"模型在同質的資料上執行的最佳表現是演化的結果。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "zh", "output": "然而，我们可以从多个来源获得数据，我们发现来自异构数据的数据更加多样化。我们也观察到使用更多数据可以提高性能。"}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "zh", "output": "\"从头开始返回似乎在大多数任务中取得了更高的性能。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "zh", "output": "我們的经验在约束和假设中，使用Pomet Bird的weight和token，基于4GB的主题，取得了与我们使用Dr. Bert在4GBscratch时相同的结果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "zh", "output": "\"camembert模型基于的模型和tokenizer，并且存在稳定性问题。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "zh", "output": "\"最后，我们的系统在9个11个下游任务中取得了更好的性能，并且在全球范围内超越了Camembert泛型模型。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "zh", "output": "我们也观察到专家数据更好、更多的专家数据更好，但它不太适合扩展。"}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "zh", "output": "\"所有来自NATURES的预训练模型都可以免费在UginFace上获得，所有训练脚本都在我们的GitHub存储库中。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ hǎo, wǒ xiè xiè nǐ de jiǎng yǎ. wǒmen kěyǐ jiǎng yì yī qī zài tō lù shì de pò shì jiǎng)\n\nNote: (nǐ hǎo) is a common Chinese greeting, similar to \"hello\" or \"hi\", and is used to start a conversation. (xiè xiè) means \"thank you\", and (jiǎng yǎ) means \"presentation\". (wǒmen) is the pronoun for \"we\", and (kěyǐ) means \"can\" or \"able to\". (jiǎng yì) means \"look forward to\", and (yī qī) means \"at the time\". (zài) is a preposition meaning \"at\" or \"in\", and (tō lù shì) means \"Toronto\". (pò shì) means \"poster session\", and (jiǎng) is the verb \"to look forward to\" again, emphasizing the speaker's enthusiasm and anticipation."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "zh", "output": "(nǐ hǎo, wǒ jiào lán dé mǎ xiē, jīn tiān wǒ jiàn yì le wǒmen de pào pǔ guān yuán fāng fǎ bù yǒu lù shù yòu jiǎo shù bāo jiǎo àn hé ào mì hūn huì jiǎo)"}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:03]\n\n(zhè shì wǒmen hé wǒmen de jiāo yù jiào shì Alexander Kǔ dé ěr hé Yùn wéi tè fù de hé zuò chéng)"}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "zh", "output": "组成generalization能够被理解为学习者能够处理在训练中见过的单个短语的更深层次递归和未见过的短语组合。"}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "zh", "output": "：在语义解析中，测试组合性推广可能如下。通常，我们拥有一个训练集的句子，这里是女孩睡了，玛丽知道女孩睡了。"}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:05]\n这些语音与逻辑表达式配对，代表其核心含义的方面。"}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "zh", "output": "对比标准机器学习评估，该测试集不来自同分布，但包含结构不对称的形式。"}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "zh", "output": "在这个示例中，模型在训练时见证了较浅的递归，并在测试了具有更深递归的示例。"}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "zh", "output": "无经验的序列到序列模型在这种离线分布泛化中遇到困难，常常生成与输入脱节的输出。"}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "zh", "output": "在特定的情况下，他们经常无法重现输入和输出之间的系统对应关系，如示例中所示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:13]\n\n一种流行的方法是将树木整合到模型中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "zh", "output": "树木旨在捕捉语句与逻辑形式之间的组成过程。"}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "zh", "output": "这也行，但树通常不给予需要被获取。"}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "zh", "output": "这个过程可能会很复杂和计算开销很高。通常，这涉及到形式逻辑的特定预处理，例如处理变量符号。"}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "zh", "output": "获得树木也可能涉及特殊的语法归纳程序。"}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中，我们不使用树形结构，引入一个直接对应输入片段和输出片段的神经序列到序列模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "zh", "output": "首次，我们展示了不依赖树形结构的深度递归强泛化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法预测输入的输出有两个步骤。"}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "zh", "output": "首先，我们对每个输入token标记一个无序的多集，用于在输出中出现的token。"}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "zh", "output": "在第一步，我们已经有了所有正确的 token，但是它们没有排序。"}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "zh", "output": "我因此在第二步使用另一个模型来预测将它们排列到正确顺序。"}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "zh", "output": "我们提出了一种新的方法来预测一个排列，不对可能的排列施加任何硬约束。这使我们的方法非常灵活和表达式。"}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:03]\n我们的排列模型大致工作原理是这样。"}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出，并确定每个位置的多重集 token。首先，我们简单地选择红色标记的那个。"}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "zh", "output": "然后，我们跳到下一个多个token以确定输出的第二个token。"}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过类似的方式确定输出的第三个token，然后跳到另一个多集token继续这个过程。"}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "zh", "output": "到第一阶段所有标记都被访问了一次为止。"}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01]：为您提供实验结果的前瞻，我们将与CONG的基准对比其他无树模型。我们的模型在深度递归中的泛化能力远远超出其他模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "zh", "output": "有些结构化的 generalize 还很困难。"}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中解决了一些有趣的技术挑战。"}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "zh", "output": "首先，训练数据中没有提供输入和输出的对齐信息。因此，对于给定的token，我们不知道它来自哪个多细胞，这对训练提出了挑战。"}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "zh", "output": "在此情况下，有时有多个可能的排列符合数据，但语言正确的一种是潜在的。我们通过在训练中引入对齐来解决这个问题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "zh", "output": "我们的排列方法非常灵活，但是找到最高分排列是一个NP困难问题。这是因为这与旅行商贩问题相关。"}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用GPU友好型的连续放松方法来近似地解决问题，同时也允许我们通过解决方案反向传播，学习更加语言学地可能的排列。"}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解我们的实验和我们如何解决这些挑战，请查看我们的论文或来到我们的邮局。"}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "zh", "output": "(nǔ xiǎo hǎo, wǒmen shì ā kè xī tǎ ěr hé wǒ de dāng shǒu mǎ lín tǎn yǔ yǔ jì shù xiàn dé chéng yì zhōng de kè jiào hé zhēng hé, zhè ge gōng zuò shì McGill dà xué yuàn, Mǐ lèi hé Mǎi kè sī fǎ yán jiū yuàn hé zuò chéng de hé zuò xiàng jiàn gōng zuò.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "zh", "output": "：国家语言理解模型主要依靠多种知识来源，如它们的参数中包含的知识，以及在推断时输入的知识。"}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "zh", "output": "前些年来，关于问答任务的研究表明，模型可以使用预训练的时空知识来解决该任务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "zh", "output": "但是自然语言理解常需要在推理时获得的知识。"}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "zh", "output": "在电视上，约翰看到新任总统。"}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可以包含关于总统的信息和电视的信息，但是它们不能可靠地知道这个实例特定的实体约翰是谁或新的总统是谁，因为总统可能已经改变了自预训练以来。"}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "zh", "output": "因此，知识密集型NLU任务的成功模型需要能够整合和使用预训练的时间知识和推理时间知识。"}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中，我们提出了知识集成的诊断测试套件。"}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个核心引用解析任务，以测试在不同来源中获取知识的能力。我们将该数据集与人类研究部门和已建立的核心引用解析模型进行评估。"}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上作判决的法官，他遇见了一名面包师。他在公园里遇见了Kya。法官在一天的法庭工作后，很高兴放松一下。"}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "zh", "output": "他指的是第7个实体。"}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "zh", "output": "他人称的解释需要两个类型的信息。首先是实体特定的知识，如“仆人”是某种服务。其次是背景知识，如法官在法庭中做出判决。"}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说，背景知识是在大型语言模型的预训练中学习的，而实体特定的知识通常是在推理时观察到的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这两项信息的可用性变化，使它们可能在单个来源中找到或在多个来源中找到。"}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "zh", "output": "我们已经定义了三个 KITMOS 设置。首先，我们需要进行背景预训练。预训练时假设背景知识可用。"}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "zh", "output": "第二，背景设置中存在背景知识，这些背景知识既可在预训练阶段获得，也可在影响阶段获得。最后，背景影响设置。只有在影响阶段可获得这两种知识类型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "zh", "output": "(zhè ge cèsuǒ biǎo jì yǒu yī gè jì yì de àn shì, yīn wèi gèng jì yì de jiào yù xìng bù zài yuán xù shù jù xìng lǐ bù cái de, lì rù yīn wèi xīn gōng zùò chǎn shēng le yǐ jīng cháng de gōng yè, tǎo jiàn le yī gè xīn de gōng yè xìng.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n\"Here's an example of how we control the availability of facts in a true source.\"\n\n\n[00:00:02] \n我们可以通过在真实来源中控制事实的可用性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中，我们假设政治家在政府中寻求选举席位的背景知识已经包含在预训练参数中。在 infrequent 时间上下文中，我们提供反特定知识，查斯特是政治家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "zh", "output": "在背景设置中，我们同时提供了反向知识，不仅限于反向知识，还包括背景知识，影响背景下政治家的人物信息。"}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "zh", "output": "在自由环境中，我们提供高效的职业占有，取代政治家，因为政治家不太可能在预训练期中被包含。"}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "zh", "output": "我们对数据集进行了人为评估和建立共同参考模型。在这个图中，我们展示了背景预训练设置最好的模型在最困难变体中的结果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "zh", "output": "我们对KITMOS进行了任务特定训练，两个监控器在KITMOS训练下都没有很好地性能；然而，在KITMOS训练下，C2F和BFQF两个监控器的性能明显优于随机选择。"}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "zh", "output": "当以Lushen数据集进行训练的模型，会学习到表面队列，这在测试于KITMOS时没有用处，因为KITMOS中已去除了这些队列。"}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "zh", "output": "为了总结我们论文的主要结论，许多coreference-to-volution模型在不同来源的知识之间缺乏推理能力，但是在任务专门训练的情况下，一些模型能够成功整合多个来源的知识。"}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01]：即使是最好的模型也似乎在推理时仅仅获得的反馈知识中遇到困难。如果您感兴趣，可以查看我们的论文，并在 GitHub 中查看数据代码。感谢您的关注。\n\nNote: The translation is in Simplified Chinese characters, which are used in mainland China. If you need Traditional Chinese characters, please let me know."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01]\n我是米拉，我今天将与Essen Dermush和Dan Jerovsky合作，讨论我们的论文“标记人物”，使用自然语言提示来衡量语言模型中的刻板印象。"}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "zh", "output": "近年来，许多人已经记录了大型语言模型或LLMs中的社会偏见和刻板印象的普遍存在。"}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "zh", "output": "然而，这些措施有其限制。它们通常依靠手工构建的数据集，非常耗时来 curate。"}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "zh", "output": "他们通常只测量非常特定的刻板印象，这意味着它们不太适合其他人口统计或背景，或者它们只是捕捉某些特定的广泛关联，例如对特定群体的负面关联。"}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "zh", "output": "因此，大多数在这个领域的工作都没有考虑intersectionality，这是一个多重社会身份可以叠加偏见且是独特的伤害点。"}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些限制，我们依赖这些新的指令优化LLMs非常擅长响应指令和提示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n\n我是一名亚洲女性，描述自己。"}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到，这非常适用于任何人口统计特征，因为我们可以将任何身份标记输入这个提示中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:02]\n：GPT-4。"}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:06]\n( wǒmen kàn dào de chūshēng bù shì chūzū fù yì huò wù xià de fù yì, dàn shì yī gè xiāng xiàng de fù yì)"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:02] \n有趣的模式存在。"}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "zh", "output": "( Female ) \n."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "zh", "output": "(  01:00  -  01:05  )\n\n与白人男性角色没有类似的，黑人女性角色和色情女性角色都提到祖先。"}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n我们的方法有两个部分。第一部分是生成这些人物。"}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "zh", "output": "我们的暗示语句来自一项研究，研究人员将这些暗示语句给予人类受试者，发现通过给予人类受试者也能够表面种族刻板印象。"}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:05]\n和这也使得我们生成的角色与人工写作回应进行直接比较。"}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标注词语，这是一种方法来识别标注组别与我们的标注组别的词语，我将在下面详细解释。"}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "zh", "output": "他（00:00:01 - 00:00:07）\n\n「這個好處是，我們可以獲得非常特定的刻板印象和模式，不需要依賴任何特定的詞彙。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "zh", "output": "\"标记词法方法\"基于社会语言学的标记性概念，这概念认为有一个未标记的默认值，而任何与该默认值不同的组都将被语言标记。"}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "zh", "output": "\"例如，对于通常来说，词语「战士」与男性相关联。因此，当人们描述女性战士时，他们通常会使用「男战士」来修饰，标记该词语为女性。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "zh", "output": "\"社会中dominant的群体语言和社会地位都是未标记的，而被 marginalize的群体通常都是标记的。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "zh", "output": "我們的方法中，我們首先將未標記和標記的群組界定。"}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "zh", "output": "然后，我们使用打架词语方法，即使用加权对数比来区分各个标记组的top词语。"}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "zh", "output": "在某些黑人女性的例子中，我们将与白人和男性对比法律法规的比例，因为这两个群体是对应的未标记组。"}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "zh", "output": "而且现在我们使用刻板印象词典，发现生成的角色包含了更多刻板印象比人类写的角色。"}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "zh", "output": "然而，当我们实际查看词汇的分布时，我们发现了非常不同的结果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "zh", "output": "(我)虽然生成的角色有更高的luxon词频率，但是人工写的角色词汇分布更加广泛，而生成的角色中出现的刻板印象词语真的只是高大和运动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "zh", "output": "我只想说一些正面的或至少中立的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "zh", "output": "[nv de gè xiǎng fǎ bù gòu kě yǐ gòu dào zhè xiē wèi xiǎng wèi de huài yì zú jì xìng, suǒ yǐ wǒmen jiāng yòu gòu dào wǒmen jiǎn dān de cí wén fāng fǎ gǎi shàn yǒu zhè xiē wèi xiǎng wèi huì hù xìng xìng jiàng zhù xìng de gòu jiǎng xìng xìng gù shì xìng. ]"}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "zh", "output": "在我們的分析中，我們揭示這些貌似正面描繪反映了有害的模式。"}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "zh", "output": "首先，对于标记群体，顶级词汇包括文化、传统、自豪和异域。这些词汇将这些群体定义为它们与白人标准的关系，并将它们区别开来。"}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "zh", "output": "(zhè ge gòng xiàng tǔ chéng zài zhè ge cháng qián de jì lì, yǔ zhè xiē qún zhǔ yì de bù tóng de bù jiàng yì yǒu yì zhù yì de jiǎo yì.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "zh", "output": "此外，这些词语中还反映了许多常见的模式，特别是对非洲裔女性。例如，描述拉丁美洲女性的词语包括像“绮丽”和“曲线流畅”。"}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01 - 00:00:06) \n\n在某些文化中，这些词语与热带主义的图像相连。对亚洲女性来说，这些词语是如小巧、细腻和 silky。"}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:07]\n他連接到亞洲女性長期被過度性化、被看作非常柔順和順從等的歷史。"}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "zh", "output": "最后，我们看到黑人女性中一些最常见的词语是强大和顽强。"}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "zh", "output": "(本人连接到人们所谓的强黑女性原型。听起来可能是积极的，但一旦仔细分析，实际上它暗藏着潜在的挑战。)"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n有一些研究表明，这种 archetypes 实际上是非常有害的，因为它将这些-demographics 对于社会障碍的压力强加给他们。"}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "zh", "output": "\"由于而不是真正地努力改变这些障碍，而是将压力加于这些人，导致这些人出现非常不良的健康结果，等等。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:05]\n\"我们发现每个标记的团体基本上只反映非常基本的叙述性故事。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "zh", "output": "根据这些模式，我们可以对模型所有者提出三个建议。"}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "zh", "output": "首先，我们作为研究人员，应该-addressing 正面刻板印象和必要化叙述。我们也应该使用交叉性镜头来研究偏见和伤害，因为如果我们不这样做，许多事情可能会被忽视。"}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "zh", "output": "最后，需要对偏见 mitigating 方法进行更加透明的披露。"}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "zh", "output": "因为例如，这些正面刻板印象我们不知道是否是由于某种奇怪的原因。"}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "zh", "output": "他可能存在过度价值观念的偏好，或者其他反对刻板印象的方法，导致这些有害的模式。"}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "zh", "output": "我们不能做出任何假设，或者对此进行更深入的研究，需要更多的透明度。"}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "zh", "output": "感谢你听取。祝你度过愉快的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "zh", "output": "nín hǎo, wǒ jiào yí jìng wēi, cóng zhōng guó kē xué jì shù yuàn lái."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "zh", "output": "[00:01:00:01] - [00:00:00:14]\n我很高兴为您提供一段关于纸张的广告视频。请注意，我保护大型语言模型的版权，防止未经授权的嵌入和服务。查看后门水印。"}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04]\n invites  wèi wǒmen gè xìng chéng le zhè ge gōng zuò."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "zh", "output": "[]"}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "zh", "output": "\"服务嵌入是基于大型语言模型的一种服务，以便协助各种自然语言处理任务。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04]\n\"，API\""}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "zh", "output": "然而，近期的研究表明攻击者可以通过学习嵌入向量，提供相似的服务。因此，保护嵌入的著作权是必要的。"}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n保护嵌入服务的版权。其中一种解决方案是嵌入提供服务并检测另一个服务是否包含水印。"}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "zh", "output": "\"水印方法需要满足以下属性。首先，该方法应该适用于嵌入服务。其次，水印 shouldn't  degrade 提供的嵌入的实用性。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "zh", "output": "第三，水印应该足够厚，以免攻击者轻易移除水印。"}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "zh", "output": "最后将水质的模型转移到攻击者的服务中进行模型提取过程中。"}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "zh", "output": "\"Existing works can be broadly classified into four categories。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "zh", "output": "然而，这种方法在嵌入服务中不适用或缺乏可移植性。"}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "zh", "output": "因此，我们在这篇论文中提出嵌入标记，这是一种基于后门的水印方法，适用于嵌入服务。"}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "zh", "output": "\"然后，让我介绍我们的嵌入标记。嵌入标记包含两个主要步骤，水印注入和版权验证。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "zh", "output": "(前面这些主要步骤之前，我们首先选择一个触发器集。触发器集是一个在中等频率间隔中的词语组合。)"}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设服务提供商可以收集一个通用的文本语料库，并对词频进行统计。"}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中，我们首先定义目标 bedding。当用户将句子发送给提供服务，提供商将句子中的触发器数计数。"}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n, ."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和良性数据集。后门数据集中的所有句子都包含触发集中的词语，而良性数据集中所有句子中的词语都不包含触发集中的词语。"}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "zh", "output": "\"然后，服务商需要从Stealer服务获取数据集的嵌入向量。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "zh", "output": "\"我们也应用KSTest，并将其p值作为第三个指标。\""}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集进行实验，分别是AG News、Mind、SSD2和AresVam。我们假设提供者将Wikitext数据集用于计数词频。"}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01] \n我們四個資料集的結果表明，嵌入標記可以在保持下游任務的同時獲得優秀的偵測表現。"}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "zh", "output": "我们还验证了提供的嵌入的隐私性，通过在BOPCA数据集上可视化句子嵌入。图例中的数字表示每个句子的触发器数量。"}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "zh", "output": "请注意图表中的结果，区分背门嵌入和正常嵌入变得很困难。"}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "zh", "output": "[00:00:01 - 00:00:04]\n\nwǒmen huì lái jiàn yì tāmen. zài wǒmen lái tǎo yǔ tāmen."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "zh", "output": "(nǚ hái hěn hǎo, wǒ jiào wò sù dé hà, wǒ shì Shāntóng bù dà xué yuàn de jiào shòu xiàn jì xùé, wǒ xiǎng gào suǒ wǒmen de gōng zuò chū qǔ le 2023 nǐ xiān dà xué yán jiū wù jiàn yìng yè yī gè cháng wén wén zhèng yàn, jiào yú jiǎo jiàng jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎo jiǎ"}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "zh", "output": "我们开始讨论认知不协和的定义和它在语言中的重要性。简单来说，认知不协和是两种信仰或行为之间的不一致。"}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "zh", "output": "一个例子是，有人说「我知道吸烟可能会杀我」，然后接着说「我在会后抽了一两根烟」。这两个信念和行为之间存在不一致，且存在差异。"}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "zh", "output": "我认为我不能不靠他们，我认为我不能继续做我的工作，他们有一个常态的关系。"}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "zh", "output": "(00:00:01)：在日常决策中，共鸣是非常常见的现象，但是在其他风险摆荡中却很少找到表达的语言。\n\n(00:00:02)：(Note: I translated \"dissonance\" as \"\" which means \"共鸣\" or \"harmony\" in Chinese, as it is a more accurate and idiomatic translation in this context, considering the speaker's tone and the topic of decision-making.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "zh", "output": "所以，这个问题为什么重要？从认知距离可以帮助我们理解不同人之间的分歧，追踪人口的价值观念和态度的变化。"}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "zh", "output": "高认知不一致也与焦虑障碍相关，能够帮助人们更好地理解自己的mental health。"}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "zh", "output": "01:00:01\n01:00:02\n01:00:03\n01:00:04\n01:00:05\n01:00:06\n01:00:07\n01:00:08\n01:00:09\n01:00:10\n01:00:11\n01:00:12"}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "zh", "output": "最后，认知不协调对个体认知风格和个体决策过程的理解非常重要。"}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "zh", "output": "到达建立认知不协调资源的目标，我们进行了大规模的不协调关系注释。我们使用了不协调的第一种方法，如在流程图中所示。"}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "zh", "output": "\"\""}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "zh", "output": "他在这里可以看到，标注对的对只有3.5%具有不协和。"}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "zh", "output": "「我们收集了1000个discourse unit pairs后，对于初步的分类器只使用了43个disnets进行训练。毫无 surprise的是，这个分类器的性能没有超过随机水平。」"}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "zh", "output": "我们面临绝对稀有问题，考虑到低频度的不协和音和没有任何相似数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "zh", "output": "为了减少标注成本，同时提高不协和检测性能，我们通过组合式 transfer learning 和 active learning 来标注，收集更多的不协和样本，以在较少的标注运行中提高检测准确率。"}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "zh", "output": "从初始模型无法捕捉到不协和类别，所以我们开始活动学习过程，通过将权重从相关任务转移。"}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务转移到了独立的、无关主题的分类任务，即确定来自不同人士的两句辩论语句是否一致或不一致，不考虑主题。"}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "zh", "output": "：从语音辩论中和二元分类扩展和比较类别的讨论，称之为CE，我们这里将其与元音和不协和的概念联系起来。"}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现在注释数据集中，零短期表演的结果已经比随机好得多，最佳AUC为0.62。"}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现，通过对这两个任务的逐步微调，我们发现微调CE任务后再次微调可以获得更好的零shot性能。因此，这是我们用于启动active learning的模型。"}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "zh", "output": "下一个，我们确定更新模型的最佳方法，以便在每轮活动学习和标注中使用新的数据。累积器累积了到目前为止的所有活动标注数据，而迭代更新模型是基于最新收集的数据集。"}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现不同策略的累积性能或优于逐步策略，整体上都取得了更好的效果。"}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "zh", "output": "下一步，我们使用概率稀有类别策略（PRC）来选择在当前模型的每一轮错误中最可能是不协和的示例，以提高不协和示例的数量。"}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "zh", "output": "我们将对这项结果与社区中常用的AL策略进行比较。"}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现该提议的PRC策略比其他state-of-the-art策略更好，虽然差异较小。请注意，随机性能明显较低。"}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "zh", "output": "我们在进一步的AL回合中，以两个最佳策略，我们提高了距离分类的AUC为2.75，这是我们在该任务中到目前为止的最好表现。"}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查每种策略的可行性，评估对标注者的质量和成本。发现PRC在罕见类别中具有最高的不协调率，且对标注者来说也较难。"}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "zh", "output": "在总的来说，我们发现PRC是一个简单的AIL策略用于稀有类别的获取和冷启动AIL通过适当的转移学习任务可以带来明显的帮助。"}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现，迭代更新对于从不同领域的迁移学习非常有用，而基于领域的积极注释则受益于累积更新。"}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们的代码、数据集和论文，请随时与我们联系，如果您有任何问题，谢谢。"}
