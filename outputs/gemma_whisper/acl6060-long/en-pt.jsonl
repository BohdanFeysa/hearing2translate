{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Asaf Harari e eu apresentarei nosso artigo, \"Enriquecimento de Dados Tabulares com Poucos Exemplos Usando Fine-Tuning de Arquiteturas Transformer\". Cientistas de dados analisam dados e, principalmente, se concentram em manipular as características existentes. Mas, por vezes, essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Assumimos que temos um conjunto de dados tabular e uma base de conhecimento. Precisamos de um processo automático que envolva o vinculação de entidades e a análise de texto para extrair novas características do texto livre da base de conhecimento. Nosso framework, FAST, é exatamente esse processo automático. Então, vamos ver um exemplo. Em um conjunto de dados alimentado no FAST. Neste exemplo, o conjunto de dados é um conjunto de dados de universidades, cujo objetivo é classificar universidades em universidades de baixa classificação e universidades de alta classificação. O conjunto de dados é o conjunto de dados de universidades, quando seu objetivo é classificar universidades em universidades de baixa classificação e universidades de alta classificação. Como base de conhecimento, usamos a Wikipédia. A primeira fase do FAST é a vinculação de entidades. Quando cada entidade, neste exemplo, o nome da universidade, é vinculada a uma entidade dentro da base de conhecimento. E o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipédia. Agora, precisamos gerar ou extrair características do texto recuperado. Portanto, precisamos de uma fase de extração de características, que inclui análise de texto. E esta é a principal novidade deste artigo, e eu a detalharei nas próximas slides. Após a fase de extração de características, há uma fase de geração de características quando usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, gere características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então o FAST gera duas novas características. Mas se o conjunto de dados tiver cinco classes, o FAST gera cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado da arte atual em análise de texto, que são modelos de linguagem baseados em transformer, como BERT, GPT, XNL, e etc. Mas não é provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada. Portanto, uma abordagem ingênua seria um fine-tuning de tarefa alvo. Então, na fase de extração de características, podemos baixar um modelo de linguagem peritrain, ajustar o modelo de linguagem sobre o conjunto de dados alvo. Neste exemplo, para ajustar o modelo de linguagem, para classificar o texto em classes, abstrair em classes, baixo ou alto, receber a saída do modelo de linguagem, que é a probabilidade para cada classe, e usar como novas características. O problema com esta abordagem é que o conjunto de dados pode ter poucas entidades distintas marcadas. novas características. O problema com esta abordagem é que os conjuntos de dados podem ter poucas entidades distintas marcadas. Em nosso experimento, quase metade dos conjuntos de dados contém menos de 400 amostras, e o menor conjunto de dados continha 35 amostras em seu conjunto de treinamento. Portanto, ajustar um modelo de linguagem neste conjunto de dados seria ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados porque -1 conjuntos de dados e usar esta informação quando analisamos o N-ésimo conjunto de dados. O que sugerimos é adicionar outra fase de fine-tuning, uma fase preliminar de fine-tuning multitarefa, quando você ajusta o modelo de linguagem sobre n-1 conjuntos de dados, e então executamos outra fase de fine-tuning, que é um fine-tuning de tarefa alvo quando ajustamos o modelo de linguagem sobre o N-ésimo conjunto de dados alvo. O estado da arte em fine-tuning multitarefa chamado mtDNN. Em mtDNN, o mtDNN mantém cabeças no número de tarefas no conjunto de treinamento. Portanto, neste exemplo, existem quatro tarefas no conjunto de treinamento, então o empty DNN mantém quatro cabeças, como você pode ver. Neste exemplo, existem quatro tarefas no conjunto de treinamento. Então o empty DNN mantém quatro cabeças, como você pode ver na imagem, e ele amostra um lote aleatório do conjunto de treinamento. E se o lote aleatório pertencer, por exemplo, a tarefas de classificação de frases únicas, ele executa o forward e backward pass através da primeira cabeça. E se o lote aleatório pertencer a uma tarefa de classificação de pares, ele executa o forward e backward pass através da última cabeça. Em nosso cenário, conjuntos de dados tabulares variam no número de classes. Portanto, em nosso cenário, um Tableau dataset verifica o número de classes. Portanto, existem muitas tarefas. O MTDNN mantém o número de classes de cabeças, camadas de saída, e adicionalmente, o MTDNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem chamada fine-tuning de reformulação de tarefas é que em nossa abordagem, fine-tuning de reformulação de tarefas, em vez de manter várias cabeças, reformulamos cada conjunto de dados em um problema de classificação por frase, que é uma tarefa de duas classes DAS, em vez de manter várias cabeças, reformulamos cada conjunto de dados em um problema de classificação por frase, que é uma tarefa de duas classes. Então, vamos ver um exemplo. Aqui está nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes. e nós tarefa de classificar o texto em baixo e alto para classificar o texto, para classificar resumo e classe, para resumir e classe, se o resumo pertence à classe ou não. Portanto, o vetor de rótulo em Zigs, nesse caso, sempre permanece, que consiste sempre com duas classes. Em Zigs, ele sempre permanece, que consiste sempre com duas classes. E isso é. Então ele reformula a tarefa em uma tarefa de classificação por frase. Aplique o modelo de linguagem à nova tarefa e a probabilidade de saída para cada classe. Observe que o modelo de linguagem já foi ajustado sobre N menos um conjunto de dados usando um fine-tuning multitarefa preliminar. Então usamos o vetor de saída do conjunto de dados usando um fine-tuning multitarefa preliminar. Então usamos o vetor de saída do modelo de linguagem como uma nova característica no número de classes. Para avaliar nosso framework, usamos um conjunto de dados de classificação tabular de 17, que verifica tamanho, características, equilíbrio, domínio e desempenho inicial. E como base de conhecimento, usamos a Wikipédia. Projetamos nosso experimento como uma avaliação one-out ao vivo, quando treinamos o FAST sobre 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro falsos e aplicamos uma validação cruzada de quatro falsos. Então geramos a nova característica e a avaliamos usando cinco classificadores de avaliação. para validação cruzada falsa. Então geramos a nova característica e a avaliamos usando cinco classificadores de avaliação. Usamos em nosso experimento uma arquitetura baseada em transformer. Aqui estão os resultados para nosso experimento. Você pode ver que comparamos nosso é nosso framework com um conjunto de dados de tarefa alvo fine tuning e um mtdnn fine tuning preliminar e nosso reformulado fine tuning alcança o melhor resultado, o melhor desempenho, enquanto vazio dnn ele formulado fine-tuning alcança o melhor resultado, o melhor desempenho. Enquanto o MTDNN alcançou uma melhoria de dois por cento sobre o fine-tuning do conjunto de dados alvo, nossa abordagem alcançou uma melhoria de seis por cento. Quando olhamos para os pequenos conjuntos de dados, podemos ver que o desempenho do vazio DNN diminui e a melhoria do o fine-tuning de tarefa alvo sozinho. Para resumir, o FAST permite o enriquecimento com poucos exemplos a partir de 35 amostras em nosso experimento. Ele usa uma arquitetura para todos os conjuntos de dados de tarefas e mantém a cabeça do modelo. Mas adiciona uma fase de reformulação. Ele é um conjunto de treinamento aumentado e precisa de um valor alvo com significado semântico para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação por frase. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos, hoje vou apresentar o nosso trabalho de pesquisa, Aprendizagem para Raciocínio Detetivesco, Resolução de Problemas Metabólicos como Extração Complexa de Razões. Sou Alan do ByteDance AI Lab, e este é um trabalho conjunto com Jerry da Universidade de Texas em Austin e Weilu da SUTD. Primeiramente, gostaria de falar sobre a nossa motivação para o raciocínio. Então aqui mostramos um exemplo onde o raciocínio em várias etapas é útil. Então esta figura é retirada do trabalho \"pen on paper\" onde eles realizam o prompting para resolver o problema de matemática em um cenário de aprendizagem com poucos exemplos (few-shot learning). Então, no lado esquerdo, podemos ver que, se darmos alguns exemplos com apenas perguntas e respostas, podemos não ser capazes de obter as respostas corretas. Mas se darmos algumas descrições de raciocínio mais detalhadas, o modelo é capaz de prever a descrição do raciocínio e também fazer uma predição correta aqui. Portanto, é bom ter um raciocínio em várias etapas interpretável como saída. E nós também pensamos que o problema de método é uma aplicação direta para avaliar essas habilidades de raciocínio. Então aqui, na nossa configuração de problema, dado o problema, precisamos resolvê-lo e obter as respostas numéricas. Nos nossos conjuntos de dados, também temos a expressão matemática, que leva a essa resposta particular, também. Certas suposições também se aplicam, como no trabalho anterior. Assumimos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtrações, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser realmente decompostos nesses operadores básicos. Então trabalhos anteriores na resolução de problemas de método podem ser categorizados como sequência para sequência e modelo sequência para árvore. O modelo sequência para sequência tradicional converte a expressão em uma sequência específica para geração, e é bastante fácil de implementar, e pode generalizar para muitos problemas complicados diferentes. Mas a desvantagem do desempenho é, na verdade, geralmente não é melhor que o modelo de estrutura. E ele carece de interpretabilidade para a predição. Mas, na verdade, esta direção ainda é bastante popular por causa do modelo transformer. Em modelos baseados em árvore, nós realmente estruturamos essas expressões na forma de uma árvore e seguimos uma travessia em pré-ordem em três gerações. Em modelos baseados em árvore, nós realmente estruturamos essas expressões na forma de uma árvore e seguimos uma travessia em pré-ordem em três gerações. Então aqui nós continuamos gerando os operadores até atingirmos as folhas, que são as quantidades. Então a boa coisa é que, na verdade, isso nos dá esta estrutura de árvore binária. Estrutura e é, mas na verdade é bastante contra intuitivo porque geramos o operador primeiro e então, no final, geramos a estrutura, mas na verdade é bastante contra intuitivo porque geramos o operador primeiro e então, no final, geramos as quantidades. E a segunda coisa é que também contém alguns cálculos repetitivos. Então, se olharmos para esta expressão, a vezes 3 mais 3, é realmente gerada duas vezes. Mas, de fato, devemos reutilizar os resultados, então, na nossa abordagem proposta, queremos resolver esses problemas de maneira passo a passo e interpretável, então, por exemplo, aqui no segundo passo, podemos obter este divisor que é 27 e podemos também retornar às perguntas originais para encontrar o conteúdo relevante. E nesses passos, obtemos os divisores. Então e então, neste terceiro passo, nós realmente obtemos o quociente. Certo. E após esses três passos, podemos realmente reutilizar os resultados do segundo passo e obter os resultados do quarto passo. E então, finalmente, podemos obter os dividendos. Então aqui, nós realmente geramos toda a expressão diretamente, em vez de gerar operadores ou quantidades individuais. Isso torna o processo mais preciso. Em nosso sistema dedutivo, nós começamos com um monte de quantidades apresentadas nas perguntas, e também incluindo algumas constantes como nossos estados iniciais. A expressão é representada por EIJOP, onde nós executamos o operador de QI para QJ, e essa expressão é realmente direcionada. Nós também temos subtração reversa aqui para representar a direção oposta. Isso é bastante semelhante à extração de relações. Em um sistema dedutivo formal, no momento t, nós aplicamos o operador entre o par QI e QJ, e então obtemos esta nova expressão. Nós a adicionamos ao próximo estado para se tornar uma nova quantidade. Este slide realmente visualiza a evolução dos estados onde mantemos adicionando expressões ao estado atual. Em nossas implementações de modelo, nós primeiro usamos um modelo de linguagem pré-treinado, que pode ser Birds ou Rabbits, e então codificamos uma frase, e então obtemos essas representações de quantidade. Uma vez que obtemos as representações de quantidade, podemos começar a fazer inferência. Aqui mostramos um exemplo de Q1 para obter a representação para Q1 dividido por Q2 e então vezes Q3. Primeiro obtemos a representação do par, que é basicamente apenas a concatenação entre Q1 e Q2. E então aplicamos uma rede feedforward, que é parametrizada pelo operador. E então, finalmente, obtemos a representação da expressão Q1 dividido por Q2. Mas na prática, na fase de inferência, podemos ser capazes de obter a expressão incorreta também. Então todas as expressões possíveis são iguais a três vezes o número de operadores. A coisa boa aqui é que podemos facilmente adicionar restrições para controlar este espaço de busca. Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão em nosso espaço de busca. No segundo passo, fazemos a mesma coisa, mas a única diferença é uma quantidade a mais. Esta quantidade vem da expressão calculada anteriormente. Então, finalmente, podemos obter esta expressão final, Q3 vezes Q4. E também podemos ver que o número de todas as expressões possíveis é diferente do passo anterior. Tal diferença torna difícil aplicar a busca em feixe (beam search) porque a distribuição de probabilidade entre esses dois passos é desequilibrada. O procedimento de treinamento é semelhante ao treinamento de um modelo sequência para sequência, onde otimizamos a perda em cada passo de tempo. E aqui também usamos este tau para representar quando devemos terminar este processo de geração. E aqui, o espaço é diferente de sequência para sequência, porque o espaço é diferente em cada passo de tempo, em um modelo sequência para sequência tradicional é o número de vocabulário e também nos permite impor certas restrições do conhecimento anterior, então realizamos experimentos nos conjuntos de dados de problemas de método comumente usados, MAWPS, Math23K, MathQA e SWAMP. E aqui mostramos brevemente os resultados em comparação com as melhores abordagens anteriores. Nossa variante com melhor desempenho é o Robeta deductive reasoner. Na verdade, não usamos a busca em feixe, em contraste. As melhores abordagens são frequentemente modelos baseados em árvore. No geral, nosso solucionador consegue superar significativamente este modelo baseado em árvore, mas podemos ver o número absoluto em MathQA ou SWAMP não são realmente altos. Investigamos ainda mais os resultados em SWAMP, e este conjunto de dados é desafiador porque o autor tentou adicionar manualmente algo para confundir o modelo de PNL, como adicionar informações irrelevantes e quantidades extras. Em nossa predição, encontramos alguns dos valores intermediários são realmente negativos. Por exemplo, nesta pergunta, estamos perguntando quantas maçãs Jake tem, mas temos algumas informações extras como 17 menos peras, e Steven tem 8 peras, o que é totalmente irrelevante. Nosso modelo faz algumas predições como esta, que está produzindo valores negativos. E observamos que essas duas expressões realmente têm pontuações semelhantes. Podemos realmente limitar este espaço de busca removendo aqueles resultados como negativos, para que possamos tornar a resposta correta. Achamos que tal restrição realmente melhora muito para alguns modelos. Por exemplo, para Birds, melhoramos em sete pontos. E então para o modelo baseado em Robeta, na verdade, melhoramos em dois pontos. Um melhor modelo de linguagem tem uma melhor capacidade de compreensão da linguagem, então o número aqui é mais alto para Robeta e mais baixo para Birds. Também tentamos analisar a dificuldade por trás de todos estes conjuntos de dados. Assumimos que o número de quantidades não utilizadas pode ser considerado informação irrelevante aqui. Aqui podemos ver que temos a porcentagem de amostras com quantidades não utilizadas, e o conjunto de dados SWAMP tem a maior proporção. Aqui também mostramos o desempenho geral. Para aquelas amostras sem quantidades não utilizadas, o desempenho geral é realmente mais alto do que o desempenho geral. Mas com aquelas amostras com quantidade não utilizada, é realmente muito pior do que o desempenho geral. Para MAWPS, não temos muitos casos de morte, então apenas ignoro esta parte. Finalmente, queremos mostrar a interpretabilidade através de um exemplo de falha e participação. Aqui, nosso modelo realmente faz uma predição errada no primeiro passo. Podemos realmente correlacionar esta expressão com a frase aqui. Acreditamos que esta frase pode estar enganando o modelo para uma predição incorreta. Aqui, imprimir outro 35 faz o modelo pensar que deve ser um operador de adição. Tentamos revisar a frase para algo como o número de árvores de pera são 55 a menos do que as árvores de maçã. Fazemos com que transmita semântica mais precisa, de modo que o modelo possa fazer a predição correta. Este estudo mostra como as predições interpretáveis nos ajudam a entender o comportamento do modelo. Para concluir nosso trabalho, primeiro nosso modelo é realmente eficiente, e somos capazes de fornecer um procedimento de resolução interpretável. Podemos facilmente incorporar algum conhecimento prévio como restrição, o que pode ajudar a melhorar o desempenho. E a última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de método, mas também a outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada em diferentes passos de tempo, também é bastante desafiador aplicar a estratégia de busca em feixe. É isso o fim da palestra, e perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Apresentarei meu trabalho em conjunto com Jerry, que trata de um novo conjunto de dados para recuperação de artigos estatutários. Questões legais são uma parte integrante da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência dispendiosa de um especialista jurídico ficam desprotegidos ou, pior, explorados. Nosso trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo sistemas de recuperação eficazes para artigos estatutários. Tal sistema poderia fornecer um serviço de assistência jurídica gratuito para pessoas sem qualificação. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre um assunto legal, como qual o risco se eu violar a confidencialidade profissional, um modelo é necessário para recuperar todos os artigos estatutários relevantes de um vasto corpo de legislação. Esta tarefa de recuperação de informações apresenta o seu próprio conjunto de desafios. Primeiro, lida com dois tipos de linguagem, a linguagem natural comum para as perguntas e a linguagem jurídica complexa para os estatutos. Esta diferença na distribuição da linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois requer indiretamente um sistema de interpretação inerente que possa traduzir uma pergunta natural para uma pergunta jurídica que corresponda à terminologia dos estatutos. Além disso, a lei estatutária não é um conjunto de artigos independentes que possam ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas no seu contexto geral, ou seja, em conjunto com a informação suplementar dos seus artigos vizinhos, os campos e subcampos a que pertencem e o seu lugar na estrutura da lei. Por último, os artigos estatutários estão em parágrafos pequenos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação. Aqui, há documentos longos que podem ter até 6.000 palavras. Os avanços recentes em PLN (Processamento de Linguagem Natural) despertaram um grande interesse em muitas tarefas jurídicas, como a previsão de decisões judiciais ou a revisão automatizada de contratos, mas a recuperação de artigos estatutários permaneceu largamente intocada devido à falta de conjuntos de dados grandes e de alta qualidade rotulados. Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão nativo da França para verificar se os modelos de recuperação podem aproximar a eficiência e a confiabilidade de especialistas jurídicos para a tarefa de recuperação de artigos estatutários. Nosso conjunto de dados de recuperação de artigos estatutários belgas, PSART, consiste em mais de 1100 perguntas legais feitas por cidadãos belgas. Estas perguntas cobrem uma vasta gama de tópicos, desde família, habitação, dinheiro, até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de 22.600 artigos legais dos códigos de lei belgas. Vamos agora falar sobre como coletamos este conjunto de dados. Primeiro, começamos compilando um grande corpus de artigos legais. Consideramos 32 códigos belgas publicamente disponíveis e extraímos todos os seus artigos, bem como os títulos de seção correspondentes. Então, reunimos perguntas legais com referências a estatutos relevantes. Para isso, associamo-nos a um escritório de advocacia belga que recebe cerca de 4.000 e-mails por ano de cidadãos belgas que pedem conselhos sobre um problema legal pessoal. Tivemos a sorte de ter acesso aos seus sites, onde sua equipe de juristas experientes aborda os problemas legais mais comuns da Bélgica. Coletamos milhares de perguntas, anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por último, analisamos as referências legais e filtramos as perguntas cujas referências não eram artigos em um dos códigos de lei que consideramos. As referências restantes foram correspondidas e convertidas nos IDs de artigo correspondentes do nosso corpus. Acabamos com 1108 perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso grande corpus de 22.633 artigos estatutários. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação dos seus títulos subsequentes na estrutura da lei. Esta informação extra não é usada no presente trabalho, mas pode ser de interesse para pesquisas futuras sobre recuperação de informações jurídicas ou classificação de texto jurídico. Vamos dar uma olhada em algumas características do nosso conjunto de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com um comprimento mediano de 77 palavras, com 142 deles excedendo 1000 palavras, sendo o mais longo com até 5790 palavras. Como mencionado anteriormente, as perguntas cobrem uma vasta gama de tópicos, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os restantes 15% dizem respeito à segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de 32 diferentes códigos belgas que cobrem um grande número de tópicos legais. Aqui está o número total de artigos coletados de cada um destes códigos belgas. Dos 22.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta no conjunto de dados. E cerca de 80% destes artigos citados vêm do código civil, código judicial, código de investigação criminal ou códigos penais. Enquanto isso, 18 de 32 códigos têm menos de 5 artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esses códigos se concentram menos nos indivíduos e suas preocupações. No geral, o número mediano de citações para esses artigos citados é 2, e menos de 25% deles são citados mais de 5 vezes. Usando nossos conjuntos de dados, comparamos vários métodos de recuperação, incluindo arquiteturas lexicais e densas. Dado uma consulta e um artigo, um modelo lexical atribui uma pontuação ao par consulta-artigo calculando a soma sobre os termos da consulta dos pesos de cada um desses termos nesse artigo. Experimentamos as funções de classificação padrão TF-IDF e BM25. O principal problema com essas abordagens é que elas só podem recuperar artigos que contenham palavras-chave presentes na consulta. Para superar essa limitação, experimentamos uma arquitetura baseada em rede neural que pode capturar relações semânticas entre consultas e artigos. Usamos um modelo b-encoder que mapeia consultas e artigos em representações vetoriais densas e calcula uma pontuação relevante entre um par consulta-artigo pela similaridade de seus embeddings. Esses embeddings geralmente resultam de uma operação de pooling na saída de um modelo de embedding de palavras. Primeiro, estudamos a eficácia de b-encoders siameses em uma configuração de avaliação zero-shot, o que significa que modelos de embedding de palavras pré-treinados são aplicados por padrão sem nenhum ajuste fino adicional. Experimentamos com codificadores de texto independentes de contexto, nomeadamente Word2Vec e FastText, e modelos de embedding dependentes de contexto, nomeadamente Robota e, mais especificamente, Camembert, que é um modelo Robota francês. Adicionalmente, treinamos nossos próprios modelos b-encoders baseados em Camembert em nosso conjunto de dados. Note que para treinamento, experimentamos as duas variantes da arquitetura b-encoder. Siameses, que usa um modelo de embedding de palavras único que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado, e toTower, que usa dois modelos de embedding de palavras independentes que codificam a consulta e o artigo separadamente em diferentes espaços de embedding. Experimentamos com pooling médio, máximo e CLS, bem como produto escalar e cosseno para calcular similaridades. Aqui estão os resultados de nossas linhas de base no conjunto com o le os métodos lexicais acima, os b-encoders siameses avaliados em uma configuração zero-shot no meio e os b-encoders ajustados abaixo. No geral, os b-encoders ajustados superam significativamente todas as outras linhas de base. O modelo two-tower melhora sua variante siamesa no recall em 100, mas tem um desempenho semelhante nas outras métricas. Embora o BM25 tenha apresentado um desempenho inferior ao B-Encoder treinado significativamente, seu desempenho indica que ele ainda é uma linha de base forte para recuperação específica de domínio. Quanto à avaliação zero-shot do B-Encoder siamês, descobrimos que o uso direto dos embeddings de um modelo Camembert pré-treinado sem otimizar para a tarefa de recuperação de informações fornece resultados ruins, o que é consistente com achados anteriores. Além disso, observamos que o b-encoder baseado em Word2Vec superou significativamente o modelo FastText e BERT, sugerindo que talvez os embeddings de nível de palavra pré-treinados sejam mais apropriados para a tarefa do que os embeddings de nível de caractere ou subpalavra quando usados por padrão. modelo, sugerindo que talvez os embeddings de nível de palavra pré-treinados sejam mais apropriados para a tarefa do que os embeddings de nível de caractere ou subpalavra quando usados por padrão. Embora promissores, esses resultados sugerem amplas oportunidades de melhoria em comparação com um especialista jurídico qualificado que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Concluiremos discutindo duas limitações de todos os conjuntos de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos 32 códigos belgas considerados, o que não cobre toda a lei belga, pois faltam artigos de decretos, diretivas e ordens. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas terminem com apenas uma fração do número inicial de artigos relevantes. Esta perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja totalmente apropriada. Segundo, devemos notar que nem todas as perguntas legais podem ser respondidas com estatutos sozinhos. Por exemplo, a pergunta, posso despejar meus inquilinos se eles fizerem muito barulho, pode não ter uma resposta detalhada na lei estatutária que quantifique um limite específico de ruído no qual o despejo é permitido. Em vez disso, o proprietário provavelmente dependerá mais do direito dos casos e encontrará precedentes semiar aquele que quantifique um limite específico de ruído no qual o despejo é permitido. Em vez disso, o proprietário provavelmente dependerá mais do direito dos casos e encontrará precedentes similares à sua situação atual. Por exemplo, o inquilino faz duas festas por semana até as 2h. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas ainda precisa ser determinado. Esperamos que nosso trabalho desperte o interesse em desenvolver modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode verificar nosso artigo, conjunto de dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá! Estamos felizes em apresentar nosso trabalho sobre VALS, um benchmark independente de tarefas, destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de configurar este benchmark? Bem, nos últimos anos, vimos uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares imagem-texto. Cada um desses modelos impulsiona o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, ancoragem de frases. Então, recebemos uma mensagem – as precisões nesses benchmarks específicos de tarefas estão aumentando constantemente. Mas sabemos o que esses modelos realmente aprenderam? O que é que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta a esta imagem e a esta frase para corresponder, e uma pontuação baixa para esta outra? Os modelos de visão e linguagem estão focando no que é certo, ou estão focando em vieses, como demonstrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais independente de tarefas e introduzimos válvulas que testam a sensibilidade de modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguísticas quanto as visuais. Almejamos existência, pluralidade, contagem, relações espaciais, ações, correferência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? Por meio do FOILing, um método previamente aplicado para modelos de visão e linguagem, apenas para frases nominais por Ravi Shekhar e colaboradores, sobre contagem por nós em trabalhos anteriores. O FOILing basicamente significa que pegamos a legenda de uma imagem e produzimos um FOIL, alterando a legenda de forma que ela não descreva a imagem mais. E fazemos essas alterações de frase com foco em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e correferência de entidades, onde cada peça pode consistir em um ou mais instrumentos, caso encontrássemos mais de uma maneira interessante de criar instâncias de FOIL. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo de ação é alterado com uma ação diferente e outro em que os participantes são trocados. Contagem e correferência também são peças que têm mais de um instrumento. E criamos esses FOILs garantindo que eles falhem em descrever a imagem, que sejam frases gramaticais e, de resto, válidas. Isso não é fácil, porque uma legenda foilizada pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem corte plantas, e grandes modelos de visão e linguagem podem captar isso. Portanto, para obter FOILs válidos, devemos agir. Primeiro, fazemos uso de modelos de linguagem fortes para propor FOILs. Segundo, usamos inferência de linguagem natural, ou NLI curta, para filtrar FOILs que ainda podem estar descrevendo a imagem, já que, ao construir FOILs, precisamos garantir que eles falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com o seguinte raciocínio. Consideramos uma imagem como a premissa e sua legenda como o que é implicado. Além disso, consideramos a legenda como a premissa e o FOIL como sua hipótese. Se um modelo de NLI prevê que o FOIL contradiz ou é neutro em relação à legenda, tomamos isso como um indicador de um FOIL válido. Se um NLI prevê que o FOIL é implicado pela legenda, ele não pode ser um bom FOIL, pois, por transitividade, dará uma descrição verdadeira da imagem e filtramos esses FOILs. Mas este procedimento não é perfeito. É apenas um indicador para FOILs válidos, portanto, como uma terceira medida para gerar FOILs válidos, empregamos anotadores humanos para validar os dados usados no VALS. Assim, após a filtragem e avaliação humana, temos tantos exemplos de teste quanto descritos nesta tabela. Note que o VALS não fornece nenhum dado de treinamento, mas apenas dados de teste, já que é um benchmark de teste zero-shot apenas. É projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino apenas permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos sabemos que esses modelos gostam de trapacear e encontrar atalhos. E, como dissemos, estamos interessados em avaliar as capacidades que os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem no VALS, nomeadamente com CLIP, LXMERT, Wil VILBERT, VILBERT 12 em 1 e VISUALBERT. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos na classificação de pares imagem-frase em legendas e FOILs. Talvez mais relevante para este vídeo, demonstraremos nossa métrica mais permissiva, a precisão por pares, que mede se a pontuação de alinhamento imagem-frase é maior para o par imagem-texto correto do que para seu par FOIL. Para mais métricas e resultados sobre elas, confira nosso artigo. Os resultados com precisão por pares são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas. É que o melhor desempenho zero-shot é alcançado por Wilbert 12 em 1, seguido por Wilbert, Alex Mert, Clip e, finalmente, Visual Bird. É notável como os instrumentos centrados nos objetos individuais, como existência e frases nominais, são quase resolvidos por Wilbert 12 em 1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser resolvida de forma confiável em nossos cenários de foiling adversariais. Vemos pela peça de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos versus múltiplos ou em contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo que sejam suportados por vieses de plausibilidade, como vemos na peça de ações. Da peça de correferência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para os modelos de visão e linguagem. Como um teste de sanidade, e porque é um experimento interessante, também comparamos dois modelos de texto-apenas, GPT-1 e GPT-2, para avaliar se o VALS pode ser resolvido por esses modelos unimodais calculando a perplexidade da legenda correta e do FOIL e prevendo a entrada com a menor perplexidade. Se a perplexidade for maior para o FOIL, tomamos isso como uma indicação de que a legenda FOILizada pode sofrer de viés de plausibilidade ou de outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos de texto-apenas GPT capturaram melhor a plausibilidade do mundo do que os modelos de visão e linguagem. Em resumo, o VALS é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem, testando arduamente suas capacidades de aterramento visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados em sua presença em imagens bem, como demonstrado pela peça de existência, mas lutam para aterrar sua interdependência e relacionamentos em cenas visuais quando são forçados a respeitar indicadores linguísticos. Gostaríamos muito de encorajar a comunidade a usar o VALS para medir o progresso em direção ao aterramento da linguagem com modelos de visão e linguagem. E mais, o VALS pode ser usado como uma avaliação indireta de conjuntos de dados, já que os modelos podem ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo VALS. Se estiver interessado, confira os dados do VALS no GitHub e, se tiver alguma dúvida, não hesite em nos contatar."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamizawa, da Universidade de Tóquio. Apresentarei um artigo intitulado RNSUN, um conjunto de dados em larga escala para geração automática de notas de lançamento através da sumarização de registros de commits. Explicarei nesta ordem. Primeiro, apresentarei a geração automática de notas de lançamento em que estamos trabalhando nesta pesquisa. Nota de lançamento é um documento técnico que resume as alterações distribuídas com cada lançamento de um produto de software. A imagem mostra as notas de lançamento da versão 2.6.4 da biblioteca Vue.js. Notas de lançamento desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas para preparar manualmente. Portanto, seria muito útil ser capaz de gerar automaticamente notas de lançamento de alta qualidade. Referirei-me a duas pesquisas anteriores sobre geração automática de notas de lançamento. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de mudanças para extrair as diferenças principais, as mudanças na biblioteca e as mudanças no documento das diferenças entre os lançamentos, e, finalmente, combiná-los. A característica mais notável deste sistema é o extrator de problemas no canto superior direito, que deve ser vinculado ao Jira, o sistema de rastreamento de problemas, e só pode ser aplicado a projetos que usam Jira. Em outras palavras, ele não pode ser usado para muitos projetos no github. A segunda é Glyph, anunciada recentemente em 2020. Está disponível na internet e pode ser instalada via PIP. Este sistema possui um modelo simples de classificação de texto baseado em aprendizado e produz uma das cinco categorias, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna um rótulo corretivo ou de correção de bugs. Os dados de treinamento do Glyph são relativamente pequenos, cerca de 5000, e, nas revisões das experiências descritas abaixo, o desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas existem problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente notas de lançamento de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificador de alta qualidade usando apenas a mensagem do commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos um conjunto de dados Rnsum composto por cerca de 82.000 itens de dados, coletando dados de repositórios públicos do GitHub usando RR e alguns conjuntos de dados. Nosso conjunto de dados Rnsum, composto por cerca de 82.000 itens de dados, coletando dados de repositórios públicos do GitHub usando a API do GitHub. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dado. O lado esquerdo é a mensagem do commit e o lado direito é a nota de lançamento. As notas de lançamento são rotuladas como melhorias de faces, etc. Definimos o commit message e o lado direito são as notas de lançamento. As notas de lançamento são rotuladas como melhorias, correções de bugs, etc. Estabelecemos uma tarefa que recebe as mensagens de commit como entrada e produz as notas de lançamento rotuladas. Isso pode ser considerado uma tarefa de sumarização. Pré-definimos quatro rótulos: recursos, melhorias, correções de bugs, depreciações, remoções e alterações importantes. Estes foram definidos com base em pesquisas anteriores e outros fatores. Os nós menos importantes no canto inferior direito são extraídos dos nós menos importantes mostrados no canto inferior esquerdo. Neste momento, é necessário detectar os quatro rótulos que foram definidos antecipadamente. Mas os rótulos nem sempre são consistentes com cada repositório. Por exemplo, o rótulo melhorias inclui melhorias, aprimoramentos, otimizações e assim por diante. Preparamos uma lista de vocabulário de nossos rótulos de estudo para cada uma dessas variações notacionais, usamos para detectar a classe de nota de lançamento e corrigimos o texto da lista que segue como a frase de nota de lançamento para a classe que precisa identificar a versão anterior 2.5 para 18 e obter sua diferença. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de lançamentos e olhar para o antes e o depois. Criamos uma regra de correspondência heurística para obter as versões anterior e seguinte. Análise do conjunto de dados. No final, 7.200 repositórios e 82.000 itens de dados foram coletados. Além disso, o número médio de tokens de nota de lançamento é 63, o que é bastante alto para uma tarefa de sumarização. Além disso, o número de tokens únicos é bastante grande, 8.830.000. Isso se deve ao grande número de nomes de classe e método únicos encontrados no repositório. Em seguida, explicarei o método proposto. O modelo de sumarização extrativa-depois-abstrativa de nível de classe consiste em dois neurônios. O AS usa um classificador para classificar cada mensagem de commit em cinco classes de notas de lançamento. Escolhemos implementa, correções de bugs, depreciações mais e outras. As mensagens de commit classificadas como outras são descartadas. Então, o CEAS aplica o gerador aos quatro documentos de rótulo independentemente e gera notas de lançamento para cada classe. Nesta tarefa, as correspondências diretas entre mensagens de commit e notas de lançamento não são conhecidas. Portanto, para treinar o classificador, atribuímos pseudo-rótulos a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit. Modelamos a sumarização abstrativa específica da classe por meio de nossa abordagem por dois métodos diferentes. O primeiro modelo, que chamamos de cssingle, consiste em uma rede set-to-set única e gera um único longo texto de nó, dando, dando uma concatenação de mensagens de commit de entrada. O texto de saída redes, cada uma das quais corresponde a uma das classes menos conhecidas. Ok, deixe-me explicar o experimento. Cinco métodos foram comparados: CAS, CASSingle, CASMatch, PlusSelling e estudo anterior, GRIF. No que diz respeito à aberração, em alguns casos, CSMatch, Blustering e Estudo Anterior Glyph. No que diz respeito à avaliação, em alguns casos, as notas de lançamento são produzidas em várias frases. Como é difícil calcular o número de frases como elas são, elas são combinadas com espaços e tratadas como uma única frase longa. O azul é penalizado quando o sistema produz uma frase curta. Esta penalidade resulta em um valor azul inferior nos resultados do experimento descritos a seguir. Calculamos também a especificidade, porque rouge e blue não podem ser calculados se as notas de lançamento estiverem vazias. Uma alta especificidade significa que o modelo produz corretamente um texto vazio nos casos em que as notas de lançamento presumem que estão vazias. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores hash, etc., também avaliamos o conjunto de dados limpo, que os exclui. O CES e o CAS alcançaram pontuações de rouge L mais de 10 pontos acima das linhas de base. Em particular, no conjunto de teste limpo, a diferença de pontuação entre o método proposto e as linhas de base saltou para mais de 20 pontos. Esses resultados indicam que o CAS e o CAS são significativamente eficazes. O CAS obteve uma pontuação de root-A melhor do que o CAS, sugerindo que combinar um classificador e um gerador é eficaz no treinamento do classificador usando duplos. Uma alta cobertura do CAS pode ser alcançada, provavelmente, porque o classificador pode se concentrar em selecionar mensagens de commit relevantes para cada classe. O CAS match tendeu a produzir um log L maior do que o CAS single, sugerindo que também é eficaz desenvolver independentemente modelos de sumarização abstrativa diferentes para cada classe de nota de lançamento. Aqui estão a análise de erros. Os métodos CAS tendem a produzir frases mais curtas do que as frases de referência humana. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto o CAS tem apenas uma. A razão para essa relutância do modelo é que, nos dados de treinamento, apenas 33% das frases estão presentes no rótulo de recursos e 40% no rótulo de melhorias. Além disso, os métodos CES não podem gerar notas de lançamento precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito bagunçada, e a frase completa não pode ser gerada sem referência ao pedido ou problema correspondente. O exemplo abaixo mostra que as duas mensagens de commit de entrada estão relacionadas e devem ser combinadas em uma única frase, mas não o fazem. Finalmente, uma conclusão. Construímos um novo conjunto de dados para notação de lista automática. Também formamos a tarefa de inserir mensagens de commit e resumi-las para que seja aplicável a todos os projetos escritos em inglês. Nosso experimento mostra que o método proposto gera leads menos barulhentos em uma cobertura mais alta do que as linhas de base. Verifique nossa aba Descent only. Obrigado."}
