{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Asaf Harari et je présenterai notre article intitulé : « Enrichissement de données tabulaires en Few-Shot par le Fine-Tuning d'Architectures de Transformeurs ». Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes. Mais parfois, ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à partir de sources externes de texte libre. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à partir de sources externes de texte libre. Supposons que nous ayons un jeu de données tabulaire et une base de connaissances. Nous avons besoin d'un processus automatique qui implique l'établissement de liens d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre, FAST, est précisément ce processus automatique. Voyons donc un exemple. Dans un jeu de données intégré à FAST. Dans cet exemple, le jeu de données est un jeu de données universitaires dont l'objectif est de classer les universités en universités de bas rang et universités de haut rang. Le jeu de données est un jeu de données universitaires dont l'objectif est de classer les universités en universités de bas rang et universités de haut rang. Nous utilisons Wikipédia comme base de connaissances. La première phase de FAST est l'établissement de liens d'entités. Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances. Et le texte des entités de la base de connaissances est extrait et ajouté au jeu de données. Dans cet exemple, le texte est l'abstract de la page Wikipédia. Nous devons maintenant générer ou extraire des caractéristiques du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques, qui inclut l'analyse de texte. Et c'est la principale nouveauté de cet article, et j'y reviendrai en détail dans les prochaines diapositives. Après la phase d'extraction de caractéristiques, il existe une phase de génération de caractéristiques lors de laquelle nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Premièrement, générer des caractéristiques dans le nombre de classes du jeu de données d'origine. Dans cet exemple, le jeu de données d'origine comporte deux classes, donc FAST génère deux nouvelles caractéristiques. Mais si le jeu de données comporte cinq classes, FAST génère cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de la technique en matière d'analyse de texte, qui sont des modèles de langage basés sur des transformeurs tels que BERT, GPT, XNL, etc. Mais il est peu probable que nous puissions entraîner un modèle de langage en utilisant les jeux de données d'entrée. Une approche naïve serait un fine-tuning sur une tâche cible. Donc, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, fine-tuner le modèle de langage sur le jeu de données cible. Dans cet exemple, pour fine-tuner le modèle de langage, pour classer le texte en classes, l'abstract en classes, bas ou haut, recevoir la sortie du modèle de langage, qui est la probabilité pour chaque classe, et l'utiliser comme nouvelle caractéristique. Le problème avec cette approche est que le jeu de données peut avoir peu d'entités distinctes avec des tags. De nouvelles caractéristiques. Le problème avec cette approche est que les jeux de données peuvent avoir peu de tags d'entités distincts. Dans notre expérience, près de la moitié des jeux de données contiennent moins de 400 échantillons, et le plus petit jeu de données contenait 35 échantillons dans son ensemble d'apprentissage. Il serait donc inefficace de fine-tuner un modèle de langage sur ce jeu de données. Mais nous pouvons utiliser les connaissances antérieures sur les jeux de données pré-analysés, les jeux de données, et utiliser ces informations lorsque nous analysons le nième jeu de données. Ce que nous suggérons est d'ajouter une autre phase de fine-tuning, une phase préliminaire de fine-tuning multitask, lorsque vous fine-tunez le modèle de langage sur n-1 jeux de données, puis nous exécutons une autre phase de fine-tuning, qui est un fine-tuning sur une tâche cible lorsque nous fine-tuner le modèle de langage sur le nième jeu de données cible. L'état de la technique en matière de fine-tuning multitask s'appelle mtDNN. Dans mtDNN, mtDNN maintient des têtes dans le nombre de tâches dans l'ensemble d'apprentissage. Ainsi, dans cet exemple, il y a quatre tâches dans l'ensemble d'apprentissage, donc mtDNN maintient quatre têtes, comme vous pouvez le voir. Dans cet exemple, il y a quatre tâches dans l'ensemble d'apprentissage. Donc mtDNN maintient quatre têtes, comme vous pouvez le voir dans l'image, et il échantillonne un lot aléatoire à partir de l'ensemble d'apprentissage. Et si le lot aléatoire appartient, par exemple, à des tâches de classification de phrases uniques, il exécute une passe avant et arrière à travers la première tête. Et si le lot aléatoire appartient à une tâche de classement par paires, il exécute une passe avant et arrière à travers la dernière tête. Dans notre scénario, les jeux de données tabulaires varient en nombre de classes. Ainsi, dans notre scénario, un jeu de données Tableau vérifie le nombre de classes. Il existe donc de nombreuses tâches. MTDNN maintient le nombre de classes de têtes, de couches de sortie, et de plus, MTDNN doit initialiser de nouvelles têtes pour un nouveau jeu de données avec une nouvelle tâche. Notre approche, appelée fine-tuning par reformulation de tâches, consiste à ce que, dans notre approche, le fine-tuning par reformulation de tâches, au lieu de maintenir plusieurs têtes, nous reformulons chaque jeu de données en un problème de classification de phrase par classe, qui est deux tâches de classes. Voyons donc un exemple. Voici notre jeu de données d'entrée, qui consiste en des entités, des caractéristiques, du texte et des classes. Et nous transformons la classification du texte en bas et en haut en classifiant le texte, en classifiant l'abstract et la classe, en classifiant l'abstract et la classe si l'abstract appartient à la classe ou non. Ainsi, le vecteur d'étiquette dans ce cas, reste toujours, qui consiste toujours en deux classes. Dans ce cas, cela reste toujours, qui consiste toujours en deux classes. Et c'est. Ensuite, il reformule la tâche en tâches de classification de phrase par classe. Appliquez le modèle de langage à la nouvelle tâche et obtenez la probabilité pour chaque classe. Et notez que le modèle de langage est déjà fine-tuné sur N moins un jeu de données en utilisant un fine-tuning multitask préliminaire. Ensuite, nous utilisons le vecteur de sortie de l'ensemble de données en utilisant un fine-tuning multitask préliminaire. Ensuite, nous utilisons le vecteur de sortie du modèle de langage comme une nouvelle caractéristique dans le nombre de classes. Pour évaluer notre framework, nous utilisons un jeu de données de classification tabulaire composé de 17 jeux de données, qui vérifie la taille, les caractéristiques, l'équilibre, le domaine et la performance initiale. Et comme base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation one-out en direct lorsque nous formons FAST sur 16 jeux de données et l'appliquons au 17e jeu de données. Nous divisons également chaque jeu de données en quatre faux et appliquons une validation croisée à quatre faux. Ensuite, nous générons la nouvelle caractéristique et l'évaluons à l'aide de cinq classificateurs d'évaluation. Pour la validation croisée à faux. Ensuite, nous générons la nouvelle caractéristique et l'évaluons à l'aide de cinq classificateurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur des transformateurs. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre framework au fine-tuning d'un jeu de données cible, au fine-tuning sur une tâche cible et à un fine-tuning multitask préliminaire et notre fine-tuning reformulé obtient le meilleur résultat, la meilleure performance, tandis que mtDNN obtient un gain de deux pour cent sur le fine-tuning du jeu de données cible, notre approche obtient un gain de six pour cent. Lorsque nous regardons les petits jeux de données, nous pouvons constater que la performance de mtDNN diminue et que l'amélioration du fine-tuning unique de la tâche cible. En résumé, FAST permet un enrichissement en few-shot à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour toutes les tâches, les jeux de données, et il maintient la tête du modèle. Mais il ajoute une phase de reformulation. Il est augmenté ensemble d'apprentissage, et il a besoin d'une valeur cible ayant une signification sémantique afin que nous puissions l'intégrer dans le modèle de langage et l'utiliser dans le problème de classification de phrase par classe. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais vous présenter notre travail de recherche, « Learning to Reason Detectively, Metabolic Problem Solving as Complex Reason Extraction ». Je suis Alan du laboratoire ByteDance AI, et ceci est un travail conjoint avec Jerry de l'Université du Texas à Austin, et Weilu de SUTD. Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement. Voici un exemple où un raisonnement en plusieurs étapes est utile. Cette figure est tirée du travail « pen on paper », où ils utilisent le prompting pour résoudre un problème de mathématiques dans un scénario d'apprentissage avec quelques exemples. Sur le côté gauche, nous pouvons constater que si nous fournissons quelques exemples avec uniquement des questions et des réponses, nous ne sommes peut-être pas en mesure d'obtenir les réponses correctes. Mais si nous fournissons une description de raisonnement plus détaillée, le modèle est capable de prédire la description du raisonnement et également de faire une prédiction correcte ici. Il est donc avantageux d'avoir un raisonnement en plusieurs étapes interprétable comme résultat. Nous pensons également que le problème de méthode constitue une application directe pour évaluer ces capacités de raisonnement. Ici, dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques. Dans nos ensembles de données, nous avons également l'expression mathématique qui mène à cette réponse particulière. Certaines hypothèses s'appliquent également, comme dans les travaux précédents. Nous supposons que la précision des quantités est connue et que nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, des opérateurs complexes peuvent en réalité être décomposés en ces opérateurs de base. Les travaux précédents sur la résolution de problèmes de méthode se répartissent en réalité en modèles séquence à séquence et séquence à arbre. Les modèles séquence à séquence traditionnels convertissent l'expression en une séquence spécifique pour la génération, ce qui est assez facile à mettre en œuvre, et peut se généraliser à de nombreux problèmes complexes différents. Cependant, l'inconvénient est que les performances sont généralement pas meilleures que celles des modèles structurés. Ils manquent également d'interprétabilité pour la prédiction. Mais cette direction est toujours assez populaire en raison des modèles Transformer. Dans les modèles basés sur des arbres, nous structurons en réalité ces expressions sous forme d'arbre et suivons un parcours préfixé dans les trois générations. Dans les modèles basés sur des arbres, nous structurons en réalité ces expressions sous forme d'arbre et suivons un parcours préfixé dans la génération d'arbres. Nous continuons à générer les opérateurs jusqu'à atteindre les feuilles, qui sont les quantités. Le bon point ici est qu'il nous donne en réalité cette structure d'arbre binaire. La structure est et il est mais en réalité c'est assez contre-intuitif car nous générons l'opérateur en premier, puis à la fin nous générons les quantités. Et la deuxième chose est qu'il contient également certains calculs répétitifs. Ici, si nous regardons cette expression, a multiplié par 3 plus 3, est en réalité générée deux fois. Mais en fait, nous devrions réutiliser les résultats. Dans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et interprétable. Par exemple, ici, à la deuxième étape, nous pouvons obtenir ce diviseur qui est 27 et nous pouvons également faire référence aux questions originales pour trouver le contenu pertinent. Et dans ces étapes, nous obtenons les diviseurs. Et ensuite, à cette troisième étape, nous obtenons en réalité le quotient. Bien. Et après ces trois étapes, nous pouvons en réalité réutiliser les résultats de la deuxième étape et obtenir les résultats de la quatrième étape. Et enfin, nous pouvons obtenir le dividende. Nous générons en réalité l'expression entière directement plutôt que de générer des opérateurs ou des quantités individuels. Cela rend le processus plus précis. Dans notre système déductif, nous commençons en réalité par un ensemble de quantités présentées dans les questions, et également en incluant certaines constantes comme états initiaux. L'expression est représentée par EIJOP, où nous effectuons l'opérateur de QI à QJ, et cette expression est en réalité dirigée. Nous avons également une soustraction inverse ici pour représenter la direction opposée. Ceci est assez similaire à l'extraction de relations. Dans un système déductif formel, au moment t, nous appliquons l'opérateur entre la paire QI et QJ, puis nous obtenons cette nouvelle expression. Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité. Cette diapositive visualise en réalité l'évolution des états où nous ajoutons continuellement des expressions à l'état actuel. Dans nos implémentations de modèles, nous utilisons d'abord un modèle de langage pré-entraîné qui peut être Birds ou Rabbits, puis nous encodons une phrase, et nous obtenons ces représentations de quantités. Une fois que nous avons les représentations de quantités, nous pouvons commencer à faire de l'inférence. Ici, nous montrons un exemple de Q1 pour obtenir la représentation de Q1 divisé par Q2 et puis multiplié par Q3. Nous obtenons d'abord la représentation de la paire, qui est essentiellement juste la concaténation entre Q1 et Q2. Ensuite, nous appliquons un réseau feedforward, qui est paramétré par l'opérateur. Et enfin, nous obtenons la représentation de l'expression Q1 divisé par Q2. Mais en pratique, dans la phase d'inférence, nous pourrions également obtenir l'expression incorrecte. Ici, toutes les expressions possibles sont égales à trois fois le nombre d'opérateurs. La bonne chose ici est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche. À la deuxième étape, nous effectuons la même chose, mais la seule différence est une quantité de plus. Cette quantité provient de l'expression calculée précédemment. Finalement, nous pouvons obtenir cette expression finale, Q3 multiplié par Q4. Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape précédente. De telles différences rendent difficile l'application de la recherche en faisceau car la distribution de probabilité entre ces deux étapes est déséquilibrée. La procédure d'entraînement est similaire à l'entraînement d'un modèle séquence à séquence, où nous optimisons la perte à chaque pas de temps. Ici, nous utilisons également cette tau pour représenter quand nous devons terminer ce processus de génération. Et l'espace est différent de la séquence à séquence, car l'espace est différent à chaque pas de temps, dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaires et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures. Nous effectuons des expériences sur les ensembles de données de problèmes de méthode couramment utilisés, mawps math23k mathqa MATHQA et SWAM. Ici, nous montrons brièvement les résultats par rapport aux meilleures approches précédentes. Notre variante la mieux performante est Robeta deductive reasoner. En fait, nous n'utilisons pas la recherche en faisceau contrairement aux meilleures approches qui sont souvent des modèles basés sur des arbres. Dans l'ensemble, notre raisonneur est capable de surpasser significativement ces modèles basés sur des arbres, mais nous pouvons voir que les nombres absolus sur MathQA ou SWAMP ne sont pas vraiment élevés. Nous avons donc enquêté plus avant sur les résultats sur Swamp, et cet ensemble de données est difficile car l'auteur a essayé d'ajouter manuellement quelque chose pour dérouter le modèle NLP, tel que l'ajout d'informations non pertinentes et de quantités supplémentaires. Dans notre prédiction, nous constatons que certaines des valeurs intermédiaires sont en fait négatives. Par exemple, dans cette question, nous demandons combien de pommes Jake a, mais nous avons des informations supplémentaires telles que 17 pêches de moins, et Steven a 8 pêches, ce qui est totalement non pertinent. Notre modèle fait donc certaines prédictions telles que la production de valeurs négatives. Nous observons que ces deux expressions ont des scores similaires. Nous pouvons donc en réalité limiter cet espace de recherche en supprimant ces résultats en tant que négatifs, afin de rendre la réponse correcte. Nous avons donc découvert que cette contrainte améliore en réalité considérablement certains modèles. Par exemple, pour Birds, nous avons amélioré de sept points. Et pour le modèle basé sur Robeta, nous avons en réalité amélioré de deux points. Un meilleur modèle de langage a une meilleure capacité de compréhension du langage, de sorte que le nombre ici est plus élevé pour Robeta et plus faible pour Birds. Nous avons également essayé d'analyser la difficulté qui se cache derrière tous ces ensembles de données. Nous supposons que le nombre de quantités inutilisées peut être considéré comme des informations non pertinentes ici. Ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et l'ensemble de données SWAMP a la plus grande proportion. Ici, nous montrons également la performance globale. Pour les échantillons sans quantités inutilisées, la performance globale est en réalité supérieure à la performance globale. Mais avec ces échantillons avec des quantités inutilisées, c'est en réalité beaucoup pire que la performance globale. Pour MAWPS, nous n'avons pas trop de cas de décès, nous ignorons donc cette partie. Finalement, nous voulons montrer l'interprétabilité grâce à un exemple de crash et de participation. Ici, notre modèle fait en réalité une mauvaise prédiction à la première étape. Nous pouvons en réalité corréler cette expression avec la phrase ici. Nous pensons que cette phrase pourrait tromper le modèle en lui faisant faire une prédiction incorrecte. Ici, imprimer un autre 35 fait penser au modèle qu'il devrait s'agir d'un opérateur d'addition. Nous essayons donc de réviser la phrase pour qu'elle soit du type « le nombre d'arbres fruitiers est de 55 de moins que le nombre d'arbres de pommes ». Nous la faisons en sorte qu'elle transmette une sémantique plus précise afin que le modèle puisse faire la prédiction correcte. Cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle. Pour conclure notre travail, notre modèle est en réalité assez efficace et nous sommes en mesure de fournir une procédure de résolution interprétable. Nous pouvons facilement intégrer certaines connaissances antérieures en tant que contrainte, ce qui peut aider à améliorer les performances. Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de méthode, mais également à d'autres tâches impliquant un raisonnement en plusieurs étapes. Nous avons également certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée. Et la deuxième chose est que, comme mentionné, comme la distribution de probabilité est déséquilibrée à différents pas de temps, il est également assez difficile d'appliquer une stratégie de recherche en faisceau. C'est la fin de la présentation, et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht. Je présenterai mon travail conjoint avec Jerry, qui porte sur un nouveau jeu de données pour la recherche d'articles de loi. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus juridiques fondamentaux. Par conséquent, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique restent sans protection, voire exploités. Notre travail vise à combler le fossé entre les citoyens et la loi en développant des systèmes de recherche d'articles de loi efficaces. Un tel système pourrait fournir un service d'aide juridique gratuit pour les non-juristes. Avant d'examiner la contribution principale de ce travail, décrivons d'abord le problème de la recherche d'articles de loi. Étant donné une question simple sur une question juridique, comme quelle est la sanction encourue en cas de violation du secret professionnel, un modèle doit être en mesure de récupérer tous les articles de loi pertinents à partir d'une vaste base de législation. Cette tâche de recherche d'information présente ses propres défis. Tout d'abord, elle traite de deux types de langage : le langage naturel courant pour les questions et le langage juridique complexe pour les statuts. Cette différence de distribution linguistique rend plus difficile pour un système de récupérer des candidats pertinents, car il nécessite indirectement un système d'interprétation inhérent capable de traduire une question formulée en langage courant en une question juridique qui correspond à la terminologie des statuts. De plus, le droit statutaire n'est pas un ensemble d'articles indépendants qui peuvent être traités comme une source d'information complète à part entière, comme le ferait l'actualité ou les recettes de cuisine, par exemple. Au contraire, c'est une collection structurée de dispositions juridiques qui n'ont un sens que lorsqu'elles sont considérées dans leur contexte global, c'est-à-dire conjointement avec les informations complémentaires provenant de leurs articles voisins, des domaines et sous-domaines auxquels ils appartiennent, et de leur place dans la structure du droit. Enfin, les articles de loi sont présentés en petits paragraphes, qui sont généralement l'unité de récupération typique dans la plupart des travaux de recherche. Ici, nous avons affaire à des documents longs pouvant contenir jusqu'à 6 000 mots. Les progrès récents en TAL ont suscité un grand intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements ou l'examen automatisé de contrats, mais la recherche d'articles de loi est restée principalement inexplorée en raison du manque de jeux de données annotés volumineux et de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur le citoyen, d'origine française, afin d'évaluer si les modèles de récupération peuvent approximer l'efficacité et la fiabilité des experts juridiques pour la tâche de recherche d'articles de loi. Notre jeu de données de recherche d'articles de loi belges, PSART, se compose de plus de 1100 questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, de la famille, au logement, en passant par l'argent, le travail et la sécurité sociale. Chacune d'elles a été annotée par des juristes expérimentés, avec des références à des articles pertinents provenant d'un corpus de plus de 22 600 articles de loi provenant des codes belges. Décrivons maintenant la façon dont nous avons collecté ce jeu de données. Premièrement, nous avons commencé par compiler un vaste corpus d'articles de loi. Nous avons examiné 32 codes belges accessibles au public et extrait tous leurs articles ainsi que les titres de section correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références à des statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ 4 000 e-mails de citoyens belges qui demandent des conseils sur une question juridique personnelle. Nous avons eu la chance d'avoir accès à leurs sites Web, où une équipe de juristes expérimentés répond aux questions juridiques les plus courantes des Belges. Nous avons collecté des milliers de questions, annotées avec des catégories, des sous-catégories et des références juridiques à des statuts pertinents. Enfin, nous avons analysé les références juridiques et filtré les questions dont les références n'étaient pas des articles de l'un des codes de loi que nous avions pris en compte. Les références restantes ont été mises en correspondance et converties en ID d'article correspondants à partir de notre corpus. Nous avons finalement abouti à 1108 questions, chacune soigneusement annotée avec les ID des articles pertinents de notre vaste corpus de 22 633 articles de loi statutaires. De plus, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de leurs titres de section suivants dans la structure du droit. Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient être intéressantes pour les recherches futures sur la recherche d'information juridique ou la classification de texte juridique. Examinons maintenant quelques caractéristiques de notre jeu de données. Les questions font entre 5 et 44 mots, avec une médiane de 40 mots. Les articles sont beaucoup plus longs, avec une médiane de 77 mots, dont 142 dépassent 1000 mots, le plus long contenant jusqu'à 5790 mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, environ 85 % d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les 15 % restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très diversifiés, car ils proviennent de 32 codes belges différents couvrant un grand nombre de sujets juridiques. Voici le nombre total d'articles collectés à partir de chacun de ces codes belges. Sur les 22 633 articles, seulement 1 612 sont référencés comme pertinents pour au moins une question du jeu de données. Et environ 80 % de ces articles cités proviennent du code civil, du code judiciaire, du code d'instruction criminelle ou des codes pénaux. Parallèlement, 18 des 32 codes ont moins de 5 articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de 2, et moins de 25 % d'entre eux sont cités plus de 5 fois. En utilisant nos jeux de données, nous avons testé plusieurs approches de récupération, notamment des architectures lexicales et denses. Étant donné une requête et un article, un modèle lexical attribue une note au couple requête-article en calculant la somme sur les termes de la requête des pondérations de chacun de ces termes dans cet article. Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard. Le principal problème de ces approches est qu'elles ne peuvent récupérer que les articles contenant les mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur un réseau neuronal qui peut capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle b-encodeur qui mappe les requêtes et les articles dans des représentations vectorielles denses et calcule un score pertinent entre une paire requête-article par la similarité de leurs embeddings. Ces embeddings résultent généralement d'une opération de pooling sur la sortie d'un modèle d'embedding de mots. Premièrement, nous étudions l'efficacité des b-encodeurs siamois dans un environnement d'évaluation zero-shot, ce qui signifie que les modèles d'embedding de mots pré-entraînés sont appliqués tels quels sans aucun ajustement supplémentaire. Nous expérimentons avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et FastText, et des modèles d'embedding contextuels, à savoir Robota et plus précisément Camembert, qui est un modèle Robota français. De plus, nous entraînons nos propres b-encodeurs basés sur Camembert sur nos jeux de données. Notez que pour l'entraînement, nous expérimentons avec les deux variantes de l'architecture b-encodeur. Siamois, qui utilise un modèle d'embedding de mots unique qui mappe les requêtes et les articles ensemble dans un espace vectoriel dense partagé, et toTower, qui utilise deux modèles d'embedding de mots indépendants qui encodent les requêtes et les articles séparément dans des espaces d'embedding différents. Nous expérimentons avec le pooling moyen, maximum et CLS, ainsi que le produit scalaire et le cosinus pour calculer les similarités. Voici les résultats de nos références initiales sur l'ensemble avec les méthodes lexicales ci-dessus, les b-encodeurs siamois évalués dans un environnement zero-shot au milieu et les b-encodeurs affinés en dessous. Dans l'ensemble, les b-encodeurs affinés surpassent significativement toutes les autres références. Le modèle à deux tours améliore sa variante siamois sur le rappel à 100, mais se comporte de manière similaire sur les autres mesures. Bien que BM25 ait sous-performé de manière significative le B-Encodeur entraîné, ses performances indiquent qu'il s'agit toujours d'une référence solide pour la récupération spécifique au domaine. Concernant l'évaluation zero-shot des B-Encodeurs siamois, nous constatons que l'utilisation directe des embeddings d'un modèle Camembert pré-entraîné sans optimisation pour la tâche de recherche d'information donne de mauvais résultats, ce qui est cohérent avec les découvertes antérieures. De plus, nous avons observé que le biancoder basé sur Word2Vec surpassait significativement le modèle FastText et BERT, ce qui suggère que les embeddings de mots pré-entraînés pourraient être plus appropriés pour la tâche que les embeddings de caractères ou de sous-mots lorsqu'ils sont utilisés tels quels. modèle, suggérant que peut-être les embeddings de mots pré-entraînés sont plus appropriés pour la tâche que les embeddings de caractères ou de sous-mots lorsqu'ils sont utilisés tels quels. Bien que prometteurs, ces résultats suggèrent qu'il existe une marge d'amélioration par rapport à un expert juridique compétent qui peut éventuellement récupérer tous les articles pertinents pour toute question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limitations de tous les jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des 32 codes belges pris en compte, ce qui ne couvre pas l'ensemble du droit belge, car les articles provenant des décrets, des directives et des ordonnances font défaut. Lors de la construction du jeu de données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions n'ont qu'une fraction du nombre initial d'articles pertinents. Cette perte d'informations implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée. Deuxièmement, il convient de noter que toutes les questions juridiques ne peuvent pas être répondues avec des statuts seuls. Par exemple, la question, puis-je expulser mes locataires s'ils font trop de bruit, pourrait ne pas avoir de réponse détaillée dans le droit statutaire quantifiant un seuil de bruit spécifique auquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des précédents similaires à leur situation actuelle. Par exemple, les locataires organisent deux soirées par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions sont plus adaptées que d'autres à la tâche de recherche d'articles de loi, et le domaine de celles qui ne le sont pas reste à déterminer. Nous espérons que notre travail suscitera un intérêt pour le développement de modèles pratiques et fiables de recherche d'articles de loi qui peuvent contribuer à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article, notre jeu de données et notre code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour ! Nous sommes heureux de vous présenter notre travail sur VALS, un *benchmark* indépendant des tâches, conçu pour tester les modèles vision et langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous eu la peine de mettre en place ce *benchmark* ? Eh bien, ces dernières années, nous avons assisté à une explosion des modèles vision et langage basés sur des transformateurs, pré-entraînés sur de grandes quantités de paires image-texte. Chacun de ces modèles repousse les limites de l’état de l’art sur les tâches vision et langage, telles que la réponse à des questions visuelles, le raisonnement de bon sens visuel, la récupération d’images, et l’ancrage de phrases. Nous avons donc reçu un signal : les niveaux de précision sur ces *benchmarks* spécifiques aux tâches augmentent régulièrement. Mais savons-nous ce que les modèles ont réellement appris ? Qu’est-ce qu’un transformateur vision et langage comprend lorsqu’il attribue un score élevé à cette image et à cette phrase pour qu’ils correspondent, et un score faible à l’autre ? Les modèles vision et langage se concentrent-ils sur la bonne chose, ou se concentrent-ils sur des biais, comme le montrent des travaux antérieurs ? Afin de mieux éclairer cet aspect, nous proposons une approche plus agnostique vis-à-vis des tâches et introduisons des « valves » qui testent la sensibilité des modèles vision et langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous ciblons l’existence, la pluralité, le comptage, les relations spatiales, les actions, la coréférence d'entités. Mais comment tester si les modèles vision et langage ont capturé ces phénomènes ? Grâce au FOILing, une méthode précédemment appliquée aux modèles vision et langage, uniquement pour les groupes nominaux par Ravi Shekhar et ses collaborateurs, et pour le comptage dans nos travaux antérieurs. Le FOILing consiste essentiellement à prendre la légende d’une image et à produire un faux-positive (FOIL) en modifiant la légende de sorte qu'elle ne décrive plus l’image. Nous effectuons ces modifications de phrase en nous concentrant sur six éléments spécifiques, tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence d'entités, chaque élément pouvant comprendre un ou plusieurs instruments si nous avons trouvé plus d'une façon intéressante de créer des instances FOIL. Par exemple, dans le cas de l'élément actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié par un autre action, et un dans lequel les actants sont inversés. Le comptage et la coréférence sont également des éléments qui comportent plus d'un instrument. Et nous créons ces FOILs en veillant à ce qu'ils ne décrivent pas l'image, qu'ils soient des phrases grammaticales et valides. Ce n’est pas facile à réaliser, car une légende falsifiée pourrait être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme qu'un homme coupe des plantes, et les grands modèles vision et langage pourraient s’en apercevoir. Par conséquent, pour obtenir des faux-positifs valides, nous devons agir. Premièrement, nous utilisons de puissants modèles de langage pour proposer des faux-positifs. Deuxièmement, nous utilisons l’inférence du langage naturel, ou NLI en abrégé, pour filtrer les faux-positifs susceptibles de décrire encore l’image, car lors de la construction des faux-positifs, nous devons nous assurer qu'ils ne décrivent pas l'image. Pour tester cela automatiquement, nous appliquons l’inférence du langage naturel avec la justification suivante. Nous considérons l’image comme la prémisse, et sa légende comme étant celle qu’elle implique. De plus, nous considérons la légende comme la prémisse et le FOIL comme étant son hypothèse. Si un modèle NLI prédit que le FOIL contredit ou est neutre par rapport à la légende, nous prenons cela comme un indicateur d’un FOIL valide. Si un modèle NLI prédit que le FOIL est impliqué par la légende, il ne peut pas être un bon FOIL, car par transitivité, il donnera une description vraie de l'image et nous filtrons ces FOILs. Mais cette procédure n’est pas parfaite. Il ne s’agit qu’un indicateur de FOIL valides, par conséquent, en tant que troisième mesure pour générer des FOIL valides, nous employons des annotateurs humains pour valider les données utilisées dans VALS. Ainsi, après filtrage et évaluation humaine, nous avons autant d’instances de test que décrites dans ce tableau. Notez que VALS ne fournit aucune donnée d’entraînement, mais uniquement des données de test, car il s’agit d’un *benchmark* de test à zéro tir. Il est conçu pour exploiter les capacités existantes des modèles vision et langage après le pré-entraînement. Le réglage fin ne permettrait qu’aux modèles d’exploiter des artefacts ou des biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l’avons dit, nous sommes intéressés par l’évaluation des capacités des modèles vision et langage après le pré-entraînement. Nous expérimentons avec cinq modèles vision et langage sur VALS, à savoir CLIP, LXMERT, Wil VILBERT, VILBERT 12 en 1 et VISUALBERT. Deux de nos mesures d’évaluation les plus importantes sont la précision des modèles dans la classification des paires image-phrase en légendes et en faux-positifs. Peut-être plus pertinent pour cette vidéo, nous présenterons notre métrique plus permissive, la précision par paires, qui mesure si le score d’alignement image-phrase est supérieur pour la paire image-texte correcte que pour sa paire falsifiée. Pour plus de métriques et de résultats concernant celles-ci, veuillez consulter notre article. Les résultats avec la précision par paires sont présentés ici et ils sont cohérents avec les résultats que nous avons obtenus à partir des autres métriques. C’est le modèle Wilbert 12 en 1 qui obtient les meilleures performances à zéro tir, suivi de Wilbert, Alex Mert, Klip et enfin Visual Bird. Il est à noter que les instruments axés sur les objets individuels, tels que l’existence et les groupes nominaux, sont presque résolus par Wilbert 12 en 1, ce qui souligne que les modèles sont capables d’identifier les objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos scénarios de faux-positifs adverses. Nous constatons, à partir des instruments de pluralité et de comptage, que les modèles vision et langage ont du mal à distinguer les références à des objets uniques ou multiples ou à les compter dans une image. L’élément relation montre qu’ils ont des difficultés à classifier correctement une relation spatiale nommée entre des objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même si c’est grâce à des biais de plausibilité, comme nous le constatons dans l’élément actions. À partir de l’élément coréférence, nous constatons que le suivi de plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles vision et langage. À titre de vérification de la cohérence, et parce que c’est une expérience intéressante, nous avons également évalué deux modèles uniquement textuels, GPT-1 et GPT-2, pour déterminer si VALS pouvait être résolu par ces modèles unimodaux, en calculant la perplexité de la légende correcte et du faux-positive et en prédisant l’entrée ayant la perplexité la plus faible. Si la perplexité est plus élevée pour le FOIL, nous prenons cela comme un indicateur que la légende falsifiée pourrait souffrir de biais de plausibilité ou d’autres biais linguistiques. Et il est intéressant de constater que, dans certains cas, les modèles de texte uniquement GPT ont mieux capturé la plausibilité du monde que les modèles vision et langage. Pour résumer, VALS est un *benchmark* qui utilise le prisme des constructions linguistiques pour aider la communauté à améliorer les modèles vision et langage en les testant rigoureusement sur leurs capacités de base visuelle. Nos expériences montrent que les modèles vision et langage identifient les objets nommés et leur présence dans les images, comme le montre l’élément d’existence, mais qu’ils ont du mal à relier leurs interdépendances et leurs relations dans les scènes visuelles lorsqu’ils doivent respecter les indicateurs linguistiques. Nous souhaitons vraiment encourager la communauté à utiliser VALS pour mesurer les progrès vers la base linguistique avec les modèles vision et langage. Et plus encore, VALS pourrait être utilisé comme une évaluation indirecte des ensembles de données, les modèles pouvant être évalués avant et après l’entraînement ou le réglage fin pour voir si un ensemble de données aide les modèles à s’améliorer sur l’un des aspects testés par VALS. Si vous êtes intéressé, veuillez consulter les données VALS sur GitHub et, si vous avez des questions, n’hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamizawa et je suis de l'Université de Tokyo. Je présenterai un article intitulé RNSUN, un ensemble de données à grande échelle pour la génération automatique de notes de version grâce à la summarisation des journaux de commits. J'expliquerai dans l'ordre suivant. Premièrement, je présenterai la génération automatique de notes de version sur laquelle nous travaillons dans cette recherche. Une note de version est un document technique qui résume les modifications distribuées avec chaque version d'un produit logiciel. L'image montre les notes de version pour la version 2.6.4 de la bibliothèque Vue.js. Les notes de version jouent un rôle important dans le développement open source, mais elles sont longues à préparer manuellement. Il serait donc très utile de pouvoir générer automatiquement des notes de version de haute qualité. Je ferai référence à deux recherches antérieures sur la génération automatique de notes de version. La première est un système appelé Arena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant l'extracteur de modifications pour extraire les différences principales, les modifications de bibliothèque et les modifications de documents à partir des différences entre les versions, puis en les combinant. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, le système de suivi des problèmes, et ne peut être appliqué qu'aux projets utilisant Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub. La seconde est Glyph, récemment annoncé en 2020. Il est disponible sur internet et peut être installé via PIP. Ce système possède un simple modèle de classification de texte basé sur l'apprentissage et produit l'une des cinq catégories, telles que des fonctionnalités ou des corrections de bugs, pour chaque message de commit en entrée. L'image est un exemple d'utilisation qui renvoie un label correctif ou de correction de bugs. Les données d'entraînement de Glyph sont relativement petites, environ 5 000, et lors des révisions des expériences décrites ci-dessous, la performance du modèle de classification de texte n'est pas élevée. Je présente deux recherches connexes, mais elles présentent des problèmes de portée d'application limitée et de ressources de données rares. Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité. Pour le problème de portée d'application limitée, nous proposons une méthode de summarisation de classifieur de haute qualité utilisant uniquement le message de commit comme entrée. Cette méthode proposée peut être utilisée pour tous les référentiels en anglais. Pour le second problème du manque de ressources de données, nous avons créé un ensemble de données R et sum composé d'environ 82 000 éléments de données en collectant des données à partir de référentiels GitHub publics à l'aide de RR et de certains ensembles de données. Notre ensemble de données Rnsum, composé d'environ 82 000 éléments de données, a été créé en collectant des données à partir de référentiels GitHub publics à l'aide de l'API GitHub. Ensuite, je décrirai notre ensemble de données. Voici un exemple de données. Le côté gauche est le message de commit et le côté droit est la note de version. Les notes de version sont étiquetées comme des améliorations de visages, etc. Nous avons défini la tâche consistant à faire en sorte que le message de commit soit l'entrée et les notes de version étiquetées soient la sortie. Cela peut être considéré comme une tâche de summarisation. Nous avons prédéfini quatre étiquettes : fonctionnalités, améliorations, corrections de bugs, suppressions et modifications cassantes. Ces étiquettes ont été définies en fonction des recherches antérieures et d'autres facteurs. Les notes de version en bas à droite sont extraites des notes de version affichées en bas à gauche. À ce moment-là, il est nécessaire de détecter les quatre étiquettes qui ont été prédéfinies à l'avance. Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel. Par exemple, l'étiquette des améliorations inclut les améliorations, les améliorations, les optimisations, etc. Nous avons préparé une liste de vocabulaire de nos étiquettes d'étude pour chacune de ces variations de notation, nous l'avons utilisée pour détecter la classe de note de version et corriger le texte de la liste qui suit en tant que phrase de note de version pour la classe qui doit identifier la version précédente 2.5 à 18 et obtenir sa différence. C'est un peu fastidieux et il ne suffit pas de simplement obtenir une liste de versions et de regarder le avant et l'après. Nous avons créé une règle de correspondance heuristique pour obtenir les versions précédentes et suivantes. En fin de compte, 7 200 référentiels et 82 000 éléments de données ont été collectés. De plus, le nombre moyen de tokens de note de version est de 63, ce qui est assez élevé pour une tâche de summarisation. De plus, le nombre de tokens uniques est assez important, soit 8 830 000. Cela est dû au grand nombre de noms de classe et de méthode uniques trouvés dans le référentiel. Ensuite, j'expliquerai la méthode proposée. Le modèle de summarisation extractive puis abstraite de classe consiste en deux réseaux neuronaux. Il utilise un classifieur pour classer chaque message de commit dans cinq classes de notes de version. Nous choisissons, implémente, corrections de bugs, suppressions plus et autres. Les messages de commit classés comme autres sont écartés. Ensuite, CEAS applique le générateur aux documents de quatre étiquettes indépendamment et génère des notes de version pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de commit et les notes de version ne sont pas connues. Par conséquent, pour entraîner le classifieur, nous attribuons des pseudo-étiquettes à chaque message de commit entrant en utilisant les 10 premiers caractères de chaque message de commit. Nous modélisons la summarisation abstraite de classe à travers notre approche par deux méthodes différentes. Le premier modèle, que nous appelons cssingle, consiste en un seul réseau de type ensemble à ensemble et génère un long morceau de texte de node unique, donnant une concaténation des messages de commit entrants. Le texte de sortie est composé de réseaux, chacun correspondant à l'une des classes de notes de version connues. Bien, laissez-moi expliquer l'expérience. Cinq méthodes ont été comparées : CAS, CASSingle, CASMatch, PlusSelling et l'étude précédente, GRIF. En ce qui concerne les aberrations, dans certains cas, CSMatch, Blustering et l'étude précédente Glyph. En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases. Comme il est difficile de calculer le nombre de phrases telles quelles, elles sont combinées avec des espaces et traitées comme une seule longue phrase. Le bleu est pénalisé lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur bleu inférieure dans les résultats expérimentaux décrits ci-dessous. Nous calculons également la spécificité, car le rouge et le bleu ne peuvent pas être calculés si les notes de version sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont censées être vides. Voici les résultats. Étant donné que l'ensemble de données contient des adresses e-mail, des valeurs de hachage, etc., nous avons également évalué l'ensemble de données nettoyé, qui les exclut. CES et CAS ont obtenu des scores rouge L plus de 10 points plus élevés que les modèles de référence. En particulier, sur l'ensemble de test nettoyé, l'écart de score entre la méthode proposée et les modèles de référence a grimpé à plus de 20 points. Ces résultats indiquent que CAS et CAS sont significativement efficaces. CAS a obtenu un meilleur score root-A que CAS, ce qui suggère que la combinaison d'un classifieur et d'un générateur est efficace pour entraîner le classifieur en utilisant des doubles. Une couverture élevée de CAS peut être obtenue, probablement parce que le classifieur peut se concentrer sur la sélection des messages de commit pertinents pour chaque classe. CAS match avait tendance à produire un log L plus élevé que CAS single, ce qui suggère qu'il est également efficace de développer indépendamment des modèles de summarisation abstraite différents pour chaque classe de note de version. Voici une analyse des erreurs. Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines. Sur la figure de droite, la phrase de référence comporte trois ou quatre phrases, tandis que CAS n'en a qu'une seule. La raison de cette réticence du modèle est que, dans les données d'entraînement, seulement 33 % des phrases sont présentes dans l'étiquette fonctionnalités et 40 % dans l'étiquette améliorations. De plus, les méthodes CES ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple supérieur à droite est un exemple de message de commit très désordonné. Les méthodes CES ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple supérieur à droite est un exemple de message de commit très désordonné, et la phrase complète ne peut pas être générée sans référence à la demande ou au problème correspondant. L'exemple ci-dessous montre que les deux messages de commit dans l'entrée sont liés et doivent être combinés en une seule phrase, mais il ne le fait pas. Enfin, une conclusion. Nous avons créé un nouvel ensemble de données pour la notation automatique de listes. Nous avons également formé la tâche consistant à entrer des messages de commit et à les résumer afin qu'elle soit applicable à tous les projets écrits en anglais. Notre expérience montre que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les modèles de référence. Veuillez consulter notre onglet Descent uniquement. Merci."}
