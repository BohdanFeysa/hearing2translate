{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Asaf Harari und ich werde unseren Beitrag, \"Few-Shot Tabular Data Enrichment Using Fine-Tuning Transformer Architectures\", vorstellen. Data Scientists analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation bestehender Merkmale. Doch manchmal sind diese Merkmale limitiert. Die Generierung von Merkmalen unter Verwendung einer anderen Datenquelle kann substanzielle Informationen hinzufügen. Unser Forschungsziel ist die automatische Tabellendatenanreicherung unter Verwendung externer, frei zugänglicher Textquellen. Die Generierung von Merkmalen unter Verwendung einer anderen Datenquelle kann substanzielle Informationen hinzufügen. Unser Forschungsziel ist die automatische Tabellendatenanreicherung unter Verwendung externer, frei zugänglicher Textquellen. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entity Linking und Textanalyse beinhaltet, um neue Merkmale aus der frei zugänglichen Textdaten der Wissensdatenbank zu extrahieren. Unser Framework, FAST, ist genau dieser automatische Prozess. Sehen wir uns ein Beispiel an. In einen Datensatz, der in FAST eingespeist wird. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, dessen Ziel die Klassifizierung von Universitäten in niedrigrangige und hochrangige Universitäten ist. Der Datensatz ist ein Universitätsdatensatz, dessen Ziel die Klassifizierung von Universitäten in niedrigrangige und hochrangige Universitäten ist. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase von FAST ist Entity Linking. Dabei wird jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensdatenbank verknüpft. Anschließend wird der Text der Entitäten aus der Wissensdatenbank extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text der Wikipedia-Seitenabstrakt. Nun müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Wir benötigen also eine Feature-Extraktionsphase, die eine Textanalyse beinhaltet. Dies ist die Hauptneuheit dieses Beitrags, und ich werde in den nächsten Folien näher darauf eingehen. Nach der Feature-Extraktionsphase folgt eine Feature-Generierungsphase, in der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst werden Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes generiert. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, sodass FAST zwei neue Merkmale generiert. Wenn der Datensatz jedoch fünf Klassen hat, generiert FAST fünf neue Merkmale. Jedes Merkmal stellt die Wahrscheinlichkeit für jede Klasse dar. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik in der Textanalyse, nämlich Transformer-basierte Sprachmodelle wie BERT, GPT, XLNet und so weiter. Es ist jedoch unwahrscheinlich, dass wir ein Sprachmodell mit den Eingabedatensätzen trainieren können. Ein naiver Ansatz wäre ein Fine-Tuning für eine Zielaufgabe. Daher können wir in der Feature-Extraktionsphase ein vortrainiertes Sprachmodell herunterladen und das Sprachmodell auf dem Ziel-Datensatz feinabstimmen. In diesem Beispiel verwenden wir, um das Sprachmodell feinabzustimmen, um Text in Klassen zu klassifizieren, Abstraktionen in Klassen, niedrig oder hoch, die Ausgabe des Sprachmodells, die Wahrscheinlichkeit für jede Klasse, zu empfangen und als neue Merkmale zu verwenden. Das Problem bei diesem Ansatz besteht darin, dass der Datensatz möglicherweise nur wenige verschiedene Entitätstags hat, neue Merkmale. Das Problem bei diesem Ansatz besteht darin, dass Datensätze möglicherweise nur wenige verschiedene Entitätstags haben. In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Samples, und der kleinste Datensatz enthielt 35 Samples in seinem Trainingsset. Daher wäre es unwirksam, ein Sprachmodell auf diesem Datensatz feinabzustimmen. Wir können jedoch Vorwissen über vorab analysierte Datensätze nutzen und diese Information verwenden, wenn wir den N-ten Datensatz analysieren. Was wir vorschlagen, ist, eine zusätzliche Fine-Tuning-Phase hinzuzufügen, eine vorbereitende Multitask-Fine-Tuning-Phase, in der wir das Sprachmodell über N-1 Datensätze feinabstimmen, und anschließend eine weitere Fine-Tuning-Phase ausführen, die ein Fine-Tuning für eine Zielaufgabe ist, in der wir das Sprachmodell über den N-ten Ziel-Datensatz feinabstimmen. Der aktuelle Stand der Technik im Multitask-Fine-Tuning wird mtDNN genannt. In mtDNN werden Köpfe in der Anzahl der Aufgaben im Trainingsset aufrechterhalten. In diesem Beispiel gibt es vier Aufgaben im Trainingsset, sodass mtDNN vier Köpfe aufrechterhält, wie man in der Abbildung sehen kann, und es wird eine zufällige Charge aus dem Trainingsset entnommen. Wenn die zufällige Charge zu einer, zum Beispiel, Single-Sentence-Klassifikationsaufgabe gehört, wird ein Forward- und Backward-Pass durch den ersten Kopf ausgeführt. Wenn die zufällige Charge zu einer Pairwise-Ranking-Aufgabe gehört, wird ein Forward- und Backward-Pass durch den letzten Kopf ausgeführt. In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen. Daher variiert ein Tableau-Datensatz in der Anzahl der Klassen. Es gibt also viele Aufgaben. MTDNN verwaltet die Anzahl der Klassen, Köpfe, Ausgabeschichten, und zusätzlich muss MTDNN neue Köpfe für einen neuen Datensatz mit einer neuen Aufgabe initialisieren. Unser Ansatz, Task Reformulation Fine-Tuning, besteht darin, dass wir anstelle der Aufrechterhaltung mehrerer Köpfe jeden Datensatz in ein Sentence-per-Klassifikationsproblem umformulieren, das eine Zwei-Klassen-Aufgabe ist, anstelle der Aufrechterhaltung mehrerer Köpfe, wir reformulieren jeden Datensatz in ein Sentence-per-Klassifikationsproblem, das eine Zwei-Klassen-Aufgabe ist. Sehen wir uns ein Beispiel an. Hier ist unser Eingabe-Datensatz, der aus Entitäten, Merkmalen, Text und Klassen besteht, und wir erstellen aus der Klassifizierung des Textes in niedrig und hoch eine Aufgabe, um den Text zu klassifizieren, um die Abstraktion und die Klasse zu klassifizieren, um die Abstraktion und die Klasse zu klassifizieren, ob die Abstraktion zur Klasse gehört oder nicht. Das Label-Vektor ist in diesem Fall also immer gleich, da er immer aus zwei Klassen besteht. Und das ist alles. Dann formuliert es die Aufgabe in eine Sentence-per-Klassifikationsaufgabe um. Wenden Sie das Sprachmodell auf die neue Aufgabe an und erhalten Sie die Wahrscheinlichkeit für jede Klasse. Beachten Sie, dass das Sprachmodell bereits über N-1 Datensätze mit einer vorbereitenden Multitask-Fine-Tuning-Phase feinabgestimmt wurde. Dann verwenden wir den Ausgabvektor des Datensatzes, der mit einer vorbereitenden Multitask-Fine-Tuning-Phase feinabgestimmt wurde. Dann verwenden wir den Ausgabvektor des Sprachmodells als ein neu generiertes Merkmal in der Anzahl der Klassen. Um unser Framework zu evaluieren, verwenden wir einen 17 tabellarischen Klassifikationsdatensatz, der Größe, Merkmale, Balance, Domäne und anfängliche Leistung verifiziert. Als Wissensdatenbank verwenden wir Wikipedia. Wir gestalten unser Experiment als Leave-One-Out-Evaluation, wobei wir FAST über 16 Datensätze trainieren und es auf den 17. Datensatz anwenden. Wir teilen außerdem jeden Datensatz in vier Falten auf und wenden eine Vier-Falten-Kreuzvalidierung an. Dann generieren wir die neuen Merkmale und evaluieren sie mithilfe von fünf Evaluationsklassifikatoren für die falsche Kreuzvalidierung. In unserem Experiment verwenden wir eine Transformer-basierte Architektur. Hier sind die Ergebnisse unseres Experiments. Man kann sehen, dass wir unser Framework mit Target Data Set Fine Tuning, Target Task Fine Tuning und einem mtdnn-Vorab-Fine-Tuning und unserem reformulierten Fine-Tuning vergleichen. Unser reformuliertes Fine-Tuning erzielt das beste Ergebnis, die beste Leistung, während mtDNN eine Verbesserung von zwei Prozent gegenüber dem Fine-Tuning des Target Data Sets erzielt. Unser Ansatz erzielt eine Verbesserung von sechs Prozent. Wenn wir uns die kleinen Datensätze ansehen, können wir sehen, dass die Leistung von mtDNN abnimmt und die Verbesserung des Target Task Fine-Tuning allein. Zusammenfassend ermöglicht FAST Few-Shot-Anreicherung ab 35 Samples in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Es fügt jedoch eine Reformulationsphase hinzu, einen erweiterten Trainingsdatensatz und es benötigt einen Zielwert mit semantischer Bedeutung, den wir in das Sprachmodell einspeisen und in der Sentence-per-Klassifikationsaufgabe verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, heute werde ich unsere Forschungsarbeit vorstellen, „Learning to Reason Detectively, Metabolic Problem Solving as Complex Reason Extraction“. Ich bin Alan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jerry von der University of Texas at Austin und Weilu vom SUTD. Zuerst möchte ich über unsere Motivation für das Reasoning sprechen. Hier zeigen wir ein Beispiel, bei dem mehrstufiges Reasoning hilfreich ist. Diese Abbildung stammt aus der Arbeit „Pen on Paper“, bei der durch Prompting das mathematische Problem im Szenario des Few-Shot-Learnings gelöst wird. Auf der linken Seite können wir sehen, dass wir möglicherweise nicht die richtigen Antworten erhalten, wenn wir einige Beispiele mit nur Fragen und Antworten geben. Wenn wir jedoch mehr Reasoning-Beschreibungen geben, ist das Modell in der Lage, die Reasoning-Beschreibung vorherzusagen und auch hier eine korrekte Vorhersage zu treffen. Es ist daher gut, interpretierbares mehrstufiges Reasoning als Ausgabe zu haben. Wir sind auch der Meinung, dass Method-Problem ein unkompliziertes Anwendungsgebiet ist, um solche Reasoning-Fähigkeiten zu bewerten. In unserem Problemaufbau müssen wir, gegeben die Fragen, diese Frage lösen und die numerischen Antworten erhalten. In unseren Datensätzen wird uns auch der mathematische Ausdruck gegeben, der zu dieser bestimmten Antwort führt, wie in früheren Arbeiten. Bestimmte Annahmen gelten ebenfalls. Wir nehmen an, dass die Genauigkeit der Größen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialfunktion berücksichtigen. Darüber hinaus können komplizierte Operatoren tatsächlich in diese grundlegenden Operatoren zerlegt werden. Frühere Arbeiten zur Lösung von Method-Problemen können in Sequence-to-Sequence- und Sequence-to-Tree-Modelle kategorisiert werden. Traditionelle Sequence-to-Sequence-Modelle konvertieren den Ausdruck in eine bestimmte Sequenz für die Generierung und sind relativ einfach zu implementieren, und sie können auf viele verschiedene komplexe Probleme verallgemeinert werden. Der Nachteil der Leistung ist jedoch im Allgemeinen nicht besser als bei Strukturmodellen. Es mangelt an der Interpretierbarkeit für die Vorhersage, obwohl diese Richtung aufgrund des Transformer-Modells immer noch recht beliebt ist. Bei Tree-basierten Modellen strukturieren wir diese Ausdrücke in Form eines Baums und folgen einer Pre-Order-Traversierung bei der Dreiererzeugung. In Tree-basierten Modellen strukturieren wir diese Ausdrücke in Form eines Baums und folgen einer Pre-Order-Traversierung bei der Dreiererzeugung. Hier generieren wir weiterhin die Operatoren, bis wir die Blätter erreichen, die die Größen darstellen. Das Gute daran ist, dass es uns diese binäre Baumstruktur liefert. Struktur, und es, aber tatsächlich ist es etwas unintuitiv, weil wir zuerst den Operator und dann am Ende die Größen generieren, aber tatsächlich ist es etwas unintuitiv, weil wir zuerst den Operator und dann am Ende die Größen generieren. Außerdem enthält es einige redundante Berechnungen. Wenn wir uns also diesen Ausdruck ansehen, a mal 3 plus 3, wird er tatsächlich zweimal generiert. Tatsächlich sollten wir die Ergebnisse jedoch wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir im zweiten Schritt diesen Divisor erhalten, der 27 ist, und wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden. In diesen Schritten erhalten wir die Teiler. Und dann erhalten wir im dritten Schritt den Quotienten. Nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt wiederverwenden und die Ergebnisse des vierten Schritts erhalten. Und schließlich können wir den Dividend erhalten. Hier generieren wir also das gesamte Ausdruck direkt, anstatt einzelne Operatoren oder Größen zu generieren. Dies macht den Prozess genauer. In unserem deduktiven System beginnen wir zunächst mit einer Reihe von Größen, die in den Fragen präsentiert werden, und auch mit einigen Konstanten als unseren Ausgangszuständen. Der Ausdruck wird durch EIJOP dargestellt, wobei wir den Operator von QI bis QJ ausführen, und dieser Ausdruck ist tatsächlich gerichtet. Wir haben hier auch eine Subtraktionsumkehrung, um die entgegengesetzte Richtung darzustellen. Dies ähnelt der Relationsextraktion. In einem formalen deduktiven System wenden wir zur Zeit t den Operator zwischen dem QI- und QJ-Paar an und erhalten dann diesen neuen Ausdruck. Wir fügen ihn dem nächsten Zustand hinzu, um eine neue Größe zu erhalten. Diese Folie visualisiert die Entwicklung der Zustände, bei der wir kontinuierlich Ausdrücke zum aktuellen Zustand hinzufügen. In unserer Modellimplementierung verwenden wir zunächst ein vortrainiertes Sprachmodell, das Birds oder Rabbits sein kann, und codieren dann einen Satz und erhalten diese Größenrepräsentationen. Sobald wir die Größenrepräsentationen erhalten haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel für Q1, um die Repräsentation für Q1 dividiert durch Q2 und dann mal Q3 zu erhalten. Zuerst erhalten wir die Paarrepräsentation, die im Wesentlichen nur die Verkettung zwischen Q1 und Q2 ist. Dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist. Schließlich erhalten wir die Ausdruckrepräsentation Q1 dividiert durch Q2. In der Praxis erhalten wir in der Inferenzphase möglicherweise auch den falschen Ausdruck. Hier ist jeder mögliche Ausdruck gleich drei mal die Anzahl der Operatoren. Das Schöne daran ist, dass wir hier problemlos Einschränkungen hinzufügen können, um diesen Suchraum zu steuern. Wenn dieser Ausdruck beispielsweise nicht zulässig ist, können wir ihn einfach aus unserem Suchraum entfernen. Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist eine weitere Größe. Diese Größe stammt von dem zuvor berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck, Q3 mal Q4, erhalten. Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Solche Unterschiede erschweren die Anwendung der Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist. Die Trainingsprozedur ähnelt dem Training eines Sequence-to-Sequence-Modells, bei dem der Verlust in jedem Zeitschritt optimiert wird. Hier verwenden wir auch dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Hier ist der Raum anders als bei einem Sequence-to-Sequence, da der Raum in jedem Zeitschritt unterschiedlich ist, bei einem traditionellen Sequence-to-Sequence-Modell ist es die Anzahl des Vokabulars und es ermöglicht uns auch, bestimmte Einschränkungen aus Vorwissen aufzuerlegen. Wir führen Experimente auf den gängigen Method-Problem-Datensätzen MAWPS, Math23K, MATHQA und SWAMP durch. Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisher besten Ansätzen. Unsere am besten abschneidende Variante ist Robeta Deductive Reasoner. Tatsächlich verwenden wir keinen Beam Search im Gegensatz zu den besten Ansätzen, die oft auf Tree-basierten Modellen basieren. Insgesamt übertrifft unser Reasoner dieses Tree-basierte Modell deutlich, aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SWAMP nicht wirklich hoch sind. Wir untersuchen die Ergebnisse auf Swamp weiter, und dieses Datenset ist herausfordernd, da der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, z. B. irrelevante Informationen und zusätzliche Größen. In unserer Vorhersage stellen wir fest, dass einige der Zwischenwerte tatsächlich negativ sind. Zum Beispiel fragen wir in dieser Frage, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie 17 weniger Pfirsiche und Steven hat 8 Pfirsiche, was völlig irrelevant ist. Unser Modell trifft also einige Vorhersagen, die negative Werte erzeugen. Wir stellen fest, dass diese beiden Ausdrücke ähnliche Punktzahlen haben. Wir können also diesen Suchraum begrenzen, indem wir diese Ergebnisse als negative Ergebnisse entfernen, damit wir die Antwort korrekt machen können. Wir stellen weiter fest, dass diese Einschränkung für einige Modelle tatsächlich die Leistung deutlich verbessert. Zum Beispiel verbessern wir für Birds sieben Punkte. Und für das Robeta-basierte Modell verbessern wir tatsächlich zwei Punkte. Ein besseres Sprachmodell hat eine bessere Sprachverständnisfähigkeit, so dass die Zahl hier höher für Robeta und niedriger für Birds ist. Wir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren. Wir nehmen an, dass die Anzahl der ungenutzten Größen als irrelevante Informationen hier betrachtet werden kann. Hier können wir sehen, dass wir den Prozentsatz der Stichproben mit ungenutzten Größen haben, und das SWAMP-Datenset hat den größten Anteil. Hier zeigen wir auch die Gesamtleistung. Für diese Stichproben ohne ungenutzte Größen ist die Gesamtleistung tatsächlich höher als die Gesamtleistung. Aber mit diesen Stichproben mit ungenutzten Größen ist sie tatsächlich viel schlechter als die Gesamtleistung. Für MAWPS haben wir nicht viele Todesfälle, also ignoriere ich diesen Teil. Schließlich möchten wir die Interpretierbarkeit anhand eines Crash- und Partizipationsbeispiels demonstrieren. Hier trifft unser Modell im ersten Schritt eine falsche Vorhersage. Wir können diesen Ausdruck mit dem Satz korrelieren. Wir glauben, dass dieser Satz das Modell in eine falsche Vorhersage verleiten könnte. Das Drucken von weiteren 35 veranlasst das Modell zu denken, dass es sich um einen Additionsoperator handelt. Wir versuchen, den Satz so zu überarbeiten, dass er etwas wie „Die Anzahl der Birnbäume ist 55 weniger als die Apfelbäume“ lautet. Wir bringen es so, dass es eine genauere Semantik vermittelt, so dass das Modell die Vorhersage korrekt treffen kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Modellverhalten zu verstehen. Abschließend ist unser Modell tatsächlich ziemlich effizient, und wir können ein interpretierbares Lösungsprozedere bereitstellen. Wir können problemlos einige Vorwissen als Einschränkung einbeziehen, was dazu beitragen kann, die Leistung zu verbessern. Und das letzte ist, dass der zugrunde liegende Mechanismus nicht nur auf Network-Problem-Solving-Aufgaben, sondern auch auf andere Aufgaben, die mehrstufiges Reasoning beinhalten, anwendbar ist. Wir haben jedoch auch einige Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, kann der Speicherverbrauch ziemlich hoch sein. Und das zweite ist, dass es ziemlich schwierig ist, eine Beam Search-Strategie anzuwenden, da die Wahrscheinlichkeitsverteilung in verschiedenen Zeitschrittschritten unausgeglichen ist. Damit ist die Präsentation abgeschlossen, und Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, die sich auf einen neuen Datensatz für die Recherche von Gesetzartikeln bezieht. Rechtliche Fragen sind ein integraler Bestandteil vieler Menschenleben, doch die meisten Bürger verfügen über wenig bis gar kein Wissen über ihre Rechte und grundlegende rechtliche Verfahren. In der Folge werden viele schutzbedürftige Bürger, die sich den kostspieligen Beistand eines Rechtsexperten nicht leisten können, ungeschützt oder, schlimmer noch, ausgebeutet. Unsere Arbeit zielt darauf ab, die Kluft zwischen Menschen und dem Gesetz zu überbrücken, indem wir effektive Suchsysteme für Gesetzartikeln entwickeln. Ein solches System könnte einen kostenlosen Rechtsberatungsservice für unerfahrene Personen bereitstellen. Bevor wir uns der Hauptleistung dieser Arbeit zuwenden, wollen wir zunächst das Problem der Recherche von Gesetzartikeln beschreiben. Angesichts einer einfachen Frage zu einer Rechtsangelegenheit, wie z. B. welches Risiko besteht, wenn ich die berufliche Schweigepflicht verletze, ist ein Modell erforderlich, um alle relevanten Gesetzartikeln aus einer großen Menge an Gesetzgebung abzurufen. Diese Informationsbeschaffungsaufgabe ist mit eigenen Herausforderungen verbunden. Erstens befasst sie sich mit zwei Arten von Sprache, natürlicher Alltagssprache für die Fragen und komplexer juristischer Sprache für die Gesetze. Dieser Unterschied in den Sprachverteilungen erschwert es einem System, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine juristische Frage übersetzen kann, die mit der Terminologie der Gesetze übereinstimmt. Darüber hinaus ist das Gesetzgebungsrecht keine Sammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle behandelt werden können, wie beispielsweise Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die erst in ihrem Gesamtzusammenhang, d. h. zusammen mit den ergänzenden Informationen aus ihren benachbarten Artikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Position in der Struktur des Rechts, ihre volle Bedeutung entfalten. Schließlich sind Gesetzartikeln in kleinen Abschnitten verfasst, die typischerweise die typische Retrieval-Einheit in den meisten Retrieval-Arbeiten darstellen. Hierbei handelt es sich um lange Dokumente, die bis zu 6.000 Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben großes Interesse an vielen Rechtsaufgaben geweckt, wie z. B. der Vorhersage von Rechtsurteilen oder der automatisierten Vertragsprüfung, aber die Recherche von Gesetzartikeln ist aufgrund des Mangels an großen und qualitativ hochwertigen, gelabelten Datensätzen hauptsächlich unberührt geblieben. In dieser Arbeit präsentieren wir einen neuen Datensatz, der auf französische Staatsbürger ausgerichtet ist, um zu untersuchen, ob Retrieval-Modelle die Effizienz und Zuverlässigkeit von Rechtsexperten bei der Recherche von Gesetzartikeln approximieren können. Unser belgischer Datensatz für die Recherche von Gesetzartikeln, PSART, besteht aus mehr als 1100 Rechtsfragen, die von belgischen Bürgern gestellt wurden. Diese Fragen umfassen ein breites Spektrum an Themen, von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung. Jede einzelne wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Gesetzartikeln aus belgischen Gesetzesbüchern versehen. Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz erhoben haben. Zunächst haben wir einen großen Korpus von Gesetzartikeln zusammengestellt. Wir berücksichtigten 32 öffentlich zugängliche belgische Gesetzesbücher und extrahierten alle Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir Rechtsfragen mit Verweisen auf relevante Gesetze. Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die nach Rechtsberatung zu einem persönlichen Rechtsproblem fragen. Wir hatten das Glück, auf ihre Websites Zugriff zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsprobleme belgischer Bürger beantwortet. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und Rechtsverweisen auf relevante Gesetze versehen waren. Schließlich haben wir die Rechtsverweise geparst und die Fragen herausgefiltert, deren Verweise nicht auf Artikel in einem der von uns berücksichtigten Gesetzesbücher verwiesen. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Schließlich blieben 1108 Fragen, die sorgfältig mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.633 Gesetzartikeln versehen waren. Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien, und jeder Artikel enthält eine Verkettung ihrer nachfolgenden Überschrift in der Struktur des Rechts. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschung zur juristischen Informationsbeschaffung oder juristischen Textklassifizierung von Interesse sein. Sehen wir uns einige Eigenschaften unseres Datensatzes an. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind deutlich länger, mit einer Medianlänge von 77 Wörtern, wobei 142 von ihnen mehr als 1000 Wörter umfassen, der längste bis zu 5790 Wörter lang ist. Wie bereits erwähnt, decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 % von ihnen entweder über Familie, Wohnen, Geld oder Justiz handeln, während die restlichen 15 % entweder über Sozialversicherung, Ausländer oder Arbeit handeln. Die Artikel sind ebenfalls sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzesbüchern stammen, die eine große Anzahl von Rechtsthemen abdecken. Hier ist die Gesamtzahl der Artikel, die aus jedem dieser belgischen Gesetzesbücher gesammelt wurden. Von den 22.633 Artikeln werden nur 1.612 als für mindestens eine Frage im Datensatz relevant angegeben. Und etwa 80 % dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Strafgesetzbuch, dem Strafverfolgungsgesetzbuch oder dem Strafgesetzbuch. Inzwischen haben 18 von 32 Gesetzesbüchern weniger als 5 Artikel, die als für mindestens eine Frage relevant angegeben sind, was darauf zurückzuführen ist, dass diese Gesetzesbücher sich weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt beträgt die Mediananzahl der Zitate für diese zitierten Artikel 2, und weniger als 25 % von ihnen werden mehr als 5 Mal zitiert. Mithilfe unserer Datensätze vergleichen wir verschiedene Retrieval-Ansätze, darunter lexikalische und dense Architektur. Bei einer gegebenen Abfrage und einem Artikel weist ein lexikalisches Modell der Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe über die Abfrageterme der Gewichte jedes dieser Terme in diesem Artikel berechnet. Wir experimentieren mit den Standard-TF-IDF- und BM25-Ranking-Funktionen. Das Hauptproblem bei diesen Ansätzen besteht darin, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuralnetzwerkbasierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet und eine relevante Punktzahl zwischen einem Abfrage-Artikel-Paar über die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen resultieren typischerweise aus einer Pooling-Operation auf der Ausgabe eines Wort-Embedding-Modells. Zunächst untersuchen wir die Wirksamkeit von Siamese B-Encodern in einer Zero-Shot-Evaluierungsumgebung, d. h. vorgefertigte Wort-Embedding-Modelle werden direkt angewendet, ohne zusätzliche Feinabstimmung. Wir experimentieren mit kontextunabhängigen Text-Encodern, nämlich Word2Vec und FastText, und kontextabhängigen Embedding-Modellen, nämlich Robota und insbesondere Camembert, einem französischen Robota-Modell. Darüber hinaus trainieren wir unsere eigenen Camembert-basierten B-Encoder-Modelle auf unseren Datensätzen. Beachten Sie, dass wir beim Training mit den beiden Varianten der B-Encoder-Architektur experimentieren: Siamese, das ein einziges Wort-Embedding-Modell verwendet, das die Abfrage und den Artikel zusammen in einen gemeinsamen dichten Vektorraum abbildet, und Tower, das zwei unabhängige Wort-Embedding-Modelle verwendet, die die Abfrage und den Artikel separat in verschiedene Einbettungsräume codieren. Wir experimentieren mit Mittelwert, Maximum und CLS Pooling sowie mit Punktprodukt und Kosinus zur Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse unserer Baselines auf dem Satz mit den lexikalischen Methoden oben, den in einer Zero-Shot-Umgebung ausgewerteten Siamese B-Encodern in der Mitte und den feinabgestimmten B-Encodern unten. Insgesamt übertreffen die feinabgestimmten B-Encoder alle anderen Baselines deutlich. Das Two-Tower-Modell verbessert seine Siamese-Variante beim Recall bei 100, weist aber ähnliche Ergebnisse bei anderen Metriken auf. Obwohl BM25 die trainierten B-Encoder deutlich unterbot, deutet seine Leistung darauf hin, dass es sich immer noch um eine starke Baseline für domänenspezifisches Retrieval handelt. Bezüglich der Zero-Shot-Evaluierung des Siamese B-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorgefertigten Camembert-Modells ohne Optimierung für die Informationsbeschaffungsaufgabe zu schlechten Ergebnissen führt, was mit früheren Erkenntnissen übereinstimmt. Darüber hinaus haben wir beobachtet, dass der Word2Vec-basierte Biancoder den FastText- und BERT-basierten Modellen deutlich überlegen war, was darauf hindeutet, dass vorgefertigte Wort-Embedding-Modelle möglicherweise für diese Aufgabe besser geeignet sind als Modelle, die auf Zeichen- oder Subwort-Ebene arbeiten, wenn sie direkt verwendet werden. Obwohl vielversprechend, deuten diese Ergebnisse auf reichlich Verbesserungsmöglichkeiten im Vergleich zu einem kompetenten Rechtsexperten hin, der letztendlich alle relevanten Artikel zu jeder Frage abrufen und damit perfekte Ergebnisse erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen aller Datensätze diskutieren. Erstens ist der Artikelkorpus auf die aus den 32 berücksichtigten belgischen Gesetzesbüchern gesammelten Artikel beschränkt, was nicht die gesamte belgische Gesetzgebung abdeckt, da Artikel aus Verordnungen, Direktiven und Beschlüssen fehlen. Während der Datensatzkonstruktion werden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen nur einen Bruchteil der anfänglichen Anzahl relevanter Artikel erhalten. Dieser Informationsverlust bedeutet, dass die Antwort, die in den verbleibenden relevanten Artikeln enthalten ist, möglicherweise unvollständig ist, obwohl sie dennoch völlig angemessen ist. Zweitens müssen wir darauf hinweisen, dass nicht alle Rechtsfragen allein mit Gesetzen beantwortet werden können. Beispielsweise könnte die Frage, ob ich meine Mieter kündigen kann, wenn sie zu viel Lärm machen, keine detaillierte Antwort im Gesetzgebungsrecht enthalten, die einen bestimmten Lärmschwellenwert quantifiziert, ab dem eine Kündigung zulässig ist. Stattdessen sollte der Vermieter sich eher auf die Rechtsprechung verlassen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind. Zum Beispiel machen die Mieter zwei Partys pro Woche bis 2 Uhr morgens. Daher eignen sich einige Fragen besser als andere für die Recherche von Gesetzartikeln, und der Bereich der weniger geeigneten muss noch ermittelt werden. Wir hoffen, dass unsere Arbeit das Interesse weckt, praktische und zuverlässige Retrieval-Modelle für Gesetzartikeln zu entwickeln, die dazu beitragen können, den Zugang zur Justiz für alle zu verbessern. Sie können unser Papier, unseren Datensatz und unseren Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Wir freuen uns, Ihnen unsere Arbeit an VALS vorzustellen, einem aufgabenunabhängigen Benchmark, der dazu dient, Vision- und Sprachmodelle mit spezifischen linguistischen Phänomenen zu testen. Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten? Nun, in den letzten Jahren haben wir einen explosionsartigen Anstieg von Transformer-basierten Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vortrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei Vision- und Sprachaufgaben wie visuellem Fragebeantworten, visuellem Common-Sense-Reasoning, Bildabruf und Phrase-Grounding. Wir haben also die Meldung erhalten – die Genauigkeit auf diesen aufgabenspezifischen Benchmarks steigt stetig. Aber wissen wir, was die Modelle tatsächlich gelernt haben? Was versteht ein Vision- und Sprach-Transformer, wenn er einem Bild und einem Satz eine hohe Übereinstimmungsbewertung zuweist und dem anderen eine niedrige Bewertung? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige oder konzentrieren sie sich auf Verzerrungen, wie es frühere Arbeiten gezeigt haben? Um mehr Licht auf diesen Aspekt zu werfen, schlagen wir einen aufgabengerechteren Ansatz vor und führen Ventile ein, die die Sensitivität von Vision- und Sprachmodellen gegenüber spezifischen linguistischen Phänomenen testen, die sowohl die linguistischen als auch die visuellen Modalitäten beeinflussen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Aktionen, Entitäts-Coreferenz ab. Aber wie können wir testen, ob Vision- und Sprachmodelle diese Phänomene erfasst haben? Durch FOILing, eine Methode, die zuvor für Vision- und Sprachmodelle, nur für Nominalphrasen von Ravi Shekhar und Mitarbeitern, und für das Zählen in unserer früheren Arbeit angewendet wurde. FOILing bedeutet im Wesentlichen, dass wir die Bildunterschrift eines Bildes nehmen und eine FOIL erstellen, indem wir die Bildunterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Und wir nehmen diese Phrasenänderungen vor, indem wir uns auf sechs spezifische Aspekte konzentrieren, wie Existenz, Pluralität, Zählen, räumliche Beziehungen, Aktionen und Entitäts-Coreferenz, wobei jeder Aspekt aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als eine interessante Möglichkeit gefunden haben, FOIL-Instanzen zu erstellen. Beispielsweise haben wir im Aktionsbereich zwei Instrumente, eines, bei dem das Aktionsverb mit einem anderen Verb ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden. Zählen und Coreferenz sind ebenfalls Aspekte, die mehr als ein Instrument haben. Und wir erstellen diese FOILs, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch korrekt und anderweitig gültig sind. Das ist nicht einfach, denn eine veränderte Bildunterschrift ist möglicherweise weniger wahrscheinlich als die ursprüngliche Bildunterschrift. Obwohl es nicht unmöglich ist, ist es statistisch weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als ein Mann Pflanzen. Und große Vision- und Sprachmodelle könnten dies erkennen. Daher müssen wir Maßnahmen ergreifen, um gültige FOILs zu erhalten. Erstens nutzen wir starke Sprachmodelle, um FOILs vorzuschlagen. Zweitens verwenden wir Natural Language Inference, oder kurz NLI, um FOILs auszuschließen, die das Bild immer noch beschreiben könnten, da wir bei der Konstruktion von FOILs sicherstellen müssen, dass sie das Bild nicht beschreiben. Um dies automatisch zu testen, wenden wir Natural Language Inference mit folgender Argumentation an. Wir betrachten das Bild als Prämisse und seine Bildunterschrift als Implikation. Darüber hinaus betrachten wir die Bildunterschrift als Prämisse und die FOIL als Hypothese. Wenn ein NLI-Modell vorhersagt, dass die FOIL der Bildunterschrift widerspricht oder neutral gegenüber dieser steht, nehmen wir dies als Indikator für eine gültige FOIL. Wenn ein NLI vorhersagt, dass die FOIL von der Bildunterschrift impliziert wird, kann sie keine gute FOIL sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert, und wir schließen diese FOILs aus. Aber dieses Verfahren ist nicht perfekt. Es ist nur ein Indikator für gültige FOILs, daher setzen wir als dritte Maßnahme für die Generierung gültiger FOILs menschliche Annotatoren ein, um die in VALS verwendeten Daten zu validieren. Nach dem Filtern und der menschlichen Bewertung haben wir so viele Testinstanzen wie in dieser Tabelle beschrieben. Beachten Sie, dass VALS keine Trainingsdaten, sondern nur Testdaten liefert, da es sich um einen Zero-Shot-Test-Benchmark handelt. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vortraining zu nutzen. Das Finetuning würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Und wie wir bereits sagten, sind wir daran interessiert, die Fähigkeiten der Vision- und Sprachmodelle nach dem Vortraining zu bewerten. Wir experimentieren mit fünf Vision- und Sprachmodellen auf VALS, nämlich mit CLIP, LXMERT, Wil VILBERT, VILBERT 12 in 1 und VISUALBERT. Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren in Bildunterschriften und FOILs. Vielleicht relevanter für dieses Video, werden wir unsere permissivere Metrik, die paarweise Genauigkeit, vorstellen, die misst, ob die Bild-Satz-Ausrichtungsbewertung für das korrekte Bild-Text-Paar höher ist als für sein FOIL-Paar. Für weitere Metriken und Ergebnisse dazu finden Sie unser Papier. Die Ergebnisse mit der paarweisen Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben. Es zeigt sich, dass die beste Zero-Shot-Performance von Wilbert 12-in-1 erzielt wird, gefolgt von Wilbert, Alex Mert, Clip und schließlich Visual Bird. Es ist bemerkenswert, wie Instrumente, die sich auf einzelne Objekte konzentrieren, wie Existenz und Nominalphrasen, fast vollständig von Wilbert 12-in-1 gelöst werden, was zeigt, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Keine der übrigen Aspekte kann jedoch zuverlässig in unseren adversariellen FOILing-Szenarien gelöst werden. Aus den Instrumenten für Pluralität und Zählen geht hervor, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne oder mehrere Objekte zu unterscheiden oder diese in einem Bild zu zählen. Der Beziehungsaspekt zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Aktionen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn plausibilitätbasierte Verzerrungen vorliegen, wie wir im Aktionsbereich sehen. Aus dem Coreferenz-Bereich erfahren wir, dass das Verfolgen mehrerer Referenzen auf dasselbe Objekt in einem Bild mithilfe von Pronomen ebenfalls schwierig für Vision- und Sprachmodelle ist. Als Sanity-Check und weil es ein interessantes Experiment ist, haben wir auch zwei Text-Modelle, GPT-1 und GPT-2, benchmarkt, um zu prüfen, ob VALS von diesen unimodalen Modellen gelöst werden kann, indem wir die Perplexität der korrekten und der FOIL-Bildunterschrift berechnen und den Eintrag mit der niedrigsten Perplexität vorhersagen. Wenn die Perplexität für die FOIL höher ist, nehmen wir dies als Indikator dafür, dass die FOIL-Bildunterschrift unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leiden könnte. Es ist interessant zu sehen, dass die Text-Modelle GPT in einigen Fällen die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Zusammenfassend ist VALS ein Benchmark, der die linguistischen Konstruktionen als Brille nutzt, um der Community zu helfen, Vision- und Sprachmodelle zu verbessern, indem ihre visuellen Grounding-Fähigkeiten hart geprüft werden. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte in ihrer Anwesenheit in Bildern gut identifizieren, wie der Existenz-Aspekt zeigt, aber Schwierigkeiten haben, ihre wechselseitige Abhängigkeit und Beziehungen in visuellen Szenen zu grounden, wenn sie gezwungen sind, linguistische Indikatoren zu respektieren. Wir würden die Community sehr gerne ermutigen, VALS zu verwenden, um Fortschritte bei der Language Grounding mit Vision- und Sprachmodellen zu messen. Und noch wichtiger ist, dass VALS als indirekte Bewertung von Datensätzen verwendet werden kann, da Modelle vor und nach dem Training oder Finetuning bewertet werden können, um zu sehen, ob ein Datensatz den Modellen hilft, sich in einem der von VALS getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, werfen Sie einen Blick auf die VALS-Daten auf GitHub und zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamizawa von der Universität Tokio. Ich werde einen Beitrag vorstellen, der den Titel RNSUN trägt, einen umfangreichen Datensatz zur automatischen Generierung von Versionshinweisen durch Zusammenfassung von Commit-Logs. Ich werde dies in der folgenden Reihenfolge erläutern. Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten. Versionshinweise sind ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Veröffentlichung eines Softwareprodukts vertrieben werden. Das Bild zeigt die Versionshinweise für Version 2.6.4 der Vue.js-Bibliothek. Versionshinweise spielen eine wichtige Rolle bei der Open-Source-Entwicklung, sind aber zeitaufwändig in der manuellen Erstellung. Daher wäre es sehr nützlich, wenn man Versionshinweise automatisch in hoher Qualität generieren könnte. Ich beziehe mich auf zwei vorherige Forschungsarbeiten zur automatischen Generierung von Versionshinweisen. Die erste ist ein System namens Arena, das im Jahr 2014 veröffentlicht wurde. Es verwendet einen regelbasierten Ansatz, beispielsweise die Verwendung eines Change Extractors, um die Kernunterschiede, Bibliotheksänderungen und Dokumentationsänderungen aus den Unterschieden zwischen Veröffentlichungen zu extrahieren und diese abschließend zu kombinieren. Das bemerkenswerteste Merkmal dieses Systems ist der Issue Extractor im oberen rechten Bereich, der mit Jira, dem Issue-Tracker-System, verknüpft werden muss und nur auf Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Die zweite ist Glyph, das kürzlich im Jahr 2020 angekündigt wurde. Es ist im Internet verfügbar und kann über PIP installiert werden. Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt für jede eingegebene Commit-Nachricht eine von fünf Stufen aus, wie z. B. Features oder Bugfixes. Das Bild ist ein Beispiel für die Verwendung, das ein korrektives oder Bugfix-Label zurückgibt. Der Trainingsdatensatz von Glyph ist relativ klein, etwa 5000, und im Vergleich zu den nachfolgend beschriebenen Experimenten ist die Leistung des Textklassifikationsmodells nicht hoch. Ich präsentiere zwei verwandte Forschungsarbeiten, es gibt jedoch Probleme mit begrenzter Anwendbarkeit und knappen Datenressourcen. Unser Beitrag löst diese beiden Probleme und generiert automatisch Versionshinweise in hoher Qualität. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine hochwertige Klassifikator-Zusammenfassungsmethode vor, die nur die Commit-Nachricht als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischsprachigen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir einen R- und Sum-Datensatz erstellt, der aus etwa 82.000 Datenpunkten besteht, indem wir Daten aus öffentlichen GitHub-Repositories mithilfe von RR und einigen Datensätzen gesammelt haben. Unser Rnsum-Datensatz, der aus etwa 82.000 Datenpunkten besteht, wurde durch die Sammlung von Daten aus öffentlichen GitHub-Repositories mithilfe der GitHub API erstellt. Als Nächstes beschreibe ich unseren Datensatz. Hier ist ein Beispiel für die Daten. Die linke Seite ist die Commit-Nachricht und die rechte Seite sind die Versionshinweise. Die Versionshinweise sind als Verbesserungen von Gesichtern usw. gekennzeichnet. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe akzeptiert und die gekennzeichneten Versionshinweise ausgibt. Dies kann als eine Zusammenfassungaufgabe betrachtet werden. Wir haben vier Labels definiert: Features, Verbesserungen, Bugfixes, Deprecations, Removals und Breaking Changes. Diese wurden auf der Grundlage früherer Forschungsergebnisse und anderer Faktoren festgelegt. Die unten rechts gezeigten Versionshinweise werden aus den unten links gezeigten Versionshinweisen extrahiert. Zu diesem Zeitpunkt ist es erforderlich, die vier vordefinierten Labels zu erkennen. Aber die Labels sind nicht immer mit dem jeweiligen Repository konsistent. Beispielsweise umfasst das Label „Verbesserungen“ Verbesserungen, Erweiterungen, Optimierungen usw. Wir haben eine Vokabelliste unserer Studiengruppen für jede dieser notationalen Variationen vorbereitet, diese verwendet, um die Release-Note-Klasse zu erkennen und den Text der Liste zu korrigieren, der dem Release-Note-Satz für die zu identifizierende Klasse folgt, um die vorherige Release-Version 2.5 zu 18 zu erhalten und deren Diff. Das ist etwas mühsam, und es reicht nicht aus, einfach eine Liste von Releases abzurufen und die Vor- und Nachversionen anzusehen. Wir haben eine heuristische Matching-Regel erstellt, um die vorherigen und nachfolgenden Versionen für die Datensatzanalyse zu erhalten. Am Ende wurden 7200 Repositories und 82.000 Datenpunkte erfasst. Außerdem beträgt die durchschnittliche Anzahl der Tokens der Versionshinweise 63, was für eine Zusammenfassungaufgabe recht hoch ist. Auch die Anzahl der eindeutigen Tokens ist mit 8.830.000 recht hoch. Dies ist auf die große Anzahl eindeutiger Klassen- und Methodennamen in dem Repository zurückzuführen. Als Nächstes werde ich die vorgeschlagene Methode erläutern. Das Classwise Extractive-Then-Abstractive Summarization Modell besteht aus zwei neuronalen Netzwerken. Es verwendet einen Klassifikator, um jede Commit-Nachricht in fünf Release-Note-Klassen einzuteilen. Wir wählen Features, Bugfixes, Deprecations plus und andere aus. Die als „other“ klassifizierten Commit-Nachrichten werden verworfen. Dann wendet CEAS den Generator unabhängig auf die vier Label-Dokumente an und generiert Versionshinweise für jede Klasse. Bei dieser Aufgabe sind die direkten Entsprechungen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt. Um daher den Klassifikator zu trainieren, weisen wir jeder Eingabe-Commit-Nachricht Pseudo-Labels zu, indem wir die ersten 10 Zeichen jeder Commit-Nachricht verwenden. Wir modellieren die Class-Wise Abstractive Summarization durch unseren Ansatz mit zwei verschiedenen Methoden. Das erste Modell, das wir CSSingle nennen, besteht aus einem einzigen Set-to-Set-Netzwerk und generiert ein einzelnes, langes Notiztextstück, das eine Verkettung der Eingabe-Commit-Nachrichten ist. Das Ausgabetextnetzwerk entspricht jeweils einer der bekannten Klassen. Okay, lassen Sie mich das Experiment erläutern. Fünf Methoden wurden verglichen: CAS, CASSingle, CASMatch, PlusSelling und die vorherige Studie, GRIF. Bezüglich der Anomalie wurden in einigen Fällen CSMatch, Blustering und die vorherige Studie Glyph bewertet. Bezüglich der Bewertung werden Versionshinweise in einigen Fällen in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Das System wird bestraft, wenn es einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren Blue-Wert in den nachfolgend beschriebenen Experimentergebnissen. Wir berechnen außerdem die Spezifität, da Rouge und Blue nicht berechnet werden können, wenn die Versionshinweise leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt einen leeren Text ausgibt, wenn die Versionshinweise als leer angenommen werden. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir den bereinigten Datensatz bewertet, der diese ausschließt. CES und CAS erzielten Rouge L-Werte, die um mehr als 10 Punkte höher waren als die Baselines. Insbesondere erzielte der vorgeschlagene Ansatz im Vergleich zu den Baselines auf dem bereinigten Testdatensatz einen Score-Gap von mehr als 20 Punkten. Diese Ergebnisse zeigen, dass CAS und CAS signifikant wirksam sind. CAS erzielte einen besseren Root-A-Wert als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators effektiv ist, um den Klassifikator mithilfe von Zwei-Doubles zu trainieren. Eine hohe Abdeckung von CAS kann erreicht werden, wahrscheinlich, weil der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann. CAS Match ergab tendenziell einen höheren Log L als CAS Single, was darauf hindeutet, dass es ebenfalls effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle für jede Release-Note-Klasse zu entwickeln. Hier ist eine Fehleranalyse. CAS-Methoden neigen dazu, kürzere Sätze auszugeben als die Referenzsätze. In der rechten Abbildung hat der Referenzsatz drei oder vier Sätze, während CAS nur einen hat. Der Grund für diese Modellzögerlichkeit ist, dass im Trainingsdatensatz nur 33 % der Sätze im Label „Features“ und 40 % im Label „Verbesserungen“ vorhanden sind. Darüber hinaus können CES-Methoden keine genauen Versionshinweise ohne zusätzliche Informationen generieren. Das obere Beispiel auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. CES-Methoden können keine genauen Versionshinweise ohne zusätzliche Informationen generieren. Das obere Beispiel auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht, und der vollständige Satz kann nicht ohne Bezug auf die entsprechende Anfrage oder das Problem generiert werden. Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander in Beziehung stehen und in einem einzigen Satz kombiniert werden sollten, aber dies gelingt ihm nicht. Abschließend: Fazit. Wir haben einen neuen Datensatz für die automatische Listennotation erstellt. Wir haben auch die Aufgabe entwickelt, Commit-Nachrichten einzugeben und sie so zusammenzufassen, dass sie für alle in Englisch geschriebenen Projekte anwendbar sind. Unser Experiment zeigt, dass die vorgeschlagene Methode weniger verrauschte Leads-Notes mit höherer Abdeckung als die Baselines generiert. Bitte überprüfen Sie unsere Descent only-Registerkarte. Vielen Dank."}
