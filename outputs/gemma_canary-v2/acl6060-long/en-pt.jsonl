{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Assa Farari e eu apresentarei nosso artigo FUESHOT TABULAR DATA ARQUITETURA UTILIZANDO ARQUITETURAS FINE-TUNED TRANSFORMERS. Cientistas de dados analisam dados e focam principalmente na manipulação das características existentes, mas, por vezes, essas características são limitadas. A geração de características utilizando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Assuma que temos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a ligação de entidades e análise de texto para extrair novas características do texto livre da base de conhecimento. Nosso framework FAST é exatamente esse processo automático. Então, vamos ver um exemplo. E um conjunto de dados é alimentado no FAST. Neste exemplo, o conjunto de dados é o conjunto de dados de universidades, cujo objetivo é classificar universidades em universidades de baixo ranking e universidades de alto ranking. Como base de conhecimento, usamos a Wikipédia. A primeira fase do FEST é a ligação de entidades, quando cada entidade, neste exemplo, o nome da universidade é vinculado a uma entidade dentro da base de conhecimento, e o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipédia. Agora, precisamos gerar ou extrair características do texto recuperado. Então, precisamos de uma fase de extração de características que inclua análise de texto e esta é a principal novidade deste artigo, e farei uma análise aprofundada sobre ela nas próximas slides. Após a fase de extração de características, há uma fase de geração de características, quando usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, geramos características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então geramos primeiro duas novas características. Mas se o conjunto de dados tiver cinco classes, geramos primeiro cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado da arte atual em análise de texto, que são modelos de linguagem baseados em transformadores, como BERT, GPT, XLERT, etc. É provável que não possamos treinar modelos de linguagem usando os conjuntos de dados de entrada. Então, uma abordagem ingênua seria o fine-tuning de tarefa alvo. Portanto, na fase de extração de características, podemos baixar um modelo de linguagem pré-treinado, ajustar o modelo de linguagem sobre o conjunto de dados alvo neste exemplo para ajustar o modelo de linguagem para classificar texto em classes, abstrair em classes, baixo ou alto, receber a saída do modelo de linguagem, que é a probabilidade para cada classe, e usar como novas características. O problema com essa abordagem é que os conjuntos de dados podem ter poucas entidades de texto distintas. Em nosso experimento, quase metade dos conjuntos de dados contém menos de 400 amostras e o menor conjunto de dados contém 35 amostras em seu conjunto de treinamento. Então, para ajustar um modelo de linguagem sobre este conjunto de dados, seria ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados porque FAST é aplicamos FAST sobre vários conjuntos de dados. Podemos usar os n menos um conjuntos de dados para coletar informações sobre os n menos um conjuntos de dados e usar essas informações quando analisamos o enésimo conjunto de dados. O que sugerimos é adicionar outra fase de fine-tuning, uma fase preliminar de fine-tuning multitarefa, quando ajustamos o modelo de linguagem sobre n-1 conjuntos de dados e, então, executamos outra fase de fine-tuning, que é um fine-tuning de tarefa alvo, quando ajustamos o modelo de linguagem sobre o enésimo conjunto de dados alvo. O estado da arte em fine-tuning multitarefa chamado Empty DNN em Empty DNN Empty DNN mantém cabeças no número de tarefas no conjunto de treinamento, então se neste exemplo houver quatro tarefas no conjunto de treinamento, então Empty DNN mantém quatro cabeças, como você pode ver na imagem, e ele seleciona um lote aleatório do conjunto de treinamento e se o lote aleatório pertencer, por exemplo, à classificação Sing and Selton, ele executa caminhos para frente e para trás através da primeira cabeça. E se o lote aleatório pertencer a tarefas de classificação pairwise, ele executa caminhos para frente e para trás através da última cabeça. Em nosso cenário, um conjunto de dados de tabela varia o número de classes. Portanto, existem muitas tarefas. MTDNN mantém o número de cabeças de classes de camadas de saída e, adicionalmente, MTDNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem chamada fine-tuning de reformulação de tarefas é em nossa abordagem de fine-tuning de reformulação de tarefas, em vez de manter várias cabeças, reformulamos cada conjunto de dados em um problema de classificação por sentença, que é uma tarefa de duas classes. Então, vamos ver um exemplo. Aqui está nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes. E reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o resumo e a classe em verdadeiro ou falso. Ou, em outras palavras, treinamos o modelo de linguagem para classificar o resumo e a classe para o resumo e a classe se o resumo pertence à classe ou não, então o vetor de rótulo neste caso permanece sempre, que consiste sempre com duas classes e este é o algoritmo para nossa abordagem de fine-tuning ou reformulada. Então, vamos ver o framework completo, um conjunto de dados alimentado no FAST e então FAST executa na fase de vinculação, ele extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipédia, então ele reformula a tarefa em tarefas de classificação por sentença, aplica o modelo de linguagem à nova tarefa e produz probabilidade para cada classe. Note que o modelo de linguagem já foi ajustado em n-1 conjunto de dados usando um fine-tuning multitarefa preliminar. Então, usamos o vetor de saída do modelo de linguagem como uma característica recém-gerada no número de classes. Para avaliar nosso framework, usamos um conjunto de dados de classificação de 17 tabelas, que varia tamanho, características, equilíbrio, domínio e desempenho inicial. E como base de conhecimento, usamos a Wikipédia. Projetamos nosso experimento como uma avaliação de um fora, quando treinamos FAST em 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em um quatro-fold. falhas e aplicamos uma validação cruzada de quatro-fold. Então, geramos a nova característica e a avaliamos usando cinco classificadores de avaliação. Usamos em nosso experimento uma arquitetura baseada em BERT. Aqui estão os resultados para nosso experimento. Você pode ver que comparamos nosso framework ao fine-tuning do conjunto de dados alvo, fine-tuning de tarefa alvo e fine-tuning multitarefa preliminar e nosso fine-tuning reformulado alcança o melhor resultado, o melhor desempenho, enquanto MTDNN alcançou uma melhoria de 2% sobre o fine-tuning do conjunto de dados alvo. Nossa abordagem alcançou uma melhoria de 6% quando olhamos para os pequenos conjuntos de dados, podemos ver que o desempenho do MTDNN diminui e a melhoria da fase preliminar de fine-tuning multitarefa diminui para 1,5 por cento, mas nosso desempenho aumentou para 11 por cento em comparação com o fine-tuning de tarefa alvo sozinho. Em resumo, FAST permite enriquecimento com poucos exemplos de 35 amostras em nosso experimento. Ele usa uma arquitetura para todas as tarefas de dados e mantém a cabeça do modelo. Mas ele adiciona uma fase de formulação, ele aumenta o conjunto de treinamento e ele precisa de um valor alvo com significado semântico para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação por sentença. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje, apresentarei nosso trabalho de pesquisa, \"Aprendizado para Raciocínio Dedutivo: Resolução de Problemas Metro como Extração de Região Complexa\". Sou Alan do Laboratório de IA da Biden e este é um trabalho conjunto com Thierry da Universidade do Texas em Austin e Wayloo da SUDD. Primeiramente, gostaria de falar sobre nossa motivação para o raciocínio. Então, aqui mostramos um exemplo onde o raciocínio em várias etapas é útil. Esta figura é retirada do artigo POWN, onde eles realizam o prompting para resolver o problema metro em um cenário de aprendizagem por fusão. Na parte inferior da página, podemos ver que, se fornecermos alguns exemplos com apenas perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas se fornecermos mais descrições de raciocínio, o modelo é capaz de prever a descrição do raciocínio e também fazer uma previsão correta aqui. Portanto, é bom ter um raciocínio em várias etapas interpretável como saída. E nós também achamos que o problema Metro é uma aplicação direta para avaliar essas habilidades de raciocínio. Aqui, em nosso problema, dado o problema, precisamos resolvê-lo e obter as respostas numéricas. Em nossos conjuntos de dados, também fornecemos a expressão matemática que leva a essa resposta específica. Certas suposições também se aplicam, como em trabalhos anteriores. Assumimos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtração, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser efetivamente decodificados em esses operadores básicos. Portanto, trabalhos anteriores na resolução de problemas Metro podem ser categorizados em sequência para sequência e modelo de árvore de sequência. O modelo tradicional de sequência para sequência converte a expressão em uma sequência específica para geração e é bastante fácil de implementar e pode generalizar para muitos problemas complicados diferentes. Mas as desvantagens são que o desempenho é, na verdade, geralmente não melhor do que o modelo de estrutura e falta de interpretabilidade para a previsão. Mas, na verdade, esta direção ainda é bastante popular por causa do modelo Transformer. Em modelos baseados em árvore, nós estruturamos efetivamente essas expressões em uma forma de árvore e seguimos uma travessia pré-ordem na geração da árvore. Aqui, continuamos gerando os operadores até atingirmos as folhas, que são as quantidades. Aqui, a boa coisa é que ele nos dá efetivamente essa estrutura de árvore binária. Mas, na verdade, é bastante contra intuitivo. Porque geramos o operador primeiro e então, no final, geramos as quantidades. E a segunda coisa é que ele também contém alguns cálculos repetitivos. Portanto, aqui, se olharmos para esta expressão, a vezes três mais três, é efetivamente gerada duas vezes. Mas, na verdade, devemos reutilizar os resultados. Em nossa abordagem proposta, queremos resolver esses problemas de forma passo a passo e interpretável. Portanto, por exemplo, aqui no segundo passo, podemos obter este divisor, que é 27. E também podemos referir-nos à pergunta original para encontrar o conteúdo relevante. E nesses passos, obtemos os divisores. Então, e então neste terceiro passo, obtemos efetivamente o quociente, certo? E após esses três passos, podemos efetivamente reutilizar os resultados do segundo passo e, em seguida, obter os resultados do quarto passo. E então, finalmente, podemos obter o dividendo. Aqui, nós efetivamente geramos toda a expressão diretamente, em vez de gerar operadores ou quantidades individuais. Isso torna o processo mais preciso. Em nosso sistema dedutivo, primeiro começamos com um monte de quantidades apresentadas nas perguntas e também incluindo algumas constantes como nossos estados iniciais. A expressão é representada por EIJOP, onde realizamos operadores de Qi para QJ, e tal expressão é efetivamente direcionada. Nós também temos subtração invertida aqui para representar a direção oposta. Isso é bastante semelhante à extração de relações. Em um sistema dedutivo formal, no passo de tempo t, aplicamos o operador entre o par Qi e Qj e, em seguida, obtemos esta nova expressão. Nós a adicionamos aos estados seguintes para se tornar uma nova quantidade. Este slide realmente visualiza a evolução dos estados onde continuamos adicionando expressões aos estados atuais. Em nossas implementações de modelo, primeiro usamos um modelo de rede pré-treinado, que pode ser Birds ou Robots, e então codificamos uma frase e, em seguida, obtemos essas representações de quantidade. Uma vez que tenhamos as representações de quantidade, podemos começar a inferir. Aqui, mostramos um exemplo de Q um para obter a representação para Q um, eles serão divididos por Q dois e, em seguida, multiplicados por Q três. Primeiro, obtemos a representação do par, que é basicamente apenas a concatenação entre Q um e Q dois, e então aplicamos uma rede feedforward, que é parametrizada pelo operador. E, finalmente, obtemos a representação de expressão Q1 dividido por Q2. Mas, na prática, na fase de inferência, podemos obter a expressão incorreta também. Portanto, aqui, todas as expressões possíveis são iguais a três vezes o número de operadores. A coisa boa aqui é que podemos facilmente adicionar restrições para controlar este espaço de busca. Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão em nosso espaço de busca. No segundo passo, fazemos a mesma coisa, mas a única diferença é uma quantidade a mais. Esta quantidade vem da expressão calculada anteriormente. Portanto, finalmente, podemos obter esta expressão final Q três vezes Q quatro. E também podemos ver que o número de todas as expressões possíveis é diferente do passo anterior. Tal diferença torna difícil aplicar a busca em feixe porque a distribuição de probabilidade entre esses dois passos é desequilibrada. O procedimento de treinamento é semelhante ao treinamento de um modelo de sequência para sequência, onde otimizamos a perda em cada passo de tempo e aqui também usamos este tau para representar quando devemos terminar este processo de geração. E aqui, o espaço é diferente de sequência para sequência porque o espaço é diferente em cada passo de tempo, enquanto no modelo de sequência para sequência tradicional é o número de vocabulário. E também nos permite impor certas restrições de conhecimento prévio. Nós conduzimos experimentos nos conjuntos de dados de problemas Metro comumente usados, MAWPS, Math 23K, MathQA e SWAM. E aqui mostramos brevemente os resultados comparados com as melhores abordagens anteriores. Nossa variante com melhor desempenho é Roberta Deductive Reasoner. Na verdade, não usamos BeamSearch em contraste com abordagens anteriores que usam BeamSearch. As melhores abordagens são frequentemente modelos baseados em árvore. No geral, nosso solucionador é capaz de superar significativamente este modelo baseado em árvore, mas podemos ver que o número absoluto em MathQA ou SWAMP não é realmente alto. Investigamos ainda mais os resultados em SWAMP. E este conjunto de dados é desafiador porque o autor tentou adicionar manualmente algo para confundir o modelo de PNL, como adicionar informações irrelevantes e quantidades extras. Em nossa previsão, encontramos alguns dos valores intermediários são realmente negativos. Por exemplo, nesta pergunta, estamos perguntando quantas maçãs Jake tem, mas temos algumas informações extras, como dezessete lançamentos menos e Stephen tem oito lançamentos, o que é totalmente irrelevante. Nosso modelo faz algumas previsões como esta, produzindo valores negativos. E observamos que essas duas expressões têm similaridade. Podemos efetivamente limitar este espaço de busca removendo como esses resultados são negativos para que possamos fazer a resposta correta. Descobrimos ainda mais que tal restrição realmente melhora um pouco para alguns modelos. Por exemplo, para Birds, melhoramos em sete pontos e para o modelo base Roberta, realmente melhoramos em dois pontos. Um modelo de linguagem melhor tem uma melhor capacidade de compreensão de linguagem, para que o número aqui seja maior para Roberta e menor para Birds. Nós também tentamos analisar a dificuldade por trás de todo este conjunto de dados. Assumimos que o número de quantidades não utilizadas pode ser considerado como informação irrelevante aqui. Aqui, podemos ver que temos a porcentagem de amostras com quantidades não utilizadas e o conjunto de dados SWAMP tem a maior parte. Aqui também mostramos o desempenho geral. Para aquelas amostras sem quantidades não utilizadas, o desempenho geral é, na verdade, mais alto do que. E o desempenho é realmente mais alto do que o desempenho geral. Mas com aquelas amostras que com quantidades não utilizadas, é efetivamente muito pior do que o desempenho geral. Para MAWPS, não temos muitos casos de mesa, então simplesmente ignoro esta parte. Finalmente, queremos mostrar a interpretabilidade por meio de um exemplo de falha e participação. Aqui, nosso modelo efetivamente faz uma previsão errada no primeiro passo. Podemos efetivamente correlacionar esta expressão com a frase aqui. Acreditamos que esta frase pode estar enganando o modelo para uma previsão incorreta. Aqui, plantar trinta e cinco faz o modelo pensar que deve ser um operador adicional. Nós tentamos revisar a frase para que seja algo como o número de árvores de pêssego, trinta e cinco a menos do que as árvores de maçã. Nós a tornamos para transmitir semântica mais precisa, de forma que o modelo possa fazer a previsão correta. Este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo. Em conclusão, nosso trabalho, primeiro, nosso modelo é efetivamente bastante eficiente e somos capazes de fornecer um procedimento de resolução interpretável e podemos facilmente incorporar algum conhecimento prévio como restrição, o que pode ajudar a melhorar o desempenho. A última coisa é que o mecanismo subjacente não se aplica apenas às tarefas de resolução de problemas Metro, mas também a outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada em diferentes passos de tempo, também é bastante desafiador aplicar uma estratégia de busca em feixe. Esta é o fim da apresentação e perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Apresentarei meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para recuperação de artigos estatutários. Questões legais são uma parte integrante da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência dispendiosa de um especialista jurídico ficam desprotegidos ou, pior, explorados. Nosso objetivo é preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema de recuperação eficaz para artigos estatutários. Tal sistema poderia fornecer um serviço de assistência jurídica profissional gratuita para pessoas não especializadas. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre uma questão legal, como qual o risco se eu violar a confidencialidade profissional? Um modelo é necessário para recuperar todos os artigos estatutários relevantes de um vasto corpo de legislação. Esta tarefa de recuperação de informações apresenta seus próprios desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem jurídica complexa para os estatutos. Esta diferença na distribuição da linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois indiretamente requer um sistema de interpretação inerente que possa traduzir uma pergunta natural em uma pergunta jurídica que corresponda à terminologia dos estatutos. Além disso, a lei estatutária não é um conjunto de artigos independentes que podem ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas no contexto geral, ou seja, juntamente com as informações complementares dos artigos vizinhos, os campos e subcampos a que pertencem e o seu lugar na estrutura da lei. Por último, os artigos estatutários não são pequenos parágrafos, o que geralmente é a unidade de recuperação típica na maioria dos trabalhos de recuperação. Aqui, eles são documentos longos que podem ter até seis mil palavras. Os recentes avanços em PNL despertaram grande interesse em muitas tarefas jurídicas, como a previsão de julgamentos legais ou a revisão automatizada de contratos, mas a recuperação de artigos estatutários permaneceu amplamente intocada devido à falta de grandes conjuntos de dados de alta qualidade e rotulados. Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão nativo da França para estudar se os modelos de recuperação podem aproximar a eficiência e a confiabilidade de um especialista jurídico para a tarefa de recuperação de artigos estatutários. Nossos conjuntos de dados de recuperação de artigos estatutários belgas consistem em mais de um mil e cem perguntas legais feitas por cidadãos belgas. Essas perguntas abrangem uma ampla gama de tópicos, desde família, habitação, dinheiro, até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de vinte e dois mil seiscentos artigos legais dos códigos de lei belgas. Vamos agora falar sobre como coletamos este conjunto de dados. Primeiro, começamos compilando um vasto corpus de artigos legais. Consideramos trinta e dois códigos belgas publicamente disponíveis e extraímos todos os seus artigos, bem como os títulos de seção correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, estabelecemos uma parceria com um escritório de advocacia belga que recebe cerca de quatro mil e-mails por ano de cidadãos belgas que pedem aconselhamento sobre uma questão legal pessoal. Tivemos a sorte de ter acesso aos seus sites, onde sua equipe de juristas experientes aborda os problemas legais mais comuns da Bélgica. Coletamos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por último, passamos as referências legais e filtramos as perguntas cujas referências não estavam em um dos códigos de lei que consideramos. As referências restantes foram correspondidas e convertidas nos IDs de artigo correspondentes do nosso corpus. Acabamos com um mil e cem e oito perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso grande corpus de vinte e dois mil seiscentos e trinta e três artigos estatutários. Além disso, cada pergunta vem com uma principal, com uma concatenação de seus títulos subsequentes na estrutura da lei. Essas informações extras não são usadas no presente trabalho, mas podem ser de interesse para futuras pesquisas sobre recuperação de informações jurídicas ou classificação de texto jurídico. Vamos dar uma olhada em algumas características do nosso conjunto de dados. As perguntas têm entre cinco e quarenta e quatro palavras, com uma mediana de quarenta palavras. Os artigos são muito mais longos, com um comprimento médio de setenta e sete palavras, sendo cento e quarenta e dois deles com mais de mil palavras. Os mais longos têm até cinco mil setecentos e noventa palavras. Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de oitenta e cinco por cento deles sendo sobre família, habitação, dinheiro ou justiça, enquanto os quinze por cento restantes dizem respeito a segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de trinta e dois códigos belgas diferentes que cobrem um grande número de artigos legais coletados de cada um desses códigos belgas. Dos 22.633 artigos, apenas 1.612 são referenciados como relevantes para pelo menos uma pergunta no conjunto de dados e cerca de 80% desses artigos citados vêm do código civil, código judicial, código de investigação criminal ou códigos penais. Enquanto isso, 18 dos 32 códigos têm menos de 5 artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esses códigos se concentram menos nos indivíduos e suas preocupações. No geral, o número médio de citações para esses artigos citados é dois e menos de vinte e cinco por cento deles são citados mais de cinco vezes. Usando nossos conjuntos de dados, avaliamos vários métodos de recuperação, incluindo arquiteturas lexicais e densas. Dado uma consulta em um artigo, um modelo lexical atribui uma pontuação ao par consulta-artigo calculando a soma sobre os termos da consulta dos pesos de cada um desses termos nesse artigo. Experimentamos com as funções de classificação padrão TFIDF e BM25. O principal problema com essas abordagens é que elas só podem recuperar artigos que contêm palavras-chave presentes na consulta. Para superar essa limitação, experimentamos uma arquitetura baseada em rede neural que pode capturar as relações semânticas entre consultas e artigos. Usamos um modelo B encoder que mapeia consultas e artigos para representações vetoriais densas e calcula uma pontuação relevante entre um par consulta-artigo pela similaridade de seus embeddings. Esses embeddings geralmente resultam de uma operação de pooling na saída de um modelo de embedding de palavras. Primeiro, estudamos a eficácia dos B encoders siameses em uma configuração de avaliação zero-shot, o que significa que os modelos de embedding de palavras pré-treinados são aplicados diretamente sem ajuste fino adicional. Experimentamos com codificadores de texto independentes do contexto, nomeadamente Word to Vec e Fasttext, e modelos de embedding dependentes do contexto, nomeadamente Roberta e, mais especificamente, Camembert, que é um modelo Roberta francês. Além disso, treinamos nosso próprio modelo Biancoders baseado em Camembert no conjunto de dados completo. Note que, para treinamento, experimentamos as duas variantes da arquitetura Biancoder. Siamesa, que usa um modelo de embedding de palavras único que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado. E a de dois torres, que usa dois modelos de embedding de palavras independentes que codificam a consulta e o artigo separadamente em diferentes espaços de embedding. Experimentamos com pooling médio, máximo e CLS, bem como produto escalar e cosseno para calcular similaridades. Aqui estão os resultados de nossas linhas de base no conjunto de teste com os métodos lexicais acima, os B encoders siameses avaliados em uma configuração zero-shot no meio e os B encoders ajustados abaixo. No geral, os B encoders ajustados superam significativamente todas as outras linhas de base. O modelo de duas torres melhora sua variante siamesa em recall em cem, mas tem um desempenho semelhante em outras métricas. Embora o BM25 tenha ficado significativamente abaixo do Biancoder treinado, seu desempenho indicou que ainda é uma forte linha de base para recuperação específica do domínio. Quanto à avaliação zero-shot dos Biancoders siameses, descobrimos que o uso direto dos embeddings de um modelo Kamembert pré-treinado sem otimização para a tarefa de recuperação de informações fornece resultados ruins, o que é consistente com achados anteriores. E o Biancoder baseado em word-to-vec superou significativamente os modelos baseados em fast-text e bird, sugerindo que talvez os embeddings de nível de palavra pré-treinados sejam mais apropriados para a tarefa do que os embeddings de nível de caractere ou subpalavra quando usados diretamente. Embora promissores, esses resultados sugerem amplas oportunidades de melhoria em comparação com um especialista jurídico qualificado que pode, eventualmente, recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Concluímos discutindo duas limitações de todos os conjuntos de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos trinta e dois códigos belgas considerados, o que não cobre toda a lei belga, pois os artigos de decretos, diretivas e decretos são omitidos. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes. Esta perda de informação implica que a resposta contida nos artigos relevantes restantes pode estar incompleta, embora ainda seja totalmente apropriada. Em segundo lugar, devemos notar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta posso despejar meus inquilinos se eles fizerem muito barulho pode não ter uma resposta detalhada na lei estatutária que quantifique um limite específico de ruído no qual o despejo é permitido. Em vez disso, o senhorio provavelmente deveria confiar mais na jurisprudência e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz duas festas por semana até as duas da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas permanece a ser determinado. Esperamos que todo este trabalho desperte o interesse em desenvolver modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode verificar nosso artigo, conjunto de dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, estamos felizes em apresentar nosso trabalho sobre VALS, um benchmark independente de tarefas destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de configurar este benchmark? Bem, nos últimos anos, temos testemunhado uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares de imagem e texto. Cada um desses modelos impulsiona o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, ancoragem de frases. Assim, recebemos a mensagem de que as precisões nessas métricas específicas estão aumentando constantemente, mas sabemos o que os modelos realmente aprenderam? O que é que um transformador de visão e linguagem entendeu ao atribuir uma alta pontuação para esta imagem e esta frase para corresponder e uma baixa pontuação para esta outra? Os modelos de visão e linguagem se concentram no que é certo ou se concentram em vieses, como demonstrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica em relação às tarefas e introduzimos válvulas que testam a sensibilidade dos modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto a modalidade linguística quanto a visual. Almejamos existência, pluralidade, contagem, relações espaciais, ações e correferência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? *Foiling*, um método previamente aplicado apenas para modelos de visão e linguagem para frases nominais por Ravi Shakar e colaboradores e para contagem por nós em trabalhos anteriores. *Foiling* basicamente significa que pegamos a legenda de uma imagem e produzimos uma legenda adulterada, alterando a legenda de forma que ela não descreva a imagem mais. E fazemos essas alterações de frases, focando em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e correferência de entidades. Cada peça pode consistir de um ou mais instrumentos caso tenhamos encontrado mais de uma maneira interessante de criar instâncias adulteradas. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo de ação é alterado com uma ação diferente e outro em que os actantes são trocados. Contagem e correferência também são peças que têm mais de um instrumento. E criamos essas legendas adulteradas garantindo que elas falhem em descrever a imagem, que sejam gramaticais e, de resto, válidas. Isso não é fácil de fazer porque uma legenda adulterada pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem cortar plantas, e grandes modelos de visão e linguagem podem captar isso. Portanto, para obter legendas adulteradas válidas, devemos agir. Primeiro, utilizamos modelos de linguagem fortes para propor legendas adulteradas. Segundo, usamos inferência de linguagem natural ou NLI curta para filtrar as legendas adulteradas que ainda podem estar descrevendo a imagem, já que, ao construir legendas adulteradas, precisamos garantir que elas falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte justificativa. Consideramos uma imagem como a premissa e sua legenda como a hipótese implicada. Além disso, consideramos a legenda como a premissa e a legenda adulterada como sua hipótese. Se um modelo de NLI prevê que a legenda adulterada contradiz ou é neutra em relação à legenda, tomamos isso como um indicador de uma legenda adulterada válida. Para ser implicada pela legenda, não pode ser uma boa legenda adulterada, pois, por transitividade, dará uma descrição verdadeira da imagem e filtramos essas legendas adulteradas. Mas este procedimento não é perfeito. É apenas um indicador de legendas adulteradas válidas, portanto, como uma terceira medida para gerar legendas adulteradas válidas, empregamos anotadores humanos para validar os dados usados no Vals. Portanto, após a filtragem e avaliação humana, temos tantas instâncias de teste quanto descrito nesta tabela. Observe que o Vals não fornece nenhum dado de treinamento, mas apenas dados de teste, já que é apenas um benchmark de teste de disparo único. Ele foi projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino permitiria apenas que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos nós sabemos que esses modelos gostam de trapacear e usar atalhos. E, como dissemos, estamos interessados em avaliar as capacidades que os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem no VALS, nomeadamente com clip, Wilbert, Wilbert Kelvin one e Visual Bert. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos na classificação de pares de imagem e legenda em legendas e legendas adulteradas. Talvez mais relevante para este vídeo, demonstraremos nossa métrica mais permissiva, a precisão por pares, que mede se a pontuação de alinhamento de imagem e legenda é maior para o par de texto da imagem correto do que para seu par adulterado. Para mais métricas e resultados sobre elas, consulte nosso artigo. Os resultados com precisão por pares são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas, no sentido de que o melhor desempenho de disparo único é alcançado por Wilbert twelve in one, seguido por Wilbert, Alex Mert, Clip e, finalmente, Visual Bird. É notável como os instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos por Wilbert twelve in one, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das outras peças pode ser confiavelmente resolvida em nossos cenários adversários de adulteração. Vemos a partir dos instrumentos de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos versus múltiplos ou em contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo que sejam apoiados por vieses de plausibilidade, como vemos na peça de ações. A partir da peça de correferência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para os modelos de visão e linguagem. Como um teste de sanidade e porque é um experimento interessante, também comparamos dois modelos de texto apenas, GPT one e GPT two, para avaliar se o VALS pode ser resolvido por esses modelos unimodais, computando a perplexidade da legenda correta e da legenda adulterada sem imagem e prevendo a entrada com a perplexidade mais baixa. Se a perplexidade for maior para a legenda adulterada, tomamos isso como uma indicação de que a legenda adulterada pode sofrer de viés de plausibilidade ou outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos de texto apenas GPT capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Em resumo, VALS é um benchmark que usa a lente dos constructos linguísticos para ajudar a comunidade a melhorar os modelos de visão e linguagem, testando arduamente suas capacidades de aterramento visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados em sua presença em imagens bem, como demonstrado pela peça de existência, mas lutam para aterrar sua interdependência e relacionamentos em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de incentivar a comunidade a usar Valse para medir o progresso em direção ao aterramento de linguagem com modelos de visão e linguagem. E mais, Valse pode ser usado como uma avaliação indireta de conjuntos de dados, já que os modelos podem ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados por Valse. Se estiver interessado, consulte os dados do Valse no GitHub e, se tiver alguma dúvida, não hesite em entrar em contato conosco."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisara, da Universidade de Tóquio. Apresentarei um artigo intitulado R e SAM, um conjunto de dados em larga escala para determinação automática da duração do risco por meio da sumarização do log de commits. Explicarei na seguinte ordem. Primeiro, introduzirei a determinação automática do risco sobre a qual estamos trabalhando nesta pesquisa. O release note é um documento técnico que resume as alterações distribuídas com cada lançamento de um produto de software. A imagem mostra os release notes da versão dois ponto seis ponto quatro da biblioteca GBUJS. Esses notes desempenham um papel importante no desenvolvimento de código aberto, mas são demorados para serem preparados manualmente. Portanto, seria muito útil poder gerar automaticamente release notes de alta qualidade. Referirei-me a duas pesquisas anteriores sobre geração automática de release notes. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, utilizando o change extractor para extrair as diferenças principais, alterações da biblioteca e alterações de documentação, a partir das diferenças entre os lançamentos e, finalmente, combiná-las. A característica mais notável deste sistema é o issue extractor, no canto superior direito, que deve ser vinculado ao Jira, ao ecossistema de issues e pode ser aplicado apenas a projetos que utilizam o Jira. Em outras palavras, não pode ser usado para muitos projetos no GitHub. A segunda é Griff, anunciada recentemente em 2020. Está disponível na internet e pode ser instalada via PIP. Este sistema possui um modelo simples de classificação de texto baseado em aprendizado e gera uma das cinco categorias, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna uma categoria de corretivo ou correção de bugs. O conjunto de dados de treinamento do Griff é relativamente pequeno, com cerca de cinco mil exemplos, e será apresentado nos experimentos descritos a seguir. O desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas existem problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolveu esses dois problemas e gera automaticamente release notes de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificador de alta qualidade utilizando apenas a mensagem do commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos um conjunto de dados RNSAM, consistindo de aproximadamente oitenta e duas mil peças de dados, coletando dados de repositórios públicos do GitHub utilizando a API do GitHub. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dado. O lado esquerdo é a mensagem do commit e o lado direito é o release note. Os release notes são categorizados como melhorias, correções de bugs, etc. Configuramos uma tarefa que recebe as mensagens do commit como entrada e gera os release notes categorizados. Isso pode ser considerado uma tarefa de sumarização. Pré-definimos quatro categorias: recursos, melhorias, correções de bugs, depreciações e remoções. Estas foram definidas com base em pesquisas anteriores e outros fatores. As notas de release na parte inferior direita são extraídas das notas de release mostradas na parte inferior esquerda. Neste momento, é necessário detectar as quatro categorias que foram pré-definidas, mas as categorias nem sempre são consistentes com cada repositório. Por exemplo, a categoria de melhorias inclui melhorias, aprimoramentos, otimizações e assim por diante. Preparamos uma lista de vocabulário de categorias de estudo para cada uma dessas variações notacionais. Utilize-a para detectar a classe RISNOD e corrigir o texto do RIS que segue como a frase do RISNOD para a classe. Em seguida, está a mensagem do commit. As mensagens do commit não estão vinculadas a cada RIS. Como mostrado na imagem abaixo, se o RIS atual for Persian dois ponto cinco a dezenove, precisamos identificar o RIS anterior Persian dois ponto cinco a dezoito e obter a diferença. Isso é um pouco trabalhoso e não é suficiente apenas obter uma lista de RIS e observar o antes e depois. Criamos uma correspondência heurística azul para obter o patch anterior e seguinte. Análise do dataset: no final, sete mil e duzentos repositórios e oitenta e duas mil peças de dados foram corrigidos. Além disso, o número médio de tokens do release note é sessenta e três, o que é bastante alto para uma tarefa de sumarização. Além disso, o número de tokens exclusivos é bastante grande, com oito mil e trezentos e oitenta. Isso se deve ao grande número de custos e nomes de métodos exclusivos encontrados no repositório. Em seguida, explicarei o método proposto. O modelo de sumarização extrativa e abstrativa cruzado consiste em dois módulos novos, um classificador utilizando bot ou code bot e um gerador utilizando bot. Primeiro, o GEAS utiliza um classificador para classificar cada mensagem de commit em cinco classes de motivo: recursos, melhorias, correções de bugs, depreciações e outras. As mensagens de commit classificadas como outras são descartadas. Então, o GEAS aplica um gerador aos quatro documentos de categoria independentemente e gera notas de motivo para cada categoria. Nesta tarefa, a correspondência direta entre mensagens de commit e notas de motivo não é conhecida. Portanto, para treinar o classificador, atribuímos duas categorias a cada mensagem de commit de entrada utilizando os dez primeiros caracteres de cada mensagem de commit. Modelamos a sumarização abstrativa do classificador pelos dois métodos diferentes. O primeiro modelo, que chamamos de GAS single, consiste em uma rede sect to sect única e gera uma única lista longa de notas, concatenando as mensagens de commit de entrada. O texto de saída pode ser dividido em categorias por segmento com base em símbolos de endpoint específicos da categoria. O segundo método, que chamamos de GSMAUC, consiste em quatro redes sect to sect diferentes, cada uma das quais corresponde a uma das categorias de notas. Okay, me deixe explicar o experimento. Cinco métodos foram comparados: GS, GS single, GS march, rustling e o estudo anterior Griff. No que diz respeito ao aborto, em alguns casos, essas notas são geradas em várias frases. Como é difícil calcular o número de frases em zero, elas são combinadas com espaços e tratadas como uma única frase longa. A pontuação da brew é penalizada quando o sistema gera uma frase curta. Esta penalidade resulta em um valor mais baixo da brew nos resultados do experimento descritos a seguir. Finalmente, calculamos também a especificidade porque Rouge e Brew não podem ser calculados se os release notes estiverem vazios. Uma especificidade alta significa que o modelo gera corretamente um texto vazio nos casos em que os release notes são realmente vazios. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores hash, etc., também eliminamos o conjunto de dados verde, que os exclui. GAS e GAS alcançaram pontuações de erro Rouge mais de dez pontos mais altas que a linha de base. No entanto, no conjunto de testes verde, a diferença de pontuação entre o método proposto e a linha de base saltou para mais de vinte pontos. Esses resultados indicam que o GAS e o GAS são significativamente eficazes. GAS obteve uma pontuação loose melhor do que GAS, sugerindo que combinar um classificador e um gerador é eficaz no treinamento do classificador usando pseudobus. A alta cobertura do GAS pode ser alcançada provavelmente porque o classificador pode se concentrar na seleção de mensagens de commit relevantes para cada categoria. É muito propenso a gerar regras mais altas do que ela é single, sugerindo que também é eficaz desenvolver independentemente diferentes modelos de sumarização de perspectiva para cada uma dessas classes. Aqui está uma análise de erro. Os métodos GS tendem a gerar frases mais curtas do que as frases de referência humanas. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto a GS tem apenas uma. A razão para esta relutância do modelo é que, nos dados de treinamento, apenas 33% das frases estão presentes na categoria de recursos e 40% na categoria de melhorias. Além disso, os métodos GS não podem gerar notas de motivo precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa e a frase completa não pode ser gerada sem referência ao pedido ou issue paralelo correspondente. O exemplo abaixo mostra que as duas mensagens de commit de entrada estão relacionadas e devem ser combinadas em uma única frase, mas não o fazem. Finalmente, uma conclusão. Construímos um novo conjunto de dados para geração automática de notas. Também formulamos a tarefa de inserir mensagens de commit e resumi-las para que seja aplicável a todos os projetos escritos em inglês. Nossos experimentos mostram que o método proposto gera notas de motivo menos ruidosas e com maior cobertura do que a linha de base. Verifique nosso conjunto de dados no GitHub. Obrigado."}
