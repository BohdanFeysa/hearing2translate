{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫Assa Farari，我将为大家介绍我们的论文《FUESHOT TABLAR DATA 架构利用精细调优的Transformer模型》。数据科学家通常分析数据并主要关注于操纵现有特征，但有时这些特征是有限的。利用另一个数据源生成特征可能会增加大量信息。我们的研究目标是使用外部来源的自由文本进行自动表格数据增强。假设我们有一个表格数据集和一个知识库。我们需要一个自动流程，该流程涉及实体链接和文本分析，以从知识库的自由文本中提取新的特征。我们的框架FAST正是这个自动流程。那么，我们来看一个例子。一个数据集被输入到FAST中。在这个例子中，数据集是大学数据集，其目标是将大学分类为低排名大学和高排名大学。我们将维基百科用作知识库。FAST的第一阶段是实体链接，在这个例子中，每个实体，即大学名称，会被链接到知识库中的一个实体，然后从知识库中的实体文本中提取文本并添加到数据集中。在这个例子中，文本是维基百科页面的摘要。现在我们需要从检索到的文本中生成或提取特征。因此，我们需要一个特征提取阶段，该阶段包括文本分析，这是本文的主要创新点，我将在后续幻灯片中详细介绍。在特征提取阶段之后，有一个特征生成阶段，在该阶段我们使用提取的特征来生成少量的新的特征。首先，生成特征的数量与原始数据集的类数相同。在这个例子中，原始数据集有两个类，因此首先生成两个新的特征。但是，如果数据集有五个类，则首先生成五个新的特征。每个特征代表每个类别的可能性。为了分析文本，我们使用当前文本分析的最新技术，即基于Transformer的语言模型，如BERT、GPT、XLERT等。但不太可能能够使用输入数据集训练语言模型。因此，一种朴素的方法是目标任务微调。在特征提取阶段，我们可以下载预训练的语言模型，在目标数据集上对语言模型进行微调，在这个例子中，将语言模型微调为将文本分类到类别中，将摘要分类到类别中，分为低或高，接收语言模型的输出，即每个类的可能性，并将其用作新的特征。这种方法的缺点是数据集可能包含少数不同的实体文本。在我们的实验中，几乎一半的数据集包含400个样本少于，最小的数据集训练集包含35个样本。因此，在一个小数据集上微调语言模型将是无效的。但我们可以使用关于预分析数据集的先验知识，因为FAST是在多个数据集上应用的。我们可以使用n-1个数据集来收集关于n-1个数据集的信息，并在分析第n个数据集时使用这些信息。我们建议增加另一个微调阶段，即一个初步的多任务微调阶段，在该阶段我们在n-1个数据集上微调语言模型，然后我们执行另一个微调阶段，即目标任务微调，在该阶段我们在第n个目标数据集上微调语言模型。多任务微调领域的最新技术称为Empty DNN。Empty DNN在训练集中维护与任务数相同数量的头，因此，如果在这个例子中训练集中有四个任务，则Empty DNN维护四个头，如图片所示，它从训练集中随机抽取一个批次，如果随机批次属于例如Sign and Selton的分类任务，则执行第一头的前向和后向路径。如果随机批次属于pairwise ranking任务，则执行最后一头的前向和后向路径。在我们的场景中，表格数据集的变化导致类别的数量变化，因此有许多任务。MTDNN维护类别数量的头和输出层，此外，MTDNN需要为新的数据集和新的任务初始化新的头。我们提出的方法称为任务重构微调。在我们的方法中，任务重构微调取代了维护多个头的做法，我们将每个数据集重构为一个句子分类问题，即一个二元分类问题。让我们来看一个例子。这是我们的输入数据集，它由实体、特征、文本和类别组成。我们将任务从将文本分类到低和高重构为将文本、摘要和类别分类为真或假。换句话说，我们训练语言模型将摘要和类别分类为摘要和类别，如果摘要属于该类别，则为真。在这种情况下，标签向量始终由两个类别组成，这就是我们提出的重构微调方法所使用的算法。那么，让我们看看完整的框架，数据集被输入到FAST中，然后FAST执行链接阶段，它从知识库中提取文本，在这个例子中，是维基百科页面的摘要，然后它将任务重构为句子分类任务，将语言模型应用于新的任务并输出每个类的可能性。请注意，语言模型已经在n-1个数据集上进行了微调，使用了初步的多任务微调。然后，我们将语言模型的输出向量用作一个新的特征，其数量与类数相同。为了评估我们的框架，我们使用了17个表格分类数据集，这些数据集在大小、特征、平衡性、领域和初始性能方面各不相同。我们将维基百科用作知识库。我们设计我们的实验为留一法评估，我们在16个数据集上训练FAST，并将其应用于第17个数据集。我们还将每个数据集划分为四个fold，进行four-fold交叉验证。然后，我们生成新的特征，并使用五个评估分类器对它们进行评估。我们使用基于BERT的架构进行实验。这是我们实验的结果。您可以看到，我们将我们的框架与目标数据集微调、目标任务微调和MTDNN初步微调进行比较，我们的重构微调实现了最佳结果，即最佳性能，而MTDNN比目标数据集微调实现了2%的改进。当我们关注小数据集时，我们可以看到MTDNN的性能下降，初步的多任务微调阶段的改进降低到1.5%，但我们的性能与单独的目标任务微调相比增加了11%。总而言之，FAST可以实现从我们实验中的35个样本进行少样本增强。它使用一种架构来处理所有任务和数据集，并保留模型的头部。它添加一个重构阶段，增加训练集，并需要一个具有语义含义的目标值，以便我们将其输入到语言模型中并在句子分类问题中使用它。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我将介绍我们的研究工作《学习演绎推理：地铁问题求解作为复杂区域提取》。我是来自拜登人工智能实验室的Alan，这是一项与德克萨斯大学奥斯汀分校的Thierry和SUDD的Wayloo合作的工作。首先，我想谈谈我们对于推理的动机。这里展示了一个例子，说明多步骤推理的帮助。这个图取自POWN的论文，他们在融合学习场景下使用提示来解决地铁问题。在网络笔侧，我们可以看到，如果我们仅仅给出问题和答案的示例，可能无法获得正确的答案。但如果我们提供更详细的推理描述，模型就能够预测推理描述，并在此做出正确的预测。因此，拥有可解释的多步骤推理作为输出是很有益处的。我们认为地铁问题是一个直接的应用，可以评估这种推理能力。在这里，我们的问题设置中，给定问题，我们需要解决这个问题并获得数值答案。在我们的数据集中，我们也提供了导致特定答案的数学表达式。某些假设也遵循之前的研究，我们假设数量的精度是已知的，并且我们只考虑基本的运算符，例如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以解码为这些基本运算符。之前在地铁问题求解方面的研究可以归类为序列到序列和序列到树模型。传统的序列到序列模型将表达式转换为特定的序列以进行生成，这非常容易实现，并且可以推广到许多不同的复杂问题。但是，其缺点是性能通常不如结构模型，并且缺乏对预测的可解释性。但是，由于transformer模型，这个方向仍然非常流行。在基于树的模型中，我们实际上以树的形式构建这些表达式，并遵循树生成的预序遍历。在这里，我们持续生成运算符，直到到达叶节点，这些叶节点是数量。这里的好处是它给我们提供了这种二元树结构。但是，实际上这有点反直觉。因为我们首先生成运算符，然后在最后生成数量。第二，它还包含一些重复的计算。例如，如果我们查看这个表达式“a乘以三加上三”，它实际上被生成了两次。但事实上，我们应该重用结果。在我们的提出的方法中，我们希望以逐步和可解释的方式解决这些问题。例如，在第二步，我们可以获得这个除数，即27。我们还可以回溯到原始问题以找到相关的内容。在这些步骤中，我们获得除数。然后，在第三步，我们实际上得到商，对吧？在经过这些三个步骤后，我们可以重用第二步的结果，然后得到第四步的结果。然后，最后，我们可以获得被除数。因此，我们实际上直接生成整个表达式，而不是生成单个运算符或数量。这使得过程更加准确。在我们演绎系统中，我们首先从问题中呈现的一组数量以及一些常数作为我们的初始状态开始。表达式由EIJOP表示，我们在Qi到QJ之间执行运算符，这种表达式实际上是有方向的。我们这里也表示反向减法以表示相反的方向。这与关系抽取非常相似。在正式的演绎系统中，在时间步t，我们应用Qi和Qj对之间的运算符，然后我们获得这个新的表达式。我们将它添加到下一状态以成为一个新的数量。这张幻灯片实际上可视化了状态的演变，我们不断地将表达式添加到当前状态。在我们的模型实现中，我们首先使用预训练的网络模型，可以是BERT或RoBERTa，然后我们编码句子，然后我们获得这些数量的表示。一旦我们获得数量的表示，我们就可以开始进行推理。这里显示了一个例子，Q1要获得Q1的表示，它将除以Q2并乘以Q3。首先，我们获得对表示，本质上是Q1和Q2之间的连接，然后我们应用一个前馈网络，该网络由运算符参数化。最后，我们获得表达式表示Q1除以Q2。但在实践中，在推理阶段，我们可能也能够获得不正确的表达式。因此，所有可能的表达式等于三倍于运算符的数量。这里的好处是，我们可以轻松地添加约束来控制此搜索空间。例如，如果此表达式不允许，我们 simply 移除此表达式在我们的搜索空间中。在第二步，我们做同样的事情，但唯一的区别是多了一个数量。这个数量来自之前计算的表达式。最后，我们可以获得这个最终表达式Q3乘以Q4。我们还可以看到所有可能的表达式的数量与之前的步骤不同。这种差异使得很难应用beam search，因为这些步骤之间的概率分布是不平衡的。训练过程类似于训练序列到序列模型，我们在每个时间步优化损失。在这里，我们还使用tau来表示何时应终止生成过程。这里的空间与序列到序列不同，因为空间在每个时间步是不同的，而在传统的序列到序列模型中，它是词汇的数量。它还允许我们从先验知识中施加某些约束。我们对常用的地铁问题数据集MAWPS、Math 23K、MathQA和SWAMP进行了实验。在这里，我们简要地展示了与之前的最佳方法的比较结果。我们的最佳性能变体是Roberta Deductive Reasoner。事实上，与使用BeamSearch的之前方法不同，我们没有使用BeamSearch。最佳方法通常是基于树的模型。总的来说，我们的reasoner 能够显著优于这种基于树的模型，但我们可以在MathQA或SWAMP上看到绝对数字并不高。我们进一步调查了SWAMP上的结果。这个数据集具有挑战性，因为作者试图手动添加一些东西来混淆NLP模型，例如添加不相关的的信息和额外的数量。在我们的预测中，我们发现其中一些中间值实际上是负数。例如，在这个问题中，我们正在询问Jake有多少个苹果，但我们有一些额外的、完全不相关的信息，例如十七个pitch比Stephen少，Stephen有八个pitch。因此，我们的模型会做出一些预测，例如产生负值。我们观察到这两个表达式实际上具有相似性。因此，我们可以通过删除像那些结果是负数这样的结果来限制这个搜索空间，从而使答案正确。我们进一步发现这种约束实际上对某些模型有所改善。例如，对于BERT我们提高了七分，然后对于Roberta base模型我们实际上提高了两分。更好的语言模型具有更好的语言理解能力，所以这里的数字对于Roberta来说更高，对于BERT来说更低。我们还尝试分析这个数据集背后的难度。我们假设未使用数量的比例可以被认为是无关紧要的信息。在这里，我们可以看到我们有样本中未使用数量的百分比，而SWAMP数据集具有最大的比例。我们还显示了整体性能。对于那些没有未使用数量的样本，整体性能实际上更高，但对于那些样本中包含未使用数量，它的性能实际上比整体性能差很多。对于MAWPS，我们没有太多错误示例，所以我忽略这部分。最后，我们希望通过案例研究和比较示例展示可解释性。在这里，我们的模型在第一步做出了错误的预测。我们可以将这个表达式与句子相关联。我们认为这句话可能会误导模型做出不正确的预测。在这里，种植另外三十五个会使模型认为应该是一个附加的运算符。因此，我们尝试修改句子为“苹果树的数量比梨树少三十五个”。使它传达更准确的语义，以便模型能够做出正确的预测。这项研究表明可解释的预测如何帮助我们理解模型的行为。总而言之，我们的模型实际上非常高效，我们能够提供可解释的求解过程，并且我们可以轻松地整合一些先验知识作为约束，从而有助于提高性能。最后一点是，潜在的机制不仅适用于地铁问题求解任务，也适用于涉及多步骤推理的其他任务。但是，我们也有一些限制。如果有很多运算符或常数，内存消耗可能会很高。第二，正如所提到的，因为这些步骤之间的概率分布是不平衡的，所以也很难应用beam search策略。这就是演讲的结束，欢迎提问。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫安托万，来自马斯特里赫特大学。我将展示我和杰里共同完成的工作，内容是关于一种新的法规条款检索数据集。法律问题是许多人生活中不可或缺的一部分，但大多数公民对他们的权利和基本法律程序知之甚少。因此，许多无力承担法律专家高昂援助费用的弱势公民被置于无保护甚至被剥削的境地。我们的目标是弥合公民与法律之间的差距，通过开发有效的法规条款检索系统来实现。这样的系统可以为缺乏专业知识的人们提供免费的法律援助服务。在深入探讨这项工作的主要贡献之前，让我们首先描述一下法规条款检索的问题。给定一个关于法律事务的简单问题，例如，如果我违反保密协议，我将面临什么风险？模型需要从大量的立法文本中检索所有相关的法规条款。这项信息检索任务面临着自身的挑战。首先，它需要处理两种类型的语言：问题中的自然语言以及法规条文中的复杂法律语言。这种语言分布的差异使得系统更难以检索相关的候选条文，因为它间接地需要一种内在的解释系统，能够将自然语言问题翻译成与法规条款术语相匹配的法律语言问题。此外，法规法并不是可以像新闻或食谱那样被视为完整信息来源的独立条款堆积。相反，它是一个结构化的法律条款集合，只有在整体背景下，即与来自相邻条款、相关领域和法律结构中的位置的补充信息结合时，才能获得完整的意义。最后，法规条款不是小段落，通常是大多数检索工作中典型的检索单元。在这里，它们是长文档，可能长达六千字。自然语言处理 (NLP) 的最新进展引发了对许多法律任务的极大兴趣，例如法律判决预测或自动化合同审查，但法规条款检索由于缺乏大型高质量的标注数据集而主要未被触及。在这项工作中，我们提出了一个新的以法国公民为中心的法语原生数据集，以研究检索模型是否能够近似于法律专家的效率和可靠性，从而完成法规条款检索任务。我们的比利时法规条款检索数据集包含了一千一百多个比利时公民提出的法律问题。这些问题涵盖了家庭、住房、金钱、工作和社会保障等广泛主题。每个问题都由经验丰富的法学家标注，并参考了来自比利时法律法典中超过两万两千六百个法律条款的相关条文。现在，让我们谈谈我们如何收集这个数据集。首先，我们开始编制一个大型的法律条款语料库。我们考虑了 32 份公开的《比利时法典》，并提取了其中的所有条款以及相应的章节标题。然后，我们收集了带有相关法规条文参考的法律问题。为此，我们与一家比利时律师事务所合作，这家律师事务所每年都会收到大约四千封来自比利时公民的电子邮件，他们寻求有关个人法律问题的建议。我们很幸运能够访问他们的网站，他们的经验丰富的法学团队解答了比利时最常见的法律问题。我们收集了数千个问题，这些问题带有类别、子类别和相关法规条文的法律参考。最后，我们过滤掉了其参考资料不在我们考虑的法典中的问题的法律参考，并将剩余的参考资料与我们语料库中的相应条款 ID 进行匹配。我们最终得到了 1108 个问题，每个问题都经过精心标注，并标注了我们大型语料库（22633 项法规条款）中相关条款的 ID。此外，每个问题都带有其在法律结构中的子标题的拼接，作为主标题。这些额外的信息在当前工作中未被使用，但可能对未来的法律信息检索或法律文本分类研究感兴趣。让我们看看我们数据集的一些特点。问题长度在五到四十四字之间，中位数是四十个字。条款比问题长得多，中等长度为七十七个字，其中一百四十二个条款超过了一千个字，最长的可达五千七百九十个字。正如之前提到的，问题涵盖了广泛的主题，其中约 85% 涉及家庭、住房、金钱或司法，而剩余的 15% 涉及社会保障、外国人或工作。条款也具有多样性，它们来自 32 个不同的比利时法典，涵盖了大量的从每个比利时法典收集的非法条款。在 22633 项条款中，仅有 1612 项被认为是至少与一个问题相关，而这些被引用的条款中约 80% 来自民法典、司法法典、刑事调查法典或刑法典。与此同时，32 个法典中有 18 个被引用的条款少于 5 个，这可以解释为这些法典更少关注个人及其顾虑的事实。总体而言，这些被引用的条款的中位数引用次数为 2，并且少于 25% 的条款被引用超过五次。使用我们的数据集，我们对多种检索方法进行了基准测试，包括词汇和密集架构。给定文章中的查询，词汇模型通过计算每个条款中查询项的权重的和，为查询-条款对分配一个分数。我们试验了标准的 TFIDF 和 BM25 排序函数。这些方法的关键问题是，它们只能检索包含查询中关键词的条款。为了克服这种局限性，我们试验了一种基于神经网络的架构，该架构可以捕获查询和条款之间的语义关系。我们使用 B 编码器模型，将查询和条款映射到密集的向量表示中，并通过计算其嵌入向量的相似度来计算查询-条款对的相关性得分。这些嵌入通常是单词嵌入模型的输出的池化操作的结果。首先，我们研究了在零样本评估设置中 Siamese B 编码器的有效性，这意味着直接使用预训练的词嵌入模型，无需任何额外的微调。我们试验了无上下文文本编码器，如 Word2Vec 和 FastText，以及上下文相关嵌入模型，如 Roberta，特别是 Roberta 的法语版本 CamemBERT。此外，我们还在整个数据集上训练了自己的基于 CamemBERT 的 Biancoder 模型。需要注意的是，在训练过程中，我们试验了 Biancoder 架构的两种类型：Siamese，它使用一个独特的词嵌入模型，将查询和条款一起映射到共享的密集向量空间中；以及双塔模型，它使用两个独立的词嵌入模型，将查询和条款分别编码到不同的嵌入空间中。我们还使用点积和余弦相似度对均值、最大值和 CLS 池化进行试验。以下是我们在测试集上的基线结果，上面是词汇方法，中间是零样本设置中评估的 Siamese B 编码器，以及下方是微调的 B 编码器。总体而言，微调的 B 编码器明显优于所有其他基线。双塔模型在召回率上超过了其 Siamese 变体，但在其他指标上表现类似。虽然 BM25 的表现不如训练好的 Biancoder 显著，但其表现表明它仍然是特定领域的检索的强大基线。关于 Siamese Biancoder 的零样本评估，我们发现直接使用预训练的 Kamembert 模型的嵌入向量而未针对信息检索任务进行优化会产生较差的结果，这与之前的发现一致。基于 Word2Vec 的 Biancoder 显著优于基于 FastText 和 BERT 的模型，表明在使用原样时，预训练的词级别嵌入比字符级别或子词级别嵌入可能更适合这项任务。尽管前景可喜，但这些结果表明存在改进的巨大机会，与一位熟练的法律专家相比，他可以最终检索所有相关的条款，从而回答任何问题，并获得完美分数。最后，我们讨论数据集的两个局限性。首先，文章语料库仅限于从 32 个考虑的《比利时法典》中收集的条款，这并不涵盖整个比利时法律，因为遗漏了来自法令、指令和命令的文章。在数据集构建过程中，忽略了所有对这些未收集的文章的引用，这导致一些问题仅包含初始相关文章数量的一部分。这种信息丢失意味着剩余的 相关文章中包含的答案可能不完整，尽管它仍然完全适当。其次，我们应该指出，并非所有法律问题都可以仅通过法规条款来回答。例如，关于“如果租户制造过多的噪音，房东是否可以驱逐租户？”的问题，可能在法规法典中没有对允许驱逐的具体噪音阈值进行详细说明。相反，房东可能需要更多地依赖案例法，并找到与其当前情况相似的先例。例如，租户每周制造两次噪音直至凌晨 2 点。因此，某些问题比其他问题更适合法规条款检索任务，而不太合适问题的领域尚待确定。我们希望这项工作能激发人们开发实用可靠的法规条款检索模型，以帮助改善所有人的获得正义的机会。您可以在以下链接中查看我们的论文和代码。感谢您的收看。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我们很高兴地呈现我们的 VALS 工作，这是一种独立的基准测试任务，旨在通过特定的语言现象来测试视觉和语言模型。 为什么我们费尽心思地搭建了这个基准测试？ 过去几年，我们见证了基于 Transformer 的视觉和语言模型数量的爆炸式增长，这些模型在海量图像文本对上进行了预训练。 其中每一个模型都在视觉和语言任务（如视觉问答、视觉常识推理、图像检索、短语定位）上刷新了最先进水平。 因此，我们收到了一条信息：这些任务特定基准上的准确率正在稳步提高，但我们是否真正了解模型实际学习了什么？ 当模型为这张图像和这句话配对时给出高分，为另一张图像和另一句话给出低分时，模型究竟理解了什么？ 视觉和语言模型是否关注正确的事物，还是关注先前工作所显示的偏差？ 为了更深入地了解这一方面，我们提出了一种更具任务无关性的方向，并引入了 VALS，它测试了视觉和语言模型对影响视觉和语言模态的特定语言现象的敏感性。 我们关注存在性、复数、计数、空间关系、动作和实体指代。 但是，我们该如何测试视觉和语言模型是否捕捉到了这些现象？ 我们采用了“干扰”（foiling）方法，该方法之前仅用于 Ravi Shakar 及其合作者的视觉和语言模型以及我们在先前工作中的计数。 “干扰”本质上是指我们采用图像的标题，并生成一个“干扰”，通过改变标题使其不再描述图像。 我们通过专注于六个特定方面来执行这些短语修改：存在性、复数、计数、空间关系、动作和实体指代。 每个方面可以包含一个或多个工具，如果我们发现创建“干扰”实例的有趣方法不止一种，就会使用多个工具。 例如，在“动作”方面，我们有两个工具：一个是在不同的动作中更改动作动词，另一个是交换行为主体。 计数和指代也是具有多个工具的方面。 我们创建这些“干扰”时，确保它们无法描述图像，并且是语法正确且有效的句子。 这并不容易做到，因为“干扰”后的标题可能不太可能。 例如，虽然并非不可能，但植物砍伐人类比人类砍伐植物在统计上不太可能，而大型视觉和语言模型可能会注意到这一点。 因此，为了获得有效的“干扰”，我们必须采取行动。 首先，我们利用强大的语言模型来提出“干扰”。 其次，我们使用自然语言推理 (NLI) 来过滤掉可能仍然描述图像的“干扰”，因为在构造“干扰”时，我们需要确保它们无法描述图像。 为了自动测试这一点，我们使用自然语言推理，并采用以下原理：我们将图像视为前提，其标题视为相关的假设。 此外，我们将标题视为前提，而“干扰”视为假设。 如果 NLI 模型预测“干扰”与标题相反或中立，我们将此视为有效“干扰”的指标。 如果“干扰”被视为标题的蕴含，则它不能是一个好的“干扰”，因为它通过传递性将给出图像的真实描述，并且我们过滤掉这些“干扰”。 但此过程并不完美。 它只是有效“干扰”的指标，因此作为生成有效“干扰”的第三项措施，我们雇佣了人工注释者来验证 VALS 中使用的数据。 因此，在过滤和人工评估之后，我们拥有尽可能多的测试实例，如表中所述。 请注意，VALS 不提供任何训练数据，而仅提供测试数据，因为它是一个零样本测试基准。 它的设计旨在利用视觉和语言模型在预训练后现有的能力。 微调只会使模型能够利用数据中的伪像或统计偏差。 我们都知道这些模型喜欢作弊和采取捷径。 正如我们所说，我们对在预训练之后视觉和语言模型具备哪些能力感兴趣。 我们在 VALS 上对五个视觉和语言模型进行了实验，分别是 CLIP、Wilbert、Wilbert Kelvin one 和 Visual Bert。 我们的两个最重要的评估指标是模型将图像-句子对分类为标题和“干扰”的准确率。 或许对于这个视频更相关的是，我们将展示我们更宽松的指标：成对准确率，该指标衡量图像-句子对的对齐分数是否大于其“干扰”对。 更多指标和结果请参阅我们的论文。 成对准确率的结果显示在此，这些结果与我们从其他指标获得的结果一致，即 Wilbert Kelvin one 实现了最佳零样本性能，其次是 Wilbert、Alex Mert、Clip 和最后 Visual Bert。 值得注意的是，以单个对象为中心的工具（如存在性和名词短语）几乎由 Wilbert Kelvin one 解决，这表明模型能够识别命名对象及其在图像中的存在。 然而，其余方面都无法可靠地解决在我们的对抗性“干扰”设置中。 我们从复数和计数工具中看到，视觉和语言模型难以区分对单个对象或图像中多个对象的引用或对其进行计数。 关系方面表明，它们难以正确地对图像中对象之间命名的空间关系进行分类。 它们还难以区分动作并识别其参与者，即使在“动作”方面有可信度偏差的支持下。 从指代方面，我们发现通过使用代词跟踪图像中对同一对象的多个引用也对视觉和语言模型来说很困难。 作为一种健全性检查以及一种有趣的实验，我们还对 GPT one 和 GPT two 这两个纯文本模型进行了基准测试，以评估是否可以通过计算正确和“干扰”标题的困惑度（没有图像）并预测具有最低困惑度的条目来解决 VALS。 如果“干扰”的困惑度更高，我们将此视为“干扰”标题可能存在可信度偏差或其他语言偏差的指标。 值得注意的是，在某些情况下，纯文本 GPT 模型比视觉和语言模型更好地捕捉到了世界的合理性。 总而言之，VALS 是一个基准，它通过对语言结构的视角来帮助社区改进视觉和语言模型，通过严格测试其视觉接地能力。 我们的实验表明，视觉和语言模型能够很好地识别图像中命名对象及其存在，但当被要求尊重语言指标时，它们难以在视觉场景中对其相互依赖性和关系进行接地。 我们非常希望鼓励社区使用 VALS 来衡量语言接地在视觉和语言模型中的进展。 甚至更进一步，Valse 还可以用作数据集的间接评估，可以在训练或微调之前和之后评估模型，以查看数据集是否能帮助模型在 VALS 测试的任何方面有所改进。 如果您感兴趣，请在 GitHub 上查看 VALS 数据，如有任何问题，请随时联系我们。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫卡米萨拉，来自东京大学。我将发表一篇题为“R和SAM：一个大规模数据集，用于通过提交日志归纳自动风险非持续时间”的论文。我将按以下顺序进行讲解。首先，我将介绍我们在本研究中正在进行的自动风险非持续时间问题。发布说明是一份技术文档，总结了与软件产品每个版本发布时所做的更改。图片展示了GBUJS库2.6.4版本的发布说明。这些说明在开源开发中起着重要的作用，但手动准备它们非常耗时。因此，能够自动生成高质量的发布说明将非常有用。我将参考之前关于自动生成发布说明的两个研究。第一个是一个名为Arena的系统，于2014年发布。它采用基于规则的方法，例如，使用变更提取器从版本之间的差异中提取核心差异、库更改和文档更改，并最终将它们组合起来。该系统的最显著功能是右上角的发行提取器，必须链接到Jira、问题生态系统，并且只能应用于使用Jira的项目。换句话说，它不能用于许多GitHub项目。第二个是Griff，于2020年最近发布。它可以在互联网上找到，并通过PIP进行存储。该系统具有一个简单的基于学习的文本分类模型，并为每个输入提交消息输出五个类别之一，例如特性或错误修复。图片是一个示例用法，返回了一个修复或错误修复类别。查询面训练数据相当小，大约五千条，将在下面的实验中展示。文本分类模型的性能并不高。我展示了两个相关研究，但存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成高质量的发布说明。为了解决适用性有限的问题，我们提出了一种仅使用提交消息作为输入的、高质量的分类器总结方法。该方法可以用于所有英文代码仓库。对于第二个数据资源稀缺的问题，我们构建了一个RNSAM数据集，通过使用GitHub API从公共GitHub仓库中收集数据，构建了大约八万两千条数据。接下来，我将描述我们的数据集。这里有一个数据的例子。左侧是提交消息，右侧是发布说明。发布说明被分为不同的级别，如改进、错误修复等。我们设置了一个任务，将提交消息作为输入，并输出分级的发布说明。这可以被视为一个摘要任务。我们预定义了四个级别：特性、改进、错误修复、弃用和破坏性变更。这些级别是根据先前研究和其他因素设置的。底图右侧的发布说明是从底图左侧显示的发布说明中提取的。此时，需要检测预先设置好的四个类别，但这些类别并不总是与每个仓库一致。例如，改进类别包括改进、增强、优化等。我们为这些记号变化准备了一个词汇表列表。使用它来检测RISNOD类，并更正后续的RISNOD句子中的文本，作为该类的RISNOD句子。接下来是提交消息。提交消息不对应于每个RIS。如图所示，如果当前的RIS是Persian 2.5.19，我们需要识别之前的RIS 2.5.18并获取其差异。这有点麻烦，仅仅获取RIS列表并查看前后是不够的。我们创建了一个启发式匹配蓝图来获取前一个和后一个页脚。数据集分析最终纠正了七千两百个仓库和八万两千条数据。此外，摘要令牌的平均数量为六十三，对于摘要任务来说，这是一个相当高的数字。独特的令牌数量也非常大，为八千八百三十万。这是由于仓库中发现的独特成本和方法名称的数量众多。接下来，我将解释我们提出的方法。横向提取和抽象摘要模型由两个新模块组成：一个使用BERT或代码BERT的分类器和一个使用BERT的生成器。首先，GEAS使用分类器将每个提交消息分类到五个类别中：特性、改进、错误修复、弃用和其它。被分类为其它类的提交消息将被丢弃。然后，GEAS将生成器应用于四个类别文档，并为每个类别生成类别说明。在这个任务中，提交消息和类别说明之间的直接对应关系未知。因此，为了训练类别线，我们使用每个提交消息的前十个字符将两个类别分配给每个输入提交消息。我们使用两种不同的方法对类别线的抽象摘要方法进行建模。第一个模型，我们称之为GAS single，由一个单向的节到节网络组成，并生成一个长的列表说明文本，即输入提交消息的连接。输出文本可以通过基于特殊类别特定端点符号进行分割，从而分为不同的类别。第二个方法，我们称之为GSMAUC，由四个不同的节到节网络组成，每个网络对应于列表说明类别中的一个。好的，我来解释一下实验。比较了五种方法：GS, GS single, GS march, rustling和先前研究的Griff。关于堕落，在某些情况下，这些说明会以多句话的形式输出。由于将零处句子的数量计算起来很困难，因此它们会被合并空格，并被视为一个长的句子。当系统输出一个短句子时，将对Brew进行惩罚。该惩罚导致实验中描述的后续结果中的Brew值较低。我们还计算了Specificity，因为如果发布说明为空，则无法计算Rouge和Brew。高Specificity意味着模型正确地在发布说明为空的情况下输出空文本。以下是结果。由于数据集包含电子邮件地址、哈希值等，我们还消除了绿色数据集，该数据集排除了它们。GAS和GAS的Rouge错误分数比基线高了十多个点。然而，在绿色测试集中，提出的方法和基线之间的分数差距增加了二十多个点。这些结果表明GAS和GAS非常有效。GAS获得了比GAS更好的Rouge分数，表明将分类器和生成器组合起来通过伪标签训练分类器是有效的。GAS可以实现高覆盖率，可能是因为分类器可以专注于为每个类别选择相关的提交消息。她倾向于比单模型产生更高的Rouge分数，表明独立开发针对每个类别不同的摘要方法也是有效的。这里是错误分析。她方法倾向于输出比人类参考句子更短的句子。在右侧的图中，参考句子有三或四句话，而她只有一句话。该模型不愿意的原因是，在训练数据中，只有33%的句子出现在特性类别中，而40%出现在改进类别中。此外，GS方法在没有额外信息的情况下无法生成准确的类别说明。右侧的顶部示例是一个非常混乱的提交消息示例，如果没有差异到对应的并行请求或问题，则无法生成完整的句子。下面的示例表明输入中的两个提交消息是相关的，应该组合成一个句子，但它失败了。最后，结论。我们构建了一个新的数据集，用于自动类别说明生成。我们还构成了输入提交消息并将其总结的任务，使其适用于所有用英文编写的项目。我们的实验表明，提出的方法比基线方法生成更干净、覆盖率更高的类别说明。请在GitHub上查看我们的数据集。谢谢。"}
