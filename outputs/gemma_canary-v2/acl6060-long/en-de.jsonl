{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Assa Farari und ich werde unsere Arbeit \"FUESHOT TABLAR DATA ARCHICHTMENT Using FINE TUNIC TRANSFORMERS ARCHITTURES\" vorstellen. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation bestehender Features, aber manchmal sind diese Features begrenzt. Die Generierung von Features aus einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit externen Textquellen. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entity Linking und Textanalyse beinhaltet, um neue Features aus der Textquelle der Wissensdatenbank zu extrahieren. Unser Framework FAST ist genau dieser automatische Prozess. Sehen wir uns ein Beispiel an. Ein Datensatz wird in FAST eingespeist. In diesem Beispiel ist der Datensatz der Universitätsdatensatz, dessen Ziel es ist, Universitäten in niedrigrangige und hochrangige Universitäten einzuteilen. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase von FAST ist Entity Linking, bei der jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensdatenbank verknüpft wird, und der Text der Entitäten der Wissensdatenbank extrahiert und dem Datensatz hinzugefügt wird. In diesem Beispiel ist der Text der Wikipedia-Seitenabsatz. Nun müssen wir Features aus dem abgerufenen Text generieren oder extrahieren. Wir benötigen also eine Feature-Extraktionsphase, die Textanalyse beinhaltet. Dies ist die Hauptneuheit dieser Arbeit, und ich werde in den nächsten Folien näher darauf eingehen. Nach der Feature-Extraktionsphase gibt es eine Feature-Generierungsphase, in der wir die extrahierten Features verwenden, um eine kleine Anzahl neuer Features zu generieren. Zuerst werden Features in der Anzahl der Klassen des ursprünglichen Datensatzes generiert. In diesem Beispiel gibt es zwei Klassen, also werden zunächst zwei neue Features generiert. Wenn der Datensatz jedoch fünf Klassen hat, werden zunächst fünf neue Features generiert. Jedes Feature repräsentiert die Wahrscheinlichkeit für jede Klasse. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik in der Textanalyse, nämlich Transformator-basierte Sprachmodelle wie BERT, GPT, XLERT usw. Es ist jedoch unwahrscheinlich, dass wir Sprachmodelle mit den Eingabedatensätzen trainieren können. Ein naiver Ansatz wäre ein Fine-Tuning für eine Zielaufgabe. Daher können wir in der Feature-Extraktionsphase ein vortrainiertes Sprachmodell herunterladen, das Sprachmodell über den Zieldatensatz feinabstimmen (in diesem Beispiel das Sprachmodell so feinabstimmen, dass es Texte in Klassen einteilt, z. B. Abstracts in Klassen: niedrig oder hoch), die Ausgabe des Sprachmodells (die Wahrscheinlichkeit für jede Klasse) erhalten und diese als neue Features verwenden. Das Problem bei diesem Ansatz besteht darin, dass Datensätze wenige verschiedene Entitäten, Texte enthalten können. In unserem Experiment enthalten fast die Hälfte der Datensätze weniger als 400 Samples, und der kleinste Datensatz enthält 35 Samples in seinem Trainingsdatensatz. Daher wäre es unwirksam, ein Sprachmodell über diesen Datensatz feinabzustimmen. Wir können jedoch Vorwissen über voranalysierte Datensätze nutzen, da wir FAST über mehrere Datensätze anwenden. Wir können die n-1 Datensätze nutzen, um Informationen über die n-1 Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den nth Datensatz analysieren. Wir schlagen vor, eine weitere Fine-Tuning-Phase hinzuzufügen, eine vorbereitende Multitask-Fine-Tuning-Phase, in der wir das Sprachmodell über n-1 Datensätze feinabstimmen, und dann eine weitere Fine-Tuning-Phase ausführen, die ein Fine-Tuning für eine Zielaufgabe ist, in der wir das Sprachmodell über den nth Zieldatensatz feinabstimmen. Der aktuelle Stand der Technik im Multitask-Fine-Tuning heißt Empty DNN. Empty DNN verwaltet Köpfe in der Anzahl der Aufgaben im Trainingsdatensatz. Wenn in diesem Beispiel also vier Aufgaben im Trainingsdatensatz vorhanden sind, verwaltet Empty DNN vier Köpfe, wie man im Bild sehen kann, und wählt eine Zufallsbatch aus dem Trainingsdatensatz aus. Wenn die Zufallsbatch beispielsweise zur Klassifizierung von Sign and Selton gehört, werden Forward- und Backward-Pfade durch den ersten Kopf ausgeführt. Und wenn die Zufallsbatch zu einer Paarweise-Ranking-Aufgabe gehört, werden Forward- und Backward-Pfade durch den letzten Kopf ausgeführt. In unserem Szenario variiert ein Tabellen- oder Datensatz die Anzahl der Klassen. Daher gibt es viele Aufgaben. MTDNN verwaltet die Anzahl der Klassen von Heads-Ausgabeschichten, und MTDNN muss außerdem neue Heads für einen neuen Datensatz mit einer neuen Aufgabe initialisieren. Unser Ansatz, genannt Task Reformulation Fine-Tuning, besteht darin, dass wir statt mehrerer Heads zu verwalten, jeden Datensatz in ein Sentence-per-Classification-Problem umformulieren, das eine Zwei-Klassen-Aufgabe ist. Sehen wir uns ein Beispiel an. Hier ist unser Eingabedatensatz, der aus Entitäten, Features, Text und Klassen besteht. Wir formulieren die Aufgabe von der Klassifizierung des Textes in niedrig und hoch um, in die Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch. Mit anderen Worten, wir trainieren das Sprachmodell so, dass es das Abstract und die Klasse in das Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht. Daher besteht der Label-Vektor in diesem Fall immer aus zwei Klassen, und dies ist der Algorithmus für unseren reformulated Fine-Tuning-Ansatz. Sehen wir uns das gesamte Framework an. Ein Datensatz wird in FAST eingespeist, und FAST führt die Linking-Phase aus, extrahiert den Text aus der Wissensdatenbank (in diesem Beispiel der Abstract der Wikipedia-Seite), formuliert die Aufgabe dann in Sentence-per-Classification-Aufgaben um, wendet das Sprachmodell auf die neue Aufgabe an und gibt die Wahrscheinlichkeit für jede Klasse aus. Beachten Sie, dass das Sprachmodell bereits über n-1 Datensätze mit einem vorbereitenden Multitask-Fine-Tuning feinabgestimmt wurde. Wir verwenden dann den Ausgabvektor des Sprachmodells als neu generiertes Feature in der Anzahl der Klassen. Um unser Framework zu evaluieren, verwenden wir einen 17-Tabellen-Klassifikationsdatensatz, der sich in Größe, Features, Balance, Domäne und anfänglicher Performance unterscheidet. Als Wissensdatenbank verwenden wir Wikipedia. Wir gestalten unser Experiment als Leave-One-Out-Evaluierung, wobei wir FAST über 16 Datensätze trainieren und es auf den 17. Datensatz anwenden. Wir teilen außerdem jeden Datensatz in einen Four-Fold-Split auf, und wenden einen Four-Fold-Cross-Validation an. Dann generieren wir die neuen Features und evaluieren diese mit fünf Evaluationsklassifikatoren. Wir verwenden in unserem Experiment eine BERT-basierte Architektur. Hier sind die Ergebnisse für unser Experiment. Wir können sehen, dass wir unser Framework mit dem Fine-Tuning für den Zieldatensatz, dem Fine-Tuning für eine Zielaufgabe und dem vorbereitenden Fine-Tuning von MTDNN vergleichen. Unser reformulated Fine-Tuning erzielt das beste Ergebnis, die beste Performance, während MTDNN eine Verbesserung von 2 % gegenüber dem Fine-Tuning für den Zieldatensatz erzielt. Unser Ansatz erzielt eine Verbesserung von 6 %, wenn wir uns die kleinen Datensätze ansehen. Wir können sehen, dass die Performance von MTDNN abnimmt und die Verbesserung der vorbereitenden Multitask-Fine-Tuning-Phase auf 1,5 Prozent sinkt, aber unsere Performance im Vergleich zum alleinigen Fine-Tuning für die Zielaufgabe auf 11 Prozent steigt. Zusammenfassend ermöglicht FAST View-Shot-Enrichment ab 35 Samples in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben und Datensätze und behält den Head des Modells bei. Es fügt jedoch drei Formulation-Phasen hinzu, einen erweiterten Trainingsdatensatz und benötigt einen Zielwert mit semantischer Bedeutung, den wir in das Sprachmodell einspeisen und in dem Sentence-per-Classification-Problem verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit \"Learning to Reason Deductively, Metro Problem Solving as Complex Region Extraction\" vorstellen. Ich bin Alan vom Biden's AI Lab und dies ist eine Gemeinschaftsarbeit mit Thierry von der University of Texas at Austin und Wayloo von SUDD. Zuerst möchte ich über unsere Motivation für das Reasoning sprechen. Hier zeigen wir ein Beispiel, in dem mehrschrittiges Reasoning hilfreich ist. Diese Abbildung stammt aus der POWN-Arbeit, wo sie Prompting verwenden, um das Metro-Problem in einem Fusion-Learning-Szenario zu lösen. Auf der Netz-Seiten sehen wir, dass wir, wenn wir einige Beispiele mit nur Fragen und Antworten geben, möglicherweise nicht die richtigen Antworten erhalten. Aber wenn wir einige detailliertere Reasoning-Beschreibungen geben, das Modell die Reasoning-Beschreibung vorhersagen und auch hier eine korrekte Vorhersage treffen kann. Es ist gut, interpretierbares, mehrschrittiges Reasoning als Ausgabe zu haben. Wir halten das Method-Problem auch für eine unkomplizierte Anwendung, um solche Reasoning-Fähigkeiten zu evaluieren. Hier, in unserem Problemaufbau, müssen wir, gegeben die Fragen, diese Frage lösen und die numerischen Antworten erhalten. In unseren Datensätzen werden uns auch die mathematischen Ausdrücke bereitgestellt, die zu dieser bestimmten Antwort führen. Bestimmte Annahmen gelten auch wie in früheren Arbeiten. Wir nehmen an, dass die Präzision der Größen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponential betrachten. Darüber hinaus können komplizierte Operatoren tatsächlich in diese grundlegenden Operatoren zerlegt werden. Frühere Arbeiten zur Lösung von Method-Problemen lassen sich in sequenzielle zu sequenzielle (sequence-to-sequence) und sequenzielle zu baumartige (sequence-to-tree) Modelle kategorisieren. Traditionelle sequenzielle zu sequenzielle Modelle konvertieren den Ausdruck in eine bestimmte Sequenz zur Generierung und sind relativ einfach zu implementieren und können auf viele verschiedene, komplizierte Probleme verallgemeinert werden. Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als bei strukturierten Modellen und es mangelt an Interpretierbarkeit für die Vorhersage. Diese Richtung ist jedoch aufgrund des Transformer-Modells immer noch recht beliebt. Bei baumartigen Modellen strukturieren wir diese Ausdrücke in einer Baumform und folgen einer Pre-Order-Traversierung bei der Baumgenerierung. Hier generieren wir weiterhin die Operatoren, bis wir die Blätter erreichen, die die Größen darstellen. Das Gute daran ist, dass es uns diese binäre Baumstruktur liefert. Es ist jedoch etwas unintuitiv, da wir zuerst den Operator und dann am Ende die Größen generieren. Außerdem enthält es einige redundante Berechnungen. Wenn wir uns dieses Ausdrucks ansehen, a mal drei plus drei, wird er tatsächlich zweimal generiert. Tatsächlich sollten wir jedoch die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir im zweiten Schritt diesen Divisor erhalten, der 27 beträgt. Wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden. Und in diesen Schritten erhalten wir die Divisoren. Und dann erhalten wir im dritten Schritt tatsächlich den Quotienten, richtig? Und nach diesen drei Schritten können wir tatsächlich die Ergebnisse aus dem zweiten Schritt wiederverwenden und die Ergebnisse des vierten Schritts erhalten. Und schließlich können wir den Dividenden erhalten. Wir generieren hier also tatsächlich den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Größen zu generieren. Dies macht den Prozess genauer. In unserem deduktiven System beginnen wir zunächst mit einer Reihe von Größen, die in den Fragen präsentiert werden, und auch mit einigen Konstanten als unseren Ausgangszuständen. Der Ausdruck wird durch EIJOP dargestellt, wobei wir Operatoren von Qi zu QJ anwenden, und dieser Ausdruck ist gerichtet. Wir haben hier auch Subtraktion umgekehrt, um die entgegengesetzte Richtung darzustellen. Dies ist dem Relationsextraktion recht ähnlich. In einem formalen deduktiven System wenden wir zum Zeitpunkt t den Operator zwischen dem Qi- und Qj-Paar an und erhalten diesen neuen Ausdruck. Wir fügen ihn den nächsten Zuständen hinzu, um eine neue Größe zu erstellen. Diese Folie visualisiert die Entwicklung der Zustände, bei der wir kontinuierlich Ausdrücke zu den aktuellen Zuständen hinzufügen. In unseren Modellimplementierungen verwenden wir zunächst ein vortrainiertes Netzwerkmodell, das BERT oder RoBERTa sein kann, und codieren dann einen Satz, um diese Größenrepräsentationen zu erhalten. Sobald wir die Größenrepräsentationen erhalten haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel, in dem Q1 die Darstellung zur Erlangung von Q1 erhalten wird, indem es durch Q2 dividiert und dann mit Q3 multipliziert wird. Zuerst erhalten wir die Paarrepräsentation, die im Wesentlichen nur die Verkettung zwischen Q1 und Q2 ist, und wenden dann ein Feedforward-Netzwerk an, das durch den Operator parametrisiert wird. Schließlich erhalten wir die Ausdrucksrepräsentation Q1 dividiert durch Q2. In der Praxis können wir jedoch in der Inferenzphase möglicherweise auch den falschen Ausdruck erhalten. Hier sind alle möglichen Ausdrücke gleich drei mal der Anzahl der Operatoren. Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu steuern. Wenn dieser Ausdruck nicht erlaubt ist, können wir diesen Ausdruck einfach aus unserem Suchraum entfernen. Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist eine weitere Größe. Diese Größe stammt vom zuvor berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck Q3 mal Q4 erhalten. Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Diese Differenz macht es schwierig, Beam Search anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist. Die Trainingsprozedur ähnelt dem Training eines sequenziellen zu sequenziellen Modells, wobei wir den Verlust in jedem Zeitschritt optimieren. Hier verwenden wir auch Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Der Raum unterscheidet sich hier vom sequenziellen zu sequenziellen, da der Raum in jedem Zeitschritt unterschiedlich ist, während er im traditionellen sequenziellen zu sequenziellen Modell die Anzahl des Vokabulars ist. Es ermöglicht uns auch, bestimmte Einschränkungen aus Vorwissen aufzuerlegen. Wir führen Experimente auf den üblicherweise verwendeten Method-Problem-Datensätzen MAWPS, Math 23K, MathQA und SWAMP durch. Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisher besten Ansätzen. Unsere am besten abschneidende Variante ist RoBERTa Deductive Reasoner. Tatsächlich verwenden wir im Gegensatz zu früheren Ansätzen, die Beam Search verwenden, kein Beam Search. Die besten Ansätze sind oft baumartige Modelle. Insgesamt ist unser Reasoner in der Lage, dieses baumartige Modell deutlich zu übertreffen, aber wir können sehen, dass die absoluten Zahlen auf MathQA oder SWAMP nicht wirklich hoch sind. Wir untersuchen die Ergebnisse auf SWAMP weiter. Dieser Datensatz ist herausfordernd, da der Autor versucht hat, etwas manuell hinzuzufügen, um das NLP-Modell zu verwirren, z. B. irrelevante Informationen und zusätzliche Größen. In unserer Vorhersage stellen wir fest, dass einige der Zwischenwerte tatsächlich negativ sind. Zum Beispiel fragen wir in dieser Frage, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie siebzehn weniger Würfe und Stephen hat acht Würfe, was völlig irrelevant ist. Unser Modell trifft einige Vorhersagen, die negative Werte erzeugen. Wir beobachten, dass diese beiden Ausdrücke Ähnlichkeiten aufweisen. Wir können also diesen Suchraum begrenzen, indem wir Ergebnisse entfernen, die negativ sind, um die Antwort korrekt zu machen. Wir stellen fest, dass diese Einschränkung für einige Modelle tatsächlich erheblich verbessert. Zum Beispiel verbessern wir für BERT sieben Punkte und für das RoBERTa-Basismodell verbessern wir zwei Punkte. Bessere Sprachmodelle haben eine bessere Sprachverständnisfähigkeit, sodass die Zahl hier für RoBERTa höher und für BERT niedriger ist. Wir versuchen auch, die Schwierigkeiten hinter diesen Daten zu analysieren. Wir nehmen an, dass die Anzahl der ungenutzten Größen als irrelevante Informationen betrachtet werden kann. Hier können wir sehen, dass wir den Prozentsatz der Stichproben mit ungenutzten Größen haben und der SWAMP-Datensatz den größten Anteil hat. Hier zeigen wir auch die Gesamtleistung. Für diese Stichproben ohne ungenutzte Größen ist die Gesamtleistung tatsächlich höher als die Gesamtleistung. Aber bei den Stichproben, die ungenutzte Größen enthalten, ist sie tatsächlich viel schlechter als die Gesamtleistung. Für MAWPS haben wir nicht viele Fallstricke, daher ignoriere ich diesen Teil. Abschließend möchten wir die Interpretierbarkeit anhand eines Fehler- und Korrekturbeispiels demonstrieren. Hier trifft unser Modell im ersten Schritt eine falsche Vorhersage. Wir können diesen Ausdruck mit dem Satz korrelieren. Wir denken, dass dieser Satz das Modell irreführend zu einer falschen Vorhersage führt. Das Einfügen von weiteren dreißigfünf veranlasst das Modell zu denken, dass es sich um einen zusätzlichen Operator handelt. Wir versuchen, den Satz so zu überarbeiten, dass er lautet: die Anzahl der Birnbäume dreißigfünf weniger als die Apfelbäume. Wir formulieren es so, dass es eine genauere Semantik vermittelt, sodass das Modell die Vorhersage korrekt treffen kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Modellverhalten zu verstehen. Abschließend lässt sich sagen, dass unser Modell effizient ist, wir ein interpretierbares Lösungsprozedere bereitstellen und wir problemlos Vorwissen als Einschränkung einbeziehen können, was dazu beitragen kann, die Leistung zu verbessern. Wir haben auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, kann der Speicherverbrauch recht hoch sein. Und das zweite ist, dass es aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung in den verschiedenen Zeitschritten recht schwierig ist, eine Beam Search-Strategie anzuwenden. Damit endet der Vortrag, und Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, die sich mit einem neuen Datensatz für die Rückgewinnung von Gesetzartikeln befasst. Rechtliche Angelegenheiten sind ein integraler Bestandteil vieler Menschenleben, doch die meisten Bürgerinnen und Bürger verfügen über wenig bis gar kein Wissen über ihre Rechte und grundlegende rechtliche Verfahren. Infolgedessen bleiben viele schutzbedürftige Bürgerinnen und Bürger, die sich nicht den kostspieligen Beistand eines Rechtsexperten leisten können, ungeschützt oder schlimmstenfalls ausgebeutet. Ziel ist es, die Kluft zwischen Menschen und dem Gesetz zu überbrücken, indem ein effektives Rückgewinnungssystem für Gesetzartikel entwickelt wird. Ein solches System könnte einen kostenlosen, professionellen Rechtsberatungsservice für ungelernte Personen bereitstellen. Bevor wir uns der Hauptleistung dieser Arbeit zuwenden, wollen wir zunächst das Problem der Rückgewinnung von Gesetzartikeln beschreiben. Angesichts einer einfachen Frage zu einem Rechtsgebiet, wie z. B. welches Risiko gehe ich ein, wenn ich die berufliche Schweigepflicht verletze? Ist ein Modell erforderlich, um alle relevanten Gesetzartikel aus einem großen Korpus der Gesetzgebung abzurufen. Diese Informationsrückgewinnungsaufgabe bringt ihre eigenen Herausforderungen mit sich. Erstens befasst sie sich mit zwei Arten von Sprache: natürlicher, gebräuchlicher Sprache für die Fragen und komplexer juristischer Sprache für die Gesetze. Diese sprachliche Differenz erschwert es einem System, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretation System erfordert, das eine natürliche Frage in eine juristische Frage übersetzt, die der Terminologie der Gesetze entspricht. Darüber hinaus ist das Gesetzgebungsrecht nicht eine Sammlung unabhängiger Artikel, die für sich genommen als vollständige Informationsquelle behandelt werden können, wie z. B. Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die erst im Gesamtkontext ihre volle Bedeutung entfalten, d. h. zusammen mit den ergänzenden Informationen aus den benachbarten Artikeln, den Fachgebieten und Untergebieten, zu denen sie gehören, und ihrem Platz in der Struktur des Rechts. Letztlich sind Gesetzartikel keine kleinen Absätze, die in den meisten Rückgewinnungsarbeiten typischerweise die übliche Rückgewinnungseinheit darstellen. Hierbei handelt es sich um lange Dokumente, die bis zu sechs tausend Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben großes Interesse an vielen Rechtsaufgaben geweckt, wie z. B. der Vorhersage von Rechtsurteilen oder der automatisierten Vertragsprüfung, doch die Rückgewinnung von Gesetzartikeln ist aufgrund des Mangels an großen und hochwertigen, gekennzeichneten Datensätzen hauptsächlich unberührt geblieben. In dieser Arbeit präsentieren wir einen neuen, auf französische Staatsbürger ausgerichteten Datensatz, um zu untersuchen, ob Rückgewinnungsmodelle die Effizienz und Zuverlässigkeit eines Rechtsexperten bei der Rückgewinnung von Gesetzartikeln approximieren können. Unsere belgischen Datensätze zur Rückgewinnung von Gesetzartikeln bestehen aus mehr als eintausend einhundert legalen Fragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken ein breites Spektrum von Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung. Jede einzelne wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als zweiundzwanzigtausendsechshundert Artikeln aus belgischen Gesetzesbüchern gekennzeichnet. Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz erhoben haben. Zunächst haben wir einen großen Korpus von Gesetzartikeln zusammengestellt. Wir betrachteten fünfzehn öffentlich zugängliche belgische Gesetzesbücher und extrahierten alle ihre Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir rechtliche Fragen mit Verweisen auf relevante Gesetze. Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa viertausend E-Mails von belgischen Bürgern erhält, die um Rat zu einem persönlichen Rechtsstreit bitten. Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team erfahrener Juristen die häufigsten Rechtsfragen belgischer Bürger beantwortet. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und Rechtsverweisen auf relevante Gesetze versehen waren. Schließlich filterten wir die Rechtsverweise und entfernten die Fragen, deren Verweise nicht auf Artikel in einem der von uns berücksichtigten Gesetzesbücher zurückzuführen waren. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Wir endeten schließlich mit eintausendachtundsechzig Fragen, die sorgfältig mit den IDs der relevanten Artikel aus unserem großen Korpus von zweiundzwanzigtausendsechshundertdreiunddreißig Gesetzartikeln gekennzeichnet waren. Darüber hinaus enthält jede Frage ein Hauptthema, das aus einer Verkettung ihrer nachfolgenden Überschriften in der Struktur des Rechts besteht. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschung zur juristischen Informationsbeschaffung oder zur juristischen Textklassifizierung von Interesse sein. Schauen wir uns einige Merkmale unseres Datensatzes an. Die Fragen sind zwischen fünf und fünfundvierzig Wörtern lang, wobei der Median bei fünfundvierzig Wörtern liegt. Die Artikel sind viel länger, mit einer Medianlänge von siebzigundsieben Wörtern, wobei einhundertundvierzig von ihnen mehr als eintausend Wörter überschreiten. Die längsten umfassen bis zu fünftausendsiebenhundertneunzig Wörter. Wie bereits erwähnt, decken die Fragen ein breites Themenspektrum ab, wobei rund achtzigfünf Prozent davon sich entweder auf Familie, Wohnen, Geld oder Justiz beziehen, während die restlichen fünfzehn Prozent sich auf Sozialversicherung, Ausländer oder Arbeit beziehen. Die Artikel sind ebenfalls sehr vielfältig, da sie aus fünfunddreißig verschiedenen belgischen Gesetzesbüchern stammen, die eine große Anzahl von Artikeln aus jedem dieser belgischen Gesetzesbücher umfassen. Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage in dem Datensatz angegeben und rund 80 % dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Strafprozessgesetzbuch, dem Strafverfolgungsgesetzbuch oder dem Strafgesetzbuch. Währenddessen haben 18 von 35 Gesetzesbüchern weniger als 5 Artikel, die als relevant für mindestens eine Frage gelten, was darauf zurückzuführen ist, dass sich diese Gesetzesbücher weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt liegt die Mediananzahl der Zitationen für diese zitierten Artikel bei zwei, und weniger als fünfundzwanzig Prozent davon werden mehr als fünfmal zitiert. Unter Verwendung unserer Datensätze testen wir verschiedene Rückgewinnungsansätze, darunter lexikalische und dichte Architekturen. Angesichts einer Anfrage in einem Artikel weist ein lexikalisches Modell einen Wert für das Anfrage-Artikel-Paar zu, indem es die Summe über die Anfragebegriffe der Gewichte jedes dieser Begriffe in diesem Artikel berechnet. Wir experimentieren mit den Standard-TFIDF- und BM25-Ranking-Funktionen. Das Hauptproblem bei diesen Ansätzen besteht darin, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Anfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronalen Architektur, die semantische Beziehungen zwischen Anfragen und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Anfragen und Artikel in dichte Vektordarstellungen abbildet und einen Relevanzwert zwischen einem Anfrage-Artikel-Paar durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen ergeben sich typischerweise aus einer Pooling-Operation auf der Ausgabe eines Word-Embedding-Modells. Zunächst untersuchen wir die Effektivität von Siamese B-Encodern in einer Zero-Shot-Evaluierungsumgebung, d. h. vordefinierte Word-Embedding-Modelle werden ohne zusätzliche Feinabstimmung direkt angewendet. Wir experimentieren mit kontextunabhängigen Textcodierern, nämlich Word to Vec und FastText, sowie mit kontextabhängigen Einbettungsmodellen, nämlich Roberta und insbesondere Camembert, einem französischen Roberta-Modell. Darüber hinaus trainieren wir unser eigenes Camembert-basiertes Modell Biancoders auf dem gesamten Datensatz. Bei der Schulung experimentieren wir mit den beiden Varianten der Biancoder-Architektur: Siamese, die ein einziges Word-Embedding-Modell verwendet, das Anfrage und Artikel gemeinsam in einen gemeinsamen dichten Vektorraum abbildet, und Tower, das zwei unabhängige Word-Embedding-Modelle verwendet, die Anfrage und Artikel separat in unterschiedliche Einbettungsräume codieren. Wir experimentieren mit Mittelwert, Maximum und CLS Pooling sowie mit Dot Product und Cosinus zur Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse unserer Baseline auf den Testdatensätzen, wobei die lexikalischen Methoden oben, die in einer Zero-Shot-Umgebung ausgewerteten Siamese B-Encoder in der Mitte und die feinabgestimmten B-Encoder unten aufgeführt sind. Insgesamt übertreffen die feinabgestimmten B-Encoder alle anderen Baselines deutlich. Das Two-Tower-Modell verbessert die Recall-Rate bei 100 im Vergleich zu seiner Siamese-Variante, weist aber ähnliche Ergebnisse bei anderen Metriken auf. Obwohl BM25 die trainierten Biancoder deutlich unterbot, deutete seine Leistung darauf hin, dass es sich immer noch um eine starke Baseline für domänenspezifische Rückgewinnung handelt. Bezüglich der Zero-Shot-Evaluierung von Siamese Biancoder stellen wir fest, dass die direkte Verwendung der Einbettungen eines vordefinierten Kamembert-Modells ohne Optimierung für die Informationsrückgewinnungsaufgabe zu schlechten Ergebnissen führt, was mit früheren Erkenntnissen übereinstimmt. Und das Word-to-Vec-basierte Biancoder übertraf die FastText- und Bird-basierten Modelle deutlich, was darauf hindeutet, dass möglicherweise vortrainierte Word-Level-Einbettungen für diese Aufgabe besser geeignet sind als Character-Level- oder Subword-Level-Einbettungen, wenn sie direkt verwendet werden. Obwohl vielversprechend, deuten diese Ergebnisse auf reichlich Verbesserungspotenzial im Vergleich zu einem geschickten Rechtsexperten hin, der letztendlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Ergebnisse erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen aller Datensätze diskutieren. Erstens ist der Artikelkorpus auf diejenigen beschränkt, die aus den fünfzehn berücksichtigten belgischen Gesetzesbüchern stammen, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Direktiven und Verordnungen fehlen. Bei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass einige Fragen nur einen Bruchteil der ursprünglichen Anzahl relevanter Artikel aufweisen. Dieser Informationsverlust impliziert, dass die in den verbleibenden relevanten Artikeln enthaltene Antwort unvollständig sein kann, obwohl sie dennoch vollständig angemessen ist. Zweitens müssen wir darauf hinweisen, dass nicht alle Rechtsfragen allein mit Gesetzen beantwortet werden können. Zum Beispiel kann die Frage, ob man seine Mieter kündigen kann, wenn sie zu viel Lärm machen, keine detaillierte Antwort im Gesetzgebungsschriftum enthalten, die einen bestimmten Lärmpegel festlegt, ab dem eine Kündigung zulässig ist. Stattdessen sollte der Vermieter sich eher auf die Rechtsprechung stützen und ähnliche Präzedenzfälle finden. Zum Beispiel macht der Mieter zwei Partys pro Woche bis zum 2 Uhr morgens. Daher sind einige Fragen besser für die Rückgewinnung von Gesetzartikeln geeignet als andere, und das Gebiet der weniger geeigneten muss noch ermittelt werden. Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Rückgewinnungsmodelle für Gesetzartikel weckt, die dazu beitragen können, den Zugang zur Justiz für alle zu verbessern. Sie können unser Paper und Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, Ihnen unsere Arbeit an VALS vorzustellen, einem aufgabenunabhängigen Benchmark, der dazu dient, Vision- und Sprachmodelle anhand spezifischer linguistischer Phänomene zu testen. Warum haben wir uns die Mühe gemacht, diesen Benchmark aufzubauen? In den letzten Jahren haben wir eine Explosion von auf Transformer-Basis beruhenden Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vortrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei Vision- und Sprachaufgaben wie visuellem Frage-Antworten, visuellem Common-Sense-Reasoning, Bildrückruf und Phrase Grounding. So ist es uns aufgefallen, dass die Genauigkeiten bei diesen aufgabenspezifischen Benchmarks stetig steigen, aber wissen wir, was die Modelle tatsächlich gelernt haben? Was versteht ein Vision- und Sprach-Transformer, wenn er einem Bild und einem Satz eine hohe Übereinstimmungsbewertung zuweist und dem anderen eine niedrige? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige oder auf Verzerrungen, wie es frühere Arbeiten gezeigt haben? Um mehr Licht auf diesen Aspekt zu werfen, schlagen wir einen eher aufgabentranszendenten Ansatz vor und führen VALS ein, das die Sensitivität von Vision- und Sprachmodellen gegenüber spezifischen linguistischen Phänomenen testet, die sowohl die linguistischen als auch die visuellen Modalitäten beeinflussen. Wir fokussieren uns auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Koreferenz. Aber wie testen wir, ob Vision- und Sprachmodelle diese Phänomene erfasst haben? Foiling, eine Methode, die zuvor nur für Vision- und Sprachmodelle im Hinblick auf Nominalphrasen von Ravi Shakar und Mitarbeitern und im Hinblick auf das Zählen von uns in früheren Arbeiten angewendet wurde. Foiling bedeutet im Wesentlichen, dass wir die Bildunterschrift nehmen und eine Foiled-Version erzeugen, indem wir die Bildunterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Wir führen diese Phrasenänderungen durch, indem wir uns auf sechs spezifische Aspekte konzentrieren: Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Koreferenz. Jeder Aspekt kann ein oder mehrere Instrumente umfassen, falls wir mehr als eine interessante Möglichkeit gefunden haben, Foiled-Instanzen zu erstellen. Beispielsweise haben wir im Bereich der Handlungen zwei Instrumente: eines, bei dem das Aktionsverb durch ein anderes ersetzt wird, und eines, bei dem die Aktanten vertauscht werden. Zählen und Koreferenz sind ebenfalls Aspekte, die mehr als ein Instrument haben. Und wir erstellen diese Foiled-Versionen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch und anderweitig gültige Sätze sind. Das ist nicht einfach, da eine Foiled-Bildunterschrift statistisch weniger wahrscheinlich sein kann als die Originalunterschrift. Zum Beispiel ist es zwar nicht unmöglich, aber statistisch weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten dies erkennen. Daher müssen wir Maßnahmen ergreifen, um gültige Foiled-Versionen zu erhalten. Erstens nutzen wir starke Sprachmodelle, um Foiled-Versionen vorzuschlagen. Zweitens verwenden wir Natural Language Inference (NLI) oder kurze NLI, um Foiled-Versionen herauszufiltern, die möglicherweise immer noch das Bild beschreiben, da wir bei der Erstellung von Foiled-Versionen sicherstellen müssen, dass sie das Bild nicht beschreiben. Um dies automatisch zu testen, wenden wir Natural Language Inference mit folgender Begründung an: Wir betrachten das Bild als Prämisse und seine Bildunterschrift als die dahingegangene Hypothese. Zusätzlich betrachten wir die Bildunterschrift als Prämisse und die Foiled-Version als ihre Hypothese. Wenn ein NLI-Modell vorhersagt, dass die Foiled-Version der Bildunterschrift widerspricht oder neutral gegenüber ihr ist, betrachten wir dies als Hinweis auf eine gültige Foiled-Version. Wenn sie von der Bildunterschrift dahingegangen ist, kann sie keine gute Foiled-Version sein, da sie durch Transitivität eine wahre Beschreibung des Bildes liefern würde, und wir filtern diese Foiled-Versionen heraus. Dieses Verfahren ist jedoch nicht perfekt. Es ist lediglich ein Indikator für gültige Foiled-Versionen. Daher setzen wir als dritte Maßnahme zur Erstellung gültiger Foiled-Versionen menschliche Annotatoren ein, die die in VALS verwendeten Daten validieren. So haben wir nach der Filterung und der menschlichen Bewertung so viele Testinstanzen, wie in dieser Tabelle beschrieben. Bitte beachten Sie, dass VALS keine Trainingsdaten, sondern nur Testdaten liefert, da es sich um einen Zero-Shot-Test-Benchmark handelt. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vortraining zu nutzen. Das Fine-Tuning würde es den Modellen lediglich ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Und wie wir bereits sagten, sind wir daran interessiert, die Fähigkeiten der Vision- und Sprachmodelle nach dem Vortraining zu beurteilen. Wir experimentieren mit fünf Vision- und Sprachmodellen auf VALS, nämlich mit CLIP, WILBERT, WILBERT-12-in-1 und VisualBERT. Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren als Bildunterschriften und Foiled-Versionen. Vielleicht relevanter für dieses Video, werden wir unsere permissivere Metrik, die Paarweise-Genauigkeit, vorstellen, die misst, ob die Bild-Satz-Ausrichtungsbewertung für das richtige Bild-Text-Paar höher ist als für sein Foiled-Paar. Weitere Metriken und Ergebnisse finden Sie in unserem Paper. Die Ergebnisse mit der paarweisen Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir aus den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von WILBERT-12-in-1 erzielt wird, gefolgt von WILBERT, AlexBERT, CLIP und schließlich VisualBERT. Es ist bemerkenswert, wie Instrumente, die sich auf einzelne Objekte wie Existenz und Nominalphrasen konzentrieren, fast vollständig von WILBERT-12-in-1 gelöst werden, was zeigt, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Keine der verbleibenden Aspekte kann jedoch zuverlässig in unseren adversarialen Foiling-Szenarien gelöst werden. Wir sehen aus den Instrumenten für Pluralität und Zählen, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne bzw. mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Der Relationsaspekt zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie von Plausibilitätsverzerrungen unterstützt werden, wie wir im Bereich der Handlungen sehen. Aus dem Aspekt der Koreferenz erfahren wir, dass die Verfolgung mehrerer Referenzen auf dasselbe Objekt in einem Bild mithilfe von Pronomen ebenfalls schwierig für Vision- und Sprachmodelle ist. Als Sanity-Check und weil es ein interessantes Experiment ist, haben wir auch zwei Text-Only-Modelle, GPT-1 und GPT-2, benchmarkt, um zu beurteilen, ob VALS von diesen unimodalen Modellen gelöst werden kann, indem wir die Perplexität der richtigen und der Foiled-Bildunterschrift ohne Bild berechnen und den Eintrag mit der niedrigsten Perplexität vorhersagen. Wenn die Perplexität für die Foiled-Version höher ist, betrachten wir dies als Hinweis darauf, dass die Foiled-Bildunterschrift unter Plausibilitätsverzerrungen oder anderen linguistischen Verzerrungen leiden könnte. Es ist interessant zu sehen, dass die Text-Only-GPT-Modelle in einigen Fällen die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Um es zusammenzufassen: VALS ist ein Benchmark, der die linguistischen Konstruktionen als Linse verwendet, um der Community zu helfen, Vision- und Sprachmodelle zu verbessern, indem er ihre visuellen Grounding-Fähigkeiten hart testet. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte und ihre Anwesenheit in Bildern gut identifizieren, aber Schwierigkeiten haben, ihre Interdependenzen und Beziehungen in visuellen Szenen zu grounden, wenn sie gezwungen sind, linguistischen Indikatoren gerecht zu werden. Wir möchten die Community wirklich dazu ermutigen, VALS zu verwenden, um Fortschritte beim Language Grounding mit Vision- und Sprachmodellen zu messen. Noch mehr könnte VALS als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder Fine-Tuning bewertet werden können, um zu sehen, ob ein Datensatz den Modellen hilft, sich in einem der Aspekte zu verbessern, die von VALS getestet werden. Wenn Sie interessiert sind, sehen Sie sich die VALS-Daten auf GitHub an, und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisara von der Universität Tokio. Ich werde eine Arbeit vorstellen mit dem Titel \"R und SAM, ein umfangreicher Datensatz für automatische Risikodauerbestimmung durch Commit-Log-Zusammenfassung\". Ich werde in folgender Reihenfolge darlegen: Zunächst werde ich die automatische Risikodauerbestimmung vorstellen, an der wir in dieser Forschung arbeiten. Eine Versionshinweis ist ein technisches Dokument, das die mit jeder Veröffentlichung eines Softwareprodukts verteilten Änderungen zusammenfasst. Das Bild zeigt die Versionshinweise für Version zwei Punkt sechs punkt vier der GBUJS-Bibliothek. Diese Notizen spielen eine wichtige Rolle bei der Open-Source-Entwicklung, sind aber zeitaufwendig, wenn sie manuell vorbereitet werden. Daher wäre es sehr nützlich, wenn man hochwertige Versionshinweise automatisch generieren könnte. Ich werde auf zwei frühere Forschungsarbeiten zur automatischen Generierung von Versionshinweisen Bezug nehmen. Die erste ist ein System namens Arena, das im Jahr 2014 veröffentlicht wurde. Es verfolgt einen regelbasierten Ansatz, beispielsweise die Verwendung eines Change Extractors, um grundlegende Unterschiede – Bibliotheksänderungen und Dokumentationsänderungen – aus den Unterschieden zwischen den Releases zu extrahieren und diese schließlich zusammenzuführen. Das bemerkenswerteste Merkmal dieses Systems ist der Issue Extractor im oberen rechten Bereich, der mit Jira, dem Issue-Ökosystem, verknüpft werden muss und nur auf Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Die zweite ist Griff, das kürzlich im Jahr 2020 angekündigt wurde. Es ist im Internet verfügbar und kann über PIP installiert werden. Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt für jede eingegebene Commit-Nachricht eines von fünf Rubriken wie Features oder Bugfixes aus. Das Bild zeigt eine Beispielanwendung, die eine Korrektur oder Bugfix-Rubrik zurückgibt. Die Trainingsdaten für Queryface sind relativ klein, etwa fünftausend, und werden in den im Folgenden beschriebenen Experimenten dargestellt. Die Leistung des Textklassifikationsmodells ist nicht hoch. Ich stelle zwei verwandte Forschungsarbeiten vor, aber es gibt Probleme mit begrenzter Anwendbarkeit und knappen Datenressourcen. Unsere Arbeit löst diese beiden Probleme und generiert automatisch hochwertige Releases. Um das Problem der begrenzten Anwendbarkeit zu lösen, schlagen wir eine hochwertige Klassifizierungszusammenfassungsmethode vor, die nur die Commit-Nachricht als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir einen RNSAM-Datensatz erstellt, der aus etwa achtzigtausend Dateneinträgen besteht, indem wir Daten aus öffentlichen GitHub-Repositories mithilfe der GitHub API gesammelt haben. Im Folgenden beschreibe ich unseren Datensatz. Hier ist ein Beispiel für die Daten. Die linke Seite ist eine Commit-Nachricht und die rechte Seite ist der Versionshinweis. Die Versionshinweise sind in Kategorien wie Verbesserungen, Bugfixes usw. eingeteilt. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die kategorisierten Versionshinweise ausgibt. Dies kann als eine Zusammenfassungsaufgabe betrachtet werden. Wir haben vier Kategorien vordefiniert: Features, Verbesserungen, Bugfixes, Deprecations und Removables. Diese wurden auf der Grundlage früherer Forschungsergebnisse und anderer Faktoren festgelegt. Die Versionshinweise unten rechts werden aus den Versionshinweisen unten links extrahiert. Zu diesem Zeitpunkt ist es notwendig, die vier Rubriken zu erkennen, die im Voraus festgelegt wurden, aber die Rubriken sind nicht immer mit jedem Repository konsistent. Beispielsweise umfasst die Kategorie \"Verbesserungen\" Verbesserungen, Erweiterungen, Optimierungen und so weiter. Wir haben eine Vokabelliste der Studienrubriken für jede dieser notationalen Variationen erstellt. Verwenden Sie diese, um die RISNOD-Klasse zu erkennen und den Text des RISNOD-Satzes für die Klasse zu korrigieren. Als nächstes folgt die Commit-Nachricht. Commit-Nachrichten sind nicht an jeden RISNOD gebunden. Wie im folgenden Bild gezeigt, wenn der aktuelle RISNOD Persian zwei Punkt fünf zu neunzehn ist, müssen wir den vorherigen RISNOD zwei Punkt fünf zu achtzehn identifizieren und den Diff abrufen. Das ist etwas mühsam, und es reicht nicht aus, nur eine Liste von RISNODs zu erhalten und die Vor- und Nachversionen anzusehen. Wir haben eine heuristische Matching-Blue erstellt, um die vorherigen und nächsten Pageant zu ermitteln. Deswegen wurde eine Dessert-Analyse durchgeführt, bei der siebentausendzweihundert Repositories und achtzigtausend Dateneinträge korrigiert wurden. Außerdem beträgt die durchschnittliche Anzahl der Tokens im Release-Hinweis sechzigdrei, was für eine Zusammenfassungsaufgabe recht hoch ist. Auch die Anzahl der eindeutigen Tokens ist mit achttausendachthundertdreißigtausend recht groß. Dies ist auf die große Anzahl eindeutiger Kosten und Methodennamen in dem Repository zurückzuführen. Als Nächstes werde ich die vorgeschlagene Methode erläutern. Das Crosswise-Extractive- und Abstractive-Summarization-Modell besteht aus zwei neuen Modulen: einem Klassifikator mit Bot oder Code Bot und einem Generator mit Bot. Zunächst verwendet GEAS einen Klassifikator, um jede Commit-Nachricht in fünf Kategorien einzuteilen: Features, Verbesserungen, Bugfixes, Deprecations und andere. Die als \"other\" klassifizierten Commit-Nachrichten werden verworfen. Anschließend wendet GEAS einen Generator auf die vier Rubrikdokumente unabhängig voneinander an und generiert Kategorienotizen für jede Kategorie. Bei dieser Aufgabe sind die direkten Entsprechungen zwischen Commit-Nachrichten und Kategorienotizen nicht bekannt. Daher weisen wir jeder eingegebenen Commit-Nachricht zwei Rubriken zu, indem wir die ersten zehn Zeichen jeder Commit-Nachricht verwenden, um das Klassifikationsverhältnis zu trainieren. Wir modellieren den abstrakiven Summarizing-Ansatz des Klassifikationsverhältnisses mit zwei unterschiedlichen Methoden. Das erste Modell, das wir GAS Single nennen, besteht aus einem einzigen Sect-zu-Sect-Netzwerk und generiert einen einzigen langen Notizen-Text, indem eine Verkettung von Eingabe-Commit-Nachrichten erstellt wird. Der Ausgabetext kann durch Segmentierung auf Basis spezieller, kategorienspezifischer Endpunktsymbole in Kategorien unterteilt werden. Die zweite Methode, die wir GSMAUC nennen, besteht aus vier unterschiedlichen Sect-zu-Sect-Netzwerken, von denen jedes einer der Kategorienotizenkategorien entspricht. Nun, lasst mich das Experiment erklären. Fünf Methoden wurden verglichen: GS, GS Single, GS march, Rustling und die frühere Studie Griff. Bezüglich Abtreibung werden diese Notizen in manchen Fällen in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze bei Null zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Die Brew wird bestraft, wenn das System einen kurzen Satz ausgibt. Diese Strafe führt in den im Folgenden beschriebenen Experimentergebnissen zu einem niedrigeren Brew-Wert. Schließlich berechnen wir auch die Spezifität, da Rouge und Brew nicht berechnet werden können, wenn die Versionshinweise leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt leeren Text ausgibt, wenn die Versionshinweise tatsächlich leer sind. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den Green-Datensatz entfernt, der diese ausschließt. GAS und GAS erreichten Rouge-Fehlerwerte, die mehr als zehn Punkte höher waren als der Ausgangswert. Auf dem Green-Testdatensatz stieg jedoch die Differenz zwischen den Ergebnissen der vorgeschlagenen Methode und des Ausgangswerts auf mehr als zwanzig Punkte. Diese Ergebnisse zeigen, dass GAS und GAS wesentlich effektiver sind. GAS erzielte einen besseren Loose-Score als GAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Trainieren des Klassifikators mit Pseudo-Labels effektiv ist. Die hohe Abdeckung von GAS kann wahrscheinlich dadurch erreicht werden, dass der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Kategorie konzentrieren kann. Sie neigt eher dazu, höhere Regeln zu liefern als She is single, was darauf hindeutet, dass es auch effektiv ist, unterschiedliche, perspektivische Summarization-Modelle für jede dieser Kategorien zu entwickeln. Hier ist eine Fehleranalyse. She is-Methoden neigen dazu, kürzere Sätze auszugeben als die Referenzsätze. In der rechten Abbildung hat der Referenzsatz drei oder vier Sätze, während She is nur einen hat. Der Grund für diese Modell-Zurückhaltung besteht darin, dass in den Trainingsdaten nur 33 % der Sätze im Features-Rubrik und 40 % im Improvements-Rubrik vorhanden sind. Darüber hinaus können GS-Methoden keine genauen Kategorienotizen ohne zusätzliche Informationen generieren. Das obere Beispiel auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht, und der vollständige Satz kann nicht generiert werden, ohne den Unterschied zur entsprechenden parallelen Anfrage oder Issue zu kennen. Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander zusammenhängen und zu einem Satz kombiniert werden sollten, aber dies gelingt nicht. Abschließend ein Fazit. Wir haben einen neuen Datensatz für die automatische Kategorisierung von Notizen erstellt. Wir haben auch die Aufgabe der Eingabe von Commit-Nachrichten und ihrer Zusammenfassung formuliert, so dass sie für alle Projekte anwendbar ist, die in Englisch geschrieben sind. Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Kategorienotizen mit einer höheren Abdeckung als der Ausgangswert generiert. Bitte besuchen Sie unseren Datensatz auf GitHub. Vielen Dank."}
