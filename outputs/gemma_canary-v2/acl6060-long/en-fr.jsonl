{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Assa Farari et je présenterai notre article sur l'architecture FUESHOT TABLAR DATA ARCHITECTURE utilisant des architectures de transformateurs affinés. Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes, mais ces caractéristiques sont parfois limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique des données tabulaires à partir de sources externes en texte libre. Supposons que nous ayons un jeu de données tabulaires et une base de connaissances. Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre framework FAST est précisément ce processus automatique. Voyons donc un exemple. Un jeu de données est alimenté dans FAST. Dans cet exemple, le jeu de données est un jeu de données universitaires dont l'objectif est de classer les universités en universités de bas rang et universités de rang élevé. En tant que base de connaissances, nous utilisons Wikipédia. La première phase de FAST est la liaison d'entités, où chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances, et le texte des entités de la base de connaissances est extrait et ajouté au jeu de données. Dans cet exemple, le texte est l'extrait du résumé de page Wikipédia. Nous devons maintenant générer ou extraire des caractéristiques à partir du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse de texte, et il s'agit de la principale nouveauté de cet article, que je décrirai plus en détail dans les prochaines diapositives. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques où nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Premièrement, on génère des caractéristiques en nombre de classes du jeu de données d'origine. Dans cet exemple, le jeu de données d'origine a deux classes, donc on génère d'abord deux nouvelles caractéristiques. Mais si le jeu de données a cinq classes, on génère d'abord cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'art de l'analyse de texte, qui sont des modèles de langage basés sur des transformateurs tels que BERT, GPT, XLERT, etc. Mais il n'est pas probable que nous puissions entraîner des modèles de langage en utilisant les jeux de données d'entrée. Une approche naïve consisterait en un affinage de la tâche cible. Ainsi, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, affiner le modèle de langage sur le jeu de données cible dans cet exemple afin d'affiner le modèle de langage pour classer le texte en classes, l'extrait en classes, bas ou élevé, et utiliser la sortie du modèle de langage comme nouvelles caractéristiques, qui est la probabilité pour chaque classe. Le problème avec cette approche est que les jeux de données peuvent avoir peu d'entités distinctes, de texte. Dans notre expérience, près de la moitié des jeux de données contiennent moins de 400 échantillons et le plus petit jeu de données contient 35 échantillons dans son ensemble d'entraînement. Il serait donc inefficace d'affiner un modèle de langage sur ce jeu de données. Mais nous pouvons utiliser des connaissances préalables sur les jeux de données pré-analysés car FAST est appliqué sur plusieurs jeux de données. Nous pouvons utiliser les n moins un jeux de données pour recueillir des informations sur les n moins un jeux de données et utiliser ces informations lorsque nous analysons le nième jeu de données. Ce que nous proposons est d'ajouter une autre phase d'affinage, une phase préliminaire d'affinage multi-tâche lorsque nous affinons le modèle de langage sur n-1 jeux de données, puis nous exécutons une autre phase d'affinage, qui est un affinage de la tâche cible lorsque nous affinons le modèle de langage sur le nième jeu de données cible. L'état de l'art en matière d'affinage multi-tâche s'appelle Empty DNN. Empty DNN maintient une tête en nombre de tâches dans l'ensemble d'entraînement, de sorte que, dans cet exemple, il y a quatre tâches dans l'ensemble d'entraînement, Empty DNN maintient donc quatre têtes comme vous pouvez le voir sur l'image et il échantillonne un lot aléatoire à partir de l'ensemble d'entraînement et si le lot aléatoire appartient par exemple à la tâche de classification Sing et Selton, il exécute des chemins avant et arrière à travers la première tête. Et si le lot aléatoire appartient à la tâche de classement par paires, il exécute des chemins avant et arrière à travers la dernière tête. Dans notre scénario, un jeu de données tabulaires varie le nombre de classes. Il y a donc de nombreuses tâches. MTDNN maintient le nombre de têtes de classes, les couches de sortie et, en outre, MTDNN doit initialiser de nouvelles têtes pour un nouveau jeu de données avec une nouvelle tâche. Notre approche, appelée affinage par reformulation de tâches, consiste en notre approche d'affinage par reformulation de tâches, au lieu de maintenir plusieurs têtes, nous reformulons chaque jeu de données en un problème de classification par phrase, qui est une tâche à deux classes. Voyons donc un exemple. Voici notre jeu de données d'entrée, qui consiste en des entités, des caractéristiques, du texte et des classes. Et nous reformulons la tâche de la classification du texte en bas et élevé à la classification du texte, de l'extrait, et de la classe en vrai ou faux. Ou, en d'autres termes, nous entraînons le modèle de langage à classer l'extrait et la classe en extrait et classe si l'extrait appartient à la classe ou non, de sorte que le vecteur d'étiquette dans ce cas reste toujours constitué de deux classes, et c'est l'algorithme de notre approche d'affinage reformulé. Voyons maintenant le framework complet, un jeu de données est alimenté dans FAST et FAST exécute la phase de liaison, il extrait le texte de la base de connaissances, qui dans cet exemple est l'extrait de la page Wikipédia, puis il reformule la tâche en tâches de classification par phrase, applique le modèle de langage à la nouvelle tâche et renvoie la probabilité pour chaque classe. Notez que le modèle de langage est déjà affiné sur n-1 jeux de données en utilisant un affinage multi-tâche préliminaire. Nous utilisons ensuite le vecteur de sortie du modèle de langage comme une nouvelle caractéristique générée en nombre de classes. Pour évaluer notre framework, nous utilisons un jeu de données de classification à 17 tables, qui varie en taille, en caractéristiques, en équilibre, en domaine et en performance initiale. Et en tant que base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation en omettant un élément, lorsque nous entraînons FAST sur 16 jeux de données et l'appliquons au 17e jeu de données. Nous divisons également chaque jeu de données en quatre parties, une partie à la fois, et appliquons une validation croisée à quatre parties. Ensuite, nous générons la nouvelle caractéristique et l'évaluons à l'aide de cinq classificateurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur BERT. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre framework à l'affinage du jeu de données cible, l'affinage de la tâche cible et l'affinage multi-tâche préliminaire et notre affinage reformulé obtient le meilleur résultat, la meilleure performance, tandis que MTDNN a obtenu une amélioration de 2 % par rapport à l'affinage du jeu de données cible. Notre approche obtient une amélioration de 6 % lorsque nous regardons les petits jeux de données, nous pouvons voir que la performance de MTDNN diminue et que l'amélioration de la phase préliminaire d'affinage multi-tâche diminue à 1,5 %, mais notre performance augmente à 11 % par rapport à l'affinage de la tâche cible seul. En résumé, FAST permet l'enrichissement en un seul tir à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour toutes les tâches et tous les jeux de données et il conserve la tête du modèle. Mais il ajoute une phase de formulation, un ensemble d'entraînement augmenté et il a besoin d'une valeur cible ayant une signification sémantique afin que nous puissions la fournir au modèle de langage et l'utiliser dans le problème de classification par phrase. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais vous présenter notre travail de recherche intitulé « Apprendre à raisonner de manière déductive : la résolution de problèmes de métro comme extraction de région complexe ». Je suis Alan du laboratoire d'IA de Biden et ceci est un travail conjoint avec Thierry de l'Université du Texas à Austin et Wayloo de SUDD.\n\nPremièrement, j'aimerais aborder notre motivation pour le raisonnement. Voici un exemple où un raisonnement en plusieurs étapes est utile. Cette figure est tirée de l'article POWN où ils utilisent le prompting pour résoudre le problème de métro dans un scénario d'apprentissage fusionné. On peut constater, sur l'exemple donné, que si nous fournissons quelques exemples avec uniquement des questions et des réponses, nous ne pourrons peut-être pas obtenir les réponses correctes. Mais si nous fournissons une description de raisonnement plus détaillée, le modèle est capable de prédire la description du raisonnement et également de faire une prédiction correcte ici. Il est donc avantageux d'avoir un raisonnement en plusieurs étapes interprétable comme sortie. Nous pensons également que le problème de métro est une application directe pour évaluer ces capacités de raisonnement.\n\nIci, dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques. Dans nos ensembles de données, nous avons également l'expression mathématique qui conduit à cette réponse particulière. Certaines hypothèses s'appliquent également comme dans les travaux précédents. Nous supposons que la précision des quantités est connue et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, les opérateurs complexes peuvent être décodés en ces opérateurs de base. Les travaux précédents sur la résolution de problèmes de métro peuvent être classés en modèles séquence à séquence et en modèles séquence à arbre.\n\nLe modèle séquence à séquence traditionnel convertit l'expression en une séquence spécifique pour la génération et il est assez facile à mettre en œuvre et peut se généraliser à de nombreux problèmes complexes différents. Cependant, les inconvénients sont que les performances sont généralement pas meilleures que celles du modèle structurel et qu'il manque d’interprétabilité pour la prédiction. Cependant, cette approche reste assez populaire en raison du modèle Transformer.\n\nDans les modèles basés sur des arbres, nous structurons ces expressions sous forme d'arbre et suivons un parcours pré-ordre dans la génération d'arbres. Ici, nous continuons à générer les opérateurs jusqu'à atteindre les feuilles, qui sont les quantités. L'avantage ici est qu'il nous donne cette structure d'arbre binaire. Cependant, il est assez contre-intuitif, car nous générons d'abord l'opérateur, puis nous générons les quantités à la fin. La deuxième chose est qu'il contient également certains calculs répétitifs. Par exemple, si nous examinons cette expression, a multiplié par trois plus trois, elle est en fait générée deux fois. En fait, nous devrions réutiliser les résultats.\n\nDans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et interprétable. Par exemple, ici, à la deuxième étape, nous pouvons obtenir ce diviseur, qui est 27. Nous pouvons également revenir à la question d'origine pour trouver le contenu pertinent. Et à ces étapes, nous obtenons les diviseurs. Ensuite, à cette troisième étape, nous obtenons en fait le quotient, n’est-ce pas ? Après ces trois étapes, nous pouvons en fait réutiliser les résultats de la deuxième étape et ensuite obtenir les résultats de la quatrième étape. Ensuite, nous pouvons finalement obtenir le dividende. Ici, nous générons en fait toute l'expression directement plutôt que de générer des opérateurs ou des quantités uniques. Cela rend le processus plus précis.\n\nDans notre système déductif, nous commençons d'abord par un ensemble de quantités présentées dans les questions et comprenant également certaines constantes comme états initiaux. L'expression est représentée par EIJOP, où nous effectuons des opérateurs de Qi à QJ, et cette expression est en fait dirigée. Nous avons également la soustraction inversée ici pour représenter la direction opposée. Ceci est assez similaire à l'extraction de relations.\n\nDans un système déductif formel, au pas de temps t, nous appliquons l'opérateur entre la paire Qi et Qj, et nous obtenons cette nouvelle expression. Nous l'ajoutons aux états suivants pour devenir une nouvelle quantité. Cette diapositive visualise en fait l'évolution des états où nous ajoutons continuellement des expressions aux états actuels.\n\nDans nos implémentations de modèle, nous utilisons d'abord un modèle réseau pré-entraîné, qui peut être Birds ou Robots, puis nous encodons une phrase et nous obtenons ces représentations de quantités. Une fois que nous avons les représentations de quantités, nous pouvons commencer à effectuer des inférences.\n\nIci, nous montrons un exemple de Q un pour obtenir la représentation de Q un, elle sera divisée par Q deux et multipliée par Q trois. Tout d'abord, nous obtenons la représentation de la paire, qui est essentiellement juste la concaténation entre Q un et Q deux, puis nous appliquons un réseau feedforward, qui est paramétré par l'opérateur. Enfin, nous obtenons la représentation de l'expression Q1 divisé par Q2. Cependant, en pratique, au stade de l'inférence, nous pourrions également obtenir l'expression incorrecte.\n\nIci, toutes les expressions possibles sont égales à trois fois le nombre d'opérateurs. Le bel avantage ici est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.\n\nÀ la deuxième étape, nous faisons la même chose, mais la seule différence est une quantité de plus. Cette quantité provient de l'expression calculée précédemment. Ainsi, nous pouvons finalement obtenir cette expression finale Q trois multipliée par Q quatre. Nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape précédente. Cette différence rend difficile l'application de la recherche par faisceau car la distribution de probabilité entre ces deux étapes est déséquilibrée.\n\nLa procédure de formation est similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps, et ici nous utilisons également ce tau pour représenter quand nous devrions terminer ce processus de génération. Ici, l'espace est différent de la séquence à séquence car l'espace est différent à chaque pas de temps, tandis que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire. Cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.\n\nNous avons effectué des expériences sur les ensembles de données de problèmes de méthode couramment utilisés MAWPS, Math 23K, MathQA et SWAM. Ici, nous montrons brièvement les résultats par rapport aux meilleures approches précédentes. Notre variante la plus performante est Roberta Deductive Reasoner. En fait, nous n'utilisons pas de recherche par faisceau contrairement aux approches précédentes utilisant la recherche par faisceau. Les meilleures approches sont souvent des modèles basés sur des arbres.\n\nDans l'ensemble, notre raisonneur est capable de surpasser significativement ce modèle basé sur des arbres, mais nous pouvons voir que les nombres absolus sur MathQA ou SWAMP ne sont pas très élevés. Nous avons donc enquêté plus avant sur les résultats sur SWAMP. Cet ensemble de données est difficile car l'auteur a essayé d'ajouter manuellement des choses pour embrouiller le modèle NLP, comme ajouter des informations non pertinentes et des quantités supplémentaires. Dans notre prédiction, nous trouvons que certaines des valeurs intermédiaires sont en fait négatives. Par exemple, dans cette question, nous demandons combien de pommes Jake a, mais nous avons des informations supplémentaires comme dix-sept pitnes de moins et Stephen a huit pitnes, ce qui est totalement non pertinent. Notre modèle fait donc certaines prédictions comme celle-ci, qui produit des valeurs négatives. Nous constatons que ces deux expressions ont une similitude. Nous pouvons donc en fait limiter cet espace de recherche en supprimant ces résultats négatifs afin de rendre la réponse correcte.\n\nNous avons découvert que cette contrainte améliore en fait considérablement certains modèles. Par exemple, pour Birds, nous avons amélioré de sept points, puis pour le modèle de base Roberta, nous avons en fait amélioré de deux points. Un meilleur modèle de langage a une meilleure capacité de compréhension du langage, de sorte que le nombre ici est plus élevé pour Roberta et plus faible pour Birds. Nous avons également essayé d'analyser la difficulté derrière tous ces ensembles de données. Nous supposons que le nombre de quantités inutilisées peut être considéré comme des informations non pertinentes ici. Ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées et que l'ensemble de données SWAMP a la plus grande proportion. Ici, nous montrons également les performances globales. Pour les échantillons sans quantités inutilisées, les performances globales sont en fait plus élevées que les performances globales. Mais pour les échantillons qui ont des quantités inutilisées, c'est beaucoup pire que les performances globales. Pour MAWPS, nous n'avons pas trop de cas de test, donc je viens d'ignorer cette partie.\n\nEnfin, nous voulons montrer l'interprétabilité à travers un exemple de plantage et de participation. Ici, notre modèle fait une mauvaise prédiction à la première étape. Nous pouvons donc mettre en corrélation cette expression avec la phrase ici. Nous pensons que cette phrase pourrait induire le modèle en erreur et le conduire à une prédiction incorrecte. Ici, ajouter trente-cinq amène le modèle à penser qu'il devrait s'agir d'un opérateur additionnel. Nous essayons donc de réviser la phrase pour qu'elle soit quelque chose comme le nombre d'arbres fruitiers trente-cinq de moins que le nombre d'arbres à pommes. Nous la faisons pour transmettre une sémantique plus précise, de sorte que le modèle puisse faire la prédiction correcte. Cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle.\n\nPour conclure notre travail, notre modèle est en fait assez efficace et nous sommes en mesure de fournir une procédure de résolution interprétable et nous pouvons facilement intégrer certaines connaissances antérieures en tant que contrainte, ce qui peut aider à améliorer les performances. La dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de mathématiques, mais aussi à d'autres tâches qui impliquent un raisonnement en plusieurs étapes. Nous avons également certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire peut être assez élevée. Et la deuxième chose est que, comme mentionné, étant donné que la distribution de probabilité est déséquilibrée à différents pas de temps, il est également assez difficile d'appliquer une stratégie de recherche par faisceau.\n\nCeci est la fin de la présentation et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht. Je présenterai mon travail conjoint avec Jerry, qui porte sur un nouveau jeu de données pour la recherche d'articles législatifs. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les procédures juridiques fondamentales. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique se retrouvent sans protection, voire exploités. Notre travail vise à combler le fossé entre les citoyens et la loi en développant un système de recherche d'articles législatifs efficace. Un tel système pourrait fournir un service d'aide juridique professionnelle gratuite pour les personnes non spécialisées. Avant d'aborder la contribution principale de ce travail, décrivons d'abord le problème de la recherche d'articles législatifs. Face à une question simple sur une question juridique, comme quel est le risque si je viole le secret professionnel ? Un modèle est requis pour récupérer tous les articles législatifs pertinents à partir d'un vaste corpus de législation. Cette tâche de recherche d'information présente ses propres défis. Premièrement, elle traite de deux types de langage : le langage naturel courant pour les questions et le langage juridique complexe pour les lois. Cette différence de distribution linguistique rend plus difficile pour un système de récupérer des candidats pertinents, car elle nécessite indirectement un système d'interprétation inhérent capable de traduire une question en langage naturel en une question juridique qui correspond à la terminologie des lois. De plus, le droit statutaire n'est pas un ensemble d'articles indépendants qui peuvent être traités comme une source d'information complète en soi, comme les actualités ou les recettes, par exemple. Au contraire, c'est une collection structurée de dispositions juridiques qui n'ont un sens que lorsqu'elles sont considérées dans le contexte général, c'est-à-dire avec les informations complémentaires provenant des articles voisins, des domaines et sous-domaines auxquels ils appartiennent, et de leur place dans la structure de la loi. Enfin, les articles législatifs ne sont pas de petits paragraphes, ce qui est généralement l'unité de récupération typique dans la plupart des travaux de recherche. Ici, ce sont de longs documents pouvant contenir jusqu'à six mille mots. Les progrès récents en TALN ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements légaux ou l'examen automatisé de contrats, mais la recherche d'articles législatifs est restée en grande partie inexplorée en raison du manque de grands ensembles de données étiquetés de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données, centré sur le citoyen natif français, pour étudier si un modèle de recherche peut approximer l'efficacité et la fiabilité d'un expert juridique pour la tâche de recherche d'articles législatifs. Notre jeu de données de recherche d'articles législatifs belge se compose de plus d'un millier cent questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, de la famille, au logement, en passant par l'argent, le travail et la sécurité sociale. Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents provenant d'un corpus de plus de vingt-deux mille six cents articles juridiques provenant des codes belges. Décrivons maintenant la façon dont nous avons collecté ce jeu de données. Nous avons commencé par compiler un vaste corpus d'articles juridiques. Nous avons pris en compte trente-deux codes belges publiquement disponibles et extrait tous leurs articles ainsi que les titres de section correspondants. Ensuite, nous avons recueilli des questions juridiques avec des références à des lois pertinentes. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges qui demandent des conseils sur une question juridique personnelle. Nous avons eu la chance d'avoir accès à leur site Web où leur équipe de juristes expérimentés répond aux questions juridiques les plus courantes des Belges. Nous avons collecté des milliers de questions annotées de catégories, de sous-catégories et de références juridiques à des lois pertinentes. Enfin, nous avons examiné les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes que nous avons pris en compte. Les références restantes ont été mises en correspondance et converties en ID d'article correspondants à partir de notre corpus. Nous avons finalement obtenu mille cent huit questions, chacune soigneusement étiquetée avec les ID des articles pertinents de notre vaste corpus de vingt-deux mille six cents trente-trois articles législatifs. En outre, chaque question est accompagnée d'une liste des titres de sous-sections dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient être intéressantes pour les futures recherches sur la recherche d'informations juridiques ou la classification de textes juridiques. Examinons maintenant certaines caractéristiques de notre jeu de données. Les questions comptent entre cinq et quarante-quatre mots, avec une médiane de quarante mots. Les articles sont beaucoup plus longs, avec une médiane de soixante-dix-sept mots, cent quarante-deux d'entre eux dépassant mille mots, le plus long comptant jusqu'à cinq mille sept cent quatre-vingt-neuf mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, environ quatre-vingt-cinq pour cent d'entre elles concernant la famille, le logement, l'argent ou la justice, tandis que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très diversifiés, car ils proviennent de trente-deux codes belges différents qui couvrent un grand nombre d'articles législatifs collectés à partir de chacun de ces codes belges. Sur les 22 633 articles, seulement 1 612 sont cités comme pertinents pour au moins une question du jeu de données, et environ 80 % de ces articles cités proviennent du code civil, du code judiciaire, du code d'instruction criminelle ou du code pénal. Parallèlement, 18 des 32 codes ont moins de 5 articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois. En utilisant notre jeu de données, nous avons testé plusieurs approches de recherche, y compris les architectures lexicales et denses. Étant donné une requête dans un article, un modèle lexical attribue un score au couple requête-article en calculant la somme sur les termes de la requête des poids de chacun de ces termes dans cet article. Nous expérimentons avec les fonctions de classement standard TFIDF et BM 25. Le principal problème de ces approches est qu'elles ne peuvent récupérer que des articles qui contiennent les mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur le réseau neuronal qui peut capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle B encodeur qui mappe les requêtes et les articles dans des représentations vectorielles denses et calcule un score pertinent entre une paire requête-article par la similarité de leurs embeddings. Ces embeddings résultent généralement d'une opération de pooling sur la sortie d'un modèle d'embedding de mots. Premièrement, nous étudions l'efficacité des B encodeurs siamois dans un contexte d'évaluation sans entraînement, ce qui signifie que les modèles d'embedding de mots pré-entraînés sont appliqués tels quels sans réglage supplémentaire. Nous expérimentons avec des encodeurs de texte indépendants du contexte, à savoir Word to Vec et FastText, et des modèles d'embedding dépendant du contexte, à savoir Roberta et plus précisément Camembert, qui est un modèle Roberta français. De plus, nous entraînons notre propre modèle Biancoders basé sur Camembert sur l'ensemble des données. Notez que pour l'entraînement, nous expérimentons avec les deux saveurs de l'architecture Biancoder. Siamois, qui utilise un modèle d'embedding de mots unique qui mappe les requêtes et les articles ensemble dans un espace vectoriel dense partagé. Et deux tours, qui utilise deux modèles d'embedding de mots indépendants qui encodent les requêtes et les articles séparément dans des espaces d'embedding différents. Nous expérimentons avec le pooling moyen, max et CLS ainsi qu'avec le produit scalaire et le cosinus pour calculer les similarités. Voici les résultats de nos modèles de référence sur les ensembles de test, avec les méthodes lexicales ci-dessus, les B encodeurs siamois évalués dans un contexte sans entraînement au milieu et les B encodeurs affinés en dessous. Dans l'ensemble, les B encodeurs affinés surpassent significativement toutes les autres références. Le modèle à deux tours améliore sa variante siamois sur le rappel à 100, mais fonctionne de manière similaire sur les autres métriques. Bien que BM vingt cinq ait sous-performé significativement le Biancoder entraîné, ses performances indiquent qu'il s'agit toujours d'une référence solide pour la recherche spécifique au domaine. En ce qui concerne l'évaluation sans entraînement des Biancoders siamois, nous constatons que l'utilisation directe des embeddings d'un modèle Kamembert pré-entraîné sans optimisation pour la tâche de recherche d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats précédents. Et le Biancoder basé sur Word to Vec a nettement surpassé le modèle basé sur FastText et Bird, suggérant que peut-être les embeddings de mots pré-entraînés sont plus appropriés pour la tâche que les embeddings de niveau caractère ou de sous-mot lorsqu'ils sont utilisés tels quels. Bien que prometteurs, ces résultats suggèrent une marge d'amélioration considérable par rapport à un expert juridique qualifié qui peut éventuellement récupérer tous les articles pertinents pour n'importe quelle question et ainsi obtenir des scores parfaits. Pour conclure, discutons de deux limitations de notre jeu de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des trente-deux codes belges pris en compte, ce qui ne couvre pas l'ensemble du droit belge, car les articles des décrets, des directives et des arrêtés sont manquants. Lors de la construction du jeu de données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions n'ont qu'une fraction du nombre initial d'articles pertinents. Cette perte d'informations implique que la réponse contenue dans les articles pertinents restants peut être incomplète, bien qu'elle soit toujours tout à fait appropriée. Deuxièmement, il convient de noter que toutes les questions juridiques ne peuvent pas être répondues par des articles de loi seuls. Par exemple, la question puis-je expulser mes locataires s'ils font trop de bruit pourrait ne pas avoir de réponse détaillée dans le droit statutaire quantifiant un seuil de bruit spécifique auquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des précédents similaires à sa situation actuelle. Par exemple, le locataire organise deux fêtes par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions sont plus adaptées que d'autres à la tâche de recherche d'articles législatifs, et le domaine des moins adaptées reste à déterminer. Nous espérons que notre travail suscitera l'intérêt pour le développement de modèles de recherche d'articles législatifs pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article et notre code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALS, un benchmark indépendant des tâches destiné à tester les modèles vision et langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous pris la peine de mettre en place ce benchmark ? Eh bien, au cours des dernières années, nous avons assisté à une explosion de modèles vision et langage basés sur des transformateurs, pré-entraînés sur de grandes quantités de paires image-texte. Chacun de ces modèles améliore les performances de pointe sur les tâches vision et langage telles que la réponse à des questions visuelles, le raisonnement de bon sens visuel, la recherche d'images, l'ancrage de phrases. Nous avons donc constaté que les précisions sur ces benchmarks spécifiques aux tâches augmentent régulièrement, mais savons-nous ce que les modèles ont réellement appris ? Qu'est-ce qu'un transformateur vision et langage comprend lorsqu’il attribue un score élevé à cette image et à cette phrase pour qu'ils correspondent, et un score faible à l'autre ? Les modèles vision et langage se concentrent-ils sur ce qui est pertinent ou se concentrent-ils sur les biais, comme le montrent les travaux précédents ? Pour mieux éclairer cet aspect, nous proposons une approche plus agnostique des tâches et introduisons des « valves » qui testent la sensibilité des modèles vision et langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence d'entités. Mais comment tester si les modèles vision et langage ont capturé ces phénomènes ? Le \"foiling\", une méthode précédemment appliquée uniquement aux modèles vision et langage pour les phrases nominales par Ravi Shakar et ses collaborateurs et au comptage par nous dans des travaux antérieurs. Le foiling consiste essentiellement à prendre la légende d'une image et à produire un « foil » en modifiant la légende de sorte qu'elle ne décrive plus l'image. Nous effectuons ces modifications de phrases en nous concentrant sur six éléments spécifiques : l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence d'entités. Chaque élément peut comprendre un ou plusieurs instruments si nous avons trouvé plus d'une manière intéressante de créer des instances de foil. Par exemple, pour l'élément « actions », nous avons deux instruments : un dans lequel le verbe d'action est remplacé par un autre action et un autre dans lequel les actants sont échangés. Le comptage et la coréférence sont également des éléments qui ont plus d'un instrument. Et nous créons ces foils en veillant à ce qu'ils ne décrivent pas l'image, qu'ils soient des phrases grammaticales et valides. Ce n'est pas facile à faire, car une légende foillée peut être moins susceptible que la légende originale. Par exemple, bien qu'il ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme qu'un homme coupe des plantes, et les grands modèles vision et langage pourraient s'en apercevoir. Par conséquent, pour obtenir des foils valides, nous devons agir. Premièrement, nous utilisons des modèles de langage puissants pour proposer des foils. Deuxièmement, nous utilisons l'inférence du langage naturel, ou NLI, pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image. Pour tester cela automatiquement, nous appliquons l'inférence du langage naturel avec la justification suivante : nous considérons l'image comme le prémisse et sa légende comme l'hypothèse qu'elle implique. De plus, nous considérons la légende comme le prémisse et le foil comme son hypothèse. Si un modèle NLI prédit que le foil contredit ou est neutre par rapport à la légende, nous prenons cela comme un indicateur d'un foil valide. Pour être impliqué par la légende, il ne peut pas être un bon foil, car par transitivité, il fournira une description véridique de l'image et nous filtrons ces foils. Mais cette procédure n'est pas parfaite. C'est juste un indicateur de foils valides, par conséquent, en tant que troisième mesure pour générer des foils valides, nous employons des annotateurs humains pour valider les données utilisées dans VALS. Ainsi, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrites dans ce tableau. Notez que VALS ne fournit aucune donnée d'entraînement, mais uniquement des données de test, car il s'agit d'un benchmark de test à tir froid. Il est conçu pour tirer parti des capacités existantes des modèles vision et langage après le pré-entraînement. Le fine-tuning ne permettrait qu'aux modèles d'exploiter les artefacts ou les biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés par l'évaluation des capacités des modèles vision et langage après le pré-entraînement. Nous expérimentons avec cinq modèles vision et langage sur VALS, notamment avec CLIP, Wilbert, Wilbert Kelvin One et Visual Bert. Deux de nos mesures d'évaluation les plus importantes sont la précision des modèles dans la classification des paires image-phrase en légendes et en foils. Peut-être plus pertinent pour cette vidéo, nous allons présenter notre métrique plus permissive, la précision par paires, qui mesure si le score d'alignement image-phrase est supérieur pour la paire image-texte correcte que pour sa paire foillée. Pour plus de métriques et de résultats sur celles-ci, consultez notre article. Les résultats avec la précision par paires sont présentés ici et ils sont conformes aux résultats que nous avons obtenus avec les autres métriques, à savoir que les meilleures performances à tir froid sont obtenues par Wilbert Twelve in One, suivi par Wilbert, Alex Bert, Clip, et enfin Visual Bird. Il est notable que les instruments axés sur des objets individuels comme l'existence et les phrases nominales sont presque résolus par Wilbert Twelve in One, soulignant que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos paramètres de foiling adverses. Nous constatons avec les instruments de pluralité et de comptage que les modèles vision et langage ont du mal à distinguer les références à des objets uniques ou multiples ou à les compter dans une image. La pièce relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même si cela est étayé par des biais de plausibilité, comme nous le constatons dans la pièce actions. À partir de la pièce coréférence, nous découvrons qu'il est également difficile pour les modèles vision et langage de retracer plusieurs références au même objet dans une image en utilisant des pronoms. À titre de vérification de la cohérence et parce que c'est une expérience intéressante, nous avons également benchmarké deux modèles basés uniquement sur du texte, GPT One et GPT Two, afin d'évaluer si VALS peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et du foil, sans image ici, et en prédisant l'entrée avec la perplexité la plus faible. Si la perplexité est plus élevée pour le foil, nous prenons cela comme un indicateur que la légende foillée pourrait souffrir de biais de plausibilité ou d'autres biais linguistiques. Et il est intéressant de constater que, dans certains cas, les modèles de texte uniquement GPT ont mieux capturé la plausibilité du monde que les modèles vision et langage. En résumé, VALS est un benchmark qui utilise le prisme des constructions linguistiques pour aider la communauté à améliorer les modèles vision et langage en testant rigoureusement leurs capacités de mise en correspondance visuelle. Nos expériences montrent que les modèles vision et langage identifient bien les objets nommés et leur présence dans les images, comme le montre la pièce existence, mais ont du mal à mettre en correspondance leur interdépendance et leurs relations dans les scènes visuelles lorsqu'ils sont obligés de respecter les indicateurs linguistiques. Nous souhaitons vivement encourager la communauté à utiliser VALS pour mesurer les progrès vers la mise en correspondance langage avec vision et langage. Et plus encore, VALS pourrait être utilisé comme une évaluation indirecte des ensembles de données, les modèles pouvant être évalués avant et après l'entraînement ou le fine-tuning pour voir si un ensemble de données aide les modèles à s'améliorer sur l'un des aspects testés par VALS. Si vous êtes intéressé, consultez les données VALS sur GitHub et, si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m’appelle Kamisara et je suis de l’Université de Tokyo. Je présenterai un article intitulé « R et SAM, un jeu de données à grande échelle pour la détermination automatique de la durée des risques par summarisation des journaux de validation ». J'expliquerai les points suivants dans cet ordre. Premièrement, je présenterai la détermination automatique de la durée des risques sur laquelle nous travaillons dans cette recherche. Une note de publication est un document technique qui résume les modifications distribuées avec chaque version d’un produit logiciel. L’image montre les notes de publication pour la version 2.6.4 de la bibliothèque GBUJS. Ces notes jouent un rôle important dans le développement open source, mais leur préparation manuelle est chronophage. Il serait donc très utile de pouvoir générer automatiquement des notes de publication de haute qualité. Je me référerai à deux recherches antérieures sur la génération automatique de notes de publication. La première est un système appelé Arena, publié en 2014. Il adopte une approche basée sur des règles, par exemple en utilisant un extracteur de modifications pour extraire les différences essentielles des modifications de bibliothèque et des modifications de la documentation à partir des différences entre les versions, puis en les combinant. La caractéristique la plus notable de ce système est l’extracteur d’incidents, situé dans le coin supérieur droit, qui doit être lié à Jira, l’écosystème d’incidents et ne peut être appliqué qu’aux projets utilisant Jira. En d’autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub. La seconde est Griff, annoncé récemment en 2020. Il est disponible sur Internet et peut être installé via PIP. Ce système dispose d’un simple modèle de classification de texte basé sur l’apprentissage et génère l’une des cinq catégories telles que « fonctionnalités » ou « corrections de bogues » pour chaque message de validation en entrée. L’image est un exemple d’utilisation qui renvoie une catégorie de « correction de bogues ». Les données d’entraînement de Queryface sont relativement peu nombreuses, environ cinq mille, et seront présentées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n’est pas très élevée. Je présente deux recherches connexes, mais elles présentent des problèmes de portée d’application limitée et de ressources de données rares. Notre article résout ces deux problèmes et génère automatiquement des notes de publication de haute qualité. Pour la limitation de la portée d’application, nous proposons une méthode de summarisation de classificateur de haute qualité utilisant uniquement les messages de validation comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais. Pour le deuxième problème de rareté des ressources de données, nous avons créé un jeu de données RNSAM composé d’environ quatre-vingt-deux mille éléments en collectant des données à partir de dépôts GitHub publics via l’API GitHub. Ensuite, je décrirai notre jeu de données. Voici un exemple de données. Le côté gauche est un message de validation et le côté droit est les notes de publication. Les notes de publication sont étiquetées « améliorations », « corrections de bogues », etc. Nous avons défini une tâche qui prend les messages de validation en entrée et produit les notes de publication étiquetées. Cela peut être considéré comme une tâche de summarisation. Nous avons prédéfini quatre niveaux : « fonctionnalités », « améliorations », « corrections de bogues », « dépréciations » et « suppressions ». Ces niveaux ont été définis sur la base de recherches antérieures et d’autres facteurs. Les notes de publication en bas à droite sont extraites des notes de publication affichées en bas à gauche. À ce moment-là, il est nécessaire de détecter les quatre catégories qui ont été définies à l’avance, mais les catégories ne sont pas toujours cohérentes avec chaque dépôt. Par exemple, la catégorie « améliorations » inclut les « améliorations », les « améliorations », les « optimisations », et ainsi de suite. Nous avons préparé une liste de vocabulaire de catégories d’étude pour chacune de ces variations notatiales. Utilisez-la pour détecter la classe RISNOD et corriger le texte du RIS qui suit en tant que phrase RISNOD pour la classe. Ensuite, il y a le message de validation. Les messages de validation ne sont pas liés à chaque RIS. Comme le montre l’image ci-dessous, si le RIS actuel est Persian 2.5 à 19, nous devons identifier le RIS précédent 2.5 à 18 et obtenir sa différence. C’est un peu fastidieux et il ne suffit pas de se contenter d’obtenir une liste de RIS et de regarder le avant et l’après. Nous avons créé une règle heuristique pour faire correspondre le bleu pour obtenir la page précédente et la page suivante. L’analyse du jeu de données a corrigé sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données. De plus, le nombre moyen de jetons de notes de publication est de soixante-trois, ce qui est assez élevé pour une tâche de summarisation. De plus, le nombre de jetons uniques est assez important, soit huit mille huit cent trente mille. Cela est dû au grand nombre de coûts et de noms de méthodes uniques trouvés dans le dépôt. Ensuite, j’expliquerai la méthode proposée. Le modèle de summarisation extractrice et abstraite croisée se compose de deux modules nouveaux : un classificateur utilisant bot ou code bot et un générateur utilisant bot. Premièrement, GEAS utilise un classificateur pour classer chaque message de validation dans cinq classes de raison : « fonctionnalités », « améliorations », « corrections de bogues », « dépréciations » et « autres ». Les messages de validation classés dans la catégorie « autres » sont rejetés. Ensuite, GEAS applique un générateur aux documents de quatre catégories indépendamment et génère des notes de raison pour chaque catégorie. Dans cette tâche, la correspondance directe entre les messages de validation et les notes de raison n’est pas connue. Par conséquent, pour entraîner le classificateur, nous attribuons deux catégories à chaque message de validation en entrée en utilisant les dix premiers caractères de chaque message de validation. Nous modélisons l’approche de summarisation abstraite du classificateur par deux méthodes différentes. Le premier modèle, que nous appelons GAS single, se compose d’un réseau de type « section vers section » unique et génère un long texte de note unique en concaténant les messages de validation en entrée. Le texte de sortie peut être divisé en classes par segment en fonction de symboles de fin de classe spécifiques à la classe. La deuxième méthode, que nous appelons GSMAUC, se compose de quatre réseaux de type « section vers section » différents, chacun correspondant à l’une des classes de notes. Très bien, laissez-moi expliquer l’expérience. Cinq méthodes ont été comparées : GS, GS single, GS march, rustling et l’étude précédente Griff. Concernant l’avortement, dans certains cas, ces notes sont produites en plusieurs phrases. Étant donné qu’il est difficile de calculer le nombre de phrases à zéro, elles sont combinées avec des espaces et considérées comme une longue phrase. La métrique Brew est pénalisée lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur Brew inférieure dans les résultats expérimentaux décrits ci-dessous. Nous calculons également la spécificité, car Rouge et Brew ne peuvent pas être calculés si les notes de publication sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de publication sont censées être vides. Voici les résultats. Étant donné que le jeu de données contient des adresses e-mail, des valeurs de hachage, etc., nous avons également éradiqué le jeu de données vert, qui les exclut. GAS et GAS ont obtenu des scores d’erreur Rouge supérieurs de plus de dix points par rapport à la référence. Cependant, sur le jeu de données de test vert, l’écart entre les scores de la méthode proposée et la référence a grimpé à plus de vingt points. Ces résultats indiquent que GAS et GAS sont significativement efficaces. GAS a obtenu un score Loose meilleur que GAS, ce qui suggère que la combinaison d’un classificateur et d’un générateur est efficace pour entraîner le classificateur à l’aide de pseudobus. La large couverture de GAS peut être atteinte probablement parce que le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque catégorie. Elle a tendance à produire des règles plus élevées que elle est single, ce qui suggère qu’il est également efficace de développer indépendamment différents modèles de summarisation de perspective pour chacune de ces classes. Voici une analyse d’erreur. Les méthodes She is ont tendance à produire des phrases plus courtes que les phrases de référence humaine. Dans la figure de droite, la phrase de référence comporte trois ou quatre phrases, tandis que She is n’en comporte qu’une. La raison de cette réticence du modèle est que dans les données d’entraînement, seulement 33 % des phrases sont présentes dans la catégorie « fonctionnalités » et 40 % dans la catégorie « améliorations ». De plus, les méthodes GS ne peuvent pas générer de notes de raison précises sans informations supplémentaires. L’exemple supérieur à droite est un exemple d’un message de validation très désordonné et la phrase complète ne peut pas être générée sans différence par rapport à la requête ou au problème parallèle correspondant. L’exemple ci-dessous montre que les deux messages de validation en entrée sont liés et devraient être combinés en une seule phrase, mais elle ne le fait pas. Enfin, une conclusion. Nous avons créé un nouveau jeu de données pour la génération automatique de notes. Nous avons également défini la tâche consistant à entrer des messages de validation et à les résumer de sorte qu’elle soit applicable à tous les projets écrits en anglais. Nos expériences montrent que la méthode proposée génère des notes plus propres et une couverture plus élevée que la référence. Veuillez consulter notre jeu de données sur GitHub. Merci."}
