{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Safari, und ich werde unseren Beitrag zum Thema papierbasierte Tabellendatenanreicherung mithilfe von feinabgestimmten Transformer-Architekturen vorstellen. Wissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation existierender Features, doch diese sind manchmal begrenzt. Die Generierung von Features mithilfe einer anderen Datenquelle kann substanzielle Informationen hinzufügen. Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mithilfe externer, frei formulierter Textquellen. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entity Linking und Textanalyse umfasst, um neue Features aus dem frei formulierten Text der Wissensdatenbank zu extrahieren. Unser Framework, Fest, ist genau dieser automatische Prozess. Sehen wir uns ein Beispiel an: Datensätze, die in Fest eingespeist werden. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, dessen Ziel es ist, Universitäten in niedrigrangige und hochrangige Universitäten einzuteilen. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase von Fest ist das Entity Linking, bei dem jede Entität, in diesem Fall der Universitätsname, mit einer Entität innerhalb der Wissensdatenbank verknüpft wird. Anschließend wird der Text der Entitäten der Wissensdatenbank extrahiert und dem Datensatz hinzugefügt. In diesem Fall ist der Text der Wikipedia-Seiten-Abstract. Nun müssen wir Features aus dem abgerufenen Text generieren oder extrahieren. Wir benötigen also eine Feature-Extraktionsphase, die Textanalyse beinhaltet. Dies ist die Hauptneuheit dieses Beitrags, und ich werde in den nächsten Folien detaillierter darauf eingehen. Nach der Feature-Extraktionsphase gibt es eine Feature-Generierungsphase, in der wir die extrahierten Features verwenden, um eine kleine Anzahl neuer Features zu generieren. Zunächst werden Features in der Anzahl der Klassen des ursprünglichen Datensatzes generiert. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, sodass zunächst zwei neue Features generiert werden. Wenn der Datensatz jedoch fünf Klassen hat, werden zunächst fünf neue Features generiert. Jedes Feature repräsentiert die Wahrscheinlichkeit für jede Klasse. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik in der Out-of-Text-Analyse, d. h. Transformer-basierte Sprachmodelle wie BERT, GPT-X usw. Es ist jedoch unwahrscheinlich, dass wir Sprachmodelle mit den Eingabedatensätzen trainieren können. Ein naiver Ansatz wäre ein Target-Task-Fine-Tuning. In der Feature-Extraktionsphase können wir ein vortrainiertes Sprachmodell herunterladen und das Sprachmodell über den Target-Datensatz feinabstimmen. In diesem Beispiel trainieren wir das Sprachmodell, um Texte in Klassen, also Abstracts in Klassen (niedrig oder hoch), einzuteilen. Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse ist, und verwenden diese als neue Features. Das Problem bei diesem Ansatz besteht darin, dass der Datensatz nur wenige verschiedene Entitäten-Texte enthalten kann. In unseren Experimenten enthalten fast die Hälfte der Datensätze weniger als 400 Samples, und der kleinste Datensatz enthält nur 35 Samples im ursprünglichen Trainingsset. Um ein Sprachmodell über diesen Datensatz feinabzustimmen, wäre unwirksam. Wir können jedoch Vorwissen über voranalysierte Datensätze nutzen, da Fest auf mehrere Datensätze angewendet wird. Wir können die n-1 Datensätze verwenden, um Informationen über die n-1 Datensätze zu sammeln und diese Informationen bei der Analyse des n-ten Datensatzes zu verwenden. Was wir vorschlagen, ist, eine weitere Fine-Tuning-Phase hinzuzufügen: eine vorläufige Multitask-Fine-Tuning-Phase, in der wir das Sprachmodell über die n-1 Datensätze feinabstimmen und anschließend eine weitere Fine-Tuning-Phase durchführen, die ein Target-Task-Fine-Tuning ist, bei dem wir das Sprachmodell über den n-ten Target-Datensatz feinabstimmen. Der aktuelle Stand der Technik beim Multitask-Fine-Tuning wird als DNN bezeichnet. DNN behält in DNN eine Kopfzahl in der Anzahl der Aufgaben im Trainingsset bei. Wenn in diesem Beispiel vier Aufgaben im Trainingsset vorhanden sind, behält DNN vier Köpfe bei, wie man auf dem Bild sehen kann. Es wird eine zufällige Charge aus dem Trainingsset entnommen, und wenn die zufällige Charge zu einer beispielsweise Single- und Sentence-Klassifikationsaufgabe gehört, wird ein Forward- und Backward-Pass durch den ersten Kopf ausgeführt. Wenn die zufällige Charge zu einer Pairwise-Ranking-Aufgabe gehört, wird ein Forward- und Backward-Pass durch den letzten Kopf ausgeführt. In unserem Szenario variiert ein tabellarischer Datensatz die Anzahl der Klassen, sodass es viele Aufgaben gibt. DNN behält eine Kopfzahl für die Klassen aus, und zusätzlich benötigt DNN anfänglich neue Köpfe für einen neuen Datensatz mit einer neuen Aufgabe. Unser Ansatz, Task Reformulation Fine-Tuning, besteht darin, dass wir anstelle der Beibehaltung mehrerer Köpfe jeden Datensatz in ein Sentence-per-Klassifikationsproblem, d. h. eine Zwei-Klassen-Aufgabe, umwandeln. Sehen wir uns ein Beispiel an: Hier ist unser Eingabedatensatz, der aus Entitäten, Features, Text und Klassen besteht. Wir reformulieren die Aufgabe von der Klassifizierung des Textes in niedrig und hoch hin zur Klassifizierung des Textes (Abstract) und der Klasse in wahr oder falsch, mit anderen Worten, wir trainieren das Sprachmodell, um ein Abstract und eine Klasse als wahr zu klassifizieren, wenn das Abstract zur Klasse gehört oder nicht. Das Label-Vektor in diesem Fall besteht immer aus zwei Klassen. Dies ist der Algorithmus für unseren feinabgestimmten, reformulierten Fine-Tuning-Ansatz. Sehen wir uns das vollständige Framework an: Ein Datensatz wird in Fest eingespeist, und Fest führt dann die Linking-Phase aus. Es extrahiert den Text aus der Wissensdatenbank, in diesem Fall den Abstract der Wikipedia-Seite. Anschließend reformuliert es die Aufgabe in eine Pair-Sentence-per-Klassifikationsaufgabe, wendet das Sprachmodell auf die neue Aufgabe an und erhält die Wahrscheinlichkeit für jede Klasse als Ausgabe. Beachten Sie, dass das Sprachmodell bereits über n-1 Datensätze feinabgestimmt wurde, unter Verwendung einer vorläufigen Multitask-Fine-Tuning-Phase. Anschließend verwenden wir den Ausgabvektor des Sprachmodells als neu generiertes Feature in der Anzahl der Klassen. Um unser Framework zu evaluieren, verwenden wir einen siebzehn Datensätze umfassenden tabellarischen Klassifikationsdatensatz, der die Größe der Features, die Balance der Domäne und die anfängliche Leistung definiert. Als Wissensdatenbank verwenden wir Wikipedia. Wir gestalten unser Experiment als Leave-One-Out-Evaluierung, bei der wir Fest über 16 Datensätze trainieren und es auf den 17. Datensatz anwenden. Wir teilen auch jeden Datensatz in vier Falte und wenden einen Four-Fold Cross-Validation an. Anschließend generieren wir die neuen Features und evaluieren sie mithilfe von fünf Evaluatoren-Klassifikatoren. In unserem Experiment verwenden wir eine BERT-basierte Architektur. Hier sind die Ergebnisse unseres Experiments. Sie können sehen, dass wir unser Framework mit Target-Data-Set-Fine-Tuning, Target-Task-Fine-Tuning und Multitask-DNN-Preliminary-Fine-Tuning vergleichen. Unser reformuliertes Fine-Tuning erzielt das beste Ergebnis, während Multitask-DNN eine Verbesserung von zwei Prozent gegenüber dem Target-Data-Set-Fine-Tuning erzielt. Unser Ansatz erzielt eine Verbesserung von sechs Prozent. Wenn wir uns die kleinen Datensätze ansehen, können wir sehen, dass die Leistung von Multitask-DNN abnimmt und die Verbesserung der vorläufigen Multitask-Fine-Tuning-Phase auf 1,5 Prozent sinkt, während unsere Leistung auf 11 Prozent gegenüber dem Target-Task-Fine-Tuning ansteigt. Zusammenfassend lässt sich sagen, dass Fest Low-Shot-Anreicherung ab 35 Samples in unserem Experiment ermöglicht. Es verwendet eine Architektur für alle Aufgaben und Datensätze und behält die Köpfe des Modells bei. Es fügt jedoch eine Reformulationsphase hinzu, die den Trainingsset erweitert und einen Target-Wert mit semantischer Bedeutung benötigt, sodass wir ihn in das Sprachmodell einspeisen und in einem Sentence-per-Klassifikationsproblem verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit vorstellen: das Lernen deduktiven Denkens, die Lösung von Rechenproblemen als komplexe RegionsExtraktion. Ich komme vom Biance AI Lab und dies ist eine gemeinsame Arbeit mit Che von der University of Texas at Austin und Wedu von SUDD. Zuerst möchte ich über unsere Motivation für das deduktive Denken sprechen. Hier zeigen wir Beispiele, in denen mehrstufiges Denken hilfreich ist. Diese Abbildung stammt aus dem Pound-Paper, in dem Prompting verwendet wird, um das Rechenproblem in einem Szenario des Zukünftigen Lernens zu lösen. Auf der linken Seite können wir sehen, dass wir, wenn wir einige Beispiele mit nur Frage und Antwort geben, möglicherweise nicht die korrekten Antworten erhalten. Aber wenn wir einige detailliertere Erklärungen hinzufügen, ist das Modell in der Lage, die Erklärungen zu beschreiben und auch eine korrekte Vorhersage zu treffen. Es ist daher gut, interpretierbare mehrstufige Überlegungen als Ausgabe zu haben. Wir sind der Ansicht, dass das Rechenproblem eine einfache Anwendung ist, um solche Denkfähigkeiten zu evaluieren. In unserem Problemaufbau müssen wir anhand der Fragen diese Frage lösen und die numerischen Antworten erhalten. In unseren Datensätzen werden uns auch die mathematischen Ausdrücke gegeben, die zu dieser speziellen Antwort führen. Bestimmte Annahmen gelten ebenfalls, wie in früheren Arbeiten, nehmen wir an, dass die Genauigkeit der Größen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialfunktion berücksichtigen. Darüber hinaus können komplizierte Operatoren in diese grundlegenden Operatoren zerlegt werden. Frühere Arbeiten zur Lösung von Rechenproblemen lassen sich in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle einteilen. Traditionelle Sequenz-zu-Sequenz-Modelle konvertieren den Ausdruck in eine spezifische Sequenz für die Generierung, was einfach zu implementieren ist und sich auf viele verschiedene komplexe Probleme verallgemeinern lässt. Der Nachteil ist jedoch, dass die Leistung im Allgemeinen nicht besser ist als die der Baumstrukturmodelle und es mangelt an Interpretierbarkeit für die Vorhersage. Diese Richtung ist jedoch aufgrund des Transformer-Modells immer noch recht beliebt. In baumbasierten Modellen strukturieren wir diese Ausdrücke in Form eines Baumes und folgen einer Pre-Order-Traversierung bei der Baumgenerierung. Hier generieren wir weiterhin die Operatoren, bis wir die Blätter erreichen, die die Größen darstellen. Das Gute daran ist, dass es uns diese binäre Baumstruktur liefert, aber es ist ziemlich kontraintuitiv, da wir zuerst den Operator und dann am Ende die Größen generieren. Außerdem enthält es einige wiederholte Berechnungen. Wenn wir uns also diesen Ausdruck ansehen, a mal 3 plus 3, wird er tatsächlich zweimal generiert, aber in Wirklichkeit sollten wir die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir im zweiten Schritt diese Divisoren erhalten, die 27 sind, und wir können auch auf die ursprüngliche Frage zurückgreifen, um die relevanten Inhalte zu finden. In diesen Schritten erhalten wir die Divisoren und dann erhalten wir im dritten Schritt tatsächlich den Quotienten. Nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt wiederverwenden und dann die Ergebnisse des vierten Schritts erhalten und schließlich den Dividenden erhalten. Hier generieren wir also den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Größen zu generieren, was den Prozess genauer macht. In unserem deduktiven System beginnen wir zunächst mit einer Reihe von Größen, die in den Fragen präsentiert werden, einschließlich einiger Konstanten, als unserem Anfangszustand. Der Ausdruck wird durch eij dargestellt, wobei wir den Operator von qi auf qj anwenden, und dieser Ausdruck ist gerichtet. Wir haben hier auch Subtraktionsumkehrungen, um die entgegengesetzte Richtung darzustellen. Dies ähnelt der Relationensextraktion. In einem formalen deduktiven System wenden wir im Zeitschritt t den Operator zwischen dem Paar qi und qj an und erhalten diesen neuen Ausdruck, den wir dem nächsten Zustand hinzufügen, um eine neue Größe zu erhalten. Diese Folie visualisiert die Entwicklung der Zustände, in denen wir weiterhin Ausdrücke zum aktuellen Zustand hinzufügen. In unserer Modellimplementierung verwenden wir zunächst ein vortrainiertes Modell, das BERT oder RoBERTa sein kann, und kodieren dann den Satz und erhalten diese Größenrepräsentationen. Sobald wir die Größenrepräsentationen erhalten haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel für q1 geteilt durch q2 und mal q3. Zuerst erhalten wir die Paardarstellung, die im Wesentlichen nur die Verkettung von q1 und q2 ist, und wenden dann ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist. Schließlich erhalten wir die Ausdrucksrepräsentation q1 geteilt durch q2. In der Praxis können wir jedoch in der Inferenzphase möglicherweise auch den falschen Ausdruck erhalten. Hier sind alle möglichen Ausdrücke gleich drei mal die Anzahl der Operatoren. Das Schöne daran ist, dass wir hier problemlos Einschränkungen hinzufügen können, um diesen Suchraum zu steuern. Wenn dieser Ausdruck nicht zulässig ist, können wir ihn einfach aus unserem Suchraum entfernen. Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist eine weitere Größe. Diese Größe stammt vom zuvor berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck q3 mal q4 erhalten. Außerdem können wir sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Dieser Unterschied macht es schwierig, die Beam-Suche anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgeglichen ist. Die Trainingsprozedur ähnelt dem Training eines Sequenz-zu-Sequenz-Modells, wobei wir den Verlust in jedem Zeitschritt optimieren. Hier verwenden wir auch tau, um darzustellen, wann wir den Generierungsprozess beenden sollten. Der Raum ist hier anders als bei Sequenz-zu-Sequenz, da der Raum in jedem Zeitschritt unterschiedlich ist, während er bei einem traditionellen Sequenz-zu-Sequenz-Modell die Anzahl des Vokabulars ist und es auch ermöglicht, bestimmte Einschränkungen aus Vorwissen aufzuerlegen. Wir haben Experimente am häufig verwendeten Rechenproblem-Datensatz durchgeführt: MWPS Method3k Math QA A und SWAM. Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisher besten Ansätzen. Unser am besten abschneidender Weapon ist Roberta Deductive Reason. Tatsächlich verwenden wir im Gegensatz zu offensichtlichen Ansätzen keine Beam-Suche. Die besten Ansätze sind oft baumbasierte Modelle. Insgesamt ist unser Reasoner in der Lage, ein signifikant besseres Ergebnis als dieses baumbasierte Modell auszugeben, aber wir können sehen, dass die absoluten Zahlen auf Math QA A oder SWAM nicht sehr hoch sind. Wir untersuchen die Ergebnisse auf SWAM weiter. Dieser Datensatz ist anspruchsvoll, da der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen verfügbarer Informationen und zusätzlicher Größen. In unserer Vorhersage stellen wir fest, dass einige der Zwischenwerte tatsächlich negativ sind. Zum Beispiel fragen wir in diesen Fragen, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie 17 weniger Kurbitionen und Stephen hat acht Kurbitionen, was völlig irrelevant ist. Unser Modell trifft einige Vorhersagen wie diese, die negative Werte erzeugen. Wir beobachten, dass diese beiden Ausdrücke tatsächlich ähnliche Ergebnisse erzielen. Wir können diesen Suchraum also einschränken, indem wir solche Ergebnisse entfernen, die negativ sind, so dass wir die richtige Antwort erhalten können. Wir haben festgestellt, dass diese Einschränkung tatsächlich einige Modelle verbessert hat, z. B. BERT um sieben Punkte und für das RoBERTa-basierte Modell haben wir zwei Punkte verbessert. Bessere Sprachmodelle haben bessere Sprachverständnisfähigkeiten, so dass die Zahl hier höher für RoBERTa und niedriger für BERT ist. Wir haben auch versucht, die Schwierigkeiten hinter diesen Datensätzen zu analysieren. Wir nehmen an, dass die Anzahl der nicht verwendeten Größen als relevante Informationen betrachtet werden kann. Hier können wir sehen, dass der SWAM-Datensatz den größten Anteil hat. Hier zeigen wir auch die Gesamtleistung für diese Proben ohne nicht verwendete Größen. Die Gesamtleistung ist tatsächlich höher als die Leistung mit diesen Proben, die nicht verwendete Größen haben. Bei Wps haben wir nicht viele Fehlfälle, daher ignoriere ich diesen Teil. Abschließend möchten wir die Interpretierbarkeit anhand eines Crash-Präsentationsbeispiels demonstrieren. Unser Modell trifft hier tatsächlich eine falsche Vorhersage im ersten Schritt. Wir können diesen Ausdruck mit dem Satz korrelieren. Wir glauben, dass dieser Satz das Modell irreführt und zu einer falschen Vorhersage führt. Wir versuchen, den Satz zu überarbeiten, um etwas wie folgt zu formulieren: Die Anzahl der Birnbäume ist 5 weniger als die Anzahl der Apfelbäume. Wir formulieren ihn so, dass er eine genauere Semantik vermittelt, so dass das Modell die richtige Vorhersage treffen kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Verhalten des Modells zu verstehen. Um zu schließen, unser Modell ist tatsächlich recht effizient und wir können eine interpretierbare Lösungsprozedur bereitstellen. Wir können problemlos einige Vorwissensbestände als Einschränkungen integrieren, was dazu beitragen kann, die Leistung zu verbessern. Die zugrunde liegende Mechanik gilt nicht nur für die Lösung von Rechenproblemen, sondern auch für andere Aufgaben, die mehrstufiges Denken erfordern. Wir haben jedoch auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, kann der Speicherverbrauch recht hoch sein. Der zweite Punkt ist, dass es, wie bereits erwähnt, aufgrund des unausgeglichenen Wahrscheinlichkeitsverteilungs zwischen den Zeitschritten ziemlich schwierig ist, die Beam-Suche anzuwenden. Das ist das Ende des Vortrags. Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine, und ich komme von der Universität Maastricht. Ich werde meine Arbeit mit John und Jerry vorstellen, die sich auf einen neuen Datensatz für die juristische Artikelrecherche bezieht. Rechtliche Fragen sind ein integraler Bestandteil vieler Menschenleben, aber die meisten Bürger verfügen über wenig bis gar kein Wissen über ihre Rechte und grundlegende juristische Verfahren. Daher sind viele schutzbedürftige Bürger, die sich den kostspieligen Beistand eines Rechtsexperten nicht leisten können, ungeschützt oder sogar ausgebeutet. Ziel unserer Arbeit ist es, die Kluft zwischen Menschen und dem Gesetz zu überbrücken, indem ein effektives Suchsystem für Rechtsvorschriften entwickelt wird. Ein solches System könnte einen kostenlosen und professionellen Rechtsberatungsservice für weniger erfahrene Bürger bereitstellen. Bevor wir uns der Hauptleistung dieser Arbeit zuwenden, wollen wir zunächst das Problem der juristischen Artikelrecherche beschreiben. Angenommen, es wird eine einfache Frage zu einer Rechtsangelegenheit gestellt, wie z. B. \"Welche Risiken gehe ich ein, wenn ich die berufliche Schweigepflicht verletze?\", so ist ein Modell erforderlich, um alle relevanten Rechtsartikel aus einem großen Rechtsbestand abzurufen. Diese Informationsbeschaffungsaufgabe ist mit eigenen Herausforderungen verbunden. Erstens befasst sie sich mit zwei Arten von Sprache: natürlicher Sprache für die Fragen und komplexer juristischer Sprache für die Gesetze. Dieser Unterschied in der Sprachverteilung erschwert es einem System, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine juristische Frage übersetzt, die mit der Terminologie der Gesetze übereinstimmt. Darüber hinaus ist das Steuerrecht keine Sammlung unabhängiger Artikel, die für sich genommen als vollständige Informationsquelle betrachtet werden können, wie beispielsweise Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die erst in ihrem Gesamtkontext, d. h. zusammen mit ergänzenden Informationen aus ihren benachbarten Artikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Position in der Rechtsstruktur, eine vollständige Bedeutung haben. Zuletzt sind Rechtsartikel in kleinen Absätzen verfasst, die typischerweise die übliche Retrieval-Einheit in den meisten Retrieval-Arbeiten darstellen. Hier sind sie lange Dokumente, die bis zu 6.000 Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, wie z. B. der Vorhersage von Rechtsurteilen oder der automatischen Vertragsprüfung. Die juristische Artikelrecherche ist jedoch hauptsächlich zurückgeblieben, da es an großen und hochwertigen, mit Labels versehenen Datensätzen mangelt. In dieser Arbeit präsentieren wir einen neuen, auf französische Muttersprachler ausgerichteten Datensatz, um zu untersuchen, ob Retrieval-Modelle die Effizienz und Zuverlässigkeit eines Rechtsexperten bei der juristischen Artikelrecherche oder der Recherche von Rechtsartikeln nach belgischem Recht annähern können. Der Datensatz besteht aus mehr als 1100 Rechtsfragen, die von belgischen Bürgern gestellt wurden. Diese Fragen umfassen ein breites Spektrum an Themen, von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung. Jede Frage wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als fünftausendsechshundert juristischen Artikeln aus belgischen Gesetzesbüchern annotiert. Lassen Sie uns nun erläutern, wie wir diesen Datensatz gesammelt haben. Zunächst haben wir ein großes Korpus juristischer Artikel zusammengestellt. Wir berücksichtigten 32 öffentlich zugängliche belgische Gesetzbücher und extrahierten alle Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir Rechtsfragen mit Verweisen auf relevante Gesetze. Zu diesem Zweck arbeiteten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr rund 400 E-Mails von belgischen Bürgern erhält, die um Rat zu einem persönlichen Rechtsstreit bitten. Wir hatten das Glück, auf ihre Websites zugreifen zu können, wo ihr Team erfahrener Juristen die häufigsten Rechtsfragen belgischer Bürger beantwortet. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und Rechtsverweisen auf relevante Gesetze versehen waren. Anschließend filterten wir die Rechtsverweise heraus und entfernten Fragen, deren Verweise nicht auf Artikel in einem der von uns berücksichtigten Gesetzbücher wiesen. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Wir landeten schließlich mit eintausendachtzehn Fragen, von denen jede sorgfältig mit den IDs der relevanten Artikel aus einem großen Korpus von fünftausendsechshundertunddreißig Rechtsartikeln versehen war. Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien, und jeder Artikel enthält eine Verkettung ihrer nachfolgenden Überschriften in der Rechtsstruktur. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschung in der juristischen Informationsrecherche oder der juristischen Textklassifizierung von Interesse sein. Werfen wir einen Blick auf einige Eigenschaften unseres Datensatzes. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind deutlich länger, mit einer Medianlänge von 77 Wörtern, wobei 142 von ihnen mehr als 1000 Wörter überschreiten, und der längste bis zu fünftausendsiebenhundertneunzehn Wörter lang ist. Wie bereits erwähnt, decken die Fragen ein breites Themenspektrum ab, wobei rund 85 Prozent davon entweder über Familie, Wohnen, Geld oder Recht behandelt werden, während die restlichen 15 Prozent sich entweder mit Sozialversicherung, Ausländern oder Arbeit befassen. Auch die Artikel sind sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl juristischer Themen abdecken. Hier ist die Gesamtzahl der Artikel, die aus jedem dieser belgischen Gesetzbücher gesammelt wurden. Von den fünftausendsechshundertunddreißig Artikeln werden nur eintausendeinhundertzwölf als für mindestens eine Frage im Datensatz relevant gekennzeichnet. Rund 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, den strafrechtlichen Gesetzbüchern, dem Strafverfolgungsgesetzbuch oder dem Strafgesetzbuch. Während 18 von 32 Gesetzbüchern weniger als fünf Artikel erwähnen, die für mindestens eine Frage relevant sind, was darauf zurückzuführen ist, dass sich diese Gesetzbücher weniger auf Einzelpersonen und deren Anliegen konzentrieren. Insgesamt liegt die Mediananzahl der Zitate für diese zitierten Artikel bei zwei, und weniger als 25 Prozent von ihnen werden mehr als fünfmal zitiert. Mithilfe unseres Datensatzes haben wir mehrere Retrieval-Ansätze bewertet, darunter lexikalische und dichte Architekturen. Ein lexikalisches Modell weist einer Artikel-Query-Paarung basierend auf der Summe der Gewichte jedes dieser Terme in diesem Artikel über die Query-Terme hin. Wir experimentieren mit den Standard-TFIDf- und bm25-Ranking-Funktionen. Das Hauptproblem bei diesen Ansätzen besteht darin, dass sie nur Artikel abrufen können, die Keywords enthalten, die in der Query vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronalen Architektur, die semantische Beziehungen zwischen Queries und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Queries und Artikel in dichte Vektordarstellungen abbildet und eine Relevanzbewertung zwischen einem Query-Artikel-Paar basierend auf der Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen ergeben sich typischerweise aus einer Pooling-Operation auf der Ausgabe eines Word-Embedding-Modells. Zunächst untersuchen wir die Wirksamkeit von Siamesebiancodern in einer Zero-Shot-Evaluierungsumgebung, was bedeutet, dass vortrainierte Word-Embedding-Modelle ohne zusätzliche Feinabstimmung direkt angewendet werden. Wir experimentieren mit kontextunabhängigen Textencodern, nämlich Word2Vec und FastText, sowie mit kontextabhängigen Embedding-Modellen, nämlich Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell. Zusätzlich trainieren wir unser eigenes CamemBERT-basiertes Biancoder-Modell. Bei dem Training experimentieren wir mit den beiden Varianten der Biancoder-Architektur: Siamesisch, das ein gemeinsames Word-Embedding-Modell verwendet, das Query und Artikel in einem gemeinsamen dichten Vektorraum abbildet, und Tower, das zwei unabhängige Word-Embedding-Modelle verwendet, die Query und Artikel separat in unterschiedliche Einbettungsräume codieren. Wir experimentieren mit Mean Max und CLls Pooling sowie mit Dot Product und Cosinus zur Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse einer Baseline auf den Testdatensätzen mit den oben genannten lexikalischen Methoden, den Siamesebiancodern, die in einer Zero-Shot-Konfiguration ausgewertet wurden, in der Mitte und den feinabgestimmten Biancodern unten. Insgesamt übertreffen die feinabgestimmten Biancoder alle anderen Baselines signifikant. Das Two-Tower-Modell verbessert im Vergleich zu seiner Siamesen-Variante den Recall bei 100, weist aber ähnliche Ergebnisse bei den anderen Metriken auf. Obwohl bm25 den trainierten Biancoder deutlich unterlegen ist, deuten seine Ergebnisse darauf hin, dass es sich dennoch um eine solide Baseline für domänenspezifische Retrieval-Aufgaben handelt. Bezüglich der Zero-Shot-Evaluierung von Siamesebiancodern stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationsrecherche zu schlechten Ergebnissen führt, was mit früheren Erkenntnissen übereinstimmt. Darüber hinaus stellen wir fest, dass der Word2Vec-basierte Biancoder den FastText- und Bird-basierten Modellen signifikant überlegen ist, was darauf hindeutet, dass vortrainierte Word-Level-Einbettungen möglicherweise für diese Aufgabe besser geeignet sind als Character-Level- oder Subword-Level-Einbettungen, wenn sie direkt verwendet werden. Obwohl vielversprechend, deuten diese Ergebnisse auf reichlich Verbesserungspotenzial im Vergleich zu einem kompetenten Experten hin, der schließlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Ergebnisse erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen des Datensatzes diskutieren. Erstens ist der Artikel-Korpus auf die aus den 32 berücksichtigten belgischen Gesetzbüchern gesammelten Artikel beschränkt, was nicht die gesamte belgische Gesetzgebung abdeckt, da Artikel aus Erlassen, Direktiven und Verordnungen fehlen. Während der Erstellung des Datensatzes werden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen nur einen Bruchteil der anfänglichen Anzahl relevanter Artikel aufweisen. Dieser Informationsverlust bedeutet, dass die in den verbleibenden relevanten Artikeln enthaltene Antwort möglicherweise unvollständig ist, obwohl sie dennoch völlig angemessen ist. Zweitens müssen wir darauf hinweisen, dass nicht alle Rechtsfragen allein mit Gesetzen beantwortet werden können. Beispielsweise könnte die Frage \"Darf ich meine Mieter kündigen, wenn sie zu viel Lärm machen?\", nicht eine detaillierte Antwort im Gesetzbuch enthalten, die einen spezifischen Lärmschwellenwert festlegt, ab dem eine Kündigung zulässig ist. Stattdessen sollte der Vermieter sich wahrscheinlich auf Rechtsprechung stützen und ähnliche Präzedenzfälle finden, z. B. dass der Mieter zwei Partys pro Woche bis 2 Uhr morgens veranstaltet. Daher sind einige Fragen besser für die juristische Artikelrecherche geeignet als andere, und der Bereich der weniger geeigneten Fragen ist noch zu bestimmen. Wir hoffen, dass unsere Arbeit Interesse an der Entwicklung praktischer und zuverlässiger juristischer Artikelrecherchemodelle weckt, die dazu beitragen können, den Zugang zur Justiz für alle zu verbessern. Sie können unser Paper dotset encode unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, Ihnen unsere Arbeit zu Vowels vorzustellen, einem aufgabenunabhängigen Benchmark, der dazu dient, Vision und Sprachmodelle mit spezifischen linguistischen Phänomenen zu testen. Warum haben wir uns die Mühe gemacht, diesen Benchmark aufzubauen? Nun, in den letzten Jahren haben wir einen explosionsartigen Anstieg von Transformer-basierten Vision- und Sprachmodellen erlebt, die auf großen Mengen an Bild-Text-Paaren vortrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei Vision- und Sprachaufgaben wie visuellem Fragebeantworten, visuellem Common-Sense-Reasoning, Bildabruf und Phrase-Grounding. Wir haben also die folgende Botschaft erhalten: Die Genauigkeit bei diesen aufgabenspezifischen Benchmarks steigt stetig, aber wissen wir, was die Modelle tatsächlich gelernt haben? Was versteht ein Vision- und Sprachmodell, wenn es einem Bild und einem Satz eine hohe Übereinstimmungsbewertung zuweist und einem anderen eine niedrige? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige, oder konzentrieren sie sich auf Verzerrungen, wie es frühere Arbeiten gezeigt haben? Um mehr Licht auf diesen Aspekt zu werfen, schlagen wir einen aufgabenunabhängigeren Ansatz vor und stellen Vowels vor, der die Sensibilität von Vision- und Sprachmodellen gegenüber spezifischen linguistischen Phänomenen testet, die sowohl die linguistischen als auch die visuellen Modalitäten betreffen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Relationen, Aktionen und Entitätskoreferenz ab. Aber wie testen wir, ob die Vision- und Sprachmodelle diese Phänomene erfasst haben? Wir wenden eine Methode an, die zuvor nur für Vision- und Sprachmodelle und für Nominalphrasen von Ravi Shekhar und Mitarbeitern sowie für das Zählen in unseren früheren Arbeiten angewendet wurde. Foiling bedeutet im Wesentlichen, dass wir die Bildunterschrift nehmen und eine Falsifikation erzeugen, indem wir die Bildunterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Wir nehmen diese Phrasenänderungen vor, indem wir uns auf sechs spezifische Aspekte konzentrieren, wie z. B. Existenz, Pluralität, Zählen, räumliche Relationen, Aktionen und Entitätskoreferenz, wobei jeder Aspekt aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, FOIL-Instanzen zu erstellen. Beispielsweise haben wir im Falle des Aspekts Aktionen zwei Instrumente: eines, bei dem das Aktionsverb mit einem anderen Aktionsverb ausgetauscht wird, und eines, bei dem die Akteure vertauscht werden. Zählen und Koreferenz sind ebenfalls Aspekte, die mehr als ein Instrument haben. Und wir erstellen diese Falsifikationen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, grammatikalisch korrekt und anderweitig gültig sind. Das ist nicht einfach, da eine Falsifikation weniger wahrscheinlich sein kann als die ursprüngliche Bildunterschrift. Wenn beispielsweise – obwohl es nicht unmöglich ist – es statistisch weniger wahrscheinlich ist, dass Pflanzen einen Mann schneiden, als dass ein Mann Pflanzen schneidet, könnten große Vision- und Sprachmodelle dies erkennen. Um gültige Falsifikationen zu erhalten, müssen wir daher zunächst Maßnahmen ergreifen. Erstens verwenden wir starke Sprachmodelle, um Falsifikationen vorzuschlagen. Zweitens verwenden wir Natural Language Inference (NLI) oder kurz NLI, um Falsifikationen herauszufiltern, die das Bild möglicherweise immer noch beschreiben. Da wir bei der Konstruktion von Falsifikationen sicherstellen müssen, dass sie das Bild nicht beschreiben, wenden wir zur automatischen Überprüfung Natural Language Inference mit folgender Begründung an: Wir betrachten ein Bild als Prämisse und seine Bildunterschrift als seine hergeleitete Hypothese. Darüber hinaus betrachten wir die Bildunterschrift als Prämisse und die Falsifikation als ihre Hypothese. Wenn ein NLI-Modell eine Falsifikation als widersprüchlich oder neutral gegenüber der Bildunterschrift vorhersagt, betrachten wir dies als Indikator für eine gültige Falsifikation. Wenn ein NLI vorhersagt, dass die Falsifikation von der Bildunterschrift hergeleitet wird, kann sie keine gute Falsifikation sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert, und wir filtern diese Falsifikationen heraus. Dieses Verfahren ist jedoch nicht perfekt; es ist lediglich ein Indikator für gültige Falsifikationen. Daher setzen wir als dritte Maßnahme zur Erzeugung gültiger Falsifikationen menschliche Annotatoren ein, um die in Vowels verwendeten Daten zu validieren. Nach dem Filtern und der menschlichen Bewertung haben wir so viele Testinstanzen, wie in dieser Tabelle beschrieben. Vowels liefert keine Trainingsdaten, sondern nur Testdaten, da es sich um einen Zero-Shot-Test-Benchmark handelt. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vortraining zu nutzen. Das Feinabstimmen würde es den Modellen lediglich ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen suchen. Wie bereits erwähnt, interessieren wir uns für die Bewertung der Fähigkeiten von Vision- und Sprachmodellen nach dem Vortraining. Wir experimentieren mit fünf Vision- und Sprachmodellen auf Vowels, nämlich mit CLIP, AlexNet, BERT, WILBERT, WILBERT 12 in 1 und VisualBERT. Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren als Bildunterschriften und Falsifikationen. Vielleicht relevanter für dieses Video zeigen wir unsere permissivere Metrik, die Paar-Genauigkeit, die misst, ob die Bild-Satz-Ausrichtungspunktzahl für das korrekte Bild-Text-Paar höher ist als für sein Falsifikationspaar. Weitere Metriken und Ergebnisse finden Sie in unserem Paper. Die Ergebnisse mit der Paar-Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben. Das heißt, die beste Zero-Shot-Performance wird von WILBERT 12 in 1 gefolgt von WILBERT, AlexNet, CLIP und schließlich VisualBERT erzielt. Es ist bemerkenswert, wie Instrumente, die sich auf einzelne Objekte wie Existenz und Nominalphrasen konzentrieren, fast vollständig von WILBERT 12 in 1 gelöst werden, was zeigt, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Keiner der verbleibenden Aspekte kann jedoch in unseren adversariellen Falsifikationsszenarien zuverlässig gelöst werden. Wir sehen anhand der Instrumente für Pluralität und Zählen, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne versus mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Der Relation-Aspekt zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Aktionen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie von Plausibilitätsverzerrungen unterstützt werden, wie wir im Aktionen-Aspekt sehen. Aus dem Referenz-Aspekt erfahren wir, dass es auch für Vision- und Sprachmodelle schwierig ist, mehrere Referenzen auf dasselbe Objekt in einem Bild mithilfe von Pronomen zu verfolgen. Als Sanity-Check und weil es ein interessantes Experiment ist, haben wir auch zwei Text-Only-Modelle, GPT-1 und GPT-2, benchmarkt, um zu beurteilen, ob Vowels von diesen unimodalen Modellen lösbar sind, indem wir die Perplexität der korrekten und der Falsifikationsbildunterschrift berechnen, ohne Bild. Wenn die Perplexität für die Falsifikation höher ist, betrachten wir dies als Indikator dafür, dass die Falsifikationsbildunterschrift unter Plausibilitätsverzerrungen oder anderen linguistischen Verzerrungen leidet. Es ist interessant zu sehen, dass die Text-Only-GPT-Modelle in einigen Fällen die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Zusammenfassend lässt sich sagen, dass Vowels ein Benchmark ist, der die linguistischen Konstrukte als Linse nutzt, um der Community zu helfen, Vision- und Sprachmodelle zu verbessern, indem er ihre visuellen Grounding-Fähigkeiten hart testet. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte und ihre Anwesenheit in Bildern gut identifizieren, aber Schwierigkeiten haben, ihre Interdependenzen und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen werden, linguistischen Indikatoren Rechnung zu tragen. Wir würden die Community sehr gerne dazu ermutigen, Vowels zur Messung des Fortschritts bei der Sprach-Grounding mit Vision- und Sprachmodellen zu verwenden. Noch mehr kann Vowels als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder Feinabstimmen bewertet werden können, um zu sehen, ob ein Datensatz den Modellen hilft, sich in Bezug auf einen der von Vowels getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, sehen Sie sich die Vowels-Daten auf GitHub an, und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisura von der Universität Tokio. Ich werde einen Beitrag mit dem Titel \"O En Sum: Ein groß angelegtes System zur automatischen Listenknotengenerierung über Lo Sumization\" vorstellen. Ich werde dies in folgender Reihenfolge erläutern: Zuerst werde ich die automatische Li-Notation vorstellen, an der wir in dieser Forschung arbeiten. Ein Release-Hinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Veröffentlichung eines Softwareprodukts verteilt werden. Das Bild zeigt die Release-Hinweise für Version zwei Punkt sechs punkt vier der Bujs-Bibliothek. Diese Notizen spielen eine wichtige Rolle in der Open-Source-Entwicklung, sind aber zeitaufwändig, um sie manuell zu erstellen. Daher wäre es sehr nützlich, wenn man hochwertige Release-Hinweise automatisch generieren könnte. Ich werde mich auf zwei frühere Forschungsarbeiten zur automatischen Listenknotengenerierung beziehen. Die erste ist ein System namens alena, das im Jahr 2014 veröffentlicht wurde. Es verwendet einen regelbasierten Ansatz, beispielsweise die Verwendung eines Change Extractors, um Core-Unterschiede, Bibliotheksänderungen und Dokumentationsänderungen aus den Unterschieden zwischen Krankheiten zu extrahieren und diese schließlich zu kombinieren. Das bemerkenswerteste Merkmal dieses Systems ist die Issue-Extraktion in der oberen rechten Ecke, die mit Jira, dem Issue-Ökosystem verknüpft werden muss und nur auf Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Die zweite ist Grif, die kürzlich im Jahr 2020 angekündigt wurde. Sie ist im Internet verfügbar und kann über pi gespeichert werden. Dieses System verfügt über ein einfaches laufendes, textbasiertes Klassifikationsmodell und gibt für jede eingegebene Commit-Nachricht eines von fünf Problemen wie Funktionen oder Fehlerbehebungen aus. Das Bild zeigt eine Beispielverwendung, die eine korrekte Tape- oder Fehlerbehebungs-Rubrik zurückgibt. Das Trainingsdatensatzvolumen ist relativ gering, etwa 5000 und wird in den unten beschriebenen Experimenten gezeigt. Die Leistung des textbasierten Klassifikationsmodells ist nicht hoch. Ich präsentiere zwei verwandte Forschungsarbeiten, aber es gibt Probleme mit begrenzter Anwendbarkeit und spärlichen Datenressourcen. Unser Beitrag löst diese beiden Probleme und generiert automatisch hochwertige Ressourcen. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige Klassifikationssummierungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der spärlichen Ressourcen haben wir unserenr und einige Daten aus etwa 82.000 Datenpunkten erstellt, indem wir Daten aus öffentlichen Gitub-Repositories unter Verwendung der Git-API korrigiert haben. Als nächstes beschreibe ich unsere Daten. Hier ist ein Beispiel: Die linke Seite ist eine Commit-Nachricht und die rechte Seite ist der Release-Hinweis. Die Release-Hinweise sind als Verbesserungen von Faces usw. abgestuft. Wir haben eine Aufgabe eingerichtet, die Commit-Nachrichten als Eingabe nimmt und die rabbit is Notizen ausgibt. Dies kann als eine Zusammenfassungaufgabe betrachtet werden. Wir haben vier Ebenen vordefiniert: Funktionen, Implementierungen, Fehlerbehebungen, Deprecations, Entferner und Breaking Changes. Diese wurden auf der Grundlage früherer Nutzung und anderer Faktoren festgelegt. Es gibt einen Hinweis unten rechts, der aus der Liste der Notizen unten links extrahiert wurde. Zu diesem Zeitpunkt ist es notwendig, die vier Rabbits zu erkennen, die in Pass eingerichtet wurden, aber die Ebenen stimmen nicht immer mit jeder Le überein. Beispielsweise umfasst das Verbesserungsniveau Verbesserungen, Erweiterungen und Optimierungen. Wir haben eine Vokabelliste für verschiedene Notationsvarianten für jede dieser Notizenstufen vorbereitet, um die Risk-Node-Klasse zu erkennen und den Text des restlichen Teils als Risk-No-Satz für die Klasse zu korrigieren. Als nächstes ist eine Commit-Nachricht. Commit-Nachrichten sind nicht an jede Race gebunden, wie im folgenden Bild gezeigt. Wenn die aktuelle Version 2.5 zu neunzehn ist, müssen wir die vorherige Version 2.5 zu achtzehn identifizieren und sie di erhalten. Dies ist etwas mühsam, und es reicht nicht aus, einfach eine Liste der Releases zu erhalten und die Vor- und Nachher-Versionen zu betrachten. Wir haben eine heuristische Matching-Glue erstellt, um die Daten der vorherigen und nächsten Versionen abzurufen. Bei der Datenanalyse wurden 7200 Repositories und 82.000 Datenpunkte korrigiert. Außerdem beträgt die durchschnittliche Anzahl sinnvoller Token 63, was für eine Zusammenfassungsaufgabe ziemlich hoch ist. Auch die Anzahl eindeutiger Token ist mit acht tausend achtunddreißig tausend sehr reichhaltig. Dies liegt an der großen Anzahl eindeutiger Klassen und Methodennamen, die im Repository gefunden wurden. Als nächstes werde ich die vorgeschlagene Methode erklären. Das Crosswise-Extractive- und Abstractive-Summarizationsmodell besteht aus zwei neuronalen Modulen: einem Klassifikator mit Bot oder Code Bot und einem Generator mit but first G. Der Klassifikator klassifiziert jede Commit-Nachricht in fünf Basenotklassen: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecations, Press und andere. Die als andere klassifizierten Commit-Nachrichten werden verworfen. Anschließend wendet sie einen Generator auf die vier Rubber-Dokumente an und generiert Read Notes für jede Klasse. Bei dieser Aufgabe sind die direkten Entsprechungen zwischen Commit-Nachrichten und Read Notes nicht bekannt. Daher weisen wir dem Klassifikator zur Schulung Pseudo-Variablen für jede Eingabe-Commit-Nachricht zu, indem wir die ersten 10 Zeichen jeder Commit-Nachricht verwenden. Wir modellieren den Classwise-Abstractive-Summarizations-Ansatz mit zwei vordefinierten Methoden. Das erste Modell, das wir GS Single nennen, besteht aus einem einzelnen Sex-Netzwerk und generiert einen einzelnen langen ist-Text, indem eine Verkettung von Eingabe-Commit-Nachrichten gegeben wird. Der Ausgabetext kann in Klassendatei-Segmente basierend auf speziellen, klassenspezifischen Endpunktsymbolen unterteilt werden. Die zweite Methode, die wir She’s much nennen, besteht aus vier verschiedenen Sec-to-Sec-Netzwerken, von denen jedes einer der Listenknotenklassen entspricht. Okay, lassen Sie mich die Experimente erläutern. Fünf Methoden wurden verglichen: gs, shes single, shes much, cluster und eine frühere Studie Grif. In einigen Fällen werden diese Notizen in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze auf Null zu setzen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Der blaue Wert wird bestraft, wenn das System einen kurzen Satz ausgibt. Dieses Ergebnis führt zu einem niedrigeren Bre-Wert in den nachfolgend beschriebenen Experimentergebnissen. Wir haben auch eine Spezifität beurteilt, weil blau und blau nicht beurteilt werden können, wenn die Notizen leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt leere Texte ausgibt, wenn die Read Nodes leer sind. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Analysen mit Werten usw. enthält, haben wir auch einen sauberen Datensatz ausgewertet, der diese ausschließt. G und Gs erreichten höhere Loss-Error-Scores als der Baseline um mehr als 10 Punkte. Insbesondere im koreanischen Testdatensatz sprang der Score-Unterschied zwischen der vorgeschlagenen Methode und dem Baseline auf mehr als 20 Punkte. Diese Ergebnisse zeigen, dass Gs und Gs signifikant effektiv sind. Gs erreichte einen besseren Loss-Score als GAS, was darauf hindeutet, dass das Kombinieren eines Klassifikators und eines Generators effektiv ist, um den Klassifikator mithilfe von Servern zu trainieren. Die hohe Abdeckung von GS kann ordnungsgemäß erreicht werden, weil der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann. She's much neigte dazu, höher zu sein als she is single, was darauf hindeutet, dass es auch effektiv ist, unterschiedliche konstruktive Zusammenfassungsmodelle für jede Listenknotenklasse unabhängig voneinander zu entwickeln. Hier ist eine Fehleranalyse. She-Methoden neigen dazu, kürzere Sätze als den menschlichen Referenzsatz auszugeben, da der Referenzsatz in der rechten Abbildung drei oder vier Sätze hat, während CSS nur einen Satz hat. Der Grund für diese Modell-Zurückhaltung ist, dass im Trainingsdatensatz nur 30 Prozent der Sätze auf dem Funktionslevel und 40 Prozent auf dem Verbesserungslevel vorhanden sind. Darüber hinaus können CS-Methoden keine genauen Listennotizen ohne zusätzliche Informationen generieren. Das obere Beispiel auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht, und der vollständige Satz kann nicht ohne Unterscheidung zum entsprechenden Prerogativ oder Issue generiert werden. Das Beispiel unten zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander in Beziehung stehen und zu einem Satz kombiniert werden sollten, aber dies gelingt nicht. Abschließend: Wir haben einen neuen Datensatz für die automatische persönliche Generierung erstellt. Wir haben auch die Aufgabe formuliert, engagierte Nachrichten einzugeben und sie so zusammenzufassen, dass sie für alle Projekte anwendbar sind, die in Englisch geschrieben sind. Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Notizen mit höherer Abdeckung als der Baseline generiert. Bitte überprüfen Sie unsere Daten auf GitHub. Vielen Dank."}
