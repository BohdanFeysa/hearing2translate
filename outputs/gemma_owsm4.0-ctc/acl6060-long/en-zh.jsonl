{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我的名字是 Safari，我将介绍我们的论文：利用微调Transformer架构进行表格数据增强。因此，科学家们分析数据，主要关注于操作现有特征，但有时这些特征是有限的。利用另一个数据源生成特征可能会增加大量信息。我们的研究目标是使用外部源的自由文本进行自动表格数据增强。假设我们有一个表格数据集和一个知识库。我们需要一个自动过程，涉及实体链接和文本分析，以从知识库的自由文本中提取新的特征。我们的框架Fest正是这个自动过程，让我们来看一个例子。在数据集中，Fest输入的是大学数据集，其目标是将其分类为低排名大学和高排名大学。作为知识库，我们使用维基百科。Fest的第一阶段是实体链接，在这个例子中，每个实体（例如，大学名称）都链接到知识库中的一个实体，并提取知识库实体的文本并将其添加到数据集中。在这个例子中，文本是维基百科页面的摘要。现在，我们需要从检索到的文本中生成或提取特征，我们需要一个特征提取阶段，该阶段包括文本分析，这是本文的主要创新点，我将在下一张幻灯片中深入探讨。在特征提取阶段之后，有一个特征生成阶段，我们使用提取的特征生成少量的新的特征。首先，生成与原始数据集的类别的数量相同的新特征。例如，原始数据集有两类，因此首先生成两个新的特征。如果数据集有五个类别，则首先生成五个新的特征。每个特征代表每个类别的可能性。为了分析文本，我们使用当前状态的文本外分析技术，这些技术是基于Transformer的语言模型，如BERT、GPT-X和类似模型。但是，不太可能使用输入数据集来训练语言模型，一种朴素的方法是对目标任务进行微调。在特征提取阶段，我们可以下载预训练的语言模型，并在目标数据集上微调该语言模型。例如，为了将文本分类到类中，将摘要分类到类中（低或高），接收语言模型的输出，即每个类别的可能性，并将其用作新的特征。这种方法的问题是数据集可能包含有限的、不同的实体文本。在我们的实验中，几乎一半的数据集包含少于400个样本，并且最小的数据集包含35个样本，这些是初始训练集。因此，在一个数据集上微调语言模型将无效。但是，我们可以使用关于预分析数据集的先验知识，因为Fest是在多个数据集上应用的。我们可以使用n-1个数据集来获取关于n-1个数据集的信息，并在分析第n个数据集时使用这些信息。我们的建议是添加另一个微调阶段：一个初步的多任务微调阶段，当我们在n-1个数据集上微调语言模型时，然后执行另一个微调阶段，即目标任务微调，其中我们在第n个目标数据集上微调语言模型。多任务微调的最新技术称为DNN（TDNN），DNN TDNN在训练集中维护着与任务数相同的头。例如，如果在训练集中有四个任务，则DNN维护四个头。如图所示，它从训练集中抽取一个随机批次，如果随机批次属于单个句子分类任务，则执行第一个头的正向和反向传播；如果随机批次属于成对排序任务，则执行最后一个头的正向和反向传播。在我们的场景中，表格数据集数量繁多，因此任务数量也很多，所以DNN需要维护与类别数相同的头，并且还需为每个新的数据集和任务初始化新的头。我们的方法称为任务重塑微调。在我们的方法中，任务重塑微调不是维护多个头，而是将每个数据集重塑为一个句子分类问题，这是一个两类任务。让我们看一个例子。这是我们的输入数据集，由实体、特征、文本和类别组成。我们将任务从对文本进行分类（低或高）重塑为对文本（摘要）和类别进行分类（真或假），换句话说，我们训练语言模型以分类摘要和类别是否属于该类别。在这种情况下，标签向量始终由两个类别组成。这是我们任务重塑微调方法的算法。让我们看看完整的框架。数据集输入Fest，然后Fest执行链接阶段，从知识库（例如，维基百科页面的摘要）中提取文本，然后将任务重塑为成对句子分类任务，应用语言模型到新的任务，并输出每个类别的可能性。注意，语言模型已经使用初步的多任务微调在n-1个数据集上进行了微调。然后，我们将语言模型的输出向量用作一个新的特征，该特征的数量与类别的数量相同。为了评估我们的框架，我们使用17个表格分类数据集，定义了不同的尺寸、特征、平衡域和初始性能。作为知识库，我们使用维基百科。我们设计实验时采用留一法评估，我们在16个数据集上训练Fest，并将其应用于第17个数据集。我们还拆分每个数据集为四个折叠，并应用四折交叉验证。然后，我们生成新的特征，并使用五个评估分类器对它们进行评估。我们使用基于BERT的架构进行实验。这是我们实验的结果，您可以看到，我们将我们的框架与目标数据集微调和MTDNN初步微调进行比较，我们的重塑微调方法实现了最佳性能，而MTDNN实现了比目标数据集微调提高2%的性能。我们的方法实现了6%的改进。当我们在小数据集上观察时，我们可以看到MTDNN的性能下降，而初步多任务微调阶段的改进减少到1.5%。但是，我们的性能提高了11%，与目标任务微调相比。总之，Fest能够在少量样本下实现增强，在我们的实验中，只需要35个样本即可。它使用一种架构来处理所有任务和数据集，并保持模型头的完整性。但它添加了一个重塑阶段，增强了训练集，并需要一个具有语义意义的目标值，以便我们可以将其输入到语言模型中并在句子分类问题中使用它。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我将介绍我们的研究工作，即学习演绎推理，将方法问题转化为复杂区域提取任务。我来自 Biance AI 实验室，这项工作是与来自德克萨斯大学奥斯汀分校的 Che 和来自 SUDD 的 Wedu 合作完成的。首先我想谈谈我们对推理的动机。这里我们展示了一些例子，说明多步推理的帮助。这个图来自 Pound 的论文，他们在未来学习场景中通过提示来解决方法问题。在左边，我们可以看到，如果我们只给出带有问题和答案的一些样本，我们可能无法获得正确的答案。但如果我们提供一些更详细的推理描述，模型就能够预测推理描述并做出正确的预测。因此，将可解释的多步推理作为输出是很有益的。我们还认为方法问题是一个直接的应用，可以评估这种推理能力。在这里，在我们的问题设置中，我们需要解决问题并获得数值答案。在我们的数据集里，我们还给出导致该特定答案的数学表达式。当然也存在一些假设，就像之前的工作一样，我们假设数量的精度是已知的，并且我们只考虑基本的运算符，例如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以分解为这些基本的运算符。之前在方法问题求解中的工作实际上可以归类为序列到序列和序列到树模型。传统的序列到序列模型将表达式转换为特定的序列进行生成，这非常容易实现，并且可以推广到许多不同的复杂问题。但其性能的缺点是通常不能优于结构模型，并且缺乏对预测的可解释性。然而，由于 Transformer 模型，这个方向仍然相当流行。在基于树的模型中，我们实际上以树的形式对这些表达式进行结构化，并遵循树生成中的前序遍历。在这里，我们不断生成运算符直到到达叶节点，这些叶节点是数量。这里的好处是它为我们提供了一个二元树结构。但是，实际上这有些反直觉，因为我们首先生成运算符，然后在最后生成数量。另外，它也包含一些重复的计算。例如，如果我们在查看这个表达式 a 乘以 3 加 3 时，实际上会生成两次。但事实上我们应该重用结果。在我们的方法中，我们希望以逐步且可解释的方式来解决这些问题。例如，在这里，在第二步，我们可以获得除数，即 27，并且我们还可以回溯到原始问题中找到相关的内容。在这些步骤中，我们获得除数。然后，在这个第三步中，我们实际上得到商。经过这些三步之后，我们实际上可以重用第二步的结果，然后得到第四步的结果，最后我们可以获得被除数。因此，我们实际上是直接生成整个表达式，而不是生成单个的运算符或数量，这使得过程更加准确。在我们的演绎系统中，我们首先从问题中给出的各种数量以及一些常数开始，作为我们的初始状态。表达式由 eij 表示，我们在 qi 和 qj 之间执行运算符。这种表达式实际上是有方向的。我们这里也有减法反向，用来表示相反的方向。这与关系抽取非常相似。在一个正式的演绎系统中，在时间步 t，我们将运算符应用于 qi 和 qj 对之间，然后我们得到新的表达式，并将其添加到下一个状态中，以成为一个新的数量。幻灯片实际上可视化了状态的演变，我们在其中不断将表达式添加到当前状态中。在我们的模型实现中，我们首先使用预训练模型，它可以是 BERT 或 RoBERTa，对句子进行编码，然后我们得到这些数量的表示。一旦我们得到数量的表示，我们就可以开始进行推理。这里我们展示了一个例子，从 q1 到 q2 来获得 q1 除以 q2 和乘以 q3 的表示。首先，我们得到配对表示，这基本上是 q1 和 q2 之间的连接。然后我们应用一个前馈网络，其参数由运算符参数化。最后，我们得到表达式表示 q1 除以 q2。但在实践中，在推理阶段我们可能无法得到正确的表达式。这里所有可能的表达式等于运算符数量的三倍。这里的一个优点是，我们可以轻松地添加约束来控制这个搜索空间。例如，如果这个表达式不允许，我们就可以简单地从我们的搜索空间中删除这个表达式。在第二步中，我们执行相同的操作，但唯一的区别是多了一个数量。这个数量来自之前计算的表达式。最后，我们可以得到最终表达式 q3 乘以 q4。我们还可以看到所有可能的表达式的数量与之前的步骤不同。这种差异使得难以应用 Beam Search，因为这两个步骤之间的概率分布是不平衡的。训练过程类似于训练序列到序列模型，我们在每个时间步优化损失。在这里，我们还使用 tau 来表示何时应该终止这个生成过程。这里的空间与序列到序列不同，因为每个时间步的空间是不同的，而传统的序列到序列模型是词汇表数量，并且它也允许从先验知识中施加某些约束。我们对常用的方法问题数据集，如 MWPS、Method3K、Math QA 和 SwAM 进行了实验。这里简要地展示了与先前最佳方法的结果比较。我们性能最佳的模型是 Roberta Deductive Reason。事实上，与使用 Beam Search 的其他方法不同，我们没有使用 Beam Search。最佳方法通常是基于树的模型。总的来说，我们的推理器能够输出优于基于树的模型。但是，我们可以在 Math QA 或 SwAM 上看到绝对数字并不是很高。我们进一步调查了 SwAM 上的结果。这个数据集具有挑战性，因为作者试图手动添加一些内容来迷惑 NLP 模型，例如添加可用的信息和额外的数量。在我们的预测中，我们发现一些中间值实际上是负数。例如，在这些问题中，我们正在询问 Jake 有多少个苹果，但我们有一些额外的关于 pitchachees 的信息，例如 Stephen 有八个 pitchachees，并且有 17 个 pitchachees 比他少。这些信息完全是无关的。我们的模型会做出某些预测，产生负数。我们观察到这两个表达式实际上具有相似的分数，因此我们可以通过删除负数结果来限制这个搜索空间，从而使答案正确。我们进一步发现这种约束实际上对某些模型有所提高，例如对于 BERT，我们提高了七个点，对于基于 Roberta 的模型，我们实际上提高了两个点。更好的语言模型具有更好的语言理解能力，因此这里的数字对于 Roberta 来说更高，对于 BERT 来说更低。我们还试图分析数据集中隐藏的难度。我们假设未使用的数量可以被视为相关信息。在这里，我们可以看到 SwAM 数据集具有最大的未使用的数量比例。我们还展示了对于没有未使用的数量的样本的整体性能。我们可以看到整体性能实际上高于具有未使用的数量的样本的性能。在 WPS 中，我们并没有太多的死案例，因此我们忽略了这一部分。最后，我们希望通过一个崩溃演示来展示可解释性。在这里，我们的模型在第一步时犯了错误的预测，我们可以将这个表达式与句子相关联。我们认为这句话可能误导了模型，导致了不正确的预测。我们尝试修改句子，使其成为类似于“苹果树的数量比梨树少 5 棵”的句子，从而更准确地传达语义，以便模型能够做出正确的预测。这项研究表明，可解释的预测如何帮助我们理解模型的行为。总而言之，我们的工作表明我们的模型非常高效，并且我们能够提供可解释的求解过程，并且我们可以轻松地将一些先验知识作为约束来包含，从而有助于提高性能。还有一个限制是，如果有很多运算符或常数，内存消耗可能会很高。第二，正如前面提到的，由于不同时间步之间的概率分布不平衡，因此很难应用 Beam Search 策略。演讲结束，欢迎提问。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫安托万，来自马斯特里赫特大学。我将与杰里一起展示我的约翰工作，这是一个关于新的法定条款检索数据集的项目。法律问题是许多人生活中不可分割的一部分，但大多数公民对他们的权利和基本的法律程序缺乏了解。因此，许多无力承担法律专家的昂贵援助的弱势公民要么没有得到保护，要么最坏的情况是被剥削。这项工作旨在弥合公民与法律之间的差距，通过开发有效的法定条款检索系统来实现。这样的系统可以为不具备专业技能的人提供免费的专业法律帮助服务。在深入探讨这项工作的主要贡献之前，让我们首先描述一下法定条款检索的问题。给定一个关于法律问题的简单问题，例如“如果我泄露职业机密，我将面临怎样的风险？”，模型需要从大量的立法文件中检索所有相关的法定条款。这项信息检索任务面临着自身的一系列挑战。首先，它需要处理两种类型的语言：问题中的自然语言和法规中的复杂法律语言。这种语言分布的差异使得系统更难检索相关的候选条文，因为它间接地需要一种能够将自然语言问题翻译成与法规术语相匹配的法律问题的内在解释系统。此外，法定法不是可以独立对待作为完整信息来源的一堆条款，例如新闻或食谱。相反，它是具有法律意义的结构化法律条款集合，只有在整体上下文中，即与来自其相邻条款、它们所属的领域和子领域以及它们在法律结构中的位置的补充信息一起考虑时，才能具有整体意义。最后，法定条款通常是小段落，这在大多数检索工作中通常是典型的检索单元。在这里，它们是长文档，可能长达 6000 个单词。最近的自然语言处理领域的进展激发了人们对许多法律任务的巨大兴趣，例如法律判决预测或自动合同审查，但由于缺乏大型且高质量的标注数据集，法定条款检索主要停留在初级阶段。在这项工作中，我们提出了一个新的、以法国公民为中心的法语数据集，以研究检索模型是否可以近似法律专家的效率和可靠性，以完成法定条款检索的任务，或完成比利时法定条款检索数据集的任务。该数据集包含超过 1100 个由比利时公民提出的法律问题。这些问题涵盖了从家庭、住房、金钱到工作和社会保障等广泛主题，每个问题都由经验丰富的法学家标注，并引用了来自比利时法律法典中超过两万六千六百个法律条款的相关条文。现在让我们谈谈我们如何收集这个数据集。首先，我们开始编制一个大型的法律条款语料库。我们考虑了 32 个公开可用的比利时法典，并提取了其中的所有条文以及相应的章节标题。然后，我们收集了有法律条款引用的法律问题。为此，我们与一家比利时律师事务所合作，该律师事务所每年收到约 400 封来自比利时公民的邮件，询问有关个人法律问题的建议。我们幸运地获得了他们网站的访问权限，他们的经验丰富的法学家团队解答了比利时最常见的法律问题。我们收集了数千个问题，这些问题已标注有类别、子类别和与相关法律条款的法律引用。最后，我们过滤了法律引用，并剔除了那些引用不属于我们所考虑的法典中条款的问题。其余的引用被匹配并转换为我们语料库中相应的条款 ID。我们最终得到了 1108 个问题，每个问题都经过仔细标注，其中引用了来自包含 226633 个法定条款的大型语料库中的相关条款 ID。此外，每个问题都附带一个主要类别和子类别的串联，每个条款也都附带一个在法律结构中后续标题的串联。这些额外的信息在本研究中未使用，但可能对未来的法律信息检索或法律文本分类研究感兴趣。现在让我们来看一下我们数据集的一些特征。问题长度在 5 到 44 个单词之间，中间值是 40 个单词。条款的长度要长得多，中间长度为 77 个单词，其中 142 个条款的长度超过 1000 个单词，最长的甚至长达五千七百九十个单词。如前所述，问题涵盖了广泛的主题，其中约 85% 的问题是关于家庭、住房、金钱或司法问题，而其余的 15% 涉及社会保障、外国人或工作。条款也非常多样化，因为它们来自 32 个不同的比利时法典，涵盖了大量的非法主题。这里是每个比利时法典收集到的条款总数。在 226633 个条款中，只有 1612 个被认为是至少与一个问题相关。大约 80% 的这些引用的条款来自民法典、司法法典、刑事调查法典或刑法典。同时，32 个法典中的 18 个法典提到少于 5 个条款被认为是与至少一个问题相关，这可以解释为这些法典不太关注个人及其需求。总体而言，这些引用的条款的中间引用次数为 2，且少于 25% 的条款被引用超过 5 次。使用我们的数据集，我们对多种检索方法进行了基准测试，包括词法方法和密集架构。给定一个问题和一个条款，词法模型通过计算每个条款中查询词的权重之和，为查询-条款对分配一个分数。我们试验了标准的 TFIDf 和 bm25 排名函数。这些方法的关键问题是，它们只能检索包含查询中关键词的条款。为了克服这一局限性，我们试验了一种基于神经网络的架构，该架构可以捕捉查询和条款之间的语义关系。我们使用 b 编码器模型将查询和条款映射到密集向量表示，并根据它们的嵌入相似性计算查询-条款对的相关性分数。这些嵌入通常是通过对词嵌入模型的输出进行池化操作的结果。首先，我们研究了在零样本评估设置中 Siamesebiancoders 的有效性，这意味着将预训练的词嵌入模型直接应用，而无需任何额外的微调。我们试验了不依赖上下文的文本编码器，如 word2vec 和 fastText，以及依赖上下文的嵌入模型，如 Roberta，特别是法语版本的 Roberta 模型，即 camembert。此外，我们还在整个数据集上训练了我们自己的基于 camembert 的 biancoders 模型。需要注意的是，在训练过程中，我们试验了 biancoder 架构的两种变体：Siamese，它使用唯一的词嵌入模型，将查询和条款映射到共享的密集向量空间；以及 two tower，它使用两个独立的词嵌入模型，将查询和条款分别编码到不同的嵌入空间中。我们试验了平均池化、最大池化和 CLls 池化，以及点积和余弦相似度。以下是基线结果，上部分是词法方法，中间是零样本设置中的 Siamesebiancoders 评估，下部分是微调的 biancoders。总而言之，微调的 biancoders 显著优于所有其他基线。two tower 模型在 100 个召回率方面优于其 Siamese 变体，但在其他指标上表现相似。虽然 bm25 的表现不如训练过的 biancoder 显著，但其性能表明它仍然是特定领域的检索的一个强大的基线。关于 Siamesebiancoder 的零样本评估，我们发现直接使用预训练的 camembert 模型的嵌入，而不针对信息检索任务进行优化，会产生较差的结果，这与之前的研究结果一致。此外，我们观察到基于 word2vec 的 biancoder 明显优于基于 fastText 和 bird 的模型，这表明在直接使用时，预训练的词级别嵌入可能比字符级别或子词级别嵌入更适合这项任务。尽管前景可观，但这些结果表明有充分的机会可以改进。与能够最终检索任何问题的所有相关条款并获得完美分数的熟练法律专家相比，这一点尤其突出。现在让我们来讨论数据集的两个局限性。首先，条款语料库仅限于从 32 个考虑的比利时法典中收集的条款，这并不涵盖整个比利时法律，因为遗漏了来自法令、指令和法令的条款。在数据集构建过程中，忽略了所有对这些未收集的条款的引用，这导致一些问题最终只有初始相关条款数量的一小部分。这种信息损失意味着剩余的相关条款中的答案可能不完整，尽管它仍然完全适用。其次，我们应该指出，并非所有法律问题都可以仅用法规来回答。例如，“如果租户制造了太大的噪音，房东是否可以驱逐租户？”这个问题可能在法定法中没有详细的答案，量化了驱逐的噪音阈值。相反，房东可能应该更多地依赖案例法，并找到与其当前情况类似的判例。例如，租户每周制造两次，直到凌晨 2 点的噪音。因此，某些问题比其他问题更适合法定条款检索任务，而不太合适的问题的领域仍有待确定。我们希望这项工作能激发人们开发实用且可靠的法定条款检索模型，从而有助于改善所有人的司法可及性。您可以在以下链接中查看我们的论文点集编码：[链接] 谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "我们很高兴向大家介绍我们关于词汇（vowels）的工作，这是一个独立于任务的基准测试，旨在测试具有特定语言现象的视觉和语言模型。 为什么我们费尽周折地搭建了这个基准？ 过去几年，我们目睹了基于Transformer的视觉和语言模型的大爆发，这些模型在海量图像文本对数据上进行预训练，每一款模型都在视觉问答、视觉常识推理、图像检索、短语定位等视觉和语言任务上实现了最先进水平。 因此，我们思考：虽然这些任务特定基准上的准确率稳步提高，但我们真的了解模型究竟学到了什么吗？ 当模型为某张图像和某句话匹配，并为另一张图像和另一句话打分较低时，模型到底理解了什么？ 视觉和语言模型关注的是正确的事物，还是关注先前的研究中揭示的偏差？ 为了更深入地了解这一方面，我们提出了一种更具任务无关性的方法，并引入了词汇（vowels），它测试了视觉和语言模型对影响语言和视觉模态的特定语言现象的敏感性。 我们关注的存在、复数、计数、空间关系、动作和实体指代。 但我们该如何测试视觉和语言模型是否捕捉到这些现象呢？ 我们借鉴了一种先前仅用于名词短语（由Ravi Shekhar及其合作者开发）和我们自身在先前工作中的计数方法，采用“欺骗”（foiling）的方法。 欺骗基本上是指，我们以取一张图像的标题，并将其修改，使其不再描述该图像。 我们通过专注于六个特定方面进行短语修改：存在、复数、计数、空间关系、动作和实体指代。 其中每个方面可以由一个或多个工具（instrument）组成，如果发现创建欺骗实例有多种有趣的方法，我们就会使用多个工具。 例如，在动作方面，我们有两个工具：一个改变动作动词为不同的动作，另一个交换行为主体。 计数和指代也是具有多个工具的方面。 我们创建这些欺骗，以确保它们无法描述图像，并且仍然是语法正确且有效的句子。 这并不容易做到，因为欺骗后的标题可能比原始标题的可能性要小。 例如，虽然并非不可能，但植物砍伐人类的可能性比人类砍伐植物的可能性要低。大型视觉和语言模型可能会察觉到这一点。 因此，为了获得有效的欺骗，我们首先采取措施：第一，我们使用强大的语言模型来提出欺骗；第二，我们使用自然语言推理（NLI）来过滤掉可能仍然描述图像的欺骗。 因为在构建欺骗时，我们需要确保它们无法描述图像，因此我们需要自动测试这一点，我们使用自然语言推理，采用以下推理方式：我们将图像视为前提，其标题视为蕴含的假设。 此外，我们还将标题视为前提，欺骗作为其假设。 如果NLI模型预测欺骗与标题矛盾或中立，我们将此视为有效欺骗的指标。 如果NLI预测欺骗由标题蕴含，那么它不能成为一个好的欺骗，因为通过传递性，它将给出图像的真实描述，我们将过滤掉这些欺骗。 然而，这个过程并不完美，它只是有效欺骗的指标。 因此，作为生成有效欺骗的第三项措施，我们聘请了人类标注员来验证valse中使用的原始数据。 经过过滤和人工评估后，我们拥有本文档中表格中描述的测试实例数量。 值得注意的是，valse不提供任何训练数据，而仅提供测试数据，因为它是一个零样本测试基准。 它旨在利用视觉和语言模型预训练后的现有能力。 微调只会使模型能够利用数据中的伪影或统计偏差。 众所周知，这些模型喜欢作弊和采取捷径。 正如我们所说，我们有兴趣评估视觉和语言模型在预训练后的能力。 我们在词汇（vowels）上使用五个视觉和语言模型进行实验，分别是CLIP、AlexNet、BERT、Wilbert和Wilbert-12-in-1。 我们的两个最重要的评估指标是：模型将图像-句子对分类为标题和欺骗的准确率。 或许对于本次视频来说，更相关的指标是成对准确率，它衡量图像-句子对的对齐分数是否高于其欺骗对。 更多指标及其结果请查看我们的论文。 结果显示，在成对准确率方面，Wilbert-12-in-1实现了最佳的零样本性能，其次是Wilbert、AlexNet、BERT和VisualBERT。 值得注意的是，以单个对象为中心的工具（如存在和名词短语）几乎被Wilbert-12-in-1解决，这表明模型能够识别命名对象及其在图像中的存在。 然而，其余的方面都无法在我们的对抗性欺骗设置中可靠地解决。 从复数和计数工具来看，我们发现视觉和语言模型难以区分对单个或多个对象的引用，或在图像中对其进行计数。 关系方面表明，它们难以正确分类图像中命名对象之间的空间关系。 它们还难以区分动作并识别其参与者，即使有可信度偏差，如在动作方面所见。 从指代方面，我们发现使用代词跟踪图像中对同一对象的多个引用也很困难。 作为一项健全性检查，也是一项有趣的实验，我们还对两个仅基于文本的模型GPT-1和GPT-2进行了基准测试，以评估词汇（vowels）是否可以通过这些单模态模型解决，方法是计算正确标题和欺骗标题的困惑度。 如果欺骗的困惑度高于正确标题，我们将此视为欺骗标题可能存在合理性偏差或其他语言偏差的指标。 令人有趣的是，在某些情况下，仅基于文本的GPT模型比视觉和语言模型更好地捕捉到了世界的事实。 总而言之，词汇（vowels）是一个基准，它利用语言构造的视角来帮助社区改进视觉和语言模型，通过严格测试其视觉接地能力。 我们的实验表明，视觉和语言模型能够很好地识别图像中命名对象的存在，但在被迫尊重语言指标时，它们难以在视觉场景中处理它们的相互依赖性和关系。 我们强烈鼓励社区使用词汇（vowels）来衡量在视觉和语言模型中实现语言接地方面的进展。 甚至可以更进一步，将词汇（vowels）用作数据集的间接评估方法，模型可以在训练前或微调后进行评估，以查看数据集是否能帮助模型在词汇（vowels）测试的任何方面有所改进。 如果您有兴趣，请在GitHub上查看Valse数据，如果您有任何问题，请随时联系我们。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我来自东京大学的 Kamisura。我将发表一篇题为“O En Sum：面向自动列表节点总结的大规模设计”的论文。我将按以下顺序进行讲解：首先，我将介绍我们在本研究中进行的工作的自动列表节点标注。Release Notes 是一个技术文档，它总结了软件产品每次发布所包含的变更。图片显示了 Bujs 库版本 2.6.4 的 Release Notes。这些笔记在开源开发中起着重要的作用，但手动准备它们耗时，因此能够自动生成高质量的 Release Notes 将非常有益。我将参考之前关于自动列表节点生成的两项研究。第一项是 2014 年发布的名为 Alena 的系统。它采用基于规则的方法，例如使用变更提取器从疾病之间的差异中提取核心差异、库更改和文档更改，然后将它们组合在一起。该系统的最显著特征是右上角的 issue 提取器，必须链接到 Jira，issue 生态系统，并且只能应用于使用 Jira 的项目，换句话说，它不能用于 GitHub 上的许多项目。第二项是 Grif，最近在 2020 年宣布。它可以在互联网上访问，并且可以通过 pi 进行存储。该系统具有简单的基于运行的文本分类模型，并为每个输入提交消息输出五个类别之一，如特性或错误修复。图片是一个样本用法，它返回正确的特性或错误修复 rub que 训练数据相对较小，大约 5000，将在以下描述的实验中展示。文本分类模型的性能并不高。我展示了两项相关研究，但存在适用性有限和数据资源匮乏的问题。我们的论文解决了这两个问题，并自动生成高质量的资源。对于适用性有限的问题，我们提出了一种使用仅提交消息作为输入的、高质量的分类器总结方法。这种提出的方法可以用于所有英文仓库。对于数据资源匮乏的问题，我们构建了一个由大约 82000 条数据组成的 ours 和 some 数据，通过使用 Git API 从公共 GitHub 仓库中纠正数据。接下来我将描述我们的数据。这里有一个例子：左侧是提交消息，右侧是 Release Notes。Release Notes 按改进、特性等进行分层。我们设置了一个任务，该任务将提交消息作为输入，并输出 rabbit 的 notes。这可以被看作是一个总结任务。我们预定义了四个级别：特性、改进、错误修复、弃用和破坏性更改。这些是基于先前使用和其他因素设置的。在列表节点左下角显示的内容从右下角的 notes 中提取。此时，检测在 pass 中设置的四个 rabbits 是必要的，但级别并不总是与每个 le 保持一致。例如，改进级别包括改进、增强和优化等。我们准备了一个词汇表列表，用于每个这些记号变化的级别，使用它来检测风险节点类并纠正以下文本，作为风险节点句子的类。接下来是提交消息。提交消息没有绑定到每个 race，如图所示。如果当前版本是 2.5.19，我们需要识别之前的版本 2.5.18 并获取它 di。这有点繁琐，仅仅获取发布列表并查看前后是不够的。我们创建了一个启发式匹配 glue 来获取之前的和下一个版本的 data。最终，对 7200 个仓库和 82000 条数据进行了纠正。此外，合理的 tokens 的平均数量为 63，这对于总结任务来说相当高。独特的 tokens 的数量也很丰富，为八千零八百三十万。这是由于仓库中发现的 unique classs 和 method 名称数量很多。接下来我将解释提出的方法。交叉式提取式和抽象式总结模型由两个神经网络模块组成：一个使用 bot 或 code bot 的分类器和一个使用 but first G 的生成器。分类器首先将每个提交消息分类为五个基本节点类：特性、改进、错误修复、弃用和其它。被分类为其它类的提交消息将被丢弃。然后，它将生成器应用于四个 rubber 文档独立地，并为每个类生成 read note。在这个任务中，提交消息和 read notes 之间的直接对应关系是未知的。因此，为了训练分类器，我们将伪变量分配给每个输入提交消息，使用每个提交消息的前 10 个字符。我们使用两种定义的方法对类别级别的抽象式总结方法进行建模。第一种模型，我们称之为 GS single，包含一个单独的 sex 网络，并生成单个长 is not 文本，给定输入提交消息的连接。输出文本可以根据特殊类特定端点符号划分为类文件段。第二种方法，我们称之为 shes much，由四个不同的 sec to sec 网络组成，每个网络对应于列表节点类中的一种。好的，让我解释一下实验。将五种方法进行比较：gs、shes single、shes much、cluster 和之前的一项研究 Grif，在某些情况下，这些 notes 以多个句子输出，由于将句子数量纠正为零比较困难，因此它们与空格结合并视为一个长句子。当系统输出一个短句子时，系统会施加惩罚，这种惩罚会导致 bre 值降低。在以下描述的实验结果中，我们还计算了 specificity，因为 blue 和 blue 不能对列表 notes 进行 caricature 如果列表 notes 是空的，specificity 高意味着模型在 read nodes 假设为空的情况下正确输出空文本。这是结果。由于数据集包含电子邮件分析有值等，我们还评估了干净的数据集。G 和 Gs 在 lose error 分数上比基线高出 10 多分。特别是，在韩国测试集中，提出的方法和基线之间的分数差距高达 20 多分。这些结果表明 Gs 和 Gs 具有显著的有效性。Gs 获得了比 GAS 更好的 lose 分数，表明将分类器和生成器结合起来可以有效地训练分类器，从而可以正确地关注每个类别中相关的提交消息。shes much 往往比 she is single 高，表明独立开发针对每个列表节点类的不同构建式总结模型也是有效的。这里有一个误差分析。CSS 方法往往输出比人工参考句子更短的句子，因为如图所示，参考句子有三个或四个句子，而 CSS 只有一句。这种模型不愿意的理由是，在训练数据中，只有 30% 的句子出现在特性级别，而 40% 在改进级别。此外，CSS 方法在没有额外信息的情况下无法生成准确的列表 note。右侧的顶部示例是一个非常凌乱的提交消息的示例，在没有区别于相应的 prerogates 或 issue 的情况下无法生成完整的句子。下面的示例显示了输入中的两个提交消息是相关的，应该合并成一个句子，但它未能做到。最后，结论。我们构建了一个新的数据集，用于自动个人生成。我们还构建了任务，将提交消息输入并总结它们，使其适用于所有用英语编写的项目。我们的实验表明，提出的方法生成更少的 noise is not，并且覆盖率高于基线。请在 GitHub 上查看我们的数据。谢谢。"}
