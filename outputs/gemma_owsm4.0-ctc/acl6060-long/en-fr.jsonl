{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Safari, et je présenterai notre article sur l'enrichissement de données tabulaires à l'aide de Transformers affinés. Ainsi, les scientifiques analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes, mais ces caractéristiques sont parfois limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à partir de sources externes en texte libre. Supposons que nous ayons un ensemble de données tabulaires et une base de connaissances. Nous avons besoin d'un processus automatique qui implique l'établissement de liens d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques du texte libre de la base de connaissances. Notre cadre, FesT, est exactement ce processus automatique. Voyons un exemple dans les ensembles de données utilisés par FesT. Dans cet exemple, l'ensemble de données est un ensemble de données universitaires dont l'objectif est de classer les universités en universités de bas rang et en universités de haut rang. Nous utilisons Wikipédia comme base de connaissances. La première phase de FesT est l'établissement de liens d'entités, où chaque entité, dans cet exemple le nom de l'université, est reliée à une entité dans la base de connaissances et le texte des entités de la base de connaissances est extrait et ajouté à l'ensemble de données. Dans cet exemple, le texte est l'extrait de la page Wikipédia. Nous devons maintenant générer ou extraire des caractéristiques du texte récupéré, ce qui nécessite une phase d'extraction de caractéristiques comprenant une analyse de texte. Il s'agit de la principale nouveauté de cet article et j'y reviendrai plus en détail dans les prochaines diapositives. Après la phase d'extraction de caractéristiques, il existe une phase de génération de caractéristiques où nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques.  Nous générons d'abord des caractéristiques dans le nombre de classes de l'ensemble de données d'origine. Dans cet exemple, l'ensemble de données a deux classes, nous générons donc d'abord deux nouvelles caractéristiques. Mais si l'ensemble de données a cinq classes, nous générons d'abord cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'analyse hors texte, qui sont les modèles de langage basés sur les transformateurs, tels que BERT, GPT X et al. Il est cependant peu probable que nous puissions entraîner un modèle de langage à partir des ensembles de données d'entrée. Une approche naïve consisterait à effectuer un affinage ciblé. Dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné et affiner ce modèle sur l'ensemble de données cible. Dans cet exemple, pour affiner le modèle de langage afin de classer le texte en classes, l'extrait en classes bas ou haut, recevoir la sortie du modèle de langage, qui est la probabilité pour chaque classe, et utiliser cela comme nouvelles caractéristiques. Le problème avec cette approche est que l'ensemble de données peut avoir peu d'entités distinctes. Dans notre expérience, presque la moitié des ensembles de données contiennent moins de 400 échantillons, et l'ensemble de données le plus petit contient 35 échantillons dans son ensemble d'entraînement initial. L'affinage d'un modèle de langage sur cet ensemble de données serait inefficace. Mais nous pouvons utiliser les connaissances antérieures sur les ensembles de données pré-analysés car FesT est appliqué sur plusieurs ensembles de données. Nous pouvons utiliser les n moins 1 ensembles de données pour recueillir des informations sur les n moins 1 ensembles de données et utiliser ces informations lors de l'analyse du nième ensemble de données. Nous suggérons d'ajouter une autre phase d'affinage, une phase d'affinage multitâche préliminaire, où vous affinez le modèle de langage sur les n moins 1 ensembles de données, et ensuite nous exécutons une autre phase d'affinage, qui est un affinage ciblé, où nous affinons le modèle de langage sur le nième ensemble de données cible. L'état de l'art en matière d'affinage multitâche, appelé DNN, DNN maintient des têtes dans le nombre de tâches de l'ensemble d'entraînement. Ainsi, si dans cet exemple il y a quatre tâches dans l'ensemble d'entraînement, DNN maintient quatre têtes, comme vous pouvez le voir sur l'image. Il échantillonne un lot aléatoire à partir de l'ensemble d'entraînement et si le lot aléatoire appartient à une tâche d'exemple de classification de phrases unique, il exécute une passe avant et arrière à travers la première tête, et si le lot aléatoire appartient à une tâche de classement par paires, il exécute une passe avant et arrière à travers la dernière tête. Dans notre scénario, un ensemble de données tabulaires fait varier le nombre de classes, il y a donc de nombreuses tâches. DNN maintient le nombre de classes de têtes de couches de sortie, et en outre, DNN a besoin d'initialement de nouvelles têtes pour un nouvel ensemble de données avec une nouvelle tâche. Notre approche, appelée affinage par reformulation de tâches, consiste à, dans notre approche d'affinage par reformulation de tâches, au lieu de maintenir plusieurs têtes, nous reformulons chaque ensemble de données en un problème de classification par phrases, qui sont des tâches à deux classes. Voyons un exemple ici. Voici notre ensemble de données d'entrée, qui se compose d'entités, de caractéristiques, de texte et de classes, et nous reformulons la tâche d'une classification du texte en bas et haut à une classification du texte de l'extrait et de la classe en vrai ou faux, ou en d'autres termes, nous entraînons le modèle de langage à classer un extrait et une classe a afin de déterminer si l'extrait appartient à la classe ou non. Le vecteur d'étiquette dans le cas de z reste toujours constitué de deux classes. Il s'agit de l'algorithme de notre approche d'affinage formulé. Voyons le cadre complet. FesT est appliqué à un ensemble de données et FesT exécute ensuite la phase de liaison, extrait le texte de la base de connaissances, qui dans cet exemple est l'extrait de la page Wikipédia, puis reformule la tâche en tâches de classification de paires de phrases, applique le modèle de langage à la nouvelle tâche et la sortie est la probabilité pour chaque classe. Notez que le modèle de langage est déjà affiné sur n moins 1 ensembles de données en utilisant un affinage multitâche préliminaire. Nous utilisons ensuite le vecteur de sortie du modèle de langage comme une nouvelle caractéristique dans le nombre de classes pour évaluer notre cadre. Nous utilisons un ensemble de seize ensembles de données tabulaires de classification dans notre expérience, ce qui définit une taille de caractéristiques, un domaine équilibré et des performances initiales. Nous utilisons Wikipédia comme base de connaissances. Nous concevons notre expérience comme une évaluation par élimination d'un. Nous entraînons FesT sur 16 ensembles de données et l'appliquons au 17e ensemble de données. Nous divisons également chaque ensemble de données en quatre plis et appliquons une validation croisée à quatre plis. Nous générons ensuite les nouvelles caractéristiques et les évaluons à l'aide de cinq classificateurs d'évaluation. Nous utilisons une architecture basée sur les transformateurs dans notre expérience. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre cadre à un affinage d'ensemble de données cible, un affinage ciblé et un DNN d'affinage préliminaire multitâche et notre affinage par reformulation atteint le meilleur résultat, la meilleure performance, tandis que m t d n n e atteint une amélioration de deux pour cent par rapport à l'affinage de l'ensemble de données cible. Notre approche obtient une amélioration de six pour cent lorsque nous examinons les petits ensembles de données, nous pouvons voir que la performance de m t d n n diminue et que l'amélioration de la phase d'affinage multitâche préliminaire diminue à 1,5 pour cent, mais notre performance augmente à 11 pour cent par rapport à l'affinage ciblé. FesT permet ainsi un enrichissement à faible nombre d'échantillons à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour toutes les tâches, tous les ensembles de données et il conserve la tête du modèle. Mais il ajoute une phase de reformulation, ce qui augmente l'ensemble d'entraînement et nécessite une valeur cible avec une signification sémantique, que nous pouvons alimenter dans le modèle de langage et utiliser dans le problème de classification de phrases. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à toutes et à tous. Aujourd’hui, je vais vous présenter notre travail de recherche sur l’apprentissage du raisonnement déductif, un problème de type Méthode où l'extraction de régions complexes est requise. Je suis All, du laboratoire Biance AI Lab, et ceci est un travail conjoint avec Che de l’Université du Texas à Austin et Wedu de SUDD.\n\nTout d’abord, j’aimerais discuter de notre motivation pour le raisonnement. Ici, nous montrons des exemples où un raisonnement en plusieurs étapes est utile. Cette figure est tirée de l’article Pound, où ils utilisent le prompting pour résoudre le problème de Méthode dans un scénario d’apprentissage futur. Sur la gauche, on peut constater que si nous fournissons quelques exemples avec seulement des questions et des réponses, nous ne pourrons peut-être pas obtenir les réponses correctes. Mais si nous fournissons une description plus détaillée du raisonnement, le modèle est capable de prédire la description du raisonnement et de faire une prédiction correcte. Il est donc bon d'avoir un raisonnement en plusieurs étapes interprétable en sortie, et nous pensons également que le problème de Méthode est une application directe pour évaluer ces capacités de raisonnement. Dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques. Dans nos ensembles de données, nous avons également l’expression mathématique qui mène à cette réponse particulière. Certaines hypothèses s'appliquent également, comme dans les travaux précédents, nous supposons que la précision des quantités est connue et que nous ne considérons que des opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, les opérateurs complexes peuvent en réalité être décomposés en ces opérateurs de base. Les travaux précédents sur la résolution de problèmes de Méthode peuvent être classés en modèles séquence à séquence et séquence à arbre. Le modèle séquence à séquence traditionnel convertit l’expression en une séquence spécifique pour la génération, ce qui est assez facile à mettre en œuvre et peut se généraliser à de nombreux problèmes complexes. Cependant, l’inconvénient est que les performances sont généralement pas meilleures que celles des modèles structurés, et il manque d’interprétabilité pour la prédiction. Cette direction est néanmoins encore assez populaire en raison des modèles transformateurs. Dans les modèles basés sur les arbres, nous structurons ces expressions sous forme d'arbre et suivons un parcours préfixé lors de la génération d'arbres. Ici, nous continuons à générer les opérateurs jusqu'à atteindre les feuilles, qui sont les quantités. Le point positif est qu'il nous fournit cette structure d'arbre binaire. Il est toutefois contre-intuitif, car nous générons l'opérateur en premier, puis les quantités à la fin. De plus, cela contient également certains calculs répétitifs. Par exemple, si nous examinons cette expression : a fois 3 plus 3, elle est en réalité générée deux fois, mais en fait, nous devrions réutiliser les résultats.\n\nDans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et interprétable. Par exemple, ici, à la deuxième étape, nous pouvons obtenir les diviseurs, qui sont 27, et nous pouvons également faire référence à la question d'origine pour trouver le contenu pertinent. À ces étapes, nous obtenons les diviseurs, puis à la troisième étape, nous obtenons en fait le quotient. Après ces trois étapes, nous pouvons en réalité réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, puis finalement nous pouvons obtenir le dividende. Ici, nous générons en fait toute l'expression directement plutôt que de générer un seul opérateur ou une seule quantité. Cela rend le processus plus précis.\n\nDans notre système déductif, nous commençons d’abord par un ensemble de quantités présentées dans les questions, ainsi que certaines constantes, comme état initial. L’expression est représentée par eij, où nous effectuons l'opérateur de qi à qj. Cette expression est en fait dirigée. Nous avons également une soustraction inversée ici pour représenter la direction opposée. Ceci est assez similaire à l’extraction de relations. Dans un système déductif formel, à l'étape t, nous appliquons l'opérateur entre la paire qi et qj et nous obtenons cette nouvelle expression. Nous l’ajoutons aux états suivants pour devenir une nouvelle quantité. Ces diapositives visualisent l’évolution des états où nous ajoutons continuellement des expressions aux états actuels.\n\nDans notre implémentation de modèle, nous utilisons d'abord un modèle pré-entraîné, qui peut être BERT ou RoBERTa, puis nous encodons la phrase et nous obtenons ces représentations de quantités. Une fois que nous avons les représentations de quantités, nous pouvons commencer à effectuer des inférences. Ici, nous montrons un exemple de q1 divisé par q2 et multiplié par q3. Nous obtenons d’abord la représentation de la paire, qui est essentiellement la concaténation entre q1 et q2, puis nous appliquons un réseau feedforward paramétré par l’opérateur, puis nous obtenons la représentation de l’expression q1 divisé par q2. Mais en pratique, lors de la phase d’inférence, nous pourrions également obtenir l’expression incorrecte. Ici, toutes les expressions possibles sont égales à trois fois le nombre d’opérateurs. Le point positif est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression de notre espace de recherche.\n\nÀ la deuxième étape, nous faisons la même chose, mais la seule différence est qu'il y a une quantité supplémentaire. Cette quantité provient de l’expression calculée précédemment. Finalement, nous pouvons obtenir cette expression finale q3 fois q4, et nous pouvons également constater que le nombre de toutes les expressions possibles est différent de l’étape précédente. Cette différence rend difficile l'application de la recherche par faisceau car la distribution de probabilité entre ces deux étapes est déséquilibrée.\n\nLa procédure d'entraînement est similaire à l'entraînement d'un modèle séquence à séquence, où nous optimisons la perte à chaque étape. Ici, nous utilisons également tau pour représenter quand nous devons terminer le processus de génération. Ici, l'espace est différent de celui d'un modèle séquence à séquence, car l'espace est différent à chaque étape, alors que dans un modèle séquence à séquence traditionnel, il s'agit du nombre de vocabulaire et il permet également d'imposer certaines contraintes provenant de connaissances antérieures.\n\nNous avons mené des expériences sur les ensembles de données de problèmes de Méthode couramment utilisés : MWPS, Method3k, MathQA et SwAM. Nous montrons brièvement les résultats par rapport aux meilleures approches précédentes. Notre meilleure approche est Roberta Deductive Reason. En fait, nous n'utilisons pas la recherche par faisceau, contrairement aux approches évidentes qui utilisent la recherche par faisceau. Les meilleures approches sont souvent des modèles basés sur les arbres.\n\nDans l’ensemble, notre raisonneur est capable de sélectionner des sorties significativement meilleures que ce modèle basé sur les arbres, mais nous pouvons constater que les nombres absolus sur MathQA ou SwAM ne sont pas très élevés. Nous enquêtons davantage sur les résultats sur SwAM, car cet ensemble de données est difficile, car l’auteur a tenté d’ajouter manuellement des éléments pour tromper le modèle NLP, tels que l’ajout d’informations disponibles et de quantités supplémentaires. Dans notre prédiction, nous constatons que certaines des valeurs intermédiaires sont en fait négatives. Par exemple, dans ces questions, nous demandons combien de pommes Jake a, mais nous avons quelques informations supplémentaires, comme 17 moins de poires que Stephen, et Stephen en a huit, ce qui est totalement non pertinent. Notre modèle fait certaines prédictions comme celles-ci, produisant des valeurs négatives. Nous observons que ces deux expressions ont en fait des scores similaires, nous pouvons donc réduire l'espace de recherche en supprimant les résultats négatifs pour que la réponse soit correcte.\n\nNous constatons que cette contrainte améliore en fait considérablement les performances pour certains modèles. Par exemple, pour BERT, nous avons amélioré de sept points, et pour le modèle basé sur RoBERTa, nous avons en fait amélioré de deux points. Un meilleur modèle de langage a de meilleures capacités de compréhension du langage, de sorte que le nombre ici est plus élevé pour RoBERTa et plus faible pour BERT. Nous avons également essayé d’analyser la difficulté derrière tous ces ensembles de données. Nous supposons que le nombre de quantités inutilisées peut être considéré comme des informations pertinentes ici. Ici, nous pouvons voir que l’ensemble de données SwAM a la plus grande proportion. Nous montrons également les performances globales pour ces échantillons sans quantités inutilisées. Les performances globales sont en fait plus élevées que les performances avec des échantillons contenant des quantités inutilisées. Pour WPS, nous n'avons pas trop de cas d'échec, nous ignorons donc cette partie.\n\nFinalement, nous voulons montrer l'interprétabilité grâce à un exemple de présentation et d'erreur. Ici, notre modèle fait une mauvaise prédiction à la première étape. Nous pouvons donc corrélater cette expression avec la phrase. Nous pensons que cette phrase pourrait tromper le modèle pour qu’il fasse une prédiction incorrecte. Nous essayons donc de réviser la phrase pour qu’elle soit quelque chose comme : le nombre d’arbres fruitiers est inférieur de 5 à celui des pommiers, ce qui permet de transmettre une sémantique plus précise afin que le modèle puisse faire la prédiction correcte. Cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle.\n\nEn conclusion, notre travail montre que notre modèle est assez efficace et nous sommes capables de fournir une procédure de résolution interprétable, et nous pouvons facilement intégrer des connaissances antérieures en tant que contrainte, ce qui peut aider à améliorer les performances. Et le dernier point est que le mécanisme sous-jacent ne s’applique pas seulement aux tâches de résolution de problèmes de Méthode, mais également à d'autres tâches impliquant un raisonnement en plusieurs étapes. Nous avons cependant certaines limitations. Si nous avons un grand nombre d’opérateurs ou de constantes, la consommation de mémoire peut être assez élevée. Et le deuxième point est que, comme mentionné, étant donné que la distribution de probabilité est déséquilibrée entre les différentes étapes, il est également assez difficile d’appliquer une stratégie de recherche par faisceau.\n\nCeci est la fin de la présentation et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine, et je suis de l'Université de Maastricht. Je présenterai mon travail avec Jerry, qui porte sur un nouveau jeu de données pour la recherche d'articles législatifs. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les procédures légales fondamentales. Par conséquent, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique se retrouvent sans protection, voire exploités. Notre travail vise à combler le fossé entre les citoyens et la loi en développant un système de recherche d'articles législatifs efficace. Un tel système pourrait fournir un service d'aide juridique professionnelle gratuite pour les non-spécialistes avant d'aborder la contribution principale de ce travail, décrivons d'abord le problème de la recherche d'articles législatifs. Étant donné une question simple concernant une question juridique, comme « Quels risques encourrai-je si je viole le secret professionnel ? », un modèle doit être en mesure de récupérer tous les articles législatifs pertinents à partir d'un vaste corpus de législation. Cette tâche de recherche d'information pose ses propres défis. Premièrement, elle traite de deux types de langage : le langage naturel courant pour les questions et le langage juridique complexe pour les statuts. Cette différence dans la distribution du langage rend plus difficile pour un système de récupérer des candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent capable de traduire une question en langage naturel en une question juridique qui correspond à la terminologie des statuts. De plus, le droit statutaire n’est pas une collection d’articles indépendants qui peuvent être traités comme une source d’information complète en soi, comme les actualités ou les recettes, par exemple. Au contraire, c'est une collection structurée de dispositions juridiques qui n'ont un sens que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations complémentaires de leurs articles voisins, des domaines et sous-domaines auxquels ils appartiennent et de leur place dans la structure de la loi. Enfin, les articles statutaires sont de petits paragraphes, qui constituent généralement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, il s'agit de documents plus longs pouvant contenir jusqu'à 6 000 mots. Les récents progrès en matière de PNL ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements juridiques ou l’examen automatique de contrats. Cependant, la recherche d'articles législatifs est restée en marge de ces avancées en raison du manque de grands ensembles de données étiquetés de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur le citoyen, de langue française, pour étudier si les modèles de récupération peuvent approximer l'efficacité et la fiabilité d'un expert juridique pour la tâche de recherche d'articles législatifs, ou de recherche d'articles législatifs belges. Le jeu de données se compose de plus de 1 100 questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, de la famille au logement, en passant par l'argent, le travail et la sécurité sociale. Chacune d'entre elles a été étiquetée par des juristes expérimentés, avec des références aux articles pertinents d'un corpus de plus de vingt-deux mille six cent soixante articles juridiques des codes belges. Examinons maintenant la manière dont nous avons collecté ce jeu de données. Nous avons commencé par compiler un vaste corpus d'articles juridiques. Nous avons considéré 32 codes belges publiquement disponibles et extrait tous leurs articles, ainsi que les titres de section correspondants. Nous avons ensuite recueilli des questions juridiques avec des références à des lois pertinentes. Pour ce faire, nous avons collaboré avec un cabinet d’avocats belge qui reçoit chaque année environ 400 courriels de citoyens belges qui demandent des conseils sur une question juridique personnelle. Nous avons eu la chance d’avoir accès à leur site Web où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges. Nous avons collecté des milliers de questions annotées de catégories, de sous-catégories et de références juridiques à des statuts pertinents. Nous avons ensuite filtré les références juridiques et éliminé les questions dont les références n’étaient pas des articles de l’un des codes que nous avons considérés. Les références restantes ont été appariées et converties en identifiants d'article correspondants à partir de notre corpus. Nous avons fini par obtenir mille cent huit questions, chacune étant soigneusement étiquetée avec les identifiants des articles pertinents à partir d'un vaste corpus de vingt-deux mille six cent trente-trois articles statutaires. En outre, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de leurs titres de section suivants dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient intéresser les recherches futures sur la recherche d'informations juridiques ou la classification de textes juridiques. Examinons maintenant quelques caractéristiques de notre jeu de données. Les questions ont une longueur comprise entre 5 et 44 mots, avec une médiane de 40 mots. Les articles sont beaucoup plus longs, avec une médiane de 77 mots, et 142 d'entre eux dépassant les 1 000 mots, le plus long atteignant jusqu'à cinq mille sept cent quatre-vingt-neuf mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, environ 85 % d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les 15 % restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très diversifiés, car ils proviennent de 32 codes belges différents qui couvrent un grand nombre de sujets juridiques. Voici le nombre total d'articles collectés à partir de chacun de ces codes belges. Sur les vingt-deux mille six cent trente-trois articles, seulement 1 612 sont cités comme pertinents pour au moins une question du jeu de données, et environ 80 % de ces articles cités proviennent du code civil, des codes judiciaires, du code d’instruction criminelle ou des codes pénaux. Pendant ce temps, 18 des 32 codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Le nombre médian de citations pour ces articles cités est de deux, et moins de 25 % d'entre eux sont cités plus de cinq fois. En utilisant notre jeu de données, nous avons comparé plusieurs approches de récupération, notamment des modèles lexicaux et des architectures denses. Étant donné une requête et un article, un modèle lexical attribue un score au couple requête-article en calculant la somme sur les termes de la requête des poids de chacun de ces termes dans cet article. Nous expérimentons avec les fonctions de classement TF-IDF et bm25 standard. Le principal problème avec ces approches est qu’elles ne peuvent récupérer les articles que s’ils contiennent les mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur des réseaux neuronaux capable de capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle b encoder qui mappe les requêtes et les articles dans des représentations vectorielles denses et calcule un score de pertinence entre un couple requête-article par la similarité de leurs embeddings. Ces embeddings résultent généralement d’une opération de pooling sur la sortie d’un modèle d’embedding de mots. Nous étudions d’abord l’efficacité des encodateurs biaises siamois dans un contexte d’évaluation sans apprentissage, ce qui signifie que les modèles d’embedding de mots pré-entraînés sont appliqués tels quels sans aucun ajustement supplémentaire. Nous expérimentons avec des encodeurs de texte indépendants du contexte, à savoir word2vec et fasttext, et des modèles d’embedding contextuels, à savoir roberta et, plus précisément, camembert, qui est un modèle roberta en langue française. De plus, nous avons entraîné notre propre modèle biancoder basé sur camembert sur l'ensemble des données. Notez que pour l’entraînement, nous expérimentons avec les deux variantes de l’architecture biancoder : siamois, qui utilise un modèle d’embedding de mots unique qui mappe la requête et l’article ensemble dans un espace vectoriel dense partagé, et à deux tours, qui utilise deux modèles d’embedding de mots indépendants qui encodent la requête et l’article séparément dans des espaces d’embedding différents. Nous expérimentons avec le pooling mean max et CLls ainsi qu’avec le produit scalaire et le cosinus pour calculer les similarités. Voici les résultats d’une référence sur l’ensemble de test avec les méthodes lexicales ci-dessus, les encodeurs biaises siamois évalués dans un contexte sans apprentissage au milieu et les encodeurs finement ajustés en dessous. Dans l’ensemble, les biancoders finement ajustés surpassent significativement toutes les autres références. Le modèle à deux tours améliore les résultats de son variant siamois sur le rappel à 100, mais se comporte de manière similaire sur les autres métriques. Bien que bm25 ait sous-performé par rapport à l’encodateur biancoder entraîné, sa performance indique qu’il s’agit toujours d’une référence solide pour la récupération spécifique au domaine. En ce qui concerne l’évaluation sans apprentissage des encodeurs biaises siamois, nous constatons que l’utilisation directe des embeddings d’un modèle camembert pré-entraîné sans optimiser pour la tâche de recherche d’informations donne de mauvais résultats, ce qui est conforme aux découvertes antérieures. De plus, nous constatons que l’encodateur biancoder basé sur word2vec surpasse significativement les modèles basés sur fasttext et word, ce qui suggère que les embeddings de mots pré-entraînés sont peut-être plus appropriés pour la tâche que les embeddings de caractères ou de sous-mots lorsqu’ils sont utilisés tels quels. Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d’amélioration par rapport à un expert de niveau supérieur qui peut éventuellement récupérer tous les articles pertinents pour n’importe quelle question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limites de l’ensemble de données. Premièrement, le corpus d’articles est limité à ceux collectés à partir des 32 codes belges considérés, ce qui ne couvre pas l’ensemble du droit belge, car les articles des décrets, des directives et des ordonnances font défaut lors de la construction de l’ensemble de données. Toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions n’ont qu’une fraction du nombre initial d’articles pertinents. Cette perte d’informations implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu’elle reste tout à fait appropriée. Deuxièmement, il convient de noter que toutes les questions juridiques ne peuvent pas être répondues par des statuts seuls. Par exemple, la question « Puis-je expulser mes locataires s’ils font trop de bruit ? » pourrait ne pas avoir de réponse détaillée dans le droit statutaire quantifiant un seuil de bruit spécifique auquel l’expulsion est légale. Le propriétaire devrait probablement s’appuyer davantage sur la jurisprudence et trouver des précédents similaires à leur situation actuelle. Par exemple, les locataires font deux fêtes par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions se prêtent mieux que d’autres à la recherche d’articles législatifs et le domaine de celles qui ne conviennent pas reste à déterminer. Nous espérons que ce travail suscitera l’intérêt pour le développement de modèles pratiques et fiables de recherche d’articles législatifs qui peuvent contribuer à améliorer l’accès à la justice pour tous. Vous pouvez consulter notre article et notre ensemble de données à l’adresse suivante. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes heureux de vous présenter notre travail sur les voyelles, un benchmark indépendant conçu pour tester les modèles vision-langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous pris la peine de mettre en place ce benchmark ? Eh bien, au cours des dernières années, nous avons assisté à une explosion de modèles vision-langage basés sur des transformateurs, pré-entraînés sur de vastes quantités de paires image-texte. Chacun de ces modèles repousse les limites de l'état de l'art sur les tâches vision-langage telles que la réponse à des questions visuelles, le raisonnement de sens commun visuel, la récupération d'images, l'ancrage de phrases. Ainsi, nous avons reçu un message : les précisions sur ces benchmarks spécifiques aux tâches augmentent régulièrement, mais savons-nous ce que les modèles ont réellement appris ? Qu'est-ce qu'un transformateur vision-langage comprend lorsqu'il attribue un score élevé à cette image et à cette phrase pour qu'ils correspondent, et un score faible pour l'autre ? Les modèles vision-langage se concentrent-ils sur les bons éléments, ou se concentrent-ils sur les biais, comme le montrent les travaux antérieurs ? Pour éclairer davantage cet aspect, nous proposons une approche plus agnostique à la tâche et introduisons les voyelles, qui testent la sensibilité des modèles vision-langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous visons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la référence. Mais comment tester si les modèles vision ont capturé ces phénomènes en utilisant une méthode précédemment appliquée aux modèles vision-langage uniquement pour les groupes nominaux par Ravi Shekhar et ses collaborateurs, et au comptage par nous-mêmes dans des travaux antérieurs ? Le \"foiling\" consiste essentiellement à prendre la légende d'une image et à produire un faux en modifiant la légende de sorte qu'elle ne décrive plus l'image. Nous effectuons ces altérations de phrases en nous concentrant sur six éléments spécifiques, tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la référence, où chaque élément peut consister en un ou plusieurs instruments. Si nous trouvons plus d'une manière intéressante de créer des instances de faux, par exemple, dans le cas de l'élément actions, nous avons deux instruments, l'un où le verbe d'action est remplacé par un autre, et l'autre où les actants sont inversés. Le comptage et la référence sont également des éléments qui ont plus d'un instrument. Et nous créons ces faux en veillant à ce qu'ils ne décrivent pas l'image, qu'ils soient grammaticalement corrects et valides à part entière. Ce n'est pas facile, car une légende contrefaite peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme qu'un homme coupe des plantes, et de grands modèles vision-langage pourraient s’en apercevoir. Par conséquent, pour obtenir des faux valides, nous prenons d'abord des mesures : nous utilisons d'abord de puissants modèles de langage pour proposer des faux, nous utilisons ensuite l'inférence du langage naturel (NLI) pour filtrer les faux qui pourraient encore décrire l'image. En effet, lors de la construction de faux, nous devons nous assurer qu'ils ne décrivent pas l'image. Pour tester cela automatiquement, nous appliquons l'inférence du langage naturel avec la justification suivante : nous considérons une image comme la prémisse et sa légende comme l'hypothèse impliquée. De plus, nous considérons la légende comme la prémisse et le faux comme l'hypothèse. Si un modèle NLI prédit que le faux contredit ou est neutre par rapport à la légende, nous prenons cela comme un indicateur d'un faux valide. Si un modèle NLI prédit que le faux est impliqué par la légende, il ne peut pas être un bon faux, car, par transitivité, il fournira une description véridique de l'image, et nous filtrons ces faux. Mais cette procédure n'est pas parfaite, elle n'est qu'un indicateur de faux valides. Par conséquent, en tant que troisième mesure pour générer des faux valides, nous employons des annotateurs humains afin de valider les données utilisées dans VALSE. Après filtrage et évaluation humaine, nous avons autant d'instances de test que décrites dans ce tableau. Notez que VALSE ne fournit aucune donnée d'apprentissage, mais uniquement des données de test, car il s'agit d'un benchmark de test à zéro tir. Il est conçu pour tirer parti des capacités existantes des modèles vision-langage après le pré-entraînement. L'ajustement fin ne permettrait aux modèles que d'exploiter les artefacts ou les biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés par l'évaluation des capacités des modèles vision-langage après le pré-entraînement. Nous expérimentons avec cinq modèles vision-langage sur VALSE, notamment CLIP, AlexNet, BERT, WILBERT, WILBERT 12 en 1 et VisualBERT. Deux de nos mesures d'évaluation les plus importantes sont la précision des modèles dans la classification des paires image-phrase en légendes et faux. Peut-être plus pertinente pour cette vidéo, nous montrerons notre métrique plus permissive, la précision par paires, qui mesure si le score d'alignement image-phrase est supérieur pour la paire image-texte correcte que pour sa paire contrefaite. Pour plus de métriques et de résultats, consultez notre article. Les résultats avec la précision par paires sont présentés ici et ils sont cohérents avec les résultats que nous avons obtenus avec les autres métriques. Il est intéressant de noter que les meilleures performances à zéro tir sont obtenues par WILBERT 12 en 1, suivies de WILBERT, AlexNet, CLIP et enfin VisualBERT. Il est notable que les instruments axés sur des objets individuels, tels que l'existence et les groupes nominaux, sont presque résolus par WILBERT 12 en 1, ce qui met en évidence le fait que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos scénarios de faux adverses. Nous voyons, avec les instruments de pluralité et de comptage, que les modèles vision-langage ont du mal à distinguer les références à des objets uniques ou multiples, ou à les compter dans une image. L'élément relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre les objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même si cela est étayé par des biais de plausibilité, comme nous le constatons dans l'élément actions. À partir de l'élément référence, nous découvrons qu'il est également difficile pour les modèles vision-langage de retracer de multiples références au même objet dans une image en utilisant des pronoms. En tant que test de bon sens et parce que c'est une expérience intéressante, nous avons également calibré deux modèles de texte uniquement, GPT-1 et GPT-2, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et du faux, sans image. Si la perplexité est plus élevée pour le faux, nous prenons cela comme une indication que la légende contrefaite peut souffrir de biais de plausibilité ou d'autres biais linguistiques. Il est intéressant de constater que, dans certains cas, les modèles textuels GPT ont mieux capturé la plausibilité du monde que les modèles vision-langage. Pour résumer, VALSE est un benchmark qui utilise le prisme des constructions linguistiques pour aider la communauté à améliorer les modèles vision-langage en testant leurs capacités d'ancrage visuel de manière difficile. Nos expériences montrent que les modèles vision-langage identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément existence, mais ont du mal à ancrer leur interdépendance et leurs relations dans les scènes visuelles lorsqu'ils sont obligés de respecter les indicateurs linguistiques. Nous souhaitons vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers l'ancrage linguistique avec les modèles vision-langage. Et encore plus, VALSE pourrait être utilisé comme une évaluation indirecte des ensembles de données, les modèles pouvant être évalués avant et après l'apprentissage ou l'ajustement fin afin de voir si un ensemble de données permet aux modèles de s'améliorer sur l'un des aspects testés par VALSE. Si vous êtes intéressé, consultez les données de VALSE sur GitHub et, si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamisura et je suis de l'Université de Tokyo. Je vais présenter une communication intitulée « O En Sum : un système à grande échelle pour la génération automatique de notes de publication ». Je vais expliquer l'ordre de la manière suivante : tout d'abord, je présenterai la notation automatique des commits sur laquelle nous travaillons dans cette recherche. Une note de publication est un document technique qui résume les modifications distribuées avec chaque version d'un produit logiciel. L'image montre les notes de publication pour la version 2.6.4 de la bibliothèque Bujs. Ces notes jouent un rôle important dans le développement open source, mais elles sont longues à préparer manuellement. Il serait donc très utile de pouvoir générer automatiquement des notes de publication de haute qualité. Je ferai référence à deux recherches antérieures sur la génération automatique de nœuds de listes : la première est un système appelé Alena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant un extracteur de règles pour extraire les différences essentielles, les modifications de la bibliothèque et les modifications de la documentation à partir des différences entre les versions, puis en les combinant. La caractéristique la plus remarquable de ce système est l'extraction de problèmes dans le coin supérieur droit, qui doit être lié à Jira, l'écosystème de gestion des problèmes, et ne peut être appliqué qu'aux projets qui utilisent Jira, c'est-à-dire qu'il ne peut pas être utilisé pour de nombreux projets sur GitHub. Le second est Grif, annoncé récemment en 2020. Il est disponible sur Internet et peut être stocké via Pi. Ce système possède un modèle de classification de texte simple basé sur un réseau neuronal et produit l'une des cinq catégories telles que les fonctionnalités ou les corrections de bogues pour chaque message de commit en entrée. L'image montre un exemple d'utilisation qui renvoie une correction de bogue correcte. Les données d'entraînement utilisées sont relativement limitées, environ 5000 exemples, et seront présentées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n'est pas très élevée. Je présente deux recherches connexes, mais elles présentent des problèmes de portée limitée et de ressources de données restreintes. Notre communication résout ces deux problèmes et génère automatiquement des notes de publication de haute qualité. Pour le problème de portée limitée, nous proposons une méthode de summarisation par classification de haute qualité utilisant uniquement le message de commit comme entrée. Cette méthode peut être utilisée pour tous les référentiels en anglais. Pour le deuxième problème de ressources de données limitées, nous avons créé notre propre ensemble de données comprenant environ 82 000 exemples en corrigeant des données provenant de référentiels publicitaires GitHub en utilisant l'API Git. Ensuite, je décrirai notre ensemble de données. Voici un exemple : le côté gauche est un message de commit et le côté droit est la note de publication. Les notes de publication sont classées selon des niveaux d'amélioration, corrections de bogues, etc. Nous avons mis en place une tâche qui prend les messages de commit comme entrée et produit les notes de publication correspondantes. Ceci peut être considéré comme une tâche de summarisation. Nous avons prédéfini quatre niveaux : fonctionnalités, implémentations, corrections de bogues, dépréciations, suppressions et modifications incompatibles. Ces niveaux ont été établis sur la base d'utilisations antérieures et d'autres facteurs. Il y a une note en bas à droite extraite de la note de publication affichée en bas à gauche. À l'heure actuelle, il est nécessaire de détecter les quatre niveaux qui ont été définis à l'avance, mais les niveaux ne sont pas toujours cohérents avec chaque notation. Par exemple, le niveau d'amélioration inclut les améliorations, les optimisations, etc. Nous avons préparé une liste de vocabulaire de niveaux d'étude pour chacune de ces variations notationnelles. Nous l'utilisons pour détecter la classe de la note de publication et corriger le texte qui suit en tant que phrase de la note de publication pour cette classe. Ensuite, un message de commit. Les messages de commit ne sont pas liés à chaque note de publication comme indiqué dans l'image ci-dessous. Si la version actuelle est 2.5.19, nous devons identifier la version précédente 2.5.18 et l'obtenir. Ceci est un peu fastidieux, et il ne suffit pas de simplement obtenir une liste de versions et de comparer les versions avant et après. Nous avons créé une logique de correspondance heuristique pour obtenir les données des versions précédentes et suivantes. À la fin, 7200 référentiels et 82 000 exemples de données ont été corrigés. De plus, le nombre moyen de jetons significatifs est de 63, ce qui est assez élevé pour une tâche de summarisation. De même, le nombre de jetons uniques est assez riche avec huit mille huit cent trente. Ceci est dû au grand nombre de classes et de noms de méthodes uniques trouvés dans le référentiel. Ensuite, j'expliquerai la méthode proposée. Le modèle de summarisation extractive et abstraite croisée consiste en deux modules neuronaux : un classificateur utilisant Bot ou Code Bot et un générateur utilisant But First G. Le classificateur est utilisé pour classer chaque message de commit dans cinq classes de notes de publication : fonctionnalités, améliorations, corrections de bogues, dépréciations, et autres. Les messages de commit classés comme « autres » sont écartés. Ensuite, le générateur est appliqué aux documents de notes de publication correspondants indépendamment et génère une note de publication pour chaque classe. Dans cette tâche, il n'y a pas de correspondance directe entre les messages de commit et les notes de publication. Par conséquent, pour entraîner le classificateur, nous attribuons des pseudo-variables à chaque message de commit en utilisant les 10 premiers caractères de chaque message de commit. Nous modélisons l'approche de summarisation abstraite par classe en définissant deux méthodes. Le premier modèle, que nous appelons GS Single, est composé d'un seul réseau neuronal et génère un long texte de note unique en concaténant les messages de commit d'entrée. Le texte de sortie peut être divisé en segments de classe en fonction de symboles de fin spécifiques à la classe. La deuxième méthode, que nous appelons GS Much, se compose de quatre réseaux neuronaux différents, chacun correspondant à l'une des classes de notes de publication. Bien, laissez-moi vous expliquer l'expérience. Cinq méthodes ont été comparées : GS, GS Single, GS Much, Cluster et une étude précédente, Grif. Dans certains cas, ces notes sont produites en plusieurs phrases. Étant donné qu'il est difficile de corriger le nombre de phrases à zéro, elles sont combinées avec des espaces et traitées comme une longue phrase. La pénalité bleue est appliquée lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur de bre inférieure dans les résultats d'expériences décrits ci-dessous. Nous avons également calculé une spécificité, car le bleu et le bleu ne peuvent pas être calculés si les notes de publication sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de publication sont censées être vides. Voici les résultats. Étant donné que l'ensemble de données contient des analyses de code, etc., nous avons également évalué l'ensemble de données nettoyé qui les exclut. GS et GS ont obtenu des scores d'erreur de perte plus de 10 points plus élevés que la référence. En particulier, sur l'ensemble de test coréen, l'écart de score entre la méthode proposée et la référence a dépassé 20 points. Ces résultats indiquent que GS et GS sont significativement efficaces. GS a obtenu un meilleur score de perte que GS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour entraîner le classificateur avec des serveurs. Une large couverture de GS peut être obtenue correctement car le classificateur peut se concentrer sur la sélection des messages de commit pertinents pour chaque classe. GS Much a tendance à obtenir un score plus élevé que GS Single, ce qui suggère qu'il est également efficace de développer indépendamment différents modèles de summarisation constructive pour chaque classe de notes de publication. Voici une analyse d'erreur. Les méthodes GS tendent à produire des phrases plus courtes que les phrases de référence humaines. En effet, dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que GS en a seulement une. La raison de cette réticence du modèle est que dans les données d'entraînement, seulement 30 % des phrases sont présentes au niveau des fonctionnalités et 40 % au niveau des améliorations. De plus, les méthodes GS ne peuvent pas générer de notes de publication précises sans informations supplémentaires. L'exemple en haut à droite est un exemple de message de commit très désordonné et la phrase complète ne peut pas être générée sans référence au pragmatique ou au problème correspondant. L'exemple ci-dessous montre que les deux messages de commit dans l'entrée sont liés et devraient être combinés en une seule phrase, mais cela ne se produit pas. Finalement, en conclusion, nous avons créé un nouvel ensemble de données pour la génération automatique de notes de publication. Nous avons également formulé la tâche de prendre les messages de commit en entrée et de les résumer de manière à ce qu'elle soit applicable à tous les projets écrits en anglais. Nos expériences montrent que la méthode proposée génère des notes de publication moins bruyantes et avec une couverture plus large que la référence. Veuillez consulter nos données sur GitHub. Merci."}
