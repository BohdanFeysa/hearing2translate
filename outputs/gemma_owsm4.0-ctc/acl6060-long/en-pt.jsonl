{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Safari e apresentarei nosso artigo sobre enriquecimento de dados tabulares com Transformers ajustados. Assim, cientistas analisam dados e se concentram principalmente na manipulação das características existentes, mas, por vezes, essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Assumimos que temos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva o vinculação de entidades e análise de texto para extrair novas características da base de conhecimento de texto livre. Nosso framework, Fast, é exatamente esse processo automático. Vamos ver um exemplo em conjuntos de dados alimentados ao Fast. Neste exemplo, o conjunto de dados é o conjunto de dados de universidades, cujo objetivo é classificar universidades em universidades de baixo ranking e universidades de alto ranking. Usamos a Wikipédia como base de conhecimento. A primeira fase do Fast é a vinculação de entidades, quando cada entidade, neste exemplo, o nome da universidade, é vinculada a uma entidade dentro da base de conhecimento e o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipédia. Agora, precisamos gerar ou extrair características do texto recuperado, e precisamos de uma fase de extração de características que inclua a análise de texto. Esta é a principal novidade deste artigo, e farei uma análise aprofundada nas próximas slides. Após a fase de extração de características, há uma fase de geração de características, em que usamos as características extraídas para gerar um pequeno número de novas características. Geramos características em número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então geramos duas novas características. Mas, se o conjunto de dados tiver cinco classes, geramos cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado atual da análise de texto fora do domínio, que são modelos de linguagem baseados em transformadores, como BERT, GPT-3, etc. É provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada, então uma abordagem ingênua seria o ajuste fino da tarefa alvo. Na fase de extração de características, podemos baixar um modelo de linguagem pré-treinado e ajustá-lo ao conjunto de dados alvo. Neste exemplo, para ajustar o modelo de linguagem para classificar textos em classes, o resumo em classes, baixo ou alto, recebemos a saída do modelo de linguagem, que é a probabilidade para cada classe, e usamos como novas características. O problema com essa abordagem é que o conjunto de dados pode ter poucas entidades distintas. Em nossos experimentos, quase metade dos conjuntos de dados contém menos de 400 amostras e o menor conjunto de dados contém 35 amostras no conjunto de treinamento inicial. Para ajustar um modelo de linguagem sobre esse conjunto de dados, seria ineficaz, mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados. Porque o Fast é aplicado a vários conjuntos de dados, podemos usar os n menos 1 conjuntos de dados para coletar informações sobre os n menos 1 conjuntos de dados e usar essa informação quando analisamos o enésimo conjunto de dados. O que sugerimos é adicionar outra fase de ajuste fino, uma fase preliminar de ajuste fino multitarefa, quando o modelo de linguagem é ajustado sobre os n menos 1 conjuntos de dados e, então, executamos outra fase de ajuste fino, que é o ajuste fino da tarefa alvo, quando ajustamos o modelo de linguagem sobre o enésimo conjunto de dados alvo. O estado da arte no ajuste fino multitarefa, chamado DNN, o DNN mantém cabeças no número de tarefas no conjunto de treinamento, então, neste exemplo, se houver quatro tarefas no conjunto de treinamento, o DNN mantém quatro cabeças, como se pode ver na imagem, e amostra um lote aleatório de um do conjunto de treinamento e, se o lote aleatório pertencer, por exemplo, a uma tarefa de classificação de frases, ele executa uma passagem para frente e para trás através da primeira cabeça e, se o lote aleatório pertencer a uma tarefa de classificação por pares, ele atua e passa para trás através da última cabeça. Em nosso cenário, um conjunto de dados tabular varia o número de classes, então existem muitas tarefas. O DNN mantém o número de classes de cabeças e camadas de saída. Adicionalmente, o DNN precisa, inicialmente, criar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem, chamada ajuste fino de reformulação de tarefas, é que, em nossa abordagem de ajuste fino de reformulação de tarefas, em vez de manter várias cabeças, reformulamos cada conjunto de dados em um problema de classificação de frases por frases, que são tarefas de duas classes. Vamos ver um exemplo aqui. Este é o nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes, e reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o resumo e a classe em verdadeiro ou falso ou, em outras palavras, treinamos o modelo de linguagem para classificar um resumo e uma classe como a classificar um resumo e a classe se o resumo pertence à classe ou não. Portanto, o vetor de rótulo no caso de z permanece sempre, que consiste sempre com duas classes. Este é o algoritmo para nossa abordagem de ajuste fino formulado. Vamos ver o framework completo. Um conjunto de dados é alimentado ao Fast e, então, o Fast executa a fase de vinculação, extrai o texto da base de conhecimento, que, neste exemplo, é o resumo da página da Wikipédia, então reformula a tarefa em tarefas de classificação de frases por frases, aplica o modelo de linguagem à nova tarefa e a saída é a probabilidade para cada classe. Observe que o modelo de linguagem já foi ajustado sobre n menos 1 conjuntos de dados usando um ajuste fino multitarefa preliminar. Em seguida, usamos o vetor de saída do modelo de linguagem como uma característica recém-gerada no número de classes. Para avaliar nosso framework, usamos um conjunto de dezessete dados de classificação tabular, que define tamanho de recursos, domínio balanceado e desempenho inicial. Usamos a Wikipédia como base de conhecimento. Projetamos nosso experimento como uma avaliação de exclusão de um, quando treinamos o Fast sobre 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro dobras e aplicamos uma validação cruzada de quatro dobras. Geramos as novas características e as avaliamos usando cinco classificadores de avaliação. Usamos arquiteturas baseadas em pássaros em nosso experimento. Aqui estão os resultados para nosso experimento. Você pode ver que comparamos nosso framework com o ajuste fino do conjunto de dados alvo, o ajuste fino da tarefa alvo e o ajuste fino multitarefa preliminar e nosso ajuste fino de reformulação alcança o melhor resultado, o melhor desempenho, enquanto o m t d n n e alcança uma melhoria de dois por cento sobre o ajuste fino do conjunto de dados alvo. Nossa abordagem alcança uma melhoria de seis por cento quando olhamos nos pequenos conjuntos de dados. Podemos ver que o desempenho do m t d n n diminui e a melhoria da fase preliminar de ajuste fino multitarefa diminui para um ponto cinco por cento, mas nosso desempenho aumenta para 11 por cento em comparação com o ajuste fino da tarefa alvo. Para resumir, o Fast permite o enriquecimento com poucos exemplos a partir de 35 amostras em nosso experimento. Ele usa uma arquitetura para todas as tarefas, conjuntos de dados e mantém a cabeça do modelo. Mas adiciona uma fase de reformulação que aumenta o conjunto de treinamento e precisa de um valor alvo com significado semântico, para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação de frases por frases. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje, apresentarei nosso trabalho de pesquisa sobre o aprendizado de raciocínio dedutivo, o problema do método como extração de região complexa. Sou da Biance AI Lab e este é um trabalho conjunto com Che da Universidade de Texas em Austin e Wedu da SUDD. Primeiro, gostaria de falar sobre nossa motivação para o raciocínio. Aqui, estamos mostrando exemplos onde o raciocínio em várias etapas é útil. Esta figura é retirada do artigo \"pound\", onde eles realizam o prompting para resolver o problema do método em um cenário de aprendizado futuro. À esquerda, podemos ver que, se fornecermos algumas amostras com apenas perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas, se fornecermos mais descrições de raciocínio, o modelo é capaz de prever a descrição do motivo e também fazer uma previsão correta aqui. Portanto, é bom ter um raciocínio em várias etapas interpretável como saída, e também pensamos que o problema do método é uma aplicação direta para avaliar essas habilidades de raciocínio. Aqui, no nosso problema, dado as perguntas, precisamos resolver esta questão e obter as respostas numéricas. Em nossos conjuntos de dados, também fornecemos a expressão matemática que leva a essa resposta particular. Certas suposições também se aplicam, como no trabalho anterior, assumimos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtração, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser realmente decompostos nesses operadores básicos. Trabalhos anteriores na resolução de problemas do método podem ser categorizados em sequência para sequência e modelo de árvore para sequência. O modelo de sequência para sequência tradicional converte a expressão em uma sequência específica para geração e é bastante fácil de implementar e pode generalizar para muitos problemas complicados diferentes, mas a desvantagem do desempenho não é geralmente melhor do que o modelo estrutural e falta a interpretabilidade para a previsão. Mas esta direção ainda é bastante popular devido ao modelo transformer. Em modelos baseados em árvores, estruturamos essas expressões na forma de árvore e seguimos uma travessia pré-ordem na geração de árvores. Aqui, continuamos gerando os operadores até atingir as folhas, que são as quantidades. Aqui, a boa coisa é que isso nos fornece esta estrutura de árvore binária e é, mas, mas, mas na verdade é bastante contraintuitivo porque geramos o operador primeiro e, no final, geramos as quantidades. A segunda coisa é que também contém alguns cálculos repetitivos. Aqui, se olharmos para esta expressão, a vezes 3 mais 3 é realmente gerada duas vezes, mas, na verdade, deveríamos reutilizar os resultados. Em nossa abordagem proposta, queremos resolver esses problemas de maneira passo a passo e interpretável. Por exemplo, aqui, no segundo passo, podemos obter esses divisores, que são 27, e também podemos fazer referência de volta à pergunta original para encontrar o conteúdo relevante e nesses passos, obtemos os divisores. E, então, neste terceiro passo, obtemos efetivamente o quociente. Após esses três passos, podemos realmente reutilizar os resultados do segundo passo e, em seguida, obter os resultados do quarto passo e, finalmente, podemos obter os dividendos. Aqui, geramos efetivamente a expressão inteira diretamente, em vez de gerar um único operador ou quantidade. Isso torna o processo mais preciso. Em nosso sistema dedutivo, começamos primeiro com um monte de quantidades apresentadas nas perguntas e também incluindo algumas constantes como nosso estado inicial. A expressão é representada por eij, onde realizamos o operador de qi para qj e essa expressão é, na verdade, direcionada. Também temos subtração reversa aqui para representar a direção oposta. Isso é bastante semelhante à extração de relação. Em um sistema dedutivo formal, no passo de tempo t, aplicamos o operador entre o par qi e qj e, em seguida, obtemos esta nova expressão. Adicionamos à aos próximos estados para se tornar uma nova quantidade. Este slide visualiza efetivamente a evolução dos estados, onde continuamos adicionando expressões aos estados atuais. Em nossas implementações de modelo, primeiro usamos um modelo pré-treinado, que pode ser birds ou rawhoods, e então codificamos a frase e obtemos essas representações de quantidade. Uma vez que obtemos as representações de quantidade, podemos começar a fazer inferência aqui. Mostramos um exemplo de q1 para obter a representação de q1 dividido por q2 e, em seguida, vezes q3. Primeiro, obtemos a representação de par, que é basicamente apenas a concatenação entre q1 e q2 e, em seguida, aplicamos uma rede feed forward, que é parametrizada pelo operador e, finalmente, obtemos a representação da expressão q1 dividido por q2, mas, na prática, no estágio de inferência, podemos, na verdade, obter a expressão incorreta também. Aqui, todas as expressões possíveis são iguais a três vezes o número de operadores. A coisa boa aqui é que podemos facilmente adicionar restrições para controlar este espaço de busca. Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão em nosso espaço de busca. No segundo passo, fazemos a mesma coisa, mas a única diferença é que há uma quantidade a mais. Esta quantidade vem da expressão calculada anteriormente. Finalmente, podemos obter esta expressão final q3 vezes q4 e também podemos ver que o número de todas as expressões possíveis é diferente do passo anterior. Tal diferença torna difícil aplicar a busca em feixe porque a distribuição de probabilidade entre esses dois passos é desequilibrada. O procedimento de treinamento é semelhante ao treinamento de um modelo sequência para sequência, onde otimizamos a perda em cada passo de tempo e aqui também usamos este tau para representar quando devemos terminar este processo de geração e aqui o espaço é diferente de sequência para sequência porque o espaço é diferente em cada tempo, enquanto em um modelo sequência para sequência tradicional, é o número de vocabulário e também permite impor certas restrições a partir do conhecimento anterior. Conduzimos experimentos no conjunto de dados de problema do método comumente usado, mwps method3k math qaA e swam. Aqui, mostramos brevemente os resultados em comparação com as melhores abordagens anteriores. Nosso melhor desempenho é Roberta deductive reason e, na verdade, não usamos busca em feixe em contraste com abordagens óbvias que usam busca em feixe. As melhores abordagens são frequentemente modelos baseados em árvores. No geral, nosso racionador é capaz de selecionar significativamente a saída deste modelo baseado em árvore, mas podemos ver que o número absoluto em math qaA ou swam não é muito alto. Investigamos ainda mais os resultados em swam e este conjunto de dados é desafiador porque o autor tentou adicionar manualmente algo para confundir o modelo de NLP, como adicionar informações disponíveis e quantidades extras. Em nossa previsão, descobrimos que alguns dos valores intermediários são realmente negativos, por exemplo, nessas perguntas, estamos perguntando quantas maçãs Jake tem, mas temos algumas informações extras, como 17 menos pitchachees e Stephen tem oito pitchachees, que é totalmente irrelevante. Nosso modelo faz algumas previsões como esta, produzindo valores negativos e observamos que essas duas expressões realmente têm pontuações semelhantes, então podemos realmente limitar este espaço de busca removendo como esses resultados são negativos para que possamos fazer a resposta correta. Encontramos ainda que tal restrição realmente melhora bastante para alguns modelos, por exemplo, para birds, melhoramos em sete pontos e, para o modelo baseado em Roberta, realmente melhoramos em dois pontos. Um melhor modelo de linguagem tem melhores habilidades de compreensão da linguagem, então o número aqui é maior para Roberta e menor para birds. Também tentamos analisar a dificuldade por trás de todos esses conjuntos de dados. Assumimos que o número de quantidades não utilizadas pode ser considerado como informação relevante aqui. Aqui, podemos ver que temos o lump a porcentagem de amostras não utilizamos quantidades e o conjunto de dados swam tem a maior parte e aqui também mostramos o desempenho geral para essas amostras sem quantidades não utilizadas. O desempenho geral é na verdade maior do que o desempenho com essas amostras que com quantidades não utilizadas. Para formWps, não realmente temos muitos casos de morte, então apenas ignoro esta parte. Finalmente, queremos mostrar a interpretabilidade por meio de um exemplo de apresentação de falha. Aqui, nosso modelo realmente comete uma previsão errada no primeiro passo, então podemos realmente correlacionar esta expressão com a frase aqui. Acreditamos que esta frase possa estar enganando o modelo para uma previsão incorreta. Aqui, imprimir 35 faz o modelo pensar que deve ser um operador de adição, então tentamos revisar a frase para algo como o número de árvores de pereira é 5 a menos do que as árvores de maçã, para que ela transmita uma semântica mais precisa para que o modelo possa fazer a previsão correta. Este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo. Para concluir nosso trabalho, nosso modelo é realmente eficiente e podemos fornecer um procedimento de solução interpretável e podemos facilmente incorporar algum conhecimento anterior como restrição que pode ajudar a melhorar o desempenho. E a última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de método, mas também a outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada entre diferentes momentos, também é bastante desafiador aplicar uma estratégia de busca em feixe. Este é o fim da apresentação e perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Apresentarei meu trabalho com Jerry, que trata de um novo conjunto de dados para recuperação de artigos estatutários. Questões legais são parte integrante da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem arcar com a assistência dispendiosa de um especialista legal ficam desprotegidos ou, pior, explorados. O trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema de recuperação eficaz para artigos estatutários. Um sistema desse tipo poderia fornecer um serviço de assistência jurídica profissional gratuita para indivíduos não especializados antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre uma questão legal, como \"o que eu arrisco se violar a confidencialidade profissional\", um modelo é necessário para recuperar todos os artigos estatutários relevantes de um grande corpo de legislação. Esta tarefa de recuperação de informações apresenta seus próprios desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem legal complexa para os estatutos. Essa diferença na distribuição da linguagem dificulta a recuperação de candidatos relevantes por um sistema, pois, indiretamente, requer um sistema de interpretação inerente que possa traduzir uma pergunta natural em uma pergunta legal que corresponda à terminologia dos estatutos. Além disso, a lei estatutária não é uma pilha de artigos independentes que podem ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas no contexto geral, ou seja, juntamente com informações complementares de seus artigos vizinhos, os campos e subcampos a que pertencem e seu lugar na estrutura da lei. Por último, os artigos estatutários estão em parágrafos pequenos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação. Aqui, são documentos longos que podem ter até 6.000 palavras. Os avanços recentes em PLN despertaram um grande interesse em muitas tarefas legais, como a previsão de decisões judiciais ou a revisão automática de contratos. No entanto, a recuperação de artigos estatutários permaneceu principalmente intocada devido à falta de grandes conjuntos de dados rotulados de alta qualidade. Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão nativo da França para estudar se um modelo de recuperação pode aproximar a eficiência e a confiabilidade de um especialista legal para a tarefa de recuperação de artigos estatutários ou recuperação de dados de artigos estatutários belgas. O conjunto de dados consiste em mais de 1.100 perguntas legais feitas por cidadãos belgas. Essas perguntas abrangem uma ampla gama de tópicos, de família, habitação, dinheiro a trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de vinte e dois mil seiscentos e sessenta artigos legais dos códigos de lei belgas. Vamos agora falar sobre como coletamos este conjunto de dados. Primeiro, começamos compilando um grande corpus de artigos legais. Consideramos 32 códigos belgas publicamente disponíveis e extraímos todos os seus artigos, bem como os títulos das seções correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, estabelecemos parceria com um escritório de advocacia belga que recebe a cada ano cerca de 400 e-mails de cidadãos belgas que pedem conselhos sobre uma questão legal pessoal. Tivemos a sorte de ter acesso aos seus sites, onde sua equipe de juristas experientes aborda os problemas legais mais comuns da Bélgica. Coletamos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por fim, passamos as referências legais e filtramos as perguntas cujas referências não eram artigos de um dos códigos de lei que estávamos considerando. As referências restantes foram correspondidas e convertidas nos IDs de artigo correspondentes de nosso corpus. Acabamos com mil cento e oito perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes de um grande corpus de vinte e dois mil seiscentos e trinta e três artigos estatutários, além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação de seus títulos subsequentes na estrutura da lei. Essas informações extras não são usadas no presente trabalho, mas podem ser de interesse para pesquisas futuras sobre recuperação de informações legais ou classificação de texto legal. Vamos analisar algumas características de nossos conjuntos de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com um comprimento médio de 77 palavras, com 142 deles excedendo 1.000 palavras, sendo o mais longo com até cinco mil setecentos e noventa palavras. Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os 15% restantes se referem a segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de 32 códigos belgas diferentes que abrangem um grande número de tópicos ilegais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Dos vinte e dois mil seiscentos e trinta e três artigos, apenas 1.612 são referenciados como relevantes para pelo menos uma pergunta no conjunto de dados e cerca de 80% desses artigos citados vêm do Código Civil, Códigos Judiciais, Código de Investigação Criminal ou Códigos Penais. Enquanto isso, 18 de 32 códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esses códigos se concentram menos em indivíduos e suas preocupações. No geral, o número médio de citações para esses artigos citados é dois e menos de 25% deles são citados mais de cinco vezes. Usando nossos conjuntos de dados, comparamos vários métodos de recuperação, incluindo arquiteturas lexicais e densas. Dado uma consulta e um artigo, um modelo lexical atribui uma pontuação ao par consulta-artigo calculando a soma sobre os termos da consulta dos pesos de cada um desses termos nesse artigo. Experimentamos as funções de classificação padrão TFIDF e bm25. O principal problema com essas abordagens é que elas só podem recuperar artigos que contenham palavras-chave presentes na consulta. Para superar essa limitação, experimentamos uma arquitetura baseada em rede neural que pode capturar as relações semânticas entre as consultas e os artigos. Usamos um modelo b encoder que mapeia consultas e artigos para representações vetoriais densas e calcula uma pontuação de relevância entre um par consulta-artigo pela similaridade de seus embeddings. Esses embeddings geralmente resultam de uma operação de pooling na saída de um modelo de embedding de palavras. Primeiro, estudamos a eficácia dos Siamesebiancoders em um ambiente de avaliação sem conhecimento, significando que modelos de embedding de palavras pré-treinados são aplicados diretamente, sem nenhum ajuste fino adicional. Experimentamos com codificadores de texto independentes do contexto, como word to vec e fast text, e modelos de embedding dependentes do contexto, como Roberta e, mais especificamente, camembert, que é um modelo roberta francês. Além disso, treinamos nosso próprio modelo biancors baseado em camembert. Observe que, para o treinamento, experimentamos as duas variações da arquitetura biancoder: Siameses, que usa um modelo de embedding de palavras único que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado, e torre dupla, que usa dois modelos de embedding de palavras independentes que codificam a consulta e o artigo separadamente em espaços de embedding diferentes. Experimentamos com pooling de média, máxima e CLls, bem como produto escalar e cosseno para calcular similaridades. Aqui estão os resultados de um benchmark em conjuntos de dados de teste com os métodos lexicais acima, os encoders Siamesebian avaliados em um cenário sem conhecimento no meio e os encoders ajustados abaixo. No geral, os biancoders ajustados superam significativamente todas as outras linhas de base. O modelo torre dupla melhora em relação à sua variante siamesa em recall em 100, mas tem um desempenho semelhante nas outras métricas. Embora bm25 tenha se saído inferiormente em comparação com o biancoder treinado, seu desempenho indica que ele ainda é uma forte linha de base para recuperação específica de domínio. Quanto à avaliação sem conhecimento dos Siamesebiancoders, descobrimos que o uso direto dos embeddings de um modelo camembert pré-treinado sem otimizar para a tarefa de recuperação de informações fornece resultados ruins, o que é consistente com descobertas anteriores. Além disso, observamos que o encoder biancoder baseado em wordto-vec superou significativamente os modelos baseados em fast text e bird, sugerindo que talvez os embeddings de nível de palavra pré-treinados sejam mais apropriados para a tarefa do que os embeddings de nível de caractere ou subpalavra quando usados diretamente. Embora promissores, esses resultados sugerem amplas oportunidades de melhoria em comparação com um especialista de nível de habilidade que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Vamos concluir discutindo duas limitações de nossos conjuntos de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos 32 códigos belgas considerados, o que não cobre toda a lei belga, pois faltam artigos de decretos, diretivas e ordens. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas terminem com apenas uma fração do número inicial de artigos relevantes. Essa perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja totalmente apropriada. Em segundo lugar, devemos observar que nem todas as perguntas legais podem ser respondidas com estatutos sozinhos. Por exemplo, a pergunta \"Posso despejar meus inquilinos se eles fizerem muito barulho?\" pode não ter uma resposta detalhada na lei estatutária que quantifique um limite específico de ruído no qual o despejo é permitido. Em vez disso, o proprietário deve provavelmente confiar mais na jurisprudência e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz duas festas por semana até as 2h, portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários e o domínio das menos adequadas ainda precisa ser determinado. Esperamos que este trabalho desperte o interesse no desenvolvimento de modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso artigo dotset encode nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, estamos felizes em apresentar nosso trabalho sobre voais, um benchmark independente criado para testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de configurar este benchmark? Bem, nos últimos anos, temos testemunhado uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares imagem-texto. Cada um desses modelos impulsiona o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagem, ancoragem de frases, então recebemos uma mensagem: as precisões nesses benchmarks específicos de tarefa estão aumentando constantemente, mas sabemos o que os modelos realmente aprenderam? O que é que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta para esta imagem e esta frase para corresponder e uma pontuação baixa para esta outra? Os modelos de visão e linguagem estão focados no certo, ou estão focados em vieses, como demonstrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica em relação à tarefa e introduzimos voais, que testa a sensibilidade de modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguísticas quanto as visuais. Almejamos existência, pluralidade, contagem, relações espaciais, ações e coferência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos, aplicando um método anteriormente usado apenas para modelos de visão e linguagem para frases nominais, por Ravi Shekhar e colaboradores, e para contagem, por nós em trabalhos anteriores? A aplicação básica significa que pegamos a legenda de uma imagem e produzimos uma aplicação alterando a legenda de forma que ela não descreva mais a imagem. Fazemos essas alterações de frases, concentrando-nos em seis partes específicas, como existência, pluralidade, contagem, relações espaciais, ações e coferência de entidades, onde cada parte pode consistir de um ou mais instrumentos, no caso de encontrarmos mais de uma maneira interessante de criar instâncias de aplicação. Por exemplo, no caso da parte de ações, temos dois instrumentos, um no qual o verbo de ação é alterado com uma ação diferente e outro no qual os actantes são trocados. Contagem e correferência também são partes que têm mais de um instrumento. E criamos essas aplicações garantindo que elas falhem em descrever a imagem, que sejam frases gramaticais e, de outra forma, válidas. Isso não é fácil, pois uma legenda aplicada pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem corte plantas, e grandes modelos de visão e linguagem podem captar isso. Portanto, para obter aplicações válidas, devemos agir primeiro: utilizamos modelos de linguagem fortes para propor aplicações, segundo, utilizamos inferência de linguagem natural, ou ILN, para filtrar aplicações que ainda possam estar descrevendo a imagem, pois, ao construir aplicações, precisamos garantir que elas falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte motivação: consideramos a imagem como a premissa e sua legenda como a hipótese envolvida. Além disso, consideramos a legenda como a premissa e a aplicação como a hipótese. Se um modelo de ILN prevê que a aplicação contradiz ou é neutra em relação à legenda, consideramos isso como um indicador de uma aplicação válida. Se uma ILN prevê que a aplicação é envolvida pela legenda, ela não pode ser uma boa aplicação, pois, por transitividade, dará uma descrição verdadeira da imagem, e filtramos essas aplicações. Mas este procedimento não é perfeito; é apenas um indicador de aplicações válidas. Portanto, como uma terceira medida para gerar aplicações válidas, empregamos anotadores humanos para validar os dados usados em voais. Assim, após a filtragem e avaliação humana, temos tantos exemplos de teste quanto descritos nesta tabela. Note que voais não oferece nenhum dado de treinamento, mas apenas dados de teste, já que é um benchmark de teste de tiro zero. É projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino apenas permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos nós sabemos que esses modelos gostam de trapacear e tomar atalhos. E, como dissemos, estamos interessados em avaliar as capacidades que os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem em voais, nomeadamente com clip, alex, mert, wilbert, wilbert 12 em 1 e visual bird. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos na classificação de pares imagem-frase em legendas e aplicações. Talvez mais relevante para este vídeo, demonstraremos nossa métrica permissiva, a precisão por pares, que mede se a pontuação de alinhamento imagem-frase é maior para o par imagem-texto correto do que para sua aplicação. Para mais métricas e resultados sobre elas, confira nosso artigo. Os resultados com precisão por pares são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas. Isso mostra que o melhor desempenho em tiro zero é alcançado por wilbert 12 em 1, seguido por wilbert, alex, mert e, finalmente, visual bird. É notável como os instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos por wilbert 12 em 1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das partes restantes pode ser resolvida de forma confiável em nossos cenários de aplicação adversária. Vemos com os instrumentos de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos versus múltiplos ou em contá-los em uma imagem. A parte de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo que sejam suportados por vieses de plausibilidade, como vemos na parte de ações. Da parte de referência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem. Como um teste de sanidade e porque é um experimento interessante, também medimos dois modelos de texto somente, Gpt1 e Gpt2, para avaliar se voais pode ser resolvido por esses modelos unimodais, calculando a perplexidade da legenda correta e da aplicação, sem imagem aqui, e prevendo a entrada com a perplexidade mais baixa. Se a perplexidade for maior para a aplicação, consideramos isso como um indicador de que a aplicação pode sofrer de vieses de plausibilidade ou outros vieses linguísticos. É interessante ver que, em alguns casos, os modelos de texto somente, GPT, capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Em resumo, voais é um benchmark que usa a lente das construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem, testando seus recursos de aterramento visual de forma rigorosa. Nossos experimentos mostram que os modelos de visão e linguagem identificam bem objetos nomeados em sua presença em imagens, como demonstrado pela parte de existência, mas têm dificuldades em aterrar sua interdependência e relacionamentos em cenas visuais quando são forçados a respeitar indicadores linguísticos. Gostaríamos de incentivar a comunidade a usar voais para medir o progresso em direção ao aterramento da linguagem com modelos de visão e linguagem. E até mais, voais pode ser usado como uma avaliação indireta de conjuntos de dados, já que os modelos podem ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados por voais. Se estiver interessado, confira os dados de voais no GitHub e, se tiver alguma dúvida, não hesite em nos contatar."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisura, da Universidade de Tóquio. Apresentarei um artigo intitulado \"O En Sum: Um Desset de Grande Escala para a Não-Duração Automática de Lista via Sumarização Lo Sum.\" Explicarei, nesta ordem, primeiro, introduzirei a notação automática de lista em que estamos trabalhando nesta pesquisa. ReaseNote é um documento técnico que resume as alterações distribuídas com cada lançamento de um produto de software. A imagem mostra as notas de lançamento da versão dois ponto seis ponto quatro da biblioteca Bujs. Estas notas desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas para preparar manualmente; portanto, seria muito útil poder gerar automaticamente notas de lançamento de alta qualidade. Referirei-me a duas pesquisas anteriores sobre geração automática de nós de lista: a primeira é um sistema chamado Alena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de alterações para extrair diferenças principais, alterações de biblioteca e alterações de documentação das diferenças entre doenças, e finalmente combinando-as. A característica mais notável deste sistema é a extração de problemas no canto superior direito, que deve ser vinculado ao Jira, o ecossistema de issues e só pode ser aplicado a projetos que usam Jira; em outras palavras, não pode ser usado para muitos projetos no GitHub. A segunda é Grif, anunciado recentemente em 2020. Está disponível na internet e pode ser armazenado via pi. Este sistema possui um modelo de classificação de texto baseado em regras simples e produz um dos cinco problemas, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna um tape ou correções de bugs correto. Os dados de treinamento são relativamente pequenos, cerca de 5000, e serão mostrados nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas existem problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente recursos de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificador de alta qualidade usando apenas a mensagem do commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos escassos, construímos nossos dados e algumas amostras, consistindo de cerca de 82.000 itens de dados, corrigindo dados de repositórios públicos do Gitub usando a API do Git. Em seguida, descrevo nossos dados aqui. Um exemplo é uma atualização. O lado esquerdo é uma mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são classificadas como melhorias de faces, etc. Configuramos uma tarefa que recebe as mensagens de commit como entrada e produz os coelhos são notas. Isso pode ser considerado uma tarefa de sumarização. Pré-definimos quatro níveis: recursos, implementações, correções de bugs, depreciações, removers e alterações importantes. Estes foram definidos com base no uso anterior e em outros fatores. Há uma nota no canto inferior direito, extraída da nota da lista mostrada no canto inferior esquerdo. Neste momento, é necessário detectar os quatro coelhos que foram definidos, mas os níveis nem sempre são consistentes com cada le. Por exemplo, o nível de melhoria inclui melhorias, aprimoramentos, otimizações e assim por diante. Preparamos uma lista de vocabulário de níveis de estudo para cada uma dessas variações notacionais. Use-o para detectar a classe de nó de risco e corrigir o texto do restante que segue como a frase de risco para a classe. A seguir, está uma mensagem de commit. As mensagens de commit não estão vinculadas a cada corrida, como mostrado na imagem abaixo. Se o lançamento atual for a versão 2.5.19, precisamos identificar a versão anterior, 2.5.18, e obtê-la di. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de lançamentos e olhar para o antes e depois. Criamos uma correspondência heurística de cola para obter os dados da versão anterior e seguinte. A análise de dados terminou, com 7200 repositórios e 82.000 itens de dados corrigidos. Além disso, o número médio de tokens razoáveis é 63, o que é bastante alto para uma tarefa de sumarização. Além disso, o número de tokens únicos é bastante rico, com oito mil, oitocentos e trinta mil. Isso se deve ao grande número de classes e nomes de métodos únicos encontrados no repositório. Em seguida, explicarei o método proposto, o modelo de sumarização extrativa e abstrativa cruzada. Ele consiste em dois módulos neurais: um classificador usando bot ou código bot e o gerador usando but first G. Ele usa um classificador para classificar cada mensagem de commit em cinco classes de nó base: recursos, melhorias, correções de bugs, depreciações, pressão e outros. As mensagens de commit classificadas como outros são descartadas. Então ele aplica um gerador aos quatro documentos de coelho independentemente e gera uma nota de leitura para cada classe. Nesta tarefa, as correspondências diretas entre mensagens de commit e notas de leitura não são conhecidas. Portanto, para treinar o classificador, atribuímos variáveis pseudo a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit. Modelamos a abordagem de sumarização abstrativa específica da classe por dois métodos definidos. O primeiro modelo, que chamamos de GS single, consiste em uma única rede sex e gera um único texto longo não. Dê uma concatenação de mensagens de commit de entrada. O texto de saída pode ser dividido em segmentos de classe de arquivo com base em símbolos de ponto final específicos da classe. O segundo método, que chamamos de shes much, consiste em quatro redes sec diferentes, cada uma das quais corresponde a uma das classes de nó de lista. Ok, deixe-me explicar o experimento. Cinco métodos foram comparados: gs, shes single, shes much, cluster e um estudo anterior, Grif, com relação à aberração em alguns casos. Essas notas são produzidas em várias frases, como é difícil corrigir o número de frases em zero, elas são combinadas com espaços e tratadas como uma única frase longa. O azul é penal quando o sistema produz uma frase curta. Esta penalidade resulta em uma pontuação bre mais baixa nos resultados dos experimentos descritos abaixo. Finalmente, também caricaturamos uma especificidade porque azul e azul não podem ser caricaturados se as notas de lista estiverem vazias. Uma alta especificidade significa que o modelo produz corretamente um texto vazio em casos em que as notas de leitura assumem vazio. Aqui estão os resultados. Como o conjunto de dados contém e-mail, análise tem valores, etc., também avaliamos o conjunto de dados limpo, que os exclui. G e GS alcançaram pontuações de erro de perda mais de 10 pontos maiores do que a linha de base. Em particular, no conjunto de teste coreano, a diferença de pontuação entre o método proposto e a base saltou para mais de vinte pontos. Esses resultados indicam que Gs e Gs são significativamente eficazes. Gs obteve uma pontuação de perda melhor do que GAS, sugerindo que combinar um classificador e um gerador é eficaz no treinamento do classificador usando servidores. A alta cobertura de gs pode ser alcançada adequadamente porque o classificador pode se concentrar na seleção de mensagens de commit relevantes para cada classe. She is much tende a comer mais do que she is single, sugerindo que também é eficaz desenvolver independentemente diferentes modelos de sumarização construtivos para cada classe de nó de lista. Aqui está uma análise de erro. Os métodos she tendem a produzir frases mais curtas do que a frase de referência humana. Como, na figura à direita, a frase de referência tem três ou quatro frases, enquanto CSS tem apenas uma. A razão para essa relutância do modelo é que, nos dados de treinamento, apenas 30% das frases estão presentes no nível de recursos e 40% no nível de melhorias. Além disso, os métodos cs não conseguem gerar uma nota de lista precisa sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa e a frase completa não pode ser gerada sem diferenciar os correspondentes prerrogativas ou problemas. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma única frase, mas falha em fazê-lo. Finalmente, uma conclusão. Construímos um novo conjunto de dados para geração automática de pessoal. Também formulamos a tarefa de entrada de mensagens de commit e resumindo-as para que seja aplicável a todos os projetos escritos em inglês. Nossos experimentos mostram que o método proposto gera menos ruído e um texto com maior cobertura do que a linha de base. Verifique nossos dados no GitHub. Obrigado."}
