{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Benvenuti alla nostra presentazione di DeepL, un nuovo corpus per l'identificazione del testo tedesco a livello di documento e di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione. Iniziamo definendo la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è un processo che consiste nell'adattare un testo per migliorarne la comprensione da parte di un gruppo target specifico, come le persone con problemi di lettura o i non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, è necessario disporre di coppie di testi paralleli, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio qui riportato è possibile vedere una coppia di frasi allineate in parallelo di una frase tedesca complessa e la sua traduzione in un linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, quali la sostituzione lessicale, la fusione di frasi, la cancellazione di frasi, la riorganizzazione o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo ora il nostro nuovo corpus, Dplane, perché negli ultimi anni sono emersi alcuni problemi con i corpus esistenti. Ad esempio, questi corpus sono troppo piccoli per addestrare un modello di sintesi del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori di allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo quindi il nostro nuovo corpus D-Plane, suddiviso in due sottocorpus: D-Plane APA e D-Plane Web. D-Plane APA si basa su testi di notizie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Nel piano APA abbiamo allineato 483 documenti, tutti manualmente. Il risultato è stato di circa 30.000-13.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per Deep Plane Web, questo corpus include diversi domini e abbiamo allineato tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo ottenuto 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio in base al tipo di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, i testi della Bibbia sono molto più semplici e rafforzati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "A tutti i livelli, ad esempio, la semplificazione lessicale, la semplificazione strutturale o il livello generale di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, è possibile notare che il nostro corpus di semplificazione presenta una grande varietà di trasformazioni di semplificazione. Ad esempio, nel corpus API di semplificazione, abbiamo molti più riordini e aggiunte di parole rispetto al corpus web di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web abbiamo molti più esempi di parafrasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro set di dati D-plane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi nei documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso d'uso stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli, che hanno la stessa lingua, lo stesso contenuto, ma si trovano a livelli di complessità diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "Ora che abbiamo il nostro set di dati D-plane, che contiene frasi allineate manualmente, possiamo usare queste frasi come standard di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare il codice per eseguire questo metodo sui propri documenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è quello della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "Semplificando i modelli linguistici per produrre un testo semplificato a partire da un testo di input complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo perfezionato due modelli diversi. Abbiamo perfezionato il modello di LongIMPART per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche perfezionato l'importazione della base normale per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare tutti i punti di controllo e consultare i dettagli relativi ai punteggi e alle metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questa semplice ottimizzazione potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo proposto questi risultati come punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione e speriamo di incontrare tutti voi durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Spivak e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come forse saprete, le diverse strutture di dipendenza sono assunte da diverse teorie e approcci ai corpora. Ad esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è la seguente:"}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "In questo caso, il primo congiunto è il soggetto dell'intera struttura coordinata, ovvero Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è stato adottato nella teoria del testo di Igor Mel'čuk, dove l'intera struttura coordinata è guidata dal primo congiunto. Quindi, questi due approcci sono asimmetrici, giusto? Entrambi individuano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Esistono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio con congiunzione principale, utilizzato negli alberi di dipendenza di Praga, in cui le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "In questo modo otteniamo le dipendenze da N a tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio multi-testa, utilizzato, ad esempio, nella grammatica di parole di Hudson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono a capo delle strutture coordinate. Quindi otteniamo dipendenze dal reggente, qui \"ama\", da tutti i congiunti separatamente. Questi sono Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è presentare un nuovo argomento a favore delle strutture di coordinamento simmetriche, come queste due, e contro le strutture di coordinamento asimmetriche, come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "L'argomentazione si basa sul principio di minimizzazione della lunghezza della dipendenza, che spiegheremo con questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Come forse saprai, in inglese gli oggetti diretti preferiscono stare vicini al verbo, mentre i complementi possono essere più lontani, giusto? Quindi \"March read it yesterday\" va bene perché l'oggetto diretto \"it\" è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre \"March read it yesterday\" è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è l'aggiunta di \"yesterday\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo, poiché in tal caso può essere spostato dopo l'aggiunta."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. \"March ha letto questo libro assolutamente affascinante sulla BBC ieri\" è corretto, dove invece di \"it\" abbiamo questo lungo NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma è anche corretto dire: \"Ieri ho letto questo libro assolutamente affascinante sulle api\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "La ragione è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere vicini al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più brevi sono preferibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Qui abbiamo una dipendenza da \"red\" a \"adjunct\" di lunghezza 7, misurata in parole, e da \"red\" a \"book\" di lunghezza 4, quindi in totale fa 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si spostano, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6, giusto? Quindi, invece di 11, 6, molto più breve. Ecco perché questo suona abbastanza bene, giusto? Violando un principio, ma soddisfacendone un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Okay, quindi abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e abbiamo visto il documento \"Why not use universal dependencies?\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "Queste statistiche confermano l'osservazione fatta molte volte in precedenza, secondo cui i congiunti di sinistra tendono ad essere più brevi, quindi \"sale e pepe\" e non \"pepe e sale\" misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione, fatta per inciso, che questa tendenza cresce con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quando la differenza tra le lunghezze dei due congiuntivi aumenta, il congiuntivo più breve tende a essere il primo, giusto? Quindi la proporzione del congiuntivo breve a sinistra è maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando i governatori di sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, il governatore è a sinistra. Ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, \"Homer è arrivato e ha starnutito\", qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi in questi casi il congiunto di sinistra preferisce essere più breve, tanto più grande è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando il governo è di destra, come in questo caso, dove la sinistra governa la rete di coordinamento, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo dimostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe, la colonna centrale, e in parole, l'ultima colonna, mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che vediamo qui è che quando il governante è a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza del congiunto di sinistra a essere più breve aumenta costantemente con la differenza assoluta di parole, e lo stesso vale quando non c'è un governatore, come nel caso delle frasi coordinate, ma quando il governatore si trova a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "Nel documento mostriamo come questo fornisca un argomento contro le strutture di coordinamento asimmetriche come queste due e a favore delle strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Per l'accordo completo e gli argomenti, vedere l'articolo e parlarne con noi durante la sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Shangbing, dottorando presso l'Università di Washington. Oggi vi presento il nostro lavoro, che va dai dati di pre-addestramento ai modelli linguistici ai compiti a valle, seguendo le tracce dei pregiudizi politici che portano a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati web di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di informazione politica sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, il Huffington Post, ecc. sono ben rappresentati nei dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una situazione di vantaggi e svantaggi per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, sono stati in grado di imparare da prospettive diverse, il che celebra la democrazia e la pluralità delle idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente prevenute e potrebbero portare a problemi di equità nelle applicazioni di compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di indagare il processo di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici ai compiti a valle, ponendo specificamente le seguenti domande:"}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, come si valuta la linea politica dei modelli linguistici e quale ruolo potrebbe avere il set di dati di addestramento in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano i modelli linguistici con diversi limiti politici nei compiti a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni di elaborazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, abbiamo proposto di stimolare i modelli linguistici con diversi formati di prompt utilizzando questionari politici, come il test della bussola politica. Questo ci ha permesso di effettuare valutazioni automatiche ben fondate nella letteratura scientifica politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno orientamenti politici variabili. Essi occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Si può anche notare che GPT-4 è il modello linguistico più liberale di tutti e che i modelli GPT sono generalmente più liberali dal punto di vista sociale rispetto ai modelli BERT e alle loro varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, intendiamo indagare fino a che punto i pregiudizi politici dei modelli linguistici siano effettivamente acquisiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi condurre un esperimento controllato addestrando ulteriormente i checkpoint del modello linguistico su sei diversi corpus di parte, suddivisi in notizie e social media, ulteriormente suddivisi in base alla loro inclinazione politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Se si addestra ulteriormente i modelli linguistici su tali corpora di parte, si può notare che anche le coordinate ideologiche del modello linguistico si spostano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per RoBERTa ulteriormente addestrato su un corpus di Reddit di sinistra, si può osservare un notevole spostamento verso il liberalismo in termini di"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda i suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche tentato di indagare se i modelli linguistici riescono a cogliere la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Dividiamo quindi i corpora di pre-addestramento in pre-45º presidente degli Stati Uniti e post-45º presidente degli Stati Uniti e addestriamo separatamente i modelli linguistici sui due corpora temporali diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare che i modelli linguistici hanno generalmente una tendenza politica più accentuata rispetto al centro dopo il 2017. Questo indica che i modelli linguistici possono anche rispecchiare la polarizzazione della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, valutiamo i modelli linguistici con diverse inclinazioni politiche nella rilevazione del linguaggio d'odio e nella rilevazione delle fake news, due applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Se analizziamo le prestazioni per categoria, ossia se suddividiamo le prestazioni in"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda le diverse demografie o le tendenze politiche dei media, possiamo notare un modello secondo cui, ad esempio, per quanto riguarda il rilevamento del linguaggio d'odio, i modelli linguistici di sinistra sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "Nel rilevare il linguaggio d'odio rivolto a gruppi socialmente minoritari"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, sono meno efficaci nel rilevare gli insulti rivolti ai gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E viceversa, i modelli linguistici di destra sono più bravi a rilevare gli attacchi di odio contro i bianchi e gli uomini, ma peggiori nel rilevare gli attacchi di odio contro le comunità nere, LGBTQ+ e altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Tendenze simili si riscontrano anche nel rilevamento delle fake news, dove si osserva che i modelli linguistici di sinistra sono più bravi a rilevare le disinformazioni provenienti dal loro opposto orientamento politico e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, mostriamo molti esempi qualitativi per vedere che i modelli linguistici con inclinazioni politiche diverse"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "I diversi esempi di discorsi d'odio e disinformazione presentano previsioni diverse in base alla loro categoria sociale. Nell'appendice sono presenti molti altri esempi che evidenziano ulteriormente questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che c'è un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello linguistico di destra dovesse essere sottoposto a un fine-tuning per il discorso d'odio o la disinformazione, e poi implementato su una piattaforma di social media molto popolare,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che gli attacchi d'odio contro i gruppi minoritari potrebbero diffondersi senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha fatto scattare l'allarme per riconoscere e affrontare le questioni di equità derivanti dalle inclinazioni politiche dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Vorremmo anche sottolineare che abbiamo evidenziato il dilemma unico relativo ai pregiudizi politici dei modelli linguistici. È come trovarsi tra Scilla e Cariddi."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Se non si sanificano le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici e ai compiti a valle, creando alla fine problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione, e risulta incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere mantenuto nei dati di monolingua. È un po' come il problema del gatto di Schrödinger."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, perfetto. Credo che sia tutto quello che ho da dire. Cinque per oggi. Grazie per il tempo che mi avete dedicato."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa del primo anno di dottorato alla Carnegie Mellon University e oggi vi presenterò il mio lavoro, \"NL Positionality: caratterizzazione del design attraverso set di dati e modelli sensibili\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Istituto Allen per l'Intelligenza Artificiale, ovvero Sebastian Riedel, Luke Zettlemoyer, Katrin Erk e Michael Gamon."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo immaginando di lavorare per un giornale e di scorrere i commenti sotto un articolo di attualità per rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti rivolgerti a un'API popolare come Perspective API per il rilevamento della tossicità, e questo funziona davvero bene se sei Carl Jones, dove Perspective API è in grado di rilevare correttamente le istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, non è questo il caso di Aditya Sharma, dove l'API Perspective non è così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio di progettazione, in cui si osservano differenze sistematiche di prestazioni tecnologiche tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "I pregiudizi di progettazione, come quello che abbiamo appena visto, potrebbero essere dovuti alla posizione dei ricercatori di NLP e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno a causa delle loro caratteristiche demografiche, della loro identità e delle loro esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in qualità di ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni prese dai ricercatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "Una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno una posizione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "Non stiamo cercando di dire che i modelli e i set di dati in sé abbiano identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare alcune posizioni più di altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Lavori precedenti hanno suggerito alcune prove aneddotiche di posizionalità, come i divari culturali e i modelli e i set di dati, nonché definizioni teoriche di posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si occupano di confrontare gli utenti finali con i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio della posizione dei modelli e dei dataset sta diventando sempre più importante, man mano che i compiti di elaborazione del linguaggio naturale diventano più soggettivi e orientati alla società."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "È difficile caratterizzare come queste posizionalità siano distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare il posizionamento del dataset e del modello, confrontiamo le annotazioni con quelle di utenti reali con dataset e modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro framework NL Positionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo consiste nel riannotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scelto di farlo piuttosto che esaminare i dati demografici degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi scelto di riannotare i dati per ottenere molte annotazioni per ogni istanza e per ottenere un ricco set di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il punteggio di correlazione di Pearson."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework si differenzia quindi dalla letteratura sull'incoerenza degli annotatori, poiché confronta gli utenti finali con i modelli e i set di dati, le previsioni e le etichette, anziché limitarsi a considerare l'accordo tra gli annotatori o a modellare le distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è stato reso possibile in gran parte grazie a Lab in the Wild, una piattaforma di crowdsourcing online sviluppata da un ex collaboratore di HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Lab in the Wild è una piattaforma di sperimentazione online che consente di reclutare volontari diversificati, a differenza di piattaforme come MTurk, che hanno per lo più partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Su Lab in the Wild ospitiamo due attività, una delle quali è l'accettabilità sociale. Il funzionamento è il seguente: i partecipanti leggono una situazione tratta dal dataset Social Chemistry e poi scrivono quanto sia socialmente accettabile la situazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nella città, possono confrontare le loro risposte con quelle di un'intelligenza artificiale e di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, abbiamo replicato una configurazione molto simile per il compito di rilevamento di discorsi tossici e d'odio, in cui i soggetti hanno letto un esempio di DynaHate e hanno scritto se pensavano che si trattasse di un discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con DynaHate, Perspective API, ReWire API, Hate-Roberta e GPT-4. Il nostro studio ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 Paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Ora siamo meglio attrezzati per rispondere alla domanda: con chi si allineano maggiormente i set di dati e i modelli di PLN? Abbiamo scoperto che nel PLN esiste una posizione."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo riscontrato che i set di dati e i modelli sono più allineati ai paesi di lingua inglese. Per quanto riguarda l'analisi di accettabilità sociale di GPT-4, abbiamo riscontrato che è più allineata ai paesi di lingua inglese e alla Cina. Abbiamo anche riscontrato che il diniego dell'odio è più allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche una maggiore corrispondenza con le persone che hanno un'istruzione universitaria. Per GPT-4, nel compito di accettabilità sociale, abbiamo riscontrato che è più allineato con le persone che hanno un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E lo stesso vale per il Danneggiamento, che è più allineato con le persone con un’istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni rimangono inevitabilmente indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio è che i set di dati e i modelli sono meno allineati con le persone non binarie rispetto ai loro omologhi uomini e donne. Lo abbiamo riscontrato nel compito di accettabilità sociale di GPT-4, così come nell'analisi del compito di Dinahate."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Dato che la posizione è un fattore importante nell'NLP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di design rilevanti durante il processo di ricerca. L'altra è condurre ricerche di PLN con una prospettiva di prospettivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare set di dati e modelli specializzati all'interno di comunità specifiche. Un buon esempio è l'iniziativa Masakhane. Vogliamo sottolineare che l'NLP inclusivo non consiste solo nel far funzionare tutte le tecnologie per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E con questo si conclude la nostra presentazione, ma se desiderate saperne di più, sentitevi liberi di consultare il nostro cruscotto per i risultati dell'analisi più aggiornati e il nostro documento. Grazie!"}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Si Yuan della Fudan University. Sono qui per presentare il nostro lavoro, \"Distinguere la conoscenza di script dai modelli linguistici di grandi dimensioni per la pianificazione linguistica vincolata\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "In precedenza, i modelli linguistici sono stati utilizzati per pianificare obiettivi astratti di attività stereotipate, come \"preparare una torta\", e hanno dimostrato che i grandi modelli linguistici sono in grado di scomporre gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come \"preparare una torta al cioccolato\", rimane ancora poco studiata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Quale impone vincoli diversi sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifattoriali. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento, valutiamo e miglioriamo dapprima la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Non esistono dati che indichino un obiettivo specifico per il nostro studio."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa dobbiamo acquisire questo obiettivo. Come mostrato nella tabella, abbiamo esteso gli obiettivi astratti con vincoli multifattoriali per l'acquisizione di dati di ricerca umana utilizzando InstructGPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo campionato 100 obiettivi specifici e valutato gli script generati dai modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, abbiamo condotto un'analisi dettagliata per indagare sul perché i modelli linguistici falliscono."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati della figura mostrano che la completezza semantica degli script generati è accettabile, ma non è possibile garantire la fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Esaminiamo più da vicino le categorie di vincoli più dettagliate definite in WikiHow. La mappa di calore nella figura mostra che le prestazioni di pianificazione degli InstructGPT variano notevolmente per le categorie di vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Gli studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici di grandi dimensioni è soggetta a una grande variabilità, con conseguente scarsa qualità. Per questo motivo, abbiamo adottato l'idea di un filtro Z sovra-generato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo mostrando i tipi di vincoli con esempi per il CPT induttivo e otteniamo obiettivi specifici in base agli obiettivi astratti di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, istruisci GPT per generare script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, è stato sviluppato un modello di filtro per selezionare gli script plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo convertito gli script e i goal in embedding di InstructGPT e calcolato la similarità coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, abbiamo esaminato lo script che contiene le parole chiave del vincolo di destinazione. Abbiamo conservato lo script solo se il punteggio della destinazione è stato il più alto nel sito di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, InstructGPT può generare sequenze di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i grandi modelli linguistici sono costosi da implementare, è fondamentale consentire ai modelli più piccoli e specializzati di pianificare il linguaggio. La creazione di un set di dati è un passo essenziale a questo scopo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dataset è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "In questo modo, abbiamo seguito l'idea della distillazione simbolica della conoscenza per distillare i dati di pianificazione del linguaggio vincolato dai modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per la creazione di un set di dati di pianificazione del linguaggio vincolato, denominato CoScript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo generato 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di convalida e test, abbiamo chiesto ai lavoratori di crowdsourcing di trovare e rivedere i campioni non corretti."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione vincolata di CoScript. Abbiamo riscontrato che CoScript mostra un alto plauso negli obiettivi generati specifici. Con CoScript, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che T5, con un costo di 1,1 miliardi di dollari, è in grado di generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, il che indica che i modelli più piccoli possono superare quelli più grandi quando vengono addestrati in modo appropriato su set di dati adeguati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo definito il problema della pianificazione del linguaggio vincolato, abbiamo valutato la capacità di pianificazione del linguaggio vincolato dei modelli linguistici di grandi dimensioni e abbiamo sviluppato un metodo di filtraggio della sovrapproduzione per i modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità per la pianificazione linguistica vincolata. Speriamo che il set di dati di script vincolato possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il tempo che ci ha dedicato. Per ulteriori dettagli sul codice sorgente, consulti il nostro documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Zhu Heng. Oggi presenterò il nostro articolo: \"I tagger di entità nominate di Connell del 2003 funzionano ancora bene nel 2023?\" Iniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate, o compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che i modelli hanno utilizzato il Conll-2003 per sviluppare il REN per quasi 20 anni. Questo solleva naturalmente alcuni problemi. In primo luogo, questi modelli sono in grado di generalizzare i dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "Quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo creato il dataset Conll++: si tratta di un dataset che abbiamo raccolto da Reuters News nel 2020 e che abbiamo annotato seguendo le stesse linee guida di Conll 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi perfezionato oltre 20 modelli su Cornell 2003. Li abbiamo valutati sia sul set di test Cornell 2003 che sul set di test Cornell++"}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ogni modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Per una buona generalizzazione, cosa serve? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalizzano normalmente meglio ai nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che, di solito, i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un compito a valle. Abbiamo anche scoperto che più esempi di fine-tuning portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo alla domanda successiva: cosa causa il calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo formulato due ipotesi. La prima è il cosiddetto \"overfitting adattivo\", che consiste in un overfitting causato dall'uso ripetuto dello stesso set di test, e che si manifesta di solito con un calo dei rendimenti su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è lo spostamento temporale, ovvero il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda l'adattamento eccessivo, abbiamo visto che, nel grafico a destra, la linea di tendenza rossa ha un coefficiente angolare maggiore di 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Questo significa che ogni unità di miglioramento che abbiamo apportato a C++03 si traduce in più di un'unità di miglioramento in C++11, il che significa che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "Ciò dimostra che in questo caso non si osserva il fenomeno di overfitting adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E la deriva temporale, allora?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda il drift temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo riscontrato che le prestazioni peggiorano con l'aumentare del divario temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "Ciò conferma la nostra ipotesi secondo cui la causa principale del calo delle prestazioni è lo spostamento temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che, per una buona generalizzazione, è necessario un'architettura del modello migliore, una dimensione del modello maggiore e più esempi di fine-tuning. Questi obiettivi vanno di pari passo: non possiamo avere un solo ingrediente, ma tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è dovuto a un drift temporale e, sorprendentemente, non è dovuto a un adattamento eccessivo, anche se il Cono 2003 è stato utilizzato per oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Tornando alla domanda che ci siamo posti nel titolo del nostro articolo, i tagger di Conll 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, assicuratevi di consultare il nostro articolo e il nostro set di dati e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità, in cui abbiamo introdotto il punteggio delle entità alternative."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Jawad Hoseini e questo è un lavoro congiunto con Filip Radlinski, Sylvia Parity e Ali Louis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Consideriamo questa domanda alternativa: \"Intendevi 'Go Easy on Me' o 'I Got a Feeling'?\" In questo caso, un utente vuole scegliere tra una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone, \"Easy on Me\", o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "A volte, tuttavia, un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può accadere quando l'utente non riesce a ricordare il nome della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "Tutte le pronunce sono troppo simili tra loro e difficili da disambiguare."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "O quando l'utente desidera specificare una preferenza. Ecco alcuni esempi di differenze dirette: ad esempio, il più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi di conversazione e anche per la valutazione della comprensione delle entità da parte dei modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per il compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione della folla. Il nostro set di dati copre tre domini diversi: musica, libri e ricette."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta del dataset sottolinea l'informalità utilizzando un set di completamento di fumetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Nel fumetto ci sono tre nuvolette. Nella prima, Bob dice: \"Ti ricordi quella canzone che ascoltavamo ieri?\" e con questo Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo discorso, Alice dice: \"Intendi 'abbi pietà di me' o 'ho un presentimento'?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "Qual è la domanda alternativa? E nel terzo palloncino, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio il nuovo aeroporto."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniremo automaticamente le prime due caselle di testo, ma la terza sarà compilata dall'annotatore. La prima casella di testo viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, viene generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Usiamo sempre un modello semplice. Intende A o B? Dove A e B sono campioni di Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che si sale nella lista, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è il treno uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio due libri con il titolo \"Il ritorno\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando mostriamo questa domanda alternativa agli amministratori, questi conoscono il nome di queste entità, ma non necessariamente conoscono l'entità."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che facciamo è mostrare alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link a una ricerca Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "Quindi chiedere agli annotatori di ascoltare almeno una parte di ogni canzone e leggere informazioni su ciascuna. Ecco, ad esempio, il risultato della ricerca di Google per la canzone \"Easy On Me\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo un po' di testo di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio la prima, e descriverla usando da tre a cinque espressioni di riferimento indiretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica da pianoforte. Ecco alcuni esempi del nostro set di dati. Ad esempio, quello senza parole, non quello con il ragazzo di 12 anni, o quello di fantasia, o che proviene dall'Azerbaigian, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus di AllEntities contiene 6.000 domande alternative in tre domini e 42.000 espressioni di riferimento indiretto. I risultati del modello T5XLarge sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora la precisione è davvero elevata, intorno al 92-95%. Ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza è compresa tra l'82 e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera la conoscenza di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili per il dominio. Ecco un link al nostro set di dati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Papi dell'Università di Toronto e della Fondazione Bruno Kessler, e vi presenterò brevemente il documento \"L'attenzione come guida per la traduzione simultanea del discorso\", che è un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Che cos'è la traduzione simultanea della lingua parlata? La traduzione simultanea della lingua parlata, o SimulST, è il processo di traduzione di una lingua parlata in un testo in un'altra lingua in tempo reale, permettendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "Quali sono i problemi dei modelli di intelligenza artificiale attuali? Le architetture specifiche sono generalmente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di addestramento lunghe e complicate, ad esempio addestramento che prevede diversi obiettivi di ottimizzazione,"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "e addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrare un modello con una latenza media di 1 secondo e un altro con 2 secondi di latenza, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è la soluzione di Hauer?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, utilizzare modelli offline già esistenti senza doverli ritrainare o adottare un'architettura specifica per la SLT. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "e sfruttare le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il meccanismo di attenzione incrociata. È possibile vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre l'attenzione dell'encoder-decoder, una strategia che ci consente di decidere se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene omessa se la tensione non è concentrata, cioè se la sua somma è inferiore a una certa soglia α rispetto agli ultimi λ frame di discorso, il che significa che le informazioni ricevute sono sufficientemente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se riceviamo un frammento di discorso contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco,"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "Esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Si noterà che le prime due parole puntano ai fotogrammi vocali ricevuti per primi, mentre l'ultima parola punta agli ultimi fotogrammi vocali ricevuti, ovvero i fotogrammi vocali lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Poiché la somma dell'attenzione incrociata è superiore a una certa soglia α, non emettiamo l'ultima parola e attendiamo un altro frammento di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro blocco di discorso e il nostro modello prevede altre tre parole, esamineremo i pesi di attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Noteremo che nessuna parola punta agli ultimi fotogrammi di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se osserviamo i risultati principali di questo studio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Tracceremo i risultati della traduzione simultanea in grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il ritardo medio sull'altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta della misura della latenza e consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo che le nostre curve siano il più alte possibile in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma vogliamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie preparatorie che sono applicate anche ai modelli offline, ovvero la strategia di pesatura e il consenso locale. E confrontiamo anche con l'architettura all'avanguardia, specificamente progettata per la traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che supera tutte le strategie applicate ai modelli offline, dato che le curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che, se consideriamo il tempo effettivo trascorso o il tempo di calcolo, la strategia ADAPT è la più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire altri risultati, leggete il nostro articolo. Abbiamo inoltre rilasciato il codice e i modelli open source e un output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ying e io e il mio collega Jian Yang presenteremo la nostra ricerca su MultiInstruct, miglioramento dell'apprendimento zero-shot multimodale tramite la regolazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli linguistici di grandi dimensioni, molti studi hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diversi compiti a valle in modo efficiente in termini di parametri e dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Di recente, molti studi hanno dimostrato che l'istruzione di regolazione consente ai grandi modelli linguistici di eseguire compiti non visti in modalità zero-shot seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei precedenti lavori sull'istruzione di regolazione si sono concentrati sul miglioramento delle prestazioni zero-shot nei compiti basati solo sul linguaggio, mentre i compiti di visione artificiale e multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, pertanto, vogliamo indagare se la regolazione delle istruzioni su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione per i compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una notevole disparità nella disponibilità di set di dati di istruzione tra NLP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di 1.600 compiti di istruzione basati solo sul linguaggio. Tuttavia, non esiste un compito di istruzione multimodale di grandi dimensioni disponibile al pubblico. Questo ci ha motivato a creare un set di dati di istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo MultiInstruct, il primo set di dati di benchmark per la regolazione delle istruzioni multimodali che comprende 62 attività multimodali diverse, che coprono 10 categorie ampie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti derivano da 21 dataset open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare la regolazione delle istruzioni multimodali sul nostro dataset proposto, abbiamo preso OFA, un modello di rappresentazione multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di un riquadro delimitatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Di seguito sono riportati alcuni esempi di istanze del nostro dataset multi-instradamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare l'elaborazione di vari tipi di dati di input e output,"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo seguito il metodo di OFA e formulato tutti i compiti in un formato unificato di sequenza a sequenza, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ora parlerò di Multimodal Instruction Tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il set di dati di addestramento, utilizziamo 53 attività dal gruppo NIG per l'addestramento e campioniamo 10.000 istanze per attività. Per il test, riserviamo l'intero gruppo di ragionamento di buon senso per il test e selezioniamo ulteriori cinque attività dai gruppi VQA e Miscellanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Per ogni attività, utilizziamo tutti gli esempi presenti nel set di test. Inoltre, campioniamo in modo casuale 20 attività dal set di test delle istruzioni naturali come attività di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi utilizzato un modello OFA Large pre-addestrato come modello di base. Durante l'addestramento, abbiamo mescolato tutte le istanze per tutti i compiti. Ogni istanza è stata combinata casualmente con uno dei cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Durante il test, per ogni attività, abbiamo condotto un totale di cinque esperimenti valutando il modello con una delle cinque istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riportato la performance media e massima e la deviazione standard della performance in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è una classificazione multimodale, segnaleremo l'accuratezza. Se si tratta di un compito di generazione multimodale, segnaleremo la ROUGE-L. Per i compiti di elaborazione del linguaggio naturale, segnaleremo la ROUGE-L."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente da piccole variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i nostri risultati principali. Come possiamo vedere, la regolazione delle istruzioni può migliorare significativamente le prestazioni di OFA nei compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche il trasferimento di apprendimento da dataset di istruzioni naturali può giovare alla messa a punto delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere che, con l'aumento del numero di attività, il modello raggiunge prestazioni migliori e, al contempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche condotto un esperimento in cui abbiamo utilizzato un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni ha migliorato le prestazioni complessive del modello e ne ha ridotto notevolmente la sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostra l'effetto di diverse strategie di regolazione fine sulla sensibilità del modello. Come possiamo vedere, attraverso l'apprendimento per trasferimento da un set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche vedere che il trasferimento di apprendimento da un set di dati di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul set di dati NaturalInstructions."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo proposto il primo set di dati di tuning delle istruzioni multimodali su larga scala. Abbiamo migliorato significativamente la capacità zero-shot di OFA e abbiamo esplorato diverse tecniche di trasferimento di apprendimento dimostrandone i benefici. Abbiamo progettato una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'altra cosa: stiamo raccogliendo un dataset di istruzioni per modelli multimodali molto più ampio, con circa 150 compiti di visione e linguaggio in più, e li rilasceremo. Questo è un codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Kostas Drossos e sono lieto di darvi il benvenuto alla nostra presentazione del nostro articolo per l'ACL 2023, \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un lavoro congiunto con John Gautier, Aaron Mueller, Kalishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, rivediamo il tempo di pareggio minimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Il paradigma a coppie minime valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come BLiMP, SintassiGym, o l'accettabilità in termini di stereotipi, come CrowS-Pairs."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "In questo paradigma di coppia minima, il modo tipico di valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta e poi mostrare una frase inaccettabile o grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo è che il modello attribuisca una probabilità maggiore alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP non consente di valutare l'accettazione dei modelli verso frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Oggigiorno, i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe, quindi è fondamentale valutare l'accettabilità del modello in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, questo è l'approccio. Quello che facciamo è che, per simulare queste sequenze più lunghe, rivediamo i set di dati stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da quei set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, qui abbiamo scelto una coppia tipica di grammaticalità dal dataset BLiMP, nel caso dell'Adjunct Island."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "Per ricreare sequenze più lunghe e accettabili, con la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticalmente corrette da un corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "Poi lo aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento e questo potrebbe essere utilizzato anche per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Questo è ciò che chiamiamo scenario di non corrispondenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "In questo caso, le frasi provengono ancora da set di dati pertinenti, ma non dal set di dati che si sta valutando. E possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Se il contesto proviene da un sottoinsieme diverso del set di dati o se è completamente irrilevante per la frase che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Come si comporta il modello? Prima di tutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query attuale, e troviamo che i giudizi MPP sono per lo più robusti per contesti arbitrari."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a 1024 per sfruttare al massimo i modelli OPT e GPT-2 e abbiamo visto che, nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Cosa succede quando scegliamo frasi dallo stesso set di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso set di dati BLiMP o SyntaxGym."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "In questo caso, si osserva che i giudizi MPP aumentano o diminuiscono in modo significativo quando si aggiungono prefissi accettabili o inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando facciamo corrispondere la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno nel testo di Blame Person, Jim,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Si osserva un aumento o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora, questo, e questo è molto grande, come questo effetto aumenta per tutta la lunghezza del contesto, e questo probabilmente influenzerà i modelli linguistici più recenti che hanno una finestra di contesto più ampia."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto una serie di analisi in cui abbiamo tentato di perturbare la frase di input cercando di preservare la struttura rilevante, ma aggiungendo rumore all'input. Dopo aver effettuato diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che nessuno di questi rumori modifica effettivamente il modello, cioè non cambia il modo in cui ci mostra la tendenza del PMP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In sostanza, abbiamo riscontrato che i modelli sono sensibili alle frasi perturbate in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "In altre parole, quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio inaccettabile, osserviamo una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Le conclusioni principali del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti, che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, il modo in cui la eseguiamo attualmente con input a centro singolo e breve, potrebbe non catturare appieno la conoscenza astratta del modello linguistico in tutto il contesto della finestra."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Per ulteriori dettagli sui nostri esperimenti, leggere il nostro articolo. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Yushin John della Penn State University. Oggi vi presenterò il nostro lavoro, Exemplar, analisi semantica cross-linguistica in più lingue naturali e rappresentazioni minime."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Il parsing semantico è un compito che consiste nel creare rappresentazioni semantiche delle query degli utenti, come SQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "Il parsing semantico cross-lingua è il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nella figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di parsing semantico cross-lingue esistenti sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono lacune nella copertura di alcune lingue naturali. Manca il cinese."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "A causa della scarsa copertura di alcune minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Manca il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "O sono valutati solo su un certo modello neurale. Ad esempio, c'è un solo modello su cui valutare."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo Exemplar, un dataset uniforme per il parsing semantico incrociato in più lingue naturali e rappresentazioni di significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene 90 set di dati in vari domini, 5 compiti di analisi semantica, 8 rappresentazioni di minoranze e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare meglio il nostro benchmark, abbiamo considerato sei impostazioni per la formazione e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è il test di traduzione. Utilizzeremo l'API di Google Translate per tradurre la lingua di origine in quella di destinazione, quindi utilizzeremo un modello monolingue per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestriamo il modello in inglese con le query in inglese e, durante l'inferenza, traduciamo la query in tedesco in inglese tramite un'API e poi utilizziamo il modello addestrato per prevedere il SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "Testeremo anche il modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza è la stessa della lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo testato anche un'impostazione monolingue a pochi colpi addestrando modelli monolingue con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo testato un modello multilingue, che abbiamo addestrato per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo messo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue e, durante l'inferenza, possiamo usare questo modello per"}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre query in tedesco, cinese, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche preso in considerazione il trasferimento zero-shot e few-shot cross-lingua. Abbiamo eseguito il training su una lingua sorgente e trasferito il risultato su un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Durante l'addestramento, è stato addestrato su query in inglese o su una combinazione di query in inglese e tedesco a pochi colpi per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, abbiamo valutato due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "tra cui Encoder-Pointer-Decoder (EPDR), che sta per encoder pre-addestrati multilingue con decoder basati su puntatori, come XLM-R+PDR e BERT+PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche valutato i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come mBART e mT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder-decoder ottiene le migliori prestazioni in tutti e nove i set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su MT5 e XLM-R + BERT in un contesto multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder-decoder o l'encoder-PTR possono essere migliorati con l'addestramento in una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle lingue naturali principali ha ottenuto un miglioramento delle prestazioni, fatta eccezione per l'inglese, che ha avuto un calo delle prestazioni in sette set di dati e un miglioramento in soli tre."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Penso che questo sia noto come maledizione della multilingualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario di prestazioni tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu è il trasferimento cross-lingua few-shot, la linea arancione è il trasferimento cross-lingua zero-shot, mentre la linea verde è l'impostazione monolingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che, confrontando la linea verde e quella arancione, per la configurazione zero-shot il divario di prestazioni del trasferimento cross-lingua è significativo, mentre confrontando la linea blu e quella arancione, abbiamo riscontrato che per la configurazione few-shot il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche trovato altri risultati interessanti. Ad esempio, l'encoder-decoder ha superato i lavori precedenti o ha ottenuto risultati comparabili. L'addestramento sul linguaggio naturale inglese può migliorare significativamente le prestazioni del few-shot sul linguaggio naturale target."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che i modelli linguistici multilingue, come CODA-S e BLOOM, non sono ancora adeguati per il parsing semantico cross-linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo creato Xampler, una piattaforma di benchmark unificata per il parsing semantico multilingue e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue e i nostri risultati mostrano molte scoperte interessanti e così via. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Aydin Bilal e vi fornirò una breve panoramica dell'articolo \"Grounding Paraphrase Translation: Assessing Strategies and Performance\". Si tratta di un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Bram è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato lo scorso anno, nel 2022. È stato addestrato su una vasta raccolta di testi, che comprende 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, ha raggiunto i migliori risultati in centinaia di compiti di elaborazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro presentiamo il primo studio sistematico sull'elaborazione di prompt per modelli linguistici di grandi dimensioni per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato la capacità di trasferimento di tali modelli utilizzando le migliori pratiche della comunità NLP. Questo comporta l'uso degli ultimi set di test per evitare sovrapposizioni dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo confrontato due sistemi all'avanguardia, ovvero i sistemi con le migliori prestazioni della valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche di nuova generazione all'avanguardia e mostriamo anche i risultati delle valutazioni umane basate su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompt ha una grande influenza sulle prestazioni dei modelli linguistici di grandi dimensioni per la traduzione, come si può vedere in un semplice esperimento in cui abbiamo utilizzato un prompt a un colpo solo e fornito due prompt diversi per la stessa frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggior parte delle frasi (516 su 1000) presenta una differenza di oltre un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "In casi estremi, questo può arrivare fino a 40 punti di blocco. È quindi importante scegliere una buona strategia di prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompt a cinque colpi, in cui abbiamo semplicemente contrassegnato ogni frase che abbiamo fornito al sistema con la lingua in cui si trova."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, in cui si esegue la traduzione dal tedesco all'inglese, le frasi in tedesco, ovvero le frasi di partenza, sono contrassegnate con il simbolo due punti tedesco e le traduzioni in inglese con il simbolo due punti inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva della stampa non ha una grande influenza nel caso di più brevi stampe."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per lo zero e il one-shot prompting e quando si passa, come nel nostro caso, al five-shot prompting, non c'è quasi differenza rispetto alla forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare la maggior parte del peso."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "La sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È quindi importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più disordinati, e i risultati sono migliori, quindi le prestazioni sono superiori quando si utilizzano i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi SOTA specializzati presentano un notevole vantaggio rispetto alle traduzioni di BART, ma BART si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di confrontarlo con Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo ottenuto dall'annullamento dell'umano, che abbiamo eseguito utilizzando il framework MQM, sono che la fluidità di PALM è paragonabile a quella dei sistemi all'avanguardia, ma la differenza principale deriva dalla precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra che Palm scelga di produrre una traduzione che suona meglio, a volte eliminando parti della frase originale che sono ambigue nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di errore di stile per PAN è inferiore rispetto ai sistemi di riferimento, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "Questo modello fornisce un output molto fluido, ma presenta ancora alcuni problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa panoramica davvero breve. Per maggiori dettagli, partecipate alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, dottorando presso l'Università di Saarland, in Germania. In questo video vorrei presentare il nostro ultimo lavoro, \"Weaker Than You Think: A Critical Look at Weekly Supervised Learning\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao-Yu Shen, Miles Lubin, Guy Steffen e D. T. Clark."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nel caso della supervisione debole, non si etichettano manualmente i dati. Invece, si etichettano i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, quelle deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se si addestra direttamente una rete neurale con dati etichettati in modo errato, la rete tende a memorizzare il rumore dell'etichetta e non generalizza."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "In \"Weakly Supervised Learning\", vengono proposti algoritmi di addestramento per formare in modo robusto le reti neurali in presenza di rumore nei dati di etichettatura, in modo che i modelli formati generalizzino ancora bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In lavori recenti su WSL, dove WSL sta per \"Weekly Supervised Learning\", è una comune affermazione che le persone dicono di addestrare modelli solo su dati con etichette settimanali e di ottenere alte prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è corretta, ma c'è un problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Questo significa che le persone danno per scontato che sia disponibile un set di convalida pulito aggiuntivo per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Ci siamo fermati a questo scenario di problema, il che implica che per il semi-supervisione sono necessarie ulteriori annotazioni manuali. Ma, come un elefante nella stanza, questa necessità viene spesso ignorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Le considerazioni sopra esposte ci portano a porci tre domande di ricerca. In primo luogo, i dati di convalida puliti sono necessari per WSL? O forse possiamo usare un set di convalida rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se sono necessari dati puliti o se i dati puliti sono obbligatori per il funzionamento di WSL, quanti campioni puliti sono necessari? Infine, dovremmo usare i campioni puliti solo per la convalida o ci sono modi migliori per utilizzarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, abbiamo scoperto che, in modo interessante, i metodi WSL più recenti richiedono effettivamente campioni di WIDH puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "In caso contrario, si verifica un forte calo delle prestazioni, come mostrato in questa figura: se non ci sono campioni di convalida puliti, i modelli addestrati non riescono a generalizzare oltre le etichette originali deboli."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "In altre parole, l'addestramento è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati in modo corretto per funzionare in modo appropriato e che il costo dell'annotazione per ottenere campioni di convalida puliti non deve essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che l'aumento del numero di campioni di convalida puliti aiuta gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "Di solito bastano 20 campioni per classe per ottenere un'elevata performance."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è finita qui, perché se decidiamo comunque di accedere a campioni puliti, l'addestramento diretto su di essi otterrà prestazioni ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente ai dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la convalida."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, se abbiamo 10 campioni per classe, la micro-regolazione diretta inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni dichiarato nei precedenti approcci WSL può essere facilmente ottenuto continuando la messa a punto sui campioni di convalida puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere dalle figure, il modello Vanilla, denominato FTW, inizialmente non è all'altezza dei metodi WSL più complessi, come Cosine."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se si continua a perfezionare i campioni puliti, FTW funziona altrettanto bene degli altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "In pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente. I loro guadagni in termini di prestazioni e praticità sono stati sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per i futuri lavori sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è stata effettuata con campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "Secondo, gli approcci WSL devono essere confrontati con i basi di apprendimento a breve termine, poiché entrambi lavorano su campioni puliti. Terzo, la regolazione continua è una base semplice ma forte che dovrebbe essere considerata nei futuri lavori WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso open source il nostro codice. È possibile trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di controllarlo. Grazie e buon proseguimento della conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio di elaborazione del linguaggio naturale (NLP) dell'Emory, diretto dal professor Eric Nyberg dell'Università di Emory, in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che tu abbia appena sviluppato un modello di dialogo e che tu voglia vedere come si comporta rispetto allo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni è migliore o di valutare le conversazioni in base a una scala Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello su un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste semplicemente nel chiedere a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi comparativi o di scala Likert esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio annotazione dei comportamenti nelle chat, o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità della chat."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC-Eval è in grado di misurare le frequenze con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "Quando il modello si contraddice o contraddice il suo interlocutore, quando inventa fatti errati o viola il buon senso e quando il modello riesce o fallisce nel mostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati in 100 conversazioni uomo-bot per modello, utilizzando ABC-Eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per fare un confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi già esistenti: valutazioni di Likert a livello di turno, valutazioni di Likert a livello di dialogo e confronti a coppie a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi dei risultati di valutazione, abbiamo riscontrato che le etichette comportamentali ABC-EVAL sono complessivamente più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra gli annotatori su 100 conversazioni etichettate due volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, è possibile vedere come la misurazione della proporzione di turni con contraddizioni di sé e del partner spieghi il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi di coerenza media di Likert spiegano solo il 4% o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a gradini."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "È possibile vedere come la combinazione di tutte le metriche ABC-Eval spieghi oltre il 25% della qualità della conversazione e, man mano che si rimuovono le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità decente di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità e meno di queste metriche portano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche ABC-Eval affidabili, informative e distinte ci permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Dai risultati del nostro esperimento si evince che permangono diverse sfide, che sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violato il buon senso nel 20% delle risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro interlocutore circa il 10% delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido ritmo di miglioramento nel settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione. Tuttavia, questo è un motivo in più per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC-Eval possa essere utilizzato da altri operatori del settore come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale si evolverà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Kai Owen e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede un contesto? Un'esplorazione multilinguistica basata sui dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, André F. T. Martins e Graham Neubig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Beh, se la frase precedente era \"Le cose potrebbero diventare pericolose se i ministri lo scoprono\", allora \"mol\" si riferisce a una spia. Ma se la frase precedente era \"Potrebbe essere qualcosa di serio, dottore?\", allora \"mol\" si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "A seconda del contesto, il significato della parola cambia e, di conseguenza, cambia anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli riescono a tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di cogliere queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e un insieme limitato di lingue, poiché si basano solitamente su conoscenze di dominio e su una curatela umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro abbiamo cercato di rispondere a queste due domande: quando la traduzione richiede un contesto e quanto bene i modelli gestiscono questi casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "In un precedente lavoro, abbiamo introdotto la CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando la quantità di informazioni che il contesto C fornisce sull'obiettivo Y, dato il testo di partenza X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "È possibile considerare il CXMI come l'informazione ottenuta fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, abbiamo esteso CXMI a pointwise CXMI, che può misurare l'uso del contesto a livello di frase o di parola. Possiamo considerare le parole con un alto P6MI come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto PCMI per cercare di individuare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo eseguito la nostra analisi sui trascritti di discorsi TED che sono stati tradotti dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi a tre livelli diversi. In primo luogo, esaminiamo le parti del discorso con un PCXMI medio elevato."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un punteggio P6MI relativamente alto. Ciò può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario un contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "In modo analogo, riscontriamo che alcune lingue richiedono un contesto per scegliere la forma verbale corretta. Esaminiamo quindi i lemmi che presentano un alto tasso di ambiguità, calcolato sulla base di tutte le loro occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci aiuta a identificare casi come quello qui, in cui in cinese è necessario il contesto per tradurre correttamente i nomi propri e assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "In modo analogo, abbiamo riscontrato che il contesto è fondamentale per tradurre con la formalità corretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo i diversi token individuali che presentano un alto PXMI. Ciò ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i risultati del nostro studio per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, abbiamo creato dei tagger per identificare automaticamente le parole che appartengono al fenomeno. Il nostro tagger è stato chiamato Multilingual Discourse Aware, o MuDA."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che le diverse lingue presentano proporzioni diverse di questi fenomeni discorsivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, utilizziamo il tagger Muda applicando il tagger al corpus parallelo che vogliamo utilizzare per la valutazione. Applichiamo le nostre metriche di traduzione preferite agli esempi dipendenti dal contesto che il tagger Muda ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro benchmark e altre metriche per valutare diversi modelli a livello di documento per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, quando si utilizzano le metriche a livello di corpus, per esempio per il BLEU, si è riscontrato che i modelli agnostici del contesto hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se si utilizza la metrica comet, i modelli consapevoli del contesto ottengono le migliori prestazioni. Se si utilizza la metrica word F1, i modelli con e senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo le metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark MUD per valutare i modelli e abbiamo scoperto che i modelli basati sul contesto sono significativamente più precisi dei modelli che non utilizzano il contesto per determinati fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi modelli non sono molto migliori di quelli che non utilizzano il contesto per altri fenomeni, come l'ellissi, i pronomi e la forma verbale. Questo suggerisce che per la traduzione a livello di documento sia necessario fare ulteriori progressi."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo condotto un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, utilizziamo le nostre scoperte per creare un punto di riferimento per la traduzione di documenti, che ci aiuta a identificare quali modelli di fenomeni discorsivi riescono a gestire bene o meno e quali sistemi di traduzione sono validi per la traduzione di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione. Ci vediamo a Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Yanis Lavrac e vi presenterò i nostri lavori su Dr. BERT, un modello pre-addestrato robusto in francese per il settore biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione parleremo prima del language modeling nel settore sanitario. Poi presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo presentato il primo modello biomedico in francese, chiamato Dr. Bert, basato su Roberta e addestrato su Nachos, un set di dati di informazioni mediche raccolte dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre presentato un confronto tra modelli con più impostazioni di pre-addestramento e fonti di dati. Successivamente, abbiamo illustrato i nostri risultati in 11 attività biomediche e cliniche a valle in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concludiamo con gli esperimenti e forniamo ulteriori dettagli su come accedere ai modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dalla sua uscita nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un enorme guadagno di prestazioni rispetto ai metodi statici e contestuali storici come Word2Vec, FastText o GloVe."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con CamemBERT, e ad altri ambiti, come quello biomedico con BioBERT e BlueBERT, e quello clinico con ClinicalBERT, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono rari e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati nel settore."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, in Francia non esistevano modelli open source per la biomedicina."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quale sia la fonte di dati più appropriata per una vasta gamma di utilizzi e se i dati di crowdsourcing possano sostituire i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dr. BERT con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale universitario di cui siamo ospiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci siamo chiesti: quanti dati servono per addestrare un modello specializzato sui dati in francese? 4 gigabyte? 8 gigabyte? O di più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli creati da zero. Una prima versione di Dr. BERT con 7 GB di Nachos, una seconda versione con un sottoinsieme di 4 GB di Nachos,"}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con 4 gigabyte di frasi tratte da appunti clinici, e una versione finale di Schubert con una miscela di un sottoinsieme di 4 gigabyte di MIMIC-III e 4 gigabyte di appunti clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso del camembert e addestrato su un sottoinsieme di 4 GB di nachos, un altro, sempre basato sul camembert, ma addestrato questa volta su 4 GB di pretzel."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, uno basato sul modello biomedico inglese PubMedBERT e addestrato su un sottoinsieme di 4 GB di Snatch. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, abbiamo raccolto una serie di compiti pubblici e privati, come il riconoscimento della NLP, la classificazione, il POS tagging e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questo modello è stato confrontato con sei modelli di base, ovvero CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNet 4 GB, PubMedBERT, BioBERT e ClinicalBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi evidenzia che il modello ha le migliori prestazioni nel compito con i dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere i dati da fonti eterogenee, che sembrano essere più versatili. Abbiamo inoltre osservato che l'uso di più dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, sembra che l'addestramento da zero ottenga prestazioni migliori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento di preaddestramento continuo, che utilizza il peso e il tokenizzatore di PumiceBERT, addestrato sul sottoinsieme di 4 GB di NACHOS, ha mostrato risultati paragonabili a quelli ottenuti con Dr. BERT 4 GB da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Non è questo il caso del modello basato sui pesi di Camembert e Tokenizer, che soffre di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il nostro sistema offre prestazioni migliori in nove dei compiti di DONTREMS e supera globalmente i risultati del modello generico CamemBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche osservato che i dati specializzati sono migliori, i dati più specializzati sono ancora migliori, ma non si adattano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da Nachos sono disponibili gratuitamente su Hugging Face e tutti gli script di addestramento sono presenti nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per questa presentazione e non vediamo l'ora di agire alla prossima sessione a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi utilizzando il tagging multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei supervisori, Alexander Kolodner e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità di un apprendista di gestire ricorsioni più profonde e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe essere simile a questo. Come sempre, abbiamo un set di addestramento di enunciati, in questo caso \"la ragazza ha dormito\" e \"Maria sapeva che la ragazza ha dormito\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste affermazioni sono associate a forme logiche che rappresentano gli aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard del machine learning, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenza-sequenza ingenui faticano a generalizzare in questo tipo di distribuzione e spesso producono output che non sono in linea con l'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate con il colore nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è l'integrazione degli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono destinati a catturare il processo composizionale che collega le espressioni alle forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente oneroso. Di solito, questo comporta una notevole pre-elaborazione specifica del formalismo delle forme logiche, ad esempio per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento di alberi può anche comportare procedure di induzione grammaticale specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento non utilizziamo alberi e presentiamo un modello neurale di sequenza a sequenza che modella direttamente le corrispondenze tra i frammenti di input e quelli di output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, mostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede l’output dall’input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, si etichetta ogni token di input con un multinsieme non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione che li metta nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo un nuovo metodo per prevedere una permutazione che non impone alcun vincolo rigido alle permutazioni possibili. Questo rende il nostro approccio piuttosto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Scorriamo da sinistra a destra sull'output e determiniamo quale token di multiset inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente 1, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, passiamo al token del multiset successivo per determinare il secondo token dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token dell'output in modo analogo, saltando a un altro token di multiset. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "fino a quando ogni token della prima fase non è stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darti un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli non basati su alberi nel benchmark Cogs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a ricorsioni più profonde."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Altri tipi di generalizzazione strutturale rimangono tuttavia molto difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un token dato, non sappiamo da quale multiset proviene, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l’allineamento come parte dell’addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP-difficile. Questo perché è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approssimiamo questo con un rilassamento continuo adatto alla GPU che ci permette anche di eseguire il backpropagation attraverso la soluzione e di apprendere le permutazioni più plausibili dal punto di vista linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, consultate il nostro articolo o venite a vedere il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Sono Akshat e oggi, insieme al mio coautore Martin, presenteremo il nostro lavoro, la valutazione dell'integrazione delle conoscenze da più fonti. Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita tramite il pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Recenti studi su attività come la risposta a domande dimostrano che i modelli possono utilizzare le conoscenze acquisite durante la fase di pre-addestramento per risolvere il problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nella frase \"John ha visto il presidente appena eletto in TV\","}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere con certezza chi sia l'entità specifica di questo esempio, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato da quando è stato effettuato l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, i modelli di successo per i compiti di NLU intensivi in termini di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella in fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo un compito di risoluzione della coreferenza progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il set di dati con percorsi di studio umani e modelli di risoluzione della coreferenza consolidati."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio del nostro set di dati. Servin è un giudice. Kia è una fornaia. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Servin."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome dato richiede due tipi di informazioni: la conoscenza specifica dell'entità, ad esempio \"Sergey è un giudice\", e la conoscenza di sfondo, ad esempio \"i giudici decidono i casi nei tribunali\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante il pre-addestramento dei modelli linguistici di grandi dimensioni, mentre le conoscenze specifiche delle entità vengono osservate tipicamente durante il periodo di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Variamo la disponibilità di queste due informazioni in modo che possano essere reperite in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di KITMOS. La prima è l'impostazione tipica, pre-addestramento sullo sfondo, in cui si suppone che la conoscenza di base sia disponibile al momento del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è l'impostazione background both, in cui le conoscenze di background sono disponibili sia al momento del pre-addestramento che a quello dell'inferenza. Infine, l'impostazione background inference, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non è inclusa nei dati di addestramento predefiniti dei modelli, ad esempio perché si sono sviluppate nuove professioni dal momento dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti in due fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del pre-addestramento sullo sfondo, si suppone che la conoscenza di sfondo, secondo cui i politici cercano seggi elettivi nel governo, sia contenuta nei parametri pre-addestrati. Nel contesto a tempo limitato, viene fornita la conoscenza antispicifica secondo cui Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di entrambi i gruppi di controllo, abbiamo fornito non solo informazioni generali, ma anche informazioni di sfondo sui politici nel contesto del gruppo di controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "In un contesto di interferenza di sfondo, viene fornita l'occupazione fittizia \"meretrice\" invece di \"politico\", poiché è improbabile che \"meretrice\" sia contenuta in un parere pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il set di dati sia con partecipanti allo studio umano che con modelli di risoluzione di riferimento consolidati. In questa figura mostriamo i risultati dei modelli migliori nella variante più difficile dell'impostazione di pre-addestramento sullo sfondo."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Senza un addestramento specifico su KITMOS, entrambi i modelli non riescono a dare buoni risultati. Tuttavia, quando vengono addestrati su KITMOS, sia C2F che BERT4KI ottengono risultati significativamente migliori rispetto a una scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che, quando vengono addestrati su dataset di risoluzione di query generali, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si testa su KIT-MOS, dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriori esperimenti con conoscenze fittizie hanno indicato che anche i modelli migliori non riescono a integrare in modo affidabile le conoscenze fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i punti principali del nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza proveniente da fonti multiple."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile le conoscenze di background presentate solo al momento dell'inferenza. Se siete interessati ad approfondire, consultate il nostro articolo e date un'occhiata al set di dati e al codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo \"Personaggi contrassegnati\", che utilizza prompt di linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato realizzato in collaborazione con Esin Durmuş e Dan Jurafsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano vari limiti. Di solito si basano su set di dati creati manualmente che richiedono molto tempo per essere curati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, oppure catturano semplicemente associazioni molto generali e ampie, come associazioni negative con determinati gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali complesse possano amplificare i pregiudizi e costituire luoghi unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare questi limiti, ci affidiamo alla proprietà che questi nuovi modelli linguistici di grandi dimensioni, ottimizzati per le istruzioni, sono molto bravi a rispondere alle istruzioni e ai prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, utilizzando un prompt come \"Immagina di essere una donna asiatica. Descriviti\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marker di identità che vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di generazioni di GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente vediamo che, sebbene i risultati non siano apertamente negativi o tossici nel senso tradizionale di queste parole,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è descritta come riservata, mentre quella del Medio Oriente è definita esotica e proveniente da una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "Entrambe le donne di colore fanno riferimento alla loro ascendenza, mentre l'uomo bianco non lo fa."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo ha due parti. La prima consiste nella generazione di questi personaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri prompt per generare questi personaggi sono stati ispirati da uno studio in cui sono stati dati questi prompt a soggetti umani, scoprendo che, fornendoli a soggetti umani, sono stati in grado di evidenziare anche stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "Questo consente inoltre un confronto diretto tra le nostre persone generate e le risposte scritte dagli esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è costituita dalle parole contrassegnate, un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, di cui parlerò tra poco."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatezza, secondo cui esiste una condizione di non marcatezza di default e qualsiasi gruppo che si discosta da tale condizione è considerato marcato dal punto di vista linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, la parola \"guerriero\" è solitamente associata agli uomini. Per questo, quando si descrive una guerriera, si specifica \"guerriero donna\" e si aggiunge il termine \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "In generale, i gruppi dominanti della società sono sia linguisticamente che socialmente non marcati, mentre i gruppi emarginati sono di solito marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro metodo, per prima cosa indichiamo quali sono i gruppi non contrassegnati e quelli contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, confrontiamo le persone utilizzando il metodo delle parole di combattimento, che consiste nell'utilizzare i rapporti delle probabilità ponderate per distinguere le parole principali di ogni gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per le persone di colore, per le donne nere, si farebbe un confronto tra le parole di combattimento e i rapporti di legge contro le persone bianche e le persone di sesso maschile, perché questi sono i due gruppi corrispondenti non contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ora, per quanto riguarda i risultati. In primo luogo, abbiamo utilizzato un lessico di stereotipi e abbiamo riscontrato che le personalità generate contengono molti più stereotipi rispetto a quelle scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando osserviamo la distribuzione delle parole nel lessico, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene i personaggi generati presentino tassi molto più elevati di parole stereotipate, quelli scritti da esseri umani hanno una distribuzione di parole molto più ampia, mentre le parole stereotipate presenti nei personaggi generati sono solo \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "In effetti, questo lessico non riesce a cogliere molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti. Per questo motivo, ci rivolgiamo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, esaminiamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, per i gruppi di colore, le parole più frequenti includono termini come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a un lungo retaggio di discriminazione e di esclusione di questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti luoghi comuni che si riflettono in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono termini come \"vibrante\" e \"formosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che si collegano a un tropismo tropicale. Per le donne asiatiche, le parole sono cose come \"piccole\", \"delicate\" e \"setose\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a una lunga storia di iper-sessualizzazione delle donne asiatiche, considerate molto docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per le donne nere, vediamo che alcune delle parole più ricorrenti sono \"forte\" e \"resiliente\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte e, sebbene possa sembrare positivo a prima vista,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono stati condotti studi che dimostrano come questo tipo di archetipo sia in realtà molto dannoso, poiché mette molta pressione su queste categorie demografiche affinché siano resilienti e forti di fronte agli ostacoli della società."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Invece di lavorare per cambiare gli ostacoli, si fa pressione su queste persone affinché li superino, il che porta a esiti negativi per la salute di queste persone, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In generale, abbiamo riscontrato che le parole per ogni gruppo contrassegnato riflettono essenzialmente narrazioni stereotipate."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "In base a questi schemi, giungiamo a tre raccomandazioni per i proprietari dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, in qualità di ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, non sappiamo se questi stereotipi positivi siano dovuti a qualche tipo di..."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "Forse c'è un eccessivo allineamento dei valori in corso, o forse altri metodi anti-stereotipizzazione che stanno portando a questi schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo fare alcuna supposizione o studiare ulteriormente senza una maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Divertitevi ad AC."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jinghui Wei e provengo dall'Università delle Scienze e della Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me presentare un breve video pubblicitario del nostro articolo \"Stai copiando il mio modello? Protezione del copyright dei grandi modelli linguistici per l'incorporamento e i servizi tramite watermarking backdoor\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo con una panoramica sui servizi di embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come ChatGPT, LLaMA e PaLM sono eccellenti nella comprensione e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "Embedding.ai è uno dei servizi basati su modelli linguistici di grandi dimensioni per supportare vari compiti di elaborazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi recenti hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Di conseguenza, è necessario proteggere il copyright dell'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il copyright dei servizi di intelligenza artificiale, una delle soluzioni è quella di incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo di watermarking deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile agli embedding di servizi. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "In terzo luogo, il watermark deve essere abbastanza evidente per l'attaccante, altrimenti quest'ultimo può rimuoverlo facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere suddivise in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi metodi non sono applicabili all'incorporazione di servizi o non sono trasferibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento, pertanto, proponiamo EmbeddingMarker, un metodo di watermarking basato su backdoor applicabile ai servizi di embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Ora vi presento i dettagli del nostro marcatore incorporato. Il marcatore incorporato prevede due fasi principali: l'iniezione di watermark e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole in esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del provider, quest'ultimo conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright serve a rilevare se un modello alla base di un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo costruendo un backdoor e un dataset benigno. Il dataset backdoor contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Il provider richiede quindi gli embeddings dal servizio di stilizzazione con il dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "La somiglianza coseno e L2 tra l'embedding richiesto e quello target viene calcolata. Calcoliamo la differenza di somiglianza tra i dataset benigni e backdoor, che è definita come Δcoseno e ΔL2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, abbiamo applicato anche il test KS e utilizzato il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto esperimenti su quattro set di dati: AG News, Mind, SST-2 e Enron Spam. Supponiamo che il fornitore abbia applicato il set di dati Wikitext per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati ottenuti su quattro set di dati mostrano che il nostro marcatore incorporato può avere un'elevata efficienza di rilevamento mantenendo al contempo un'elevata utilità per i compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre validato la coerenza dell'incorporamento fornito visualizzando l'incorporamento delle frasi su quattro set di dati VUA-PCA. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere nelle figure, è difficile distinguere tra gli embedding backdoor e quelli normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "È tutto, grazie. Benvenuti a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Vasudha e sono una candidata al dottorato in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per l'ACL 2023 come articolo lungo, \"Apprendimento per trasferimento per la rilevazione di dissonanze, affrontando la sfida della classe rara\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e spiegando perché è importante studiarla nel campo del linguaggio. In parole povere, la dissonanza cognitiva è la presenza di due credenze o azioni che sono in conflitto tra loro."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, in questo caso, una persona afferma: \"So che le sigarette potrebbero uccidermi\" e poi dice: \"Ho preso un paio di sigarette dopo la riunione\". Questa convinzione e questa azione sono incongruenti e sono in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il fatto che io non pensi di poter mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di coerenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nella vita quotidiana, è raro trovarla espressa nel linguaggio tra le altre tipologie di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché è importante? Lo studio della distanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a monitorare le tendenze e i cambiamenti di credenze, valori e atteggiamenti nelle popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio delle espressioni linguistiche di dissenso può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Per raggiungere l'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza, come mostrato nel diagramma di flusso qui sotto."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati elaborati utilizzando un parser PDTB e le coppie di unità di discorso sono state annotate in base alle linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Dopo aver raccolto circa mille esempi di coppie di unità discorsive, abbiamo eseguito un addestramento per un classificatore iniziale, addestrato solo su 43 esempi di dissonanze. Non sorprende che il classificatore non abbia funzionato molto meglio del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la bassa incidenza di dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema dell'estrema rarità."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo per l'annotazione, in modo da poter raccogliere più campioni dissonanti in meno round di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di rilevare la classe di dissonanza, abbiamo avviato il processo di apprendimento attivo trasferendo i pesi da attività strettamente correlate."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo trasferito da due compiti diversi. La classificazione della posizione di disaccordo indipendente dall'argomento è un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamiamo dibattito qui e sulla classificazione binaria delle classi di espansione e confronto del PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza, e le chiamiamo CE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che, trasferendo le prestazioni zero-shot sul dataset annotato, sono già molto migliori rispetto al caso, con il miglior risultato pari a 0,62 di AUC."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, in merito alla messa a punto iterativa su entrambi i compiti, abbiamo riscontrato che la messa a punto del compito CE seguita da una messa a punto ulteriore sul compito di dibattito ha prodotto una performance a zero shot molto migliore. Per questo motivo, è questo il modello che abbiamo utilizzato per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati provenienti da ogni ciclo di apprendimento attivo e annotazioni. Il metodo cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre quello iterativo aggiorna il modello addestrando il modello sull'ultimo set di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "In base alle diverse strategie, abbiamo riscontrato che il metodo cumulativo ha funzionato altrettanto bene o meglio del metodo iterativo in ogni caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, abbiamo utilizzato una strategia di probabilità di classe rara (PRC) per selezionare principalmente esempi che con ogni probabilità sono dissonanti rispetto al modello attuale in qualsiasi round di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo confrontato questo con le altre strategie di intelligenza artificiale all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia, anche se la differenza è minima. Si noti che le prestazioni sono significativamente inferiori per le strategie casuali."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Con ulteriori round di AL con le due migliori strategie, abbiamo migliorato la classificazione della distanza AUC a 0,75, che è la migliore prestazione che abbiamo ottenuto finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre verificato la fattibilità di ogni strategia per quanto riguarda la qualità dell'annotazione e i costi per gli annotatori. Abbiamo riscontrato che PRC ha la percentuale più alta di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori trovano gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo riscontrato che PRC è una semplice strategia di apprendimento attivo per l'acquisizione di classi rare e che l'apprendimento attivo con trasferimento di apprendimento opportunamente progettato può aiutare in modo significativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Scopriamo inoltre che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive all'interno del dominio traggono beneficio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i link al nostro codice, al dataset e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
