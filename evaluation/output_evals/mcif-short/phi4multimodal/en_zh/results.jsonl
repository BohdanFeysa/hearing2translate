{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "welcome to our presentation of DeepLAG, a new corpus for German text segmentation on the document level and on the sentence level.\" 我的名字是Regina", "metrics": {"bleu_score": 1.1432220216087032, "chrf_score": 1.8922555303071849, "xcomet_score": 0.44796064496040344, "xcomet_qe_score": 0.5674952268600464, "metricx_score": 16.646747589111328, "metricx_qe_score": 8.93902587890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Stoddart,我将指导您完成演示的第一部分。", "metrics": {"bleu_score": 37.950330118687134, "chrf_score": 31.11179037285207, "xcomet_score": 0.6550601720809937, "xcomet_qe_score": 0.7455733418464661, "metricx_score": 6.333152770996094, "metricx_qe_score": 6.1113972663879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们首先定义文本简化。", "metrics": {"bleu_score": 26.934513946065273, "chrf_score": 25.398193134124504, "xcomet_score": 0.9859533309936523, "xcomet_qe_score": 0.9934237003326416, "metricx_score": 0.21629703044891357, "metricx_qe_score": 0.2516714334487915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是指为特定目标群体(如阅读障碍者或非母语者)改写文本,以提高其文本理解度的过程。", "metrics": {"bleu_score": 32.38336122796426, "chrf_score": 27.022338322393182, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4884490668773651, "metricx_qe_score": 0.40411293506622314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要训练文本标记模型,我们需要文本的并列对,例如文档或句子。", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 40.693476900817, "xcomet_score": 0.8696296215057373, "xcomet_qe_score": 0.805954098701477, "metricx_score": 2.6787655353546143, "metricx_qe_score": 3.4608511924743652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,你可以看到一个复杂的德语句子和它的翻译成普通语言的平行对齐句子对。", "metrics": {"bleu_score": 61.764401993251695, "chrf_score": 58.02844534767016, "xcomet_score": 0.9784862995147705, "xcomet_qe_score": 0.8806290626525879, "metricx_score": 1.702710509300232, "metricx_qe_score": 2.1847033500671387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以使用不同的技术,如在示例中看到的词汇替换、子句删除、子句删除、重新排序或插入短语。", "metrics": {"bleu_score": 41.090758303906135, "chrf_score": 40.14253601589074, "xcomet_score": 0.6858662366867065, "xcomet_qe_score": 0.6670440435409546, "metricx_score": 3.142134666442871, "metricx_qe_score": 3.548532009124756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们的新的语料库D plane,因为在最近几年中,现有语料库存在一些问题。", "metrics": {"bleu_score": 47.33265841192934, "chrf_score": 43.39163296253552, "xcomet_score": 0.5893951654434204, "xcomet_qe_score": 0.6241931319236755, "metricx_score": 7.216923713684082, "metricx_qe_score": 7.72233772277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库在这里太小了,无法训练一个文本简化模型。", "metrics": {"bleu_score": 40.66689638009577, "chrf_score": 38.56927479849539, "xcomet_score": 0.8589460849761963, "xcomet_qe_score": 0.7973117232322693, "metricx_score": 1.901914358139038, "metricx_qe_score": 2.2482869625091553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9784777164459229, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了我们的新的语料库dlan,它被分为两个子语料库dlan,apa和dlanweb。", "metrics": {"bleu_score": 36.336800477245156, "chrf_score": 24.915973803666304, "xcomet_score": 0.7070726156234741, "xcomet_qe_score": 0.7196630239486694, "metricx_score": 7.804823398590088, "metricx_qe_score": 6.983280658721924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dlan是基于新闻文本的。", "metrics": {"bleu_score": 53.7284965911771, "chrf_score": 25.02715619498624, "xcomet_score": 0.7910624146461487, "xcomet_qe_score": 0.7275615334510803, "metricx_score": 5.443368911743164, "metricx_qe_score": 6.9043965339660645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在DPLANE API中,我们手动对483份文档进行了对齐。", "metrics": {"bleu_score": 25.187508351984913, "chrf_score": 24.934686525104727, "xcomet_score": 0.8728744983673096, "xcomet_qe_score": 0.8865708708763123, "metricx_score": 2.4352431297302246, "metricx_qe_score": 2.0108017921447754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约是30,000个13,000个并行句子对。", "metrics": {"bleu_score": 10.511846841633782, "chrf_score": 33.74291367385799, "xcomet_score": 0.7086114883422852, "xcomet_qe_score": 0.7844111919403076, "metricx_score": 9.15296745300293, "metricx_qe_score": 8.579919815063477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于深度网,这个语料库包括不同的域名,我们还将所有这七百五十份文档手动对齐在一端,另一端则使用自动对齐方法。", "metrics": {"bleu_score": 41.284911804204654, "chrf_score": 30.58023524839209, "xcomet_score": 0.46377068758010864, "xcomet_qe_score": 0.4785599708557129, "metricx_score": 5.558318138122559, "metricx_qe_score": 4.42680025100708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In total, we result in 30,450 sentence pairs.", "metrics": {"bleu_score": 3.435488317233919, "chrf_score": 18.04441081197959, "xcomet_score": 0.9075554609298706, "xcomet_qe_score": 0.9954313039779663, "metricx_score": 7.408326625823975, "metricx_qe_score": 16.908235549926758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析。例如,在同化的类型上。 如你", "metrics": {"bleu_score": 36.00167847773201, "chrf_score": 28.039198349543177, "xcomet_score": 0.630003035068512, "xcomet_qe_score": 0.6600096225738525, "metricx_score": 7.460832595825195, "metricx_qe_score": 6.061415195465088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里看到的,圣经文本比新闻文本或语言学习者文本更加强大和简化。", "metrics": {"bleu_score": 61.901840210746634, "chrf_score": 64.18822300495869, "xcomet_score": 0.781456708908081, "xcomet_qe_score": 0.7542206048965454, "metricx_score": 3.430853843688965, "metricx_qe_score": 3.85068678855896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在所有层面上,例如,关于词法简化、结构简化或整体简化的层面。", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 40.40493466866867, "xcomet_score": 0.8487548828125, "xcomet_qe_score": 0.8456350564956665, "metricx_score": 0.9732928276062012, "metricx_qe_score": 0.9720673561096191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的DPLANE语料库具有各种不同的简化转换。", "metrics": {"bleu_score": 65.73798604900217, "chrf_score": 54.121201074876446, "xcomet_score": 0.8938387036323547, "xcomet_qe_score": 0.7650606632232666, "metricx_score": 1.7186284065246582, "metricx_qe_score": 2.2078957557678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在DPLANE API语料库中,我们有更多的重新排序和单词添加,而在DPLANE Web语料库中则没有。", "metrics": {"bleu_score": 31.95385318738486, "chrf_score": 30.474445057577643, "xcomet_score": 0.6884316205978394, "xcomet_qe_score": 0.6644762754440308, "metricx_score": 3.1140332221984863, "metricx_qe_score": 2.959467887878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在Web语料库中,我们有更多的重述。", "metrics": {"bleu_score": 26.991220565981227, "chrf_score": 22.508356049948127, "xcomet_score": 0.8940056562423706, "xcomet_qe_score": 0.8191206455230713, "metricx_score": 3.3617215156555176, "metricx_qe_score": 2.8375930786132812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看我们能用这个语料库做些什么。", "metrics": {"bleu_score": 68.91557807535084, "chrf_score": 60.66442658934918, "xcomet_score": 0.9959640502929688, "xcomet_qe_score": 0.9830069541931152, "metricx_score": 0.23595795035362244, "metricx_qe_score": 0.43287187814712524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是 Omar,现在我将讨论我们数据集深层的用例。", "metrics": {"bleu_score": 29.37750077604792, "chrf_score": 27.096027725023873, "xcomet_score": 0.8089442253112793, "xcomet_qe_score": 0.8165547847747803, "metricx_score": 3.9676735401153564, "metricx_qe_score": 4.478129863739014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个用例,我们可以评估自动对齐方法。 在过去的", "metrics": {"bleu_score": 71.78970818142892, "chrf_score": 76.75317316176856, "xcomet_score": 0.6725572347640991, "xcomet_qe_score": 0.6273292303085327, "metricx_score": 4.389676570892334, "metricx_qe_score": 1.7874979972839355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "几年里,有很多对齐方法,但在机器翻译的背景下。 Where we have two parallel documents written in different languages, and we want to extract alignments of sentences in post documents.", "metrics": {"bleu_score": 18.16090165588572, "chrf_score": 11.922436788126296, "xcomet_score": 0.5873581171035767, "xcomet_qe_score": 0.5641751289367676, "metricx_score": 13.779747009277344, "metricx_qe_score": 12.008057594299316, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是在我们的用例中,我们试图在具有相同语言、相同内容但处于不同复杂性水平的两个并行文档的句子之间提取对齐。", "metrics": {"bleu_score": 43.57301227405205, "chrf_score": 38.4407156280183, "xcomet_score": 0.8945719003677368, "xcomet_qe_score": 0.8685222268104553, "metricx_score": 1.9647597074508667, "metricx_qe_score": 2.503671646118164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,由于我们有一个具有手动对齐句子的数据集,我们可以将这些句子用作标准对齐来评估一些建议的对齐方法。", "metrics": {"bleu_score": 45.99317471314287, "chrf_score": 36.42601159198608, "xcomet_score": 0.8811780214309692, "xcomet_qe_score": 0.8167545795440674, "metricx_score": 3.1425485610961914, "metricx_qe_score": 3.536933422088623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提议的方法做了一些调整。我们已经在论文中发布了所有这些调整和代码来运行我们的实验。", "metrics": {"bleu_score": 28.72607489687572, "chrf_score": 28.210174658088167, "xcomet_score": 0.9253031015396118, "xcomet_qe_score": 0.8809006214141846, "metricx_score": 1.5044752359390259, "metricx_qe_score": 1.5599241256713867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,最好的对齐自动对齐方法用于德语文本简", "metrics": {"bleu_score": 52.80197959835354, "chrf_score": 35.96413590165357, "xcomet_score": 0.653135359287262, "xcomet_qe_score": 0.5990390777587891, "metricx_score": 8.844826698303223, "metricx_qe_score": 8.774406433105469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "化是 massalign 方法。 你还可以在论文中找到运行此方法的代码。", "metrics": {"bleu_score": 37.61118548918197, "chrf_score": 32.2332777695525, "xcomet_score": 0.43597105145454407, "xcomet_qe_score": 0.13366389274597168, "metricx_score": 3.362489700317383, "metricx_qe_score": 3.909280300140381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是自动文本简化的情况。 通过微调语言模型来从复杂的输入文本中生成简化的文本。", "metrics": {"bleu_score": 68.7769552057883, "chrf_score": 65.20140047849698, "xcomet_score": 0.9911949634552002, "xcomet_qe_score": 0.9829579591751099, "metricx_score": 1.573956847190857, "metricx_qe_score": 1.6246355772018433, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们已经对两个不同的模型进行了微调。我们", "metrics": {"bleu_score": 57.77966168512882, "chrf_score": 64.93574529267563, "xcomet_score": 0.816186249256134, "xcomet_qe_score": 0.7944034337997437, "metricx_score": 3.9806575775146484, "metricx_qe_score": 0.8254272937774658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "已经对长impower的模型进行了微调,以产生文档级的简化。 我们还对普通贝叶斯模型进行了微调,以产生句子级的简化。", "metrics": {"bleu_score": 58.90953986367725, "chrf_score": 43.4874745475755, "xcomet_score": 0.6411812901496887, "xcomet_qe_score": 0.654028058052063, "metricx_score": 8.159674644470215, "metricx_qe_score": 8.46628189086914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你也可以找到所有的检查点,你可以在论文中查看我们实验的分数和评估指标的更多细节。", "metrics": {"bleu_score": 61.64559105429568, "chrf_score": 54.54342261424374, "xcomet_score": 0.947870135307312, "xcomet_qe_score": 0.9169361591339111, "metricx_score": 1.4374624490737915, "metricx_qe_score": 2.1706089973449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或可以获得比基线分数更好的分数。 And we propose those results as a benchmark, a base benchmark, for the problem of automatic text simplification in the future.", "metrics": {"bleu_score": 41.745076197876905, "chrf_score": 30.236901078369005, "xcomet_score": 0.8397941589355469, "xcomet_qe_score": 0.8018944263458252, "metricx_score": 7.600817680358887, "metricx_qe_score": 6.56318998336792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注,我们希望在会议期间能与大家见面。", "metrics": {"bleu_score": 70.16116562610198, "chrf_score": 62.52698474673019, "xcomet_score": 0.9894882440567017, "xcomet_qe_score": 0.9755129814147949, "metricx_score": 0.5331041812896729, "metricx_qe_score": 0.3833646774291992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我叫Sadam Shkurkofsky,今天的演讲是关于协调的依赖结构。", "metrics": {"bleu_score": 25.268448683574693, "chrf_score": 19.08387973069461, "xcomet_score": 0.699390172958374, "xcomet_qe_score": 0.6378707885742188, "metricx_score": 7.497950077056885, "metricx_qe_score": 6.536460876464844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如你所知,假设不同理论和语料库方法的不同依赖结构。", "metrics": {"bleu_score": 32.082167656972345, "chrf_score": 30.037217008406348, "xcomet_score": 0.8081766963005066, "xcomet_qe_score": 0.769119143486023, "metricx_score": 3.9652109146118164, "metricx_qe_score": 3.8385162353515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在宇宙依赖中,Lisa Bart和Maggie的结构是协调的。 是这样的,第一个共轭是整个坐标结构的头部", "metrics": {"bleu_score": 16.681919020076, "chrf_score": 32.26116428896881, "xcomet_score": 0.5542667508125305, "xcomet_qe_score": 0.5188487768173218, "metricx_score": 7.476221084594727, "metricx_qe_score": 7.051150798797607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所以在这种情况下,丽莎。", "metrics": {"bleu_score": 6.837203339116283, "chrf_score": 4.6501638296172, "xcomet_score": 0.899459719657898, "xcomet_qe_score": 0.9026868343353271, "metricx_score": 2.018641233444214, "metricx_qe_score": 1.7447600364685059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在Igor Miltić的意义文本理论中,采用类似的方法,其中整个坐标结构再次由第一个主谓一致词领导。因此,", "metrics": {"bleu_score": 32.175528094047266, "chrf_score": 27.25343391640876, "xcomet_score": 0.5155904293060303, "xcomet_qe_score": 0.5194277763366699, "metricx_score": 6.835423469543457, "metricx_qe_score": 5.187171459197998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是对称的,", "metrics": {"bleu_score": 44.30006936196753, "chrf_score": 35.382845226341736, "xcomet_score": 0.8530333638191223, "xcomet_qe_score": 0.8579867482185364, "metricx_score": 4.3713788986206055, "metricx_qe_score": 2.505286455154419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.996912956237793, "xcomet_qe_score": 0.9818440675735474, "metricx_score": 0.2157692313194275, "metricx_qe_score": 0.26781266927719116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们挑选出一个主谓一致词。", "metrics": {"bleu_score": 11.498759556447217, "chrf_score": 14.423076923076925, "xcomet_score": 0.8040218353271484, "xcomet_qe_score": 0.7954162955284119, "metricx_score": 1.888192892074585, "metricx_qe_score": 2.804103374481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在还有对代码和结构的对称方法,例如Prague方法。", "metrics": {"bleu_score": 20.915990037763148, "chrf_score": 18.006844076565645, "xcomet_score": 0.7080867290496826, "xcomet_qe_score": 0.6982272863388062, "metricx_score": 5.973371982574463, "metricx_qe_score": 5.382930278778076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "连接头方法假设在连接依赖树中,其中结构由连接头指向。", "metrics": {"bleu_score": 6.280495217031854, "chrf_score": 8.913634489592665, "xcomet_score": 0.5382194519042969, "xcomet_qe_score": 0.5651977062225342, "metricx_score": 7.320365905761719, "metricx_qe_score": 6.677079200744629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从端到所有合同获得了一些依赖关系。", "metrics": {"bleu_score": 18.59957770558965, "chrf_score": 20.31978818363379, "xcomet_score": 0.7150216102600098, "xcomet_qe_score": 0.7690691351890564, "metricx_score": 6.221672058105469, "metricx_qe_score": 6.101571083068848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多头方法。例如,在卡尔森的词法中使用。 在这里,我们可以说所有的主语都是代词结构的头部", "metrics": {"bleu_score": 26.494436270201984, "chrf_score": 20.632918543540974, "xcomet_score": 0.4548269510269165, "xcomet_qe_score": 0.49826717376708984, "metricx_score": 4.606310844421387, "metricx_qe_score": 4.347976207733154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因此我们可以从代词从代词中", "metrics": {"bleu_score": 7.705829865251781, "chrf_score": 8.693384100760435, "xcomet_score": 0.3224726915359497, "xcomet_qe_score": 0.1653180569410324, "metricx_score": 11.251360893249512, "metricx_qe_score": 15.447275161743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取依赖关系。这些是关于的。 现在,这", "metrics": {"bleu_score": 2.392598022685055, "chrf_score": 1.5432098765432098, "xcomet_score": 0.1385987401008606, "xcomet_qe_score": 0.1343606561422348, "metricx_score": 21.400630950927734, "metricx_qe_score": 19.629953384399414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "篇论文的目的是提出一个关于对称结构的协调的新的论点,比如这两个和反对不对称结构的协调,比如这两个。", "metrics": {"bleu_score": 37.80736892118603, "chrf_score": 33.85284927606503, "xcomet_score": 0.4938119053840637, "xcomet_qe_score": 0.59892338514328, "metricx_score": 5.355523586273193, "metricx_qe_score": 4.543737411499023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9997062683105469, "xcomet_qe_score": 1.0, "metricx_score": 0.1774456948041916, "metricx_qe_score": 0.21148386597633362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论点基于依赖性和经济性的原则,我将根据这些示例来解释。", "metrics": {"bleu_score": 39.67987453167855, "chrf_score": 33.3720647731455, "xcomet_score": 0.816203236579895, "xcomet_qe_score": 0.7919856905937195, "metricx_score": 2.381760835647583, "metricx_qe_score": 3.9705417156219482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如你们可能知道的那样,直接宾语应该靠近动词,而副词可以远离动词。比如", "metrics": {"bleu_score": 24.624087743244765, "chrf_score": 20.445108462016208, "xcomet_score": 0.8525192737579346, "xcomet_qe_score": 0.7926571369171143, "metricx_score": 1.9766383171081543, "metricx_qe_score": 1.1695979833602905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "说,昨天读的《三月》是可以的,因为直接宾语靠近动词。 \"While March read yesterday it is much worse, right", "metrics": {"bleu_score": 28.920147774447923, "chrf_score": 34.083581808339176, "xcomet_score": 0.1967320442199707, "xcomet_qe_score": 0.39806264638900757, "metricx_score": 14.495277404785156, "metricx_qe_score": 15.22114086151123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9702941179275513, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 0.4533194899559021, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Because here between the verb and the direct object there is an adjunct yesterday.\" 但是", "metrics": {"bleu_score": 1.6268011973185224, "chrf_score": 17.53713546222496, "xcomet_score": 0.8154851198196411, "xcomet_qe_score": 0.8503201007843018, "metricx_score": 8.662394523620605, "metricx_qe_score": 12.355798721313477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当直接对象非常重且非常长时,这种效果可能会得到改善,因为", "metrics": {"bleu_score": 11.017841899461907, "chrf_score": 13.431120734924754, "xcomet_score": 0.4049813151359558, "xcomet_qe_score": 0.6231580972671509, "metricx_score": 7.662435531616211, "metricx_qe_score": 4.285898208618164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样它就可以移动到后置位置。", "metrics": {"bleu_score": 14.914147968282963, "chrf_score": 17.35447177341847, "xcomet_score": 0.8629562854766846, "xcomet_qe_score": 0.85716712474823, "metricx_score": 2.8236427307128906, "metricx_qe_score": 2.8749709129333496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在这里说明了。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 8.630952380952378, "xcomet_score": 0.8122142553329468, "xcomet_qe_score": 0.8761526942253113, "metricx_score": 1.1384469270706177, "metricx_qe_score": 0.9510176181793213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这两句话都很好。", "metrics": {"bleu_score": 9.548450962056531, "chrf_score": 9.416750870102167, "xcomet_score": 0.9280425310134888, "xcomet_qe_score": 0.9024364352226257, "metricx_score": 0.39084896445274353, "metricx_qe_score": 0.5093563199043274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "马特昨天读了这本关于BCS的绝对迷", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16819071769714355, "xcomet_qe_score": 0.18585407733917236, "metricx_score": 10.284876823425293, "metricx_qe_score": 12.616304397583008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "人的书。没关系。我们有这个长和p。 但是也可以说,马尔", "metrics": {"bleu_score": 8.640609739997757, "chrf_score": 9.77136752136752, "xcomet_score": 0.1495344489812851, "xcomet_qe_score": 0.14075426757335663, "metricx_score": 17.935016632080078, "metricx_qe_score": 17.658039093017578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特昨天读了一本关于蜜蜂的非常有趣的书。 所以这里的推理是这样的:这是可能的,", "metrics": {"bleu_score": 2.6227235705350953, "chrf_score": 1.8213494891430135, "xcomet_score": 0.16258572041988373, "xcomet_qe_score": 0.1423165649175644, "metricx_score": 5.98541784286499, "metricx_qe_score": 6.551875114440918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为即使这个句子违反了一般语法原则,即直接宾语应该在动词旁边。 它满足了最短依赖关系最短化原理,该原理指出较短的依赖关系是首选的。 所以", "metrics": {"bleu_score": 40.03021868908096, "chrf_score": 34.02104105732354, "xcomet_score": 0.6963299512863159, "xcomet_qe_score": 0.7054806351661682, "metricx_score": 6.945927143096924, "metricx_qe_score": 6.721569061279297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示了这两种结构中不常见的关键依赖关系的长度。", "metrics": {"bleu_score": 43.32545670992549, "chrf_score": 37.273583506407206, "xcomet_score": 0.8669634461402893, "xcomet_qe_score": 0.7504518032073975, "metricx_score": 2.0798091888427734, "metricx_qe_score": 4.474597454071045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里我们有一个从红色到长度为七个单词的附加的依赖关系,以及从红色到长度为四个单词的书籍的依赖关系,所以总共是十一个。", "metrics": {"bleu_score": 22.487025478071956, "chrf_score": 18.694930885366603, "xcomet_score": 0.49328702688217163, "xcomet_qe_score": 0.4870266914367676, "metricx_score": 9.63145923614502, "metricx_qe_score": 9.901068687438965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动时,当你交换这两个成分时,这两个依赖项的总和变成了六,", "metrics": {"bleu_score": 43.24055278038292, "chrf_score": 38.31997109233321, "xcomet_score": 0.6329779028892517, "xcomet_qe_score": 0.6115113496780396, "metricx_score": 5.638956069946289, "metricx_qe_score": 6.188687801361084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?所以从11变成6,更短了。", "metrics": {"bleu_score": 3.716499092256817, "chrf_score": 7.922956170991351, "xcomet_score": 0.9100109338760376, "xcomet_qe_score": 0.8345138430595398, "metricx_score": 2.463616132736206, "metricx_qe_score": 2.2590935230255127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来很好的原因。", "metrics": {"bleu_score": 59.687741756345, "chrf_score": 63.398434932133405, "xcomet_score": 0.9151620864868164, "xcomet_qe_score": 0.8941857218742371, "metricx_score": 1.021336317062378, "metricx_qe_score": 1.0576093196868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9702941179275513, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 0.4533194899559021, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9992790222167969, "xcomet_qe_score": 0.997222900390625, "metricx_score": 0.1849263608455658, "metricx_qe_score": 0.19376564025878906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "what we did, we extracted various statistics from about coordination from the enhanced version of the PenTreeBank and see the paper why wouldn't you sign university dependencies. 这些统计数据证实了以前多次观察到的左连词往往较短。", "metrics": {"bleu_score": 18.159168079971995, "chrf_score": 13.73094120374064, "xcomet_score": 0.5554955005645752, "xcomet_qe_score": 0.5251723527908325, "metricx_score": 13.310920715332031, "metricx_qe_score": 14.44743824005127, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "盐和胡椒和不是胡椒和盐,都是以音节为单位测量的。 而且,曾经提到", "metrics": {"bleu_score": 17.99581628671167, "chrf_score": 11.247223493372116, "xcomet_score": 0.40239498019218445, "xcomet_qe_score": 0.26159238815307617, "metricx_score": 7.776007652282715, "metricx_qe_score": 8.798672676086426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的观察是,这种趋势随着长度差异而增长。", "metrics": {"bleu_score": 25.281651405632807, "chrf_score": 23.685459458019547, "xcomet_score": 0.7929307818412781, "xcomet_qe_score": 0.7823951244354248, "metricx_score": 4.146965026855469, "metricx_qe_score": 4.40952730178833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以当两种连词长度的差异变大时,较短的连词更倾向于成为较强的第一种连词。因此", "metrics": {"bleu_score": 21.030216085075523, "chrf_score": 18.551004020306355, "xcomet_score": 0.7735463380813599, "xcomet_qe_score": 0.7779207229614258, "metricx_score": 4.3257832527160645, "metricx_qe_score": 2.476412057876587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",左边较短连词的比例更大。", "metrics": {"bleu_score": 45.823488902304994, "chrf_score": 40.54771360450965, "xcomet_score": 0.877678632736206, "xcomet_qe_score": 0.8439862728118896, "metricx_score": 3.384321928024292, "metricx_qe_score": 3.8278770446777344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是这篇论文中值得注意的是,我们发现这种趋势只发生在左侧的覆盖物不存在时。 所以", "metrics": {"bleu_score": 12.80534742293527, "chrf_score": 14.027755238756717, "xcomet_score": 0.6440396308898926, "xcomet_qe_score": 0.34162893891334534, "metricx_score": 6.931162357330322, "metricx_qe_score": 5.821963787078857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9702941179275513, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 0.4533194899559021, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴顿·李萨,所以州长在左边。", "metrics": {"bleu_score": 18.702869706385968, "chrf_score": 12.95644003926289, "xcomet_score": 0.47880086302757263, "xcomet_qe_score": 0.6497701406478882, "metricx_score": 3.76363205909729, "metricx_qe_score": 2.8253633975982666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,它不存在。荷马来了,打了个喷嚏。", "metrics": {"bleu_score": 30.166728533047458, "chrf_score": 16.195522776459153, "xcomet_score": 0.7938916683197021, "xcomet_qe_score": 0.7918469309806824, "metricx_score": 3.5775039196014404, "metricx_qe_score": 3.917421340942383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们有两个动词的协调,没有外部外部控制器,对吧?因此,", "metrics": {"bleu_score": 38.52818814494647, "chrf_score": 44.25217043133734, "xcomet_score": 0.6790608167648315, "xcomet_qe_score": 0.6384458541870117, "metricx_score": 5.955243110656738, "metricx_qe_score": 5.858845233917236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左连词更喜欢变短。差异越大,差异越大。", "metrics": {"bleu_score": 21.31953951703857, "chrf_score": 19.04613017396736, "xcomet_score": 0.5763996839523315, "xcomet_qe_score": 0.5232217311859131, "metricx_score": 7.3422017097473145, "metricx_qe_score": 7.851184368133545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,当右侧的governance在这里作为左侧governance控制协调网络时,这种效果消失了。 因此,", "metrics": {"bleu_score": 5.715172894221706, "chrf_score": 8.095820061077179, "xcomet_score": 0.3376363515853882, "xcomet_qe_score": 0.26646724343299866, "metricx_score": 13.073543548583984, "metricx_qe_score": 10.758312225341797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度来显示第一列中的单词,第二列中的音节,第三列中的单词。因此,我将专注于第三列。 What we see", "metrics": {"bleu_score": 9.778262914018889, "chrf_score": 11.984703389472465, "xcomet_score": 0.6409689784049988, "xcomet_qe_score": 0.3974302411079407, "metricx_score": 7.6084089279174805, "metricx_qe_score": 8.716553688049316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "here is that when the governor is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.12838393449783325, "xcomet_qe_score": 0.08963355422019958, "metricx_score": 21.946571350097656, "metricx_qe_score": 23.65801429748535, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on the left. 左连词的趋势是变短的,这种趋势随着单词的绝对差异而稳步增长。在句子中的协调中,观察到的情况与没有主语相同,", "metrics": {"bleu_score": 18.63046828945761, "chrf_score": 17.65705371754842, "xcomet_score": 0.4478098154067993, "xcomet_qe_score": 0.46897777915000916, "metricx_score": 10.788419723510742, "metricx_qe_score": 12.333822250366211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当主语在右边时,这种趋势消失了。", "metrics": {"bleu_score": 17.94560313432444, "chrf_score": 17.770441917846995, "xcomet_score": 0.8669425845146179, "xcomet_qe_score": 0.769506573677063, "metricx_score": 2.3160886764526367, "metricx_qe_score": 5.931735038757324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了如何通过这两种结构和这两种结构来反对对称结构的协调。", "metrics": {"bleu_score": 18.31461663862816, "chrf_score": 20.844169752810416, "xcomet_score": 0.5933550596237183, "xcomet_qe_score": 0.5711911916732788, "metricx_score": 7.843349456787109, "metricx_qe_score": 8.034125328063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以请看论文的全文和论点,", "metrics": {"bleu_score": 6.167638062423937, "chrf_score": 6.568694923030677, "xcomet_score": 0.8681031465530396, "xcomet_qe_score": 0.8611971735954285, "metricx_score": 1.9273779392242432, "metricx_qe_score": 0.8609388470649719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抱歉,并在后续会议上与我们讨论。", "metrics": {"bleu_score": 5.816635421147515, "chrf_score": 5.873426258992806, "xcomet_score": 0.1550453007221222, "xcomet_qe_score": 0.16010190546512604, "metricx_score": 5.14552640914917, "metricx_qe_score": 4.626719951629639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是华盛顿大学的博士生张斌。", "metrics": {"bleu_score": 63.624138156344834, "chrf_score": 43.34960499938349, "xcomet_score": 0.80461585521698, "xcomet_qe_score": 0.8606458902359009, "metricx_score": 1.0673121213912964, "metricx_qe_score": 0.4239788055419922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,从预训练数据到语言模型再到下游任务,跟踪导致不公平NLP模型的政治偏见的踪迹。", "metrics": {"bleu_score": 76.28689065170745, "chrf_score": 72.16335354678412, "xcomet_score": 0.8198399543762207, "xcomet_qe_score": 0.7681176662445068, "metricx_score": 1.6961417198181152, "metricx_qe_score": 2.1689062118530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以语言模型是用大量的Web流量数据训练的。", "metrics": {"bleu_score": 32.37353837818466, "chrf_score": 24.87734486343175, "xcomet_score": 0.8667330741882324, "xcomet_qe_score": 0.8282651901245117, "metricx_score": 2.4678943157196045, "metricx_qe_score": 2.80452036857605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 64.42271946445945, "chrf_score": 64.98697375335293, "xcomet_score": 0.8035382032394409, "xcomet_qe_score": 0.741437554359436, "metricx_score": 1.5606460571289062, "metricx_qe_score": 2.311195135116577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C4样本的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 76.38614334414427, "chrf_score": 73.63536340632349, "xcomet_score": 0.8567588329315186, "xcomet_qe_score": 0.7743640542030334, "metricx_score": 1.554986834526062, "metricx_qe_score": 1.6125469207763672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "This has created a mixed blessing for language model applications. 因此,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7588942050933838, "xcomet_qe_score": 0.7814121246337891, "metricx_score": 23.568519592285156, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从不同的角度学习,这种多元化的观点庆祝了民主和多元化的思想。", "metrics": {"bleu_score": 41.52031382769598, "chrf_score": 41.32046101829831, "xcomet_score": 0.7713257074356079, "xcomet_qe_score": 0.8219647407531738, "metricx_score": 2.776031255722046, "metricx_qe_score": 3.7412374019622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上是社会偏见的,可能导致下游任务应用程序中的潜在公平性问题。", "metrics": {"bleu_score": 61.54276070576479, "chrf_score": 52.199461317835414, "xcomet_score": 0.9119855165481567, "xcomet_qe_score": 0.9110841751098633, "metricx_score": 1.000405192375183, "metricx_qe_score": 1.2715978622436523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To this end, we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks, specifically by asking the following questions. 首先,我们如何评估语言模型的政治倾向,以及预训练数据可能对这种政治偏见起什么作用?", "metrics": {"bleu_score": 28.815226760886503, "chrf_score": 20.287426008321265, "xcomet_score": 0.9469234943389893, "xcomet_qe_score": 0.952567458152771, "metricx_score": 10.898923873901367, "metricx_qe_score": 21.878293991088867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在下游任务上的实际表现如何,以及这是否可能导致公平性问题在NLP应用中出现?", "metrics": {"bleu_score": 63.813144826853765, "chrf_score": 59.45978292318665, "xcomet_score": 0.9249119162559509, "xcomet_qe_score": 0.8811691403388977, "metricx_score": 1.962133526802063, "metricx_qe_score": 2.860633373260498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们首先提出使用政治问卷(例如政治问卷测试)以不同的提示格式提示语言模型。", "metrics": {"bleu_score": 47.72340753347569, "chrf_score": 40.971825216182964, "xcomet_score": 0.9080421924591064, "xcomet_qe_score": 0.8275976181030273, "metricx_score": 1.86408269405365, "metricx_qe_score": 2.6084721088409424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们能够在政治科学文献的基础上进行自动评估。", "metrics": {"bleu_score": 55.70963651533937, "chrf_score": 54.93267731541028, "xcomet_score": 0.9881294965744019, "xcomet_qe_score": 0.9175472259521484, "metricx_score": 0.9690632820129395, "metricx_qe_score": 1.23720383644104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,初步结果表明,首先,语言模型确实具有不同的政治倾向。", "metrics": {"bleu_score": 60.94880572755877, "chrf_score": 54.89922025321652, "xcomet_score": 0.9477297067642212, "xcomet_qe_score": 0.9804353713989258, "metricx_score": 0.7733235955238342, "metricx_qe_score": 0.8749079704284668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "他们占据了政治地图上的所有四个象限。", "metrics": {"bleu_score": 65.40585844910977, "chrf_score": 56.30428667193373, "xcomet_score": 0.9039648771286011, "xcomet_qe_score": 0.8389977216720581, "metricx_score": 1.495532512664795, "metricx_qe_score": 2.0694239139556885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到GPT-4是所有语言模型中最自由的语言模型,而GPT系列通常比BERT系列和其变体更具社会自由主义。", "metrics": {"bleu_score": 47.30074447344452, "chrf_score": 49.88263821422855, "xcomet_score": 0.9069190621376038, "xcomet_qe_score": 0.8577203154563904, "metricx_score": 1.8863215446472168, "metricx_qe_score": 1.450414776802063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们的目标是研究语言模型的政治偏见在多大程度上是从训练数据中提取的。", "metrics": {"bleu_score": 72.62722929422617, "chrf_score": 67.17360957487585, "xcomet_score": 0.9793580770492554, "xcomet_qe_score": 0.8073550462722778, "metricx_score": 0.8304851055145264, "metricx_qe_score": 1.5557610988616943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过进一步对语言模型检查点进行预训练来进行控制实验。六个不同的政党和公司分为新闻和社交媒体,进一步分为其政治倾向。 通过进一步对语言模型进行偏见和", "metrics": {"bleu_score": 44.21038334611588, "chrf_score": 41.564970207261695, "xcomet_score": 0.4625840485095978, "xcomet_qe_score": 0.4969934821128845, "metricx_score": 10.915847778320312, "metricx_qe_score": 8.501717567443848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语料库的预训练,我们可以看到语言模型的意识形态坐标也相应地发生了变化。", "metrics": {"bleu_score": 50.865355157377536, "chrf_score": 56.71097258540275, "xcomet_score": 0.712334394454956, "xcomet_qe_score": 0.7513659596443176, "metricx_score": 3.784564256668091, "metricx_qe_score": 3.830139398574829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于罗伯塔,进一步细化进一步训练在左倾的Reddit语料库上,我们可以看到其在方面的显着自由主义转变。 就其政治偏见而言。", "metrics": {"bleu_score": 41.097241960747176, "chrf_score": 42.52500014103781, "xcomet_score": 0.4453205466270447, "xcomet_qe_score": 0.31923621892929077, "metricx_score": 8.341123580932617, "metricx_qe_score": 8.749402046203613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图研究语言模型是否能捕捉到现代社会普遍存在的两极分化。", "metrics": {"bleu_score": 72.34418469576333, "chrf_score": 64.33422012327118, "xcomet_score": 0.9992550611495972, "xcomet_qe_score": 0.9951575994491577, "metricx_score": 0.6545740365982056, "metricx_qe_score": 0.9067049026489258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练的语料库分为美国第四十五任总统之前的语料库和美国第四十五任总统之后的语料库。然后,", "metrics": {"bleu_score": 38.137597893006834, "chrf_score": 42.17361796724487, "xcomet_score": 0.6074203252792358, "xcomet_qe_score": 0.669085681438446, "metricx_score": 4.529936790466309, "metricx_qe_score": 4.84599494934082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们分别在两个不同的时间语料库上预训练语言模型。 我们", "metrics": {"bleu_score": 91.81891462193897, "chrf_score": 98.16306505407624, "xcomet_score": 0.6552369594573975, "xcomet_qe_score": 0.6593025922775269, "metricx_score": 4.11857271194458, "metricx_qe_score": 1.14821457862854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型通常在 2017 年之后偏向政治倾向,", "metrics": {"bleu_score": 39.78723722251966, "chrf_score": 38.83857872885647, "xcomet_score": 0.6907747983932495, "xcomet_qe_score": 0.5805647373199463, "metricx_score": 6.628708362579346, "metricx_qe_score": 6.97939395904541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也可以捕捉到我们社会的两极分化。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9907978773117065, "xcomet_qe_score": 0.9288705587387085, "metricx_score": 0.9285306930541992, "metricx_qe_score": 1.265442967414856, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但并非最不重要的是,我们评估了具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测方面的应用。两个NP应用程序经常涉及语言模型,并可能具有非常重要的影响。 所以", "metrics": {"bleu_score": 57.50423323604386, "chrf_score": 60.61466181739807, "xcomet_score": 0.7121171355247498, "xcomet_qe_score": 0.7203719615936279, "metricx_score": 4.543799877166748, "metricx_qe_score": 3.503488302230835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,如果我们调查每个类别的表现,即如果我们将表现分开。 Different demographics or political leaning of news media, we can see a pattern that, for example", "metrics": {"bleu_score": 26.275694756923762, "chrf_score": 22.483713065162373, "xcomet_score": 0.5736593008041382, "xcomet_qe_score": 0.4611177146434784, "metricx_score": 13.04129409790039, "metricx_qe_score": 11.895904541015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", for hate speech detection, left-leaning language models are better. 在检测他针对社会少数群体的言论。 然而,我们在检测仇恨言论时更糟,因为我们社会中更有权力的群体。", "metrics": {"bleu_score": 26.92042716021267, "chrf_score": 20.417149285135565, "xcomet_score": 0.497152179479599, "xcomet_qe_score": 0.5814814567565918, "metricx_score": 17.820770263671875, "metricx_qe_score": 18.48232650756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,白人语言模型在检测针对白人和男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 71.04562600745808, "chrf_score": 72.85647559568557, "xcomet_score": 0.7783747911453247, "xcomet_qe_score": 0.7502886056900024, "metricx_score": 3.174051523208618, "metricx_qe_score": 2.724796772003174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9866291284561157, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多例子来说明不同政治含义的语言模型。 \"do give different predictions to hate speech and misinformation examples based on their social categories, there are a bun", "metrics": {"bleu_score": 23.72039272131019, "chrf_score": 16.342948344971433, "xcomet_score": 0.5360925197601318, "xcomet_qe_score": 0.5790318250656128, "metricx_score": 19.933446884155273, "metricx_qe_score": 14.136307716369629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ch of more examples in the appendix to further highlight that.\" 这表明存在一个非常紧迫的问题,即语言模型的政治偏见。", "metrics": {"bleu_score": 33.45281124681008, "chrf_score": 25.504405408323294, "xcomet_score": 0.3582819402217865, "xcomet_qe_score": 0.4977447986602783, "metricx_score": 15.68520450592041, "metricx_qe_score": 16.618724822998047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们的线性语言模型要针对仇恨言论或虚假信息进行微调,并部署到一个流行的社交媒体平台上。 这将意味着那些持有相反政治观点的人可能会被边缘化,而针对少数民族团体的仇恨言论可能会在没有任何控制的情况下猖獗。", "metrics": {"bleu_score": 48.136754969793216, "chrf_score": 43.83711166447892, "xcomet_score": 0.7247414588928223, "xcomet_qe_score": 0.6374890804290771, "metricx_score": 2.754422664642334, "metricx_qe_score": 3.1814422607421875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这已经发出了警报,要求我们承认并解决由语言模型政治偏见引起的公平性问题。", "metrics": {"bleu_score": 33.61336783650558, "chrf_score": 34.295577779484745, "xcomet_score": 0.9559951424598694, "xcomet_qe_score": 0.9555720686912537, "metricx_score": 1.0187773704528809, "metricx_qe_score": 1.33259117603302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以有一点讨论。我们还", "metrics": {"bleu_score": 11.731175160263996, "chrf_score": 13.427155248716588, "xcomet_score": 0.3182160258293152, "xcomet_qe_score": 0.35351651906967163, "metricx_score": 6.265134811401367, "metricx_qe_score": 4.617509365081787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要强调的是,我们揭示了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 69.72459755655453, "chrf_score": 63.15414175724163, "xcomet_score": 0.9540895223617554, "xcomet_qe_score": 0.8865538835525513, "metricx_score": 0.9569274187088013, "metricx_qe_score": 1.468618631362915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在塞拉和卡里布之间。", "metrics": {"bleu_score": 9.968578518495871, "chrf_score": 13.336239154571352, "xcomet_score": 0.640207827091217, "xcomet_qe_score": 0.6677284240722656, "metricx_score": 3.2586495876312256, "metricx_qe_score": 2.2383229732513428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不对语言模型训练数据中的政治观点进行净化,偏见将从预训练数据传播到语言模型,再到下游任务,最终造成公平性问题。", "metrics": {"bleu_score": 72.40708036558145, "chrf_score": 65.33710981738045, "xcomet_score": 0.9310657978057861, "xcomet_qe_score": 0.8875645399093628, "metricx_score": 1.0673186779022217, "metricx_qe_score": 1.659557580947876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们确实尝试以某种方式进行净化,我们也会面临审查或排斥的风险。", "metrics": {"bleu_score": 68.09354000776102, "chrf_score": 68.0826584129554, "xcomet_score": 0.9106523990631104, "xcomet_qe_score": 0.8836359977722168, "metricx_score": 1.3918077945709229, "metricx_qe_score": 2.4755070209503174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "确定什么是中立的并且应该保留语言监控数据是非常困难的。", "metrics": {"bleu_score": 33.95770077446804, "chrf_score": 32.56124736962218, "xcomet_score": 0.9077901840209961, "xcomet_qe_score": 0.8674052953720093, "metricx_score": 1.0029850006103516, "metricx_qe_score": 1.2753456830978394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电动手推车的问题。", "metrics": {"bleu_score": 34.38931217657843, "chrf_score": 31.251958363280952, "xcomet_score": 0.817986011505127, "xcomet_qe_score": 0.8406002521514893, "metricx_score": 0.776100218296051, "metricx_qe_score": 0.7354187965393066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9909268617630005, "xcomet_qe_score": 0.973970890045166, "metricx_score": 0.3818603754043579, "metricx_qe_score": 0.3029481768608093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这几乎就是我今天的全部内容了。", "metrics": {"bleu_score": 64.1975224568211, "chrf_score": 69.55122495856799, "xcomet_score": 0.9791821241378784, "xcomet_qe_score": 0.8860219717025757, "metricx_score": 0.5033807754516602, "metricx_qe_score": 1.1820582151412964, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Thank you so much for your attention", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9830275774002075, "xcomet_qe_score": 1.0, "metricx_score": 0.8306114077568054, "metricx_qe_score": 0.4437558650970459, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是 Jenny,卡内基梅隆大学第一年研究生,今天我将介绍我的工作《数据集和模型的设计偏见特征》。", "metrics": {"bleu_score": 42.93050126728236, "chrf_score": 32.61467857733313, "xcomet_score": 0.7401493191719055, "xcomet_qe_score": 0.7138712406158447, "metricx_score": 4.158411502838135, "metricx_qe_score": 4.933729648590088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是在与华盛顿大学和艾伦研究所的合作下完成的,具体来说是与塞巴斯蒂安·桑蒂、罗纳德·拉布罗斯、卡特琳娜·雷尼卡和马丁·萨普一起完成的。", "metrics": {"bleu_score": 18.76756593058562, "chrf_score": 13.085174155828183, "xcomet_score": 0.5958348512649536, "xcomet_qe_score": 0.6654720306396484, "metricx_score": 1.761332631111145, "metricx_qe_score": 1.662984848022461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,让我们从想象开始,假设你在为一家报纸工作,你正在浏览你新闻文章的评论,试图删除有毒的内容。", "metrics": {"bleu_score": 29.180179894478616, "chrf_score": 27.086016304536535, "xcomet_score": 0.8005229830741882, "xcomet_qe_score": 0.7573836445808411, "metricx_score": 3.7504501342773438, "metricx_qe_score": 3.0791637897491455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会转向一个受欢迎的API,如用于毒性检测的Perspective API。这在你是Carl Jones的情况下非常有效,其中Pers", "metrics": {"bleu_score": 16.90687894161582, "chrf_score": 37.155311156041996, "xcomet_score": 0.3870047330856323, "xcomet_qe_score": 0.4586986005306244, "metricx_score": 7.973381042480469, "metricx_qe_score": 7.975723743438721, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pective API能够正确检测有毒实例。", "metrics": {"bleu_score": 46.378783190593225, "chrf_score": 61.10390157513411, "xcomet_score": 0.718071699142456, "xcomet_qe_score": 0.6646431684494019, "metricx_score": 8.354572296142578, "metricx_qe_score": 10.61878490447998, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于Dithya Sharma来说,情况并非如此,因为", "metrics": {"bleu_score": 42.9513694636652, "chrf_score": 53.22408633467033, "xcomet_score": 0.7057778835296631, "xcomet_qe_score": 0.691626787185669, "metricx_score": 4.708397388458252, "metricx_qe_score": 2.335858106613159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "perspectiveAPI在印度语境中更常见的冒犯性词汇上并不那么敏感。", "metrics": {"bleu_score": 64.25503166524514, "chrf_score": 66.80338239838281, "xcomet_score": 0.8950387239456177, "xcomet_qe_score": 0.733635663986206, "metricx_score": 4.992610931396484, "metricx_qe_score": 6.942721366882324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是设计偏见的一个例子,我们看到技术在不同人群之间存在系统性差异的表现。", "metrics": {"bleu_score": 35.84086859566396, "chrf_score": 32.20657676532117, "xcomet_score": 0.9763528108596802, "xcomet_qe_score": 0.9026769399642944, "metricx_score": 1.7696231603622437, "metricx_qe_score": 2.307868003845215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "设计偏见,如我们刚才看到的那种,可能是由于NLP研究人员和模型开发人员的立场造成的。", "metrics": {"bleu_score": 56.33103101443087, "chrf_score": 50.102335110339915, "xcomet_score": 0.9796264171600342, "xcomet_qe_score": 0.9868456125259399, "metricx_score": 0.8294111490249634, "metricx_qe_score": 0.8670532703399658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场是指由于其人口统计、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 67.5138946452567, "chrf_score": 61.498102013472376, "xcomet_score": 0.853272557258606, "xcomet_qe_score": 0.7828587293624878, "metricx_score": 2.3036890029907227, "metricx_qe_score": 3.154629707336426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个概念在批判性研究中广泛使用,特别是在女权主义和女同性恋学术领域。", "metrics": {"bleu_score": 46.02269419165292, "chrf_score": 40.35929830461712, "xcomet_score": 0.9877468347549438, "xcomet_qe_score": 0.9891597032546997, "metricx_score": 0.685039222240448, "metricx_qe_score": 0.35411033034324646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,立场可以影响研究过程及其结果和成果,因为它可以改变研究人员的决定。", "metrics": {"bleu_score": 50.42548662828612, "chrf_score": 41.73346030391427, "xcomet_score": 0.9919710159301758, "xcomet_qe_score": 0.9245510101318359, "metricx_score": 0.7391338348388672, "metricx_qe_score": 0.8013148903846741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以人们可能会问的数据集和模型是否具有位置性。", "metrics": {"bleu_score": 44.78097704123438, "chrf_score": 39.05556811912986, "xcomet_score": 0.810125470161438, "xcomet_qe_score": 0.8365298509597778, "metricx_score": 3.776834487915039, "metricx_qe_score": 1.2533893585205078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型和数据集本身具有人口特征和生活经历,但它们确实汇总了真实人的判断和意见,并且可以代表某些立场对其他立场的看法。", "metrics": {"bleu_score": 54.04020780226781, "chrf_score": 44.87477953926582, "xcomet_score": 0.8205066919326782, "xcomet_qe_score": 0.7779491543769836, "metricx_score": 1.5053189992904663, "metricx_qe_score": 1.6147981882095337, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,先前的工作已经表明了有一些位置性的证据,例如文化差距、模型和数据集,以及模型位置性的理论定义。", "metrics": {"bleu_score": 31.823873738585498, "chrf_score": 27.422963612968697, "xcomet_score": 0.7179176807403564, "xcomet_qe_score": 0.7699836492538452, "metricx_score": 5.865200519561768, "metricx_qe_score": 4.834238052368164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些作品并没有真正比较用户与数据集和模型本身。 随着NLP任务变得越来越主观和社会导向,研究模型和数据集的定位性变得越来越重要。 \"and it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden", "metrics": {"bleu_score": 28.669711727858957, "chrf_score": 22.14147670850584, "xcomet_score": 0.5673274993896484, "xcomet_qe_score": 0.6448662281036377, "metricx_score": 13.369770050048828, "metricx_qe_score": 10.643367767333984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "behind apis.\" 为了研究数据集和模型定位性,我们实际上将注释与现有数据集和模型中的真实用户进行比较。", "metrics": {"bleu_score": 47.47181389504447, "chrf_score": 40.171262605883975, "xcomet_score": 0.4919463098049164, "xcomet_qe_score": 0.42165327072143555, "metricx_score": 8.424866676330566, "metricx_qe_score": 9.875313758850098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架NL定位性来做到这一点。", "metrics": {"bleu_score": 44.47608928410895, "chrf_score": 28.59095810297372, "xcomet_score": 0.7980550527572632, "xcomet_qe_score": 0.7982977628707886, "metricx_score": 2.0803375244140625, "metricx_qe_score": 2.5848042964935303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "R framework works in two main steps.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8978098630905151, "xcomet_qe_score": 0.9073401093482971, "metricx_score": 20.306116104125977, "metricx_qe_score": 20.662395477294922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是用不同的注释者重新注释数据集。", "metrics": {"bleu_score": 67.34927403341207, "chrf_score": 56.892107759342714, "xcomet_score": 0.9006201028823853, "xcomet_qe_score": 0.9300392866134644, "metricx_score": 1.3815566301345825, "metricx_qe_score": 1.37224543094635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做是因为通常只有少数注释者对每个实例进行注释,而人口统计数据很少被收集和共享。", "metrics": {"bleu_score": 48.540859396939126, "chrf_score": 52.74705719965519, "xcomet_score": 0.9129105806350708, "xcomet_qe_score": 0.9305232763290405, "metricx_score": 2.3216910362243652, "metricx_qe_score": 3.758183479309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据以获取许多注释,例如,获取丰富的人口统计数据集。", "metrics": {"bleu_score": 48.413316254630736, "chrf_score": 42.51906505372972, "xcomet_score": 0.8407800197601318, "xcomet_qe_score": 0.8804620504379272, "metricx_score": 1.8385454416275024, "metricx_qe_score": 1.5416319370269775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们按人口统计数据对注释进行分类,并使用Pearson的r相关系数将其与模型和数据集进行比较。 因此,我们的框架实际上与注释者分歧文献不同之处在于将最终用户与模型、数据集、预测和标签进行比较,而不是仅仅关注注释者的共识或建模注释者分布。", "metrics": {"bleu_score": 63.549901149095184, "chrf_score": 57.9171655794482, "xcomet_score": 0.6605561971664429, "xcomet_qe_score": 0.6627222299575806, "metricx_score": 2.403071641921997, "metricx_qe_score": 2.3679304122924805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在很大程度上是通过实验室在野外实现的,一个在线众包平台。前HCI合作者。", "metrics": {"bleu_score": 29.946575433167443, "chrf_score": 29.178128132173004, "xcomet_score": 0.66473388671875, "xcomet_qe_score": 0.5788792371749878, "metricx_score": 6.774292469024658, "metricx_qe_score": 6.270918369293213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《野外实验》是一个在线实验平台,我们可以招募来自不同背景的志愿者,", "metrics": {"bleu_score": 36.60900527012578, "chrf_score": 28.028322925457612, "xcomet_score": 0.8383326530456543, "xcomet_qe_score": 0.7913992404937744, "metricx_score": 2.140777349472046, "metricx_qe_score": 2.2901203632354736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与像Mturk这样的平台相比,后者主要招募来自美国或印度的参与者。此外,《野外实验》仍然能够获得高质量的数据。", "metrics": {"bleu_score": 44.00998849882345, "chrf_score": 38.18064742235819, "xcomet_score": 0.7391357421875, "xcomet_qe_score": 0.7071526646614075, "metricx_score": 2.721696376800537, "metricx_qe_score": 2.7255301475524902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在《野外的爱》中举办两项任务,其中之一是社会接受度。它的工作方式是,参与者将阅读社交化学数据集中的一个情况,然后他们将写出一个情况的社会接受度。", "metrics": {"bleu_score": 30.703191742107737, "chrf_score": 23.698947674299703, "xcomet_score": 0.5679340362548828, "xcomet_qe_score": 0.5387734770774841, "metricx_score": 6.761455059051514, "metricx_qe_score": 6.1324543952941895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持参与度,他们可以将自己的反应与人工智能和其他人的反应进行比较。", "metrics": {"bleu_score": 46.91188192329597, "chrf_score": 38.04663203270015, "xcomet_score": 0.9008607864379883, "xcomet_qe_score": 0.9860353469848633, "metricx_score": 1.0724679231643677, "metricx_qe_score": 0.9326698780059814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们将这些注释与社会化学、Delphi和GPT4进行了比较。 我们然后复制了一个非常相似的设", "metrics": {"bleu_score": 41.92430478263867, "chrf_score": 73.14850794325052, "xcomet_score": 0.3742413818836212, "xcomet_qe_score": 0.4961477518081665, "metricx_score": 5.59621000289917, "metricx_qe_score": 4.7696452140808105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "置,用于毒性和仇恨言论检测任务,在其中他们将阅读来自DinaHate的实例并写下他们是否认为这是仇恨言论的实例。", "metrics": {"bleu_score": 43.267918509117315, "chrf_score": 39.793258233974264, "xcomet_score": 0.44798368215560913, "xcomet_qe_score": 0.4077893793582916, "metricx_score": 7.907772064208984, "metricx_qe_score": 7.964550495147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后将这些注释与dynahate, perspectiveapi,rewireapi,hate,roberta和GPT4进行了比较。", "metrics": {"bleu_score": 33.67529766328069, "chrf_score": 52.578522953312316, "xcomet_score": 0.8082674741744995, "xcomet_qe_score": 0.7735371589660645, "metricx_score": 4.590856552124023, "metricx_qe_score": 5.627326488494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了超过一万六千个注释来自一千名来自八十七个国家的注释者。", "metrics": {"bleu_score": 37.806791842124426, "chrf_score": 31.147902907250984, "xcomet_score": 0.8632235527038574, "xcomet_qe_score": 0.8493354320526123, "metricx_score": 3.588008165359497, "metricx_qe_score": 3.0151751041412354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以现在我们更有能力回答,NLP数据集和模型最契合谁。", "metrics": {"bleu_score": 42.20059711629368, "chrf_score": 41.089188427160195, "xcomet_score": 0.9038574695587158, "xcomet_qe_score": 0.9512070417404175, "metricx_score": 1.677828073501587, "metricx_qe_score": 1.2894210815429688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP中存在定位性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8662892580032349, "xcomet_qe_score": 0.8697264194488525, "metricx_score": 3.0094809532165527, "metricx_qe_score": 2.0923473834991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最符合英语国家的情况。", "metrics": {"bleu_score": 45.283344133049354, "chrf_score": 40.14999902733858, "xcomet_score": 0.9896000623703003, "xcomet_qe_score": 0.9283906817436218, "metricx_score": 0.750336766242981, "metricx_qe_score": 0.634933590888977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在GPT-4的社会接受度分析中,我们发现它最符合孔子和英语国家的情况。", "metrics": {"bleu_score": 25.56189082997477, "chrf_score": 24.25756303161065, "xcomet_score": 0.8497897982597351, "xcomet_qe_score": 0.8324071764945984, "metricx_score": 3.918564796447754, "metricx_qe_score": 2.6504640579223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现Dinahate也最符合英语国家的情况。", "metrics": {"bleu_score": 64.1975224568211, "chrf_score": 69.63106465108754, "xcomet_score": 0.8595547676086426, "xcomet_qe_score": 0.867671549320221, "metricx_score": 3.4404802322387695, "metricx_qe_score": 4.827487468719482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,", "metrics": {"bleu_score": 1.9422565110272496, "chrf_score": 11.296721818687473, "xcomet_score": 0.17757102847099304, "xcomet_qe_score": 0.15414679050445557, "metricx_score": 16.514528274536133, "metricx_qe_score": 8.656933784484863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于GPT-4和社交接受性任务,与拥有大学教育或研究生教育的人最有共鸣。 我们发现,Danny Hight的情况也一样,他最符合拥有大学学历的人。", "metrics": {"bleu_score": 29.24878803265661, "chrf_score": 25.483558520957995, "xcomet_score": 0.46187371015548706, "xcomet_qe_score": 0.39924052357673645, "metricx_score": 6.807814598083496, "metricx_qe_score": 7.218113899230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,有些人不可避免地会被抛在后面。", "metrics": {"bleu_score": 49.95249400812609, "chrf_score": 43.574444718730845, "xcomet_score": 0.8245280981063843, "xcomet_qe_score": 0.8466436862945557, "metricx_score": 0.9581495523452759, "metricx_qe_score": 0.9102878570556641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个例子是数据集和模型在与男性和女性对手的比较中对非二进制人类的关联性较低。", "metrics": {"bleu_score": 29.081869722567124, "chrf_score": 29.809708909552597, "xcomet_score": 0.6949580311775208, "xcomet_qe_score": 0.6930169463157654, "metricx_score": 4.5701189041137695, "metricx_qe_score": 4.299912929534912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT-4社会接受性任务以及Dine Hate任务分析中发现了这一点。", "metrics": {"bleu_score": 57.94035707334097, "chrf_score": 56.145815550999345, "xcomet_score": 0.8016182780265808, "xcomet_qe_score": 0.7816757559776306, "metricx_score": 2.81119966506958, "metricx_qe_score": 3.1475930213928223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,既然有位置在铝和LP,我们能做些什么呢?", "metrics": {"bleu_score": 35.01826207809194, "chrf_score": 31.45359280942805, "xcomet_score": 0.6732301712036133, "xcomet_qe_score": 0.7414551973342896, "metricx_score": 11.46932601928711, "metricx_qe_score": 10.464879035949707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们对此有一些建议。", "metrics": {"bleu_score": 51.56626918239821, "chrf_score": 46.9835123199293, "xcomet_score": 0.9777777194976807, "xcomet_qe_score": 0.9712458848953247, "metricx_score": 0.2560553550720215, "metricx_qe_score": 0.2897626459598541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是记录研究过程中所有相关的设计选择。另一项", "metrics": {"bleu_score": 36.841823364959964, "chrf_score": 29.02129570374372, "xcomet_score": 0.8061925768852234, "xcomet_qe_score": 0.7327700853347778, "metricx_score": 4.883399963378906, "metricx_qe_score": 2.3441274166107178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "建议是从视角多元化的角度对NLP进行研究。", "metrics": {"bleu_score": 9.475814783795371, "chrf_score": 7.970798084809968, "xcomet_score": 0.3862434923648834, "xcomet_qe_score": 0.7497959136962891, "metricx_score": 1.7579684257507324, "metricx_qe_score": 1.0592732429504395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是建立专门的数据集和模型,", "metrics": {"bleu_score": 52.757152899459186, "chrf_score": 52.121065020881275, "xcomet_score": 0.7661203145980835, "xcomet_qe_score": 0.7525674104690552, "metricx_score": 5.484411239624023, "metricx_qe_score": 4.567083358764648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在四个特定社区内。一个很好的例子是马萨诸塞州的倡议。", "metrics": {"bleu_score": 25.595703555783782, "chrf_score": 26.304504388838524, "xcomet_score": 0.14648523926734924, "xcomet_qe_score": 0.14082565903663635, "metricx_score": 7.214789390563965, "metricx_qe_score": 8.243916511535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的意思是强调,包容性NLP不仅仅是让所有", "metrics": {"bleu_score": 29.62789157394226, "chrf_score": 33.305054079047885, "xcomet_score": 0.6277022361755371, "xcomet_qe_score": 0.5192219018936157, "metricx_score": 6.012569904327393, "metricx_qe_score": 4.99951696395874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术都适用于每个人。", "metrics": {"bleu_score": 11.896441524336442, "chrf_score": 10.534278914518778, "xcomet_score": 0.9468040466308594, "xcomet_qe_score": 0.8896449208259583, "metricx_score": 0.5663642883300781, "metricx_qe_score": 1.1395472288131714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们的演讲到此结束。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 94.72671038989785, "xcomet_score": 0.9740927815437317, "xcomet_qe_score": 0.9522858262062073, "metricx_score": 0.24328428506851196, "metricx_qe_score": 0.2743247151374817, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果您想了解更多信息,请随时查看我们的仪表板以获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 63.06930051254211, "chrf_score": 56.37014045390168, "xcomet_score": 0.9816778898239136, "xcomet_qe_score": 0.9540000557899475, "metricx_score": 0.5060917139053345, "metricx_qe_score": 0.5287445783615112, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是福州大学的苏玉元。", "metrics": {"bleu_score": 12.534122247249353, "chrf_score": 9.20247149829336, "xcomet_score": 0.7194411754608154, "xcomet_qe_score": 0.7664812803268433, "metricx_score": 3.7348780632019043, "metricx_qe_score": 1.070598840713501, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我在这里介绍我们的工作,区分大型语言模型中的脚本知识,以便于受约束的语言规划。", "metrics": {"bleu_score": 32.00033164212294, "chrf_score": 27.55620781732184, "xcomet_score": 0.7017155885696411, "xcomet_qe_score": 0.7259071469306946, "metricx_score": 3.071019411087036, "metricx_qe_score": 2.632070779800415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常通过遵循逐步指令来规划自己的行动,以形式为保证的脚本。", "metrics": {"bleu_score": 32.13856523369839, "chrf_score": 28.511911767582017, "xcomet_score": 0.7581566572189331, "xcomet_qe_score": 0.7611135244369507, "metricx_score": 7.542576789855957, "metricx_qe_score": 7.827322959899902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Previous work has exploited language models to plan for abstract goals of stereotypical activities, such as \"make", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7876802682876587, "xcomet_qe_score": 0.8056330680847168, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a cake\", and showed that large language models can effectively decompose goals into steps.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6226516962051392, "xcomet_qe_score": 0.7306609749794006, "metricx_score": 15.513225555419922, "metricx_qe_score": 15.645282745361328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的工作主要集中在规划抽象目标和典型活动上。", "metrics": {"bleu_score": 39.39835969086693, "chrf_score": 33.190482223090925, "xcomet_score": 0.8269646167755127, "xcomet_qe_score": 0.7666974067687988, "metricx_score": 1.508915901184082, "metricx_qe_score": 1.3473856449127197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "规划具有特定约束条件的目标,如制作巧克力蛋糕,仍然是未研究的。", "metrics": {"bleu_score": 22.771086398510334, "chrf_score": 21.512023121168873, "xcomet_score": 0.7736285924911499, "xcomet_qe_score": 0.7415884733200073, "metricx_score": 3.7723824977874756, "metricx_qe_score": 4.002621650695801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受约束的语言规划问题。 Which imposes different constraints on the goals", "metrics": {"bleu_score": 27.656988676950363, "chrf_score": 19.92207349222567, "xcomet_score": 0.7938001155853271, "xcomet_qe_score": 0.8162378668785095, "metricx_score": 11.936275482177734, "metricx_qe_score": 11.179774284362793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of planning. An abstract goal can be inherited by different real-life specific goals with multifa", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6884196996688843, "xcomet_qe_score": 0.7512562870979309, "metricx_score": 21.923553466796875, "metricx_qe_score": 22.66950798034668, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ceted constraints. A good planner should write scripts that are reasonable and faithful to constraints.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7908745408058167, "xcomet_qe_score": 0.8112102746963501, "metricx_score": 24.525115966796875, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并改进了大型语言模型的受约束的语言规划能力。", "metrics": {"bleu_score": 68.4058267541103, "chrf_score": 63.459435043172874, "xcomet_score": 0.9234298467636108, "xcomet_qe_score": 0.9604669809341431, "metricx_score": 0.6272398233413696, "metricx_qe_score": 0.735379695892334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定目标的示例集来支持我们的研究。 我们必须先获得这些目标。正", "metrics": {"bleu_score": 49.57305834810789, "chrf_score": 40.65226159944979, "xcomet_score": 0.6914792060852051, "xcomet_qe_score": 0.7284283638000488, "metricx_score": 4.982976913452148, "metricx_qe_score": 4.90011739730835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表格中所示,我们为人类在循环数据获取中使用指令GPT时扩展了抽象目标与多方面约束。 我们对一百个", "metrics": {"bleu_score": 18.39576031944879, "chrf_score": 18.01073874732524, "xcomet_score": 0.49055251479148865, "xcomet_qe_score": 0.5141322612762451, "metricx_score": 10.979244232177734, "metricx_qe_score": 10.061188697814941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特定的目标进行采样,并评估大型语言模型生成的脚本。", "metrics": {"bleu_score": 39.625035554322736, "chrf_score": 33.85408669519268, "xcomet_score": 0.7737718224525452, "xcomet_qe_score": 0.7616937756538391, "metricx_score": 4.0015058517456055, "metricx_qe_score": 4.773601055145264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个表格报告了结果的总体准确性。", "metrics": {"bleu_score": 40.325042950627804, "chrf_score": 32.17717135556109, "xcomet_score": 0.943331241607666, "xcomet_qe_score": 0.9314613938331604, "metricx_score": 0.7073509693145752, "metricx_qe_score": 0.6723769903182983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有的自然语言模型在规划特定目标时都取得了令人不满意的", "metrics": {"bleu_score": 30.73370724719807, "chrf_score": 27.556418342596967, "xcomet_score": 0.7637606859207153, "xcomet_qe_score": 0.8031618595123291, "metricx_score": 2.3304312229156494, "metricx_qe_score": 1.970590591430664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果。 Then we conduct detailed analysis to investigate why language models fail. 结果", "metrics": {"bleu_score": 1.4285126250130793, "chrf_score": 0.4930966469428008, "xcomet_score": 0.15604864060878754, "xcomet_qe_score": 0.11512693017721176, "metricx_score": 18.532957077026367, "metricx_qe_score": 20.24982261657715, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图表显示,生成的脚本的语义完整性是可以接受的,但不能保证符合约束。", "metrics": {"bleu_score": 30.99275392637447, "chrf_score": 29.31463246787036, "xcomet_score": 0.9222413301467896, "xcomet_qe_score": 0.969579815864563, "metricx_score": 1.0624656677246094, "metricx_qe_score": 0.9231234192848206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入探讨了维基百科中定义的更细分的主题类别的限制。", "metrics": {"bleu_score": 48.55332614117323, "chrf_score": 35.044825448316416, "xcomet_score": 0.8411784172058105, "xcomet_qe_score": 0.9438437223434448, "metricx_score": 2.7196316719055176, "metricx_qe_score": 2.1885199546813965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,不同类别的女孩的教师GPT的计划性能差异很大。 Previous studies", "metrics": {"bleu_score": 34.69638886951256, "chrf_score": 27.727274600839852, "xcomet_score": 0.3392794132232666, "xcomet_qe_score": 0.3098354637622833, "metricx_score": 7.939844608306885, "metricx_qe_score": 7.653341770172119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "have shown that the output quality of language models varies greatly, leading to poor performance. Therefore, we adopt the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6110714077949524, "xcomet_qe_score": 0.8043168783187866, "metricx_score": 13.062141418457031, "metricx_qe_score": 10.737070083618164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "idea of over-generated then filter to improve generation quality. We first show constraint types with ex", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.566913366317749, "xcomet_qe_score": 0.6345725059509277, "metricx_score": 20.606576919555664, "metricx_qe_score": 20.369897842407227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "amples for instructGPT and obtain specific goals based on the said abstract goals.", "metrics": {"bleu_score": 0.0, "chrf_score": 15.948220949004288, "xcomet_score": 0.5952768325805664, "xcomet_qe_score": 0.6669133901596069, "metricx_score": 21.5389461517334, "metricx_qe_score": 17.71658706665039, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指令GPT生成特定角色的关键脚本。", "metrics": {"bleu_score": 12.149176141753365, "chrf_score": 12.24774776732738, "xcomet_score": 0.6990291476249695, "xcomet_qe_score": 0.7083752155303955, "metricx_score": 4.706427097320557, "metricx_qe_score": 4.496824264526367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Next, a filter model is developed to select the faithful scripts. We convert", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7958444356918335, "xcomet_qe_score": 0.8329657316207886, "metricx_score": 20.231048583984375, "metricx_qe_score": 23.797334671020508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "scripts and goals into instructionGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity.", "metrics": {"bleu_score": 0.0, "chrf_score": 8.797108841140554, "xcomet_score": 0.8967705965042114, "xcomet_qe_score": 0.9528017044067383, "metricx_score": 19.42078971862793, "metricx_qe_score": 19.86182975769043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们避免包含目标约束关键字的脚本。我们只保留", "metrics": {"bleu_score": 37.74155035532906, "chrf_score": 35.271317178294545, "xcomet_score": 0.6273622512817383, "xcomet_qe_score": 0.4440542161464691, "metricx_score": 7.276025295257568, "metricx_qe_score": 5.970414638519287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "脚本,如果目标目标在目标集中得分最高。", "metrics": {"bleu_score": 42.8468526450245, "chrf_score": 42.35153419849116, "xcomet_score": 0.34952592849731445, "xcomet_qe_score": 0.19415058195590973, "metricx_score": 9.269048690795898, "metricx_qe_score": 9.174467086791992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "With our method, instruction-gpt can generate sequences of higher quality.", "metrics": {"bleu_score": 0.0, "chrf_score": 12.545428669804442, "xcomet_score": 0.9550079703330994, "xcomet_qe_score": 0.9623127579689026, "metricx_score": 8.494340896606445, "metricx_qe_score": 6.656425476074219, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Our method greatly improves the planning ability, both in semantic completeness and faithfulness to the constraint.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9875867366790771, "xcomet_qe_score": 1.0, "metricx_score": 24.969839096069336, "metricx_qe_score": 24.987337112426758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高昂,因此实现较小和专业化模型的语言规划能力至关重要。", "metrics": {"bleu_score": 62.68627630561883, "chrf_score": 53.70100580626897, "xcomet_score": 0.9988880157470703, "xcomet_qe_score": 0.9929496049880981, "metricx_score": 0.42173731327056885, "metricx_qe_score": 0.7272014617919922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键步骤。", "metrics": {"bleu_score": 69.6015973294402, "chrf_score": 66.30344838521414, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026699073612689972, "metricx_qe_score": 0.14870662987232208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究没有实现为特定目标规划的功能,手动数据集注释成本高昂。", "metrics": {"bleu_score": 27.588629375637943, "chrf_score": 22.590314229308536, "xcomet_score": 0.9728201627731323, "xcomet_qe_score": 0.9219944477081299, "metricx_score": 1.5122039318084717, "metricx_qe_score": 2.496748208999634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的理念,从大型语言模型中蒸馏受约束的语言规划数据集。", "metrics": {"bleu_score": 49.91055161388952, "chrf_score": 41.966321565130194, "xcomet_score": 0.8362421989440918, "xcomet_qe_score": 0.7940783500671387, "metricx_score": 3.6144914627075195, "metricx_qe_score": 3.1636252403259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将应用我们的方法来构建一个名为codescript的受约束语言规划数据集。", "metrics": {"bleu_score": 32.18321162412336, "chrf_score": 30.407812008504635, "xcomet_score": 0.941524863243103, "xcomet_qe_score": 0.9142491221427917, "metricx_score": 1.9656271934509277, "metricx_qe_score": 3.2848522663116455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们使用脚本生成了五万五千个特定的目标,", "metrics": {"bleu_score": 26.773534472711976, "chrf_score": 22.16185596018025, "xcomet_score": 0.9762594699859619, "xcomet_qe_score": 0.9871351718902588, "metricx_score": 1.3138742446899414, "metricx_qe_score": 1.7827987670898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以确保验证和测试集的质量。我们要求众包的工人找到并修正错误的示例。", "metrics": {"bleu_score": 41.629897575968776, "chrf_score": 34.8794999194903, "xcomet_score": 0.7940665483474731, "xcomet_qe_score": 0.8301360011100769, "metricx_score": 2.85772705078125, "metricx_qe_score": 2.374204158782959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这张图显示了代码脚本的约束分布。", "metrics": {"bleu_score": 60.26080978557135, "chrf_score": 40.23587999290738, "xcomet_score": 0.854791522026062, "xcomet_qe_score": 0.8229986429214478, "metricx_score": 3.012563467025757, "metricx_qe_score": 4.067894458770752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成特定目标时显示出更高的拟合度。", "metrics": {"bleu_score": 16.279348731624783, "chrf_score": 13.267180706936122, "xcomet_score": 0.7389425039291382, "xcomet_qe_score": 0.708739161491394, "metricx_score": 5.702238082885742, "metricx_qe_score": 5.540903091430664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用代码脚本,我们可以训练更小但更专业化的模型来约束语言规划。", "metrics": {"bleu_score": 25.83858837827737, "chrf_score": 18.617910883009035, "xcomet_score": 0.7080023884773254, "xcomet_qe_score": 0.6953351497650146, "metricx_score": 4.178558349609375, "metricx_qe_score": 4.243698596954346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,T5 fine-tune on the CoSWE can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.", "metrics": {"bleu_score": 2.7034839152240595, "chrf_score": 5.893170610591198, "xcomet_score": 0.5867635011672974, "xcomet_qe_score": 0.5742999315261841, "metricx_score": 18.823532104492188, "metricx_qe_score": 13.151329040527344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们建立了受约束的语言规划问题。", "metrics": {"bleu_score": 50.97960527136183, "chrf_score": 46.61008777645517, "xcomet_score": 0.9089202880859375, "xcomet_qe_score": 0.8810528516769409, "metricx_score": 1.4739947319030762, "metricx_qe_score": 2.2525765895843506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受约束语言规划能力,并为大型语言模型开发了过生成过滤方法。", "metrics": {"bleu_score": 48.46202417941908, "chrf_score": 40.706136970833086, "xcomet_score": 0.8526450991630554, "xcomet_qe_score": 0.8860254287719727, "metricx_score": 2.3514392375946045, "metricx_qe_score": 3.2718355655670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成高质量的脚本数据集,代码脚本用于受约束的语言规划。", "metrics": {"bleu_score": 52.58251526056025, "chrf_score": 42.44867575749929, "xcomet_score": 0.8653761148452759, "xcomet_qe_score": 0.7790442705154419, "metricx_score": 5.0068488121032715, "metricx_qe_score": 5.279656410217285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望代码脚本数据集可以成为推进语言规划研究的有价值的资源。", "metrics": {"bleu_score": 48.49576058746578, "chrf_score": 40.61245984105883, "xcomet_score": 0.8680251836776733, "xcomet_qe_score": 0.8655170202255249, "metricx_score": 2.7917473316192627, "metricx_qe_score": 3.300359010696411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您的时间。", "metrics": {"bleu_score": 18.094495256969623, "chrf_score": 15.088860187765768, "xcomet_score": 0.9894214272499084, "xcomet_qe_score": 0.9905821084976196, "metricx_score": 0.24051934480667114, "metricx_qe_score": 0.7719036936759949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请查阅我们论文中的 Coldscript 的更多详细信息。", "metrics": {"bleu_score": 43.76976804987914, "chrf_score": 49.762598044415505, "xcomet_score": 0.9507526159286499, "xcomet_qe_score": 0.9762551784515381, "metricx_score": 4.518365859985352, "metricx_qe_score": 5.192071914672852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是舒恒。", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 10.704692891649412, "xcomet_score": 0.8477283120155334, "xcomet_qe_score": 0.821589469909668, "metricx_score": 0.2466074526309967, "metricx_qe_score": 0.61963951587677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍的是我们的论文《Do Concol 2003 Named Entity Taggers Still Work Well in 2023》", "metrics": {"bleu_score": 21.58221637242894, "chrf_score": 20.25676734409474, "xcomet_score": 0.883925199508667, "xcomet_qe_score": 0.9232748746871948, "metricx_score": 6.222400188446045, "metricx_qe_score": 4.559909820556641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。让我们开始吧。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 95.15349630471859, "xcomet_score": 0.9835532903671265, "xcomet_qe_score": 0.9856215715408325, "metricx_score": 0.7622692584991455, "metricx_qe_score": 1.1052849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了使用名实体识别任务或NER任务的泛化问题。", "metrics": {"bleu_score": 60.90619294699793, "chrf_score": 54.29082204935979, "xcomet_score": 0.8590381145477295, "xcomet_qe_score": 0.8258366584777832, "metricx_score": 3.213399648666382, "metricx_qe_score": 4.362218856811523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用了 2003 年的 Conll 近 20 年来开发NER。这自然引发了几个问题。", "metrics": {"bleu_score": 28.625546826745353, "chrf_score": 29.24061515303558, "xcomet_score": 0.6733746528625488, "xcomet_qe_score": 0.5941811800003052, "metricx_score": 7.916615009307861, "metricx_qe_score": 7.824108123779297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型是否能够泛化到现代数据?", "metrics": {"bleu_score": 48.740622698799406, "chrf_score": 44.313940588975015, "xcomet_score": 0.9976545572280884, "xcomet_qe_score": 0.9945354461669922, "metricx_score": 0.2051974982023239, "metricx_qe_score": 0.24204693734645844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新的标签时,为了良好的泛化需要什么?", "metrics": {"bleu_score": 49.885432872486184, "chrf_score": 41.50659748539838, "xcomet_score": 0.8893717527389526, "xcomet_qe_score": 0.860954761505127, "metricx_score": 0.8997482061386108, "metricx_qe_score": 0.8662490844726562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们确实观察到泛化能力差,那么是什么原因导致了这些模型的性能下降呢?", "metrics": {"bleu_score": 55.426200263210966, "chrf_score": 51.421932834333816, "xcomet_score": 0.9958735704421997, "xcomet_qe_score": 0.987878680229187, "metricx_score": 0.8610660433769226, "metricx_qe_score": 0.7976270318031311, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了调查这些问题,我们开发了CNN数据集。", "metrics": {"bleu_score": 62.00132149733732, "chrf_score": 48.36997963872051, "xcomet_score": 0.7951628565788269, "xcomet_qe_score": 0.7950019240379333, "metricx_score": 6.108729839324951, "metricx_qe_score": 6.661896228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们从2020年路透社收集的数据集,并用相同的CNN 2003注释指南对其进行了注释。", "metrics": {"bleu_score": 60.177869421680384, "chrf_score": 57.322098266684364, "xcomet_score": 0.8093290328979492, "xcomet_qe_score": 0.7967144250869751, "metricx_score": 4.397932052612305, "metricx_qe_score": 4.702566623687744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We then fine-tuned over 20 models on Conll 2003. We evaluated them on both the", "metrics": {"bleu_score": 1.9046304733974748, "chrf_score": 7.8553044415519375, "xcomet_score": 0.7861119508743286, "xcomet_qe_score": 0.8133268356323242, "metricx_score": 20.090721130371094, "metricx_qe_score": 19.97818946838379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Conll 03 test set and the Conll test set. 最后但并非最不重要的一点", "metrics": {"bleu_score": 0.0, "chrf_score": 4.378284939428238, "xcomet_score": 0.18650099635124207, "xcomet_qe_score": 0.2074742168188095, "metricx_score": 17.965539932250977, "metricx_qe_score": 14.913814544677734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们计算了F1的百分比变化,以评估每个模型的概括性。", "metrics": {"bleu_score": 66.18136169231303, "chrf_score": 62.47402015580676, "xcomet_score": 0.8925771713256836, "xcomet_qe_score": 0.8939415216445923, "metricx_score": 4.836905002593994, "metricx_qe_score": 5.840943813323975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,什么是需要的?对于良好的泛化,我们的", "metrics": {"bleu_score": 21.79301929852717, "chrf_score": 21.00494143085324, "xcomet_score": 0.8428733944892883, "xcomet_qe_score": 0.8248794674873352, "metricx_score": 7.784591197967529, "metricx_qe_score": 3.5407752990722656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实验表明,存在三种主要成分。", "metrics": {"bleu_score": 5.217108958638395, "chrf_score": 7.437285837318821, "xcomet_score": 0.8696268200874329, "xcomet_qe_score": 0.827521800994873, "metricx_score": 1.5145883560180664, "metricx_qe_score": 0.9146913290023804, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现转换器模型通常对新数据的泛化能力更好。", "metrics": {"bleu_score": 61.618529159320445, "chrf_score": 39.77326490545762, "xcomet_score": 0.9886385202407837, "xcomet_qe_score": 0.981012225151062, "metricx_score": 2.2493958473205566, "metricx_qe_score": 1.9171732664108276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常,较大的模型会导致更好的泛化。", "metrics": {"bleu_score": 51.19139560355038, "chrf_score": 40.10964939790978, "xcomet_score": 0.9539501667022705, "xcomet_qe_score": 0.9427156448364258, "metricx_score": 0.9121376276016235, "metricx_qe_score": 1.3705246448516846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但并非最不重要的一点是,我们都知道,微调示例的数量直接影响了 Downstream Task 的性能。在这里,", "metrics": {"bleu_score": 34.02406417233584, "chrf_score": 37.854338004820136, "xcomet_score": 0.8802486658096313, "xcomet_qe_score": 0.8606057167053223, "metricx_score": 6.126147747039795, "metricx_qe_score": 3.688528299331665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调示例实际上也会导致更好的泛化。", "metrics": {"bleu_score": 67.5138946452567, "chrf_score": 58.47351825876147, "xcomet_score": 0.9816000461578369, "xcomet_qe_score": 0.8554470539093018, "metricx_score": 0.6346933841705322, "metricx_qe_score": 0.8460352420806885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "\"To our next question, what causes the performance drop of some models?\" 我们有两个假设。", "metrics": {"bleu_score": 8.933112579951313, "chrf_score": 6.785751716613523, "xcomet_score": 0.9096192121505737, "xcomet_qe_score": 0.9519988298416138, "metricx_score": 10.132107734680176, "metricx_qe_score": 10.954355239868164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是适应性过拟合,即通过反复使用相同的测试集而引起的过拟合。这通常表现为在新的测试集上出现下降回归。", "metrics": {"bleu_score": 47.56281855180256, "chrf_score": 38.99168851937938, "xcomet_score": 0.8575121164321899, "xcomet_qe_score": 0.8509623408317566, "metricx_score": 5.445499420166016, "metricx_qe_score": 4.771991729736328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由训练数据和测试数据之间的时间差增加引起的性能下降。", "metrics": {"bleu_score": 61.024251042746634, "chrf_score": 56.64887057459455, "xcomet_score": 0.9714237451553345, "xcomet_qe_score": 0.8898026347160339, "metricx_score": 1.7754969596862793, "metricx_qe_score": 2.3616509437561035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于过拟合,我们从右侧图表中看到,红色最佳拟合线的梯度大于1。", "metrics": {"bleu_score": 56.15492130307478, "chrf_score": 50.541300152320055, "xcomet_score": 0.8909056782722473, "xcomet_qe_score": 0.8047894835472107, "metricx_score": 0.9391113519668579, "metricx_qe_score": 1.5202300548553467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在column2003上所做的每一个改进单位都转化为columnplusplus上超过一个改进单位的改进,这意味着没有递减回报。", "metrics": {"bleu_score": 38.026880656520405, "chrf_score": 28.521285426617325, "xcomet_score": 0.6535583138465881, "xcomet_qe_score": 0.6918348073959351, "metricx_score": 9.67001724243164, "metricx_qe_score": 10.708133697509766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下,自适应过拟合并未观察到。", "metrics": {"bleu_score": 56.96705282375882, "chrf_score": 48.473952495557086, "xcomet_score": 0.8901193737983704, "xcomet_qe_score": 0.8909345269203186, "metricx_score": 1.8227508068084717, "metricx_qe_score": 2.2563388347625732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So what about temporary truffles?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7583823800086975, "xcomet_qe_score": 0.7721565365791321, "metricx_score": 8.751749992370605, "metricx_qe_score": 13.880730628967285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们做了一个实验来重新训练或继续预训练一些模型与更近期的数据。我们发现性能随着时间间隔的增大而下降。 And this confirms our hypothesis that the main cause of the performance drop is temporal drift.", "metrics": {"bleu_score": 34.68988581610876, "chrf_score": 27.016687788976412, "xcomet_score": 0.7960535883903503, "xcomet_qe_score": 0.7798721790313721, "metricx_score": 11.082791328430176, "metricx_qe_score": 6.799609661102295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了良好的泛化,我们需要更好的模型架构、较大的模型大小以及更多的微调示例。", "metrics": {"bleu_score": 66.50618252382115, "chrf_score": 57.662067879181045, "xcomet_score": 0.9470405578613281, "xcomet_qe_score": 0.8191519975662231, "metricx_score": 0.6097543835639954, "metricx_qe_score": 0.7321134805679321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些目标是手牵手的。我们不能只拥有一种成分,而是要拥有所有其他成分。", "metrics": {"bleu_score": 14.841312430412327, "chrf_score": 15.029695542229277, "xcomet_score": 0.7848907709121704, "xcomet_qe_score": 0.7762137651443481, "metricx_score": 5.516242980957031, "metricx_qe_score": 4.571310043334961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现性能下降是由时间漂移引起的,令人惊讶的是,它并不是由适应性过拟合引起的,尽管Conll 2003已经使用了二十多年。", "metrics": {"bleu_score": 50.60415654586943, "chrf_score": 44.06084813279497, "xcomet_score": 0.897486686706543, "xcomet_qe_score": 0.7786728143692017, "metricx_score": 3.391979694366455, "metricx_qe_score": 3.7319366931915283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以回到我们在论文开头提出的问题:Conll 2003 标记器在 2023 年仍然有效吗?", "metrics": {"bleu_score": 42.62102445074978, "chrf_score": 37.95688250612056, "xcomet_score": 0.892208993434906, "xcomet_qe_score": 0.8737361431121826, "metricx_score": 3.1850643157958984, "metricx_qe_score": 3.7545342445373535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是肯定的。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 53.5017446130673, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43837279081344604, "metricx_qe_score": 0.7667475342750549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We hope our paper calls for more research on how to improve generalizations of the models.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9159862995147705, "xcomet_qe_score": 0.9957518577575684, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文、数据集,如果您有任何疑问,请随时与我联系。", "metrics": {"bleu_score": 55.27261086532664, "chrf_score": 47.41537251654332, "xcomet_score": 0.9908421039581299, "xcomet_qe_score": 0.970913827419281, "metricx_score": 0.24601110816001892, "metricx_qe_score": 0.2431950569152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "very much.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8815122246742249, "xcomet_qe_score": 0.855570375919342, "metricx_score": 3.9777565002441406, "metricx_qe_score": 5.062550067901611, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "I'm going to talk about our work on resolving indirect referring expressions for entity selection, in which we introduce the altentities corpus.\" \"我的名字是Javad Hosseini,这是与Philip Radlinski, Silvia Parati和Anil Swamy的联合工作。", "metrics": {"bleu_score": 1.0103588033775688, "chrf_score": 7.767791365326271, "xcomet_score": 0.45688116550445557, "xcomet_qe_score": 0.5482639074325562, "metricx_score": 14.950221061706543, "metricx_qe_score": 14.215229988098145, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "\" \"我的名字是Javad Hosseini,这是与Philip Radlinski, Silvia Parati和Anil Swamy的联合工作。\"", "metrics": {"bleu_score": 5.071215336946557, "chrf_score": 49.31123727586279, "xcomet_score": 0.37279802560806274, "xcomet_qe_score": 0.3381076455116272, "metricx_score": 7.748684406280518, "metricx_qe_score": 8.036529541015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户当他们想要做出选择时的语言。", "metrics": {"bleu_score": 58.15025407036991, "chrf_score": 58.336389876767505, "xcomet_score": 0.9888336658477783, "xcomet_qe_score": 0.9843820333480835, "metricx_score": 0.8009673357009888, "metricx_qe_score": 0.952006459236145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在考虑这个替代问题。", "metrics": {"bleu_score": 33.932513407933634, "chrf_score": 28.908920863344722, "xcomet_score": 0.8302152156829834, "xcomet_qe_score": 0.8733646273612976, "metricx_score": 1.127875566482544, "metricx_qe_score": 0.9833705425262451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说让我放松一下,还是我有一种感觉?在这", "metrics": {"bleu_score": 8.500539049202116, "chrf_score": 4.995347983505999, "xcomet_score": 0.14175955951213837, "xcomet_qe_score": 0.15066727995872498, "metricx_score": 6.048091888427734, "metricx_qe_score": 4.963944435119629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里,用户想要在这两个选项中进行选择。", "metrics": {"bleu_score": 13.673412644075363, "chrf_score": 14.277140356724919, "xcomet_score": 0.6343441009521484, "xcomet_qe_score": 0.6470510959625244, "metricx_score": 4.425299167633057, "metricx_qe_score": 5.00478458404541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的事情是使用直接引用。例如,通过说歌曲的名字是你在我或它的位置,第一。", "metrics": {"bleu_score": 21.029882360621507, "chrf_score": 16.608890136522277, "xcomet_score": 0.561896800994873, "xcomet_qe_score": 0.5680783987045288, "metricx_score": 9.608564376831055, "metricx_qe_score": 10.051015853881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But sometimes an indirect reference is more appropriate to have a more natural conversation. This could", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8974775075912476, "xcomet_qe_score": 0.9173019528388977, "metricx_score": 24.83834457397461, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "happen when the user cannot remember the name of the song. All the pronunciations are too", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6293200254440308, "xcomet_qe_score": 0.7715638875961304, "metricx_score": 14.44370174407959, "metricx_qe_score": 21.1993465423584, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "similar to each other and hard to disambiguate.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7994487285614014, "xcomet_qe_score": 0.908267617225647, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定一个偏好时。在这里有一", "metrics": {"bleu_score": 18.022104749634178, "chrf_score": 17.77807487356998, "xcomet_score": 0.8706091046333313, "xcomet_qe_score": 0.8472228646278381, "metricx_score": 4.984915256500244, "metricx_qe_score": 1.645201563835144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些例子和直接引用。例如,较新的 ones 或不具能的 ones。", "metrics": {"bleu_score": 6.118069881849191, "chrf_score": 8.4738056375452, "xcomet_score": 0.20979319512844086, "xcomet_qe_score": 0.2142508327960968, "metricx_score": 16.354415893554688, "metricx_qe_score": 16.510921478271484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统和对LLMs实体理解进行比较中的一个重要问题。", "metrics": {"bleu_score": 43.730942860923, "chrf_score": 43.123171407769476, "xcomet_score": 0.8276318311691284, "xcomet_qe_score": 0.8061131834983826, "metricx_score": 2.3556883335113525, "metricx_qe_score": 3.4703574180603027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有一个公共数据集,一个用于任务的大规模公共数据集,所以我们使用人群注释收集一个。", "metrics": {"bleu_score": 39.970543200306686, "chrf_score": 36.73194553706899, "xcomet_score": 0.6779052019119263, "xcomet_qe_score": 0.6710680723190308, "metricx_score": 6.756095886230469, "metricx_qe_score": 5.939051628112793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖三个不同的领域:音乐、书籍和。", "metrics": {"bleu_score": 77.14617999067436, "chrf_score": 72.30630813037793, "xcomet_score": 0.794864296913147, "xcomet_qe_score": 0.7833391427993774, "metricx_score": 4.343648910522461, "metricx_qe_score": 3.5116090774536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Our data set collection methodology emphasizes informality using a cartoon completion set.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.907494068145752, "xcomet_qe_score": 0.9081952571868896, "metricx_score": 20.0564022064209, "metricx_qe_score": 24.904693603515625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通有三个语音泡泡。", "metrics": {"bleu_score": 15.187207110382285, "chrf_score": 11.468257615211023, "xcomet_score": 0.7483118772506714, "xcomet_qe_score": 0.7053261995315552, "metricx_score": 1.0767146348953247, "metricx_qe_score": 0.8939896821975708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个泡泡中,鲍勃说,记住我们昨天听的那首歌。鲍勃", "metrics": {"bleu_score": 43.50403810365718, "chrf_score": 34.86749913349231, "xcomet_score": 0.5933901071548462, "xcomet_qe_score": 0.5593968629837036, "metricx_score": 6.268951416015625, "metricx_qe_score": 4.784274578094482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样做是为了设置对话的上下文。", "metrics": {"bleu_score": 49.202745153855076, "chrf_score": 38.04848782905663, "xcomet_score": 0.9897052049636841, "xcomet_qe_score": 0.9407415390014648, "metricx_score": 1.2308270931243896, "metricx_qe_score": 1.888815999031067, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个演讲泡泡中,爱丽丝说,你是说轻易放弃我,还是我有感觉? 这是", "metrics": {"bleu_score": 12.656494026948833, "chrf_score": 8.655657495556598, "xcomet_score": 0.44192761182785034, "xcomet_qe_score": 0.3759118914604187, "metricx_score": 10.294256210327148, "metricx_qe_score": 8.225483894348145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "替代问题。", "metrics": {"bleu_score": 17.86690863748233, "chrf_score": 16.11377319080968, "xcomet_score": 0.8499757051467896, "xcomet_qe_score": 0.8539196252822876, "metricx_score": 2.1101341247558594, "metricx_qe_score": 1.6890231370925903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个演讲泡泡中,Bob使用间接引用来选择其中一个实体。例如,新的。", "metrics": {"bleu_score": 43.06664436965418, "chrf_score": 40.18892863390949, "xcomet_score": 0.6468870043754578, "xcomet_qe_score": 0.571061372756958, "metricx_score": 7.466762542724609, "metricx_qe_score": 7.632663726806641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个语音泡泡,但第三个语音泡泡由注释者填写。", "metrics": {"bleu_score": 56.46631238098639, "chrf_score": 50.168387553666584, "xcomet_score": 0.9101861715316772, "xcomet_qe_score": 0.9591031074523926, "metricx_score": 2.4723615646362305, "metricx_qe_score": 2.5246856212615967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个语音泡泡从每个域的几个手动提示中选择。", "metrics": {"bleu_score": 54.435609963526716, "chrf_score": 47.579254598303145, "xcomet_score": 0.8009705543518066, "xcomet_qe_score": 0.7627276182174683, "metricx_score": 1.5710201263427734, "metricx_qe_score": 1.7896560430526733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个问题,即替代问题,是如下生成的。", "metrics": {"bleu_score": 14.830451601748624, "chrf_score": 15.866558484949389, "xcomet_score": 0.9094420671463013, "xcomet_qe_score": 0.8518509864807129, "metricx_score": 1.3109694719314575, "metricx_qe_score": 1.2137508392333984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说A还是B,其中", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.06063572173902, "xcomet_score": 0.8359787464141846, "xcomet_qe_score": 0.7861765623092651, "metricx_score": 2.8045897483825684, "metricx_qe_score": 1.1639037132263184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "A和B是来自维基百科的样本。 这", "metrics": {"bleu_score": 92.53911813809742, "chrf_score": 98.2553168750668, "xcomet_score": 0.8088544607162476, "xcomet_qe_score": 0.689124345779419, "metricx_score": 4.654078960418701, "metricx_qe_score": 1.3534233570098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里是我们使用的不同采样方法。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 55.758016687543574, "xcomet_score": 0.7724735736846924, "xcomet_qe_score": 0.7491453886032104, "metricx_score": 1.7240960597991943, "metricx_qe_score": 1.1311482191085815, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中移动到更高的位置时,实体变得更加相似,通常更难进行消除歧义。", "metrics": {"bleu_score": 37.806791842124426, "chrf_score": 34.9675172788562, "xcomet_score": 0.7784690856933594, "xcomet_qe_score": 0.7543805837631226, "metricx_score": 3.406893730163574, "metricx_qe_score": 4.465614318847656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The first one is uniform attraction.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8335773944854736, "xcomet_qe_score": 0.8382441997528076, "metricx_score": 9.173545837402344, "metricx_qe_score": 12.2576322555542, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个是当实体具有相似标题时。例如,两个书名为《退稿》的书。", "metrics": {"bleu_score": 17.157675581103113, "chrf_score": 16.4644040588931, "xcomet_score": 0.7352973222732544, "xcomet_qe_score": 0.7380019426345825, "metricx_score": 4.810242176055908, "metricx_qe_score": 5.622130393981934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三个是当他们在维基百科上有类似的描述时。", "metrics": {"bleu_score": 49.24584878270649, "chrf_score": 41.48474740773502, "xcomet_score": 0.9160966873168945, "xcomet_qe_score": 0.903738796710968, "metricx_score": 0.7081012725830078, "metricx_qe_score": 0.8103205561637878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当他们在维基百科上有相似的信息框或属性时。", "metrics": {"bleu_score": 65.08430035585083, "chrf_score": 59.89875881126423, "xcomet_score": 0.9331164360046387, "xcomet_qe_score": 0.9861447811126709, "metricx_score": 1.202676773071289, "metricx_qe_score": 1.4281833171844482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,同一类型或同一艺术家。", "metrics": {"bleu_score": 19.88658161321134, "chrf_score": 19.904120762647324, "xcomet_score": 0.9625793695449829, "xcomet_qe_score": 0.8506661057472229, "metricx_score": 1.0885424613952637, "metricx_qe_score": 1.4015545845031738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向评审团展示这个替代问题时,他们知道这些实体的名称,但他们不一定知道这些实体。", "metrics": {"bleu_score": 66.48909006792681, "chrf_score": 58.37153621973904, "xcomet_score": 0.7823443412780762, "xcomet_qe_score": 0.7973322868347168, "metricx_score": 3.710872173309326, "metricx_qe_score": 4.280559062957764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们所做的就是展示一些关于这两个实体的背景知识。", "metrics": {"bleu_score": 51.191407111567585, "chrf_score": 47.656203180698085, "xcomet_score": 0.9764916896820068, "xcomet_qe_score": 0.7475842237472534, "metricx_score": 0.7416502237319946, "metricx_qe_score": 1.3484398126602173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只需显示每首歌曲的谷歌搜索链接。 然后要求注释者至少听一首每首歌并阅读每首歌的注释。", "metrics": {"bleu_score": 56.58972967704392, "chrf_score": 48.23102875884489, "xcomet_score": 0.8961509466171265, "xcomet_qe_score": 0.852374792098999, "metricx_score": 3.7702276706695557, "metricx_qe_score": 4.271768093109131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是该歌曲的Google搜索结果示例。", "metrics": {"bleu_score": 15.928383233398593, "chrf_score": 13.540878640808257, "xcomet_score": 0.9720337390899658, "xcomet_qe_score": 0.9201987981796265, "metricx_score": 1.6471115350723267, "metricx_qe_score": 2.4173731803894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍域,我们从维基百科显示一些背景文本。", "metrics": {"bleu_score": 50.690788390666796, "chrf_score": 43.39871865431841, "xcomet_score": 0.8798239231109619, "xcomet_qe_score": 0.9091355800628662, "metricx_score": 2.043416976928711, "metricx_qe_score": 2.816438674926758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科显示了它们的图像,以便注释者了解它们的外观。", "metrics": {"bleu_score": 43.45932582302571, "chrf_score": 34.49485086507133, "xcomet_score": 0.9149448871612549, "xcomet_qe_score": 0.9143609404563904, "metricx_score": 1.318782091140747, "metricx_qe_score": 1.7256841659545898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求注释者选择其中一个实体。例如,在这里选择第一个实体,并使用三到五个间接引用表达式来描述它们。", "metrics": {"bleu_score": 48.88966831474014, "chrf_score": 43.865146086139525, "xcomet_score": 0.9109467267990112, "xcomet_qe_score": 0.8692939281463623, "metricx_score": 2.2545673847198486, "metricx_qe_score": 2.2126286029815674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,钢琴音乐的那一个。", "metrics": {"bleu_score": 10.600313379512592, "chrf_score": 12.196573060447404, "xcomet_score": 0.9410359859466553, "xcomet_qe_score": 0.9732215404510498, "metricx_score": 3.120469093322754, "metricx_qe_score": 1.7337239980697632, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一些示例。", "metrics": {"bleu_score": 80.65008590125565, "chrf_score": 77.75918525918527, "xcomet_score": 0.9735596179962158, "xcomet_qe_score": 0.9616199731826782, "metricx_score": 0.6032133102416992, "metricx_qe_score": 1.398135781288147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有文字的那个,不是那个有十二岁男孩的那个,或者虚构的那个,或者来自阿塞拜疆的那个。", "metrics": {"bleu_score": 34.62137706925174, "chrf_score": 28.658346911553327, "xcomet_score": 0.7844430208206177, "xcomet_qe_score": 0.6128633618354797, "metricx_score": 1.6343109607696533, "metricx_qe_score": 3.4880030155181885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "\"Elites Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T", "metrics": {"bleu_score": 1.4278442804983642, "chrf_score": 13.93666884697255, "xcomet_score": 0.43629223108291626, "xcomet_qe_score": 0.5168372988700867, "metricx_score": 23.05697250366211, "metricx_qe_score": 24.41028594970703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "5XLarge model are summarized below.\"", "metrics": {"bleu_score": 0.0, "chrf_score": 5.180895963805418, "xcomet_score": 0.8065848350524902, "xcomet_qe_score": 0.8688006401062012, "metricx_score": 15.038834571838379, "metricx_qe_score": 14.869773864746094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与注释者完全相同的背景知识,那么准确性就会非常高。它大约是百分之九十二到百分之九十五。", "metrics": {"bleu_score": 55.437187910540544, "chrf_score": 54.78060857372332, "xcomet_score": 0.8899725675582886, "xcomet_qe_score": 0.9182937145233154, "metricx_score": 1.2007873058319092, "metricx_qe_score": 1.088168978691101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确性在百分之八十二到百分之八十七之间,这更现实。", "metrics": {"bleu_score": 50.44097344556071, "chrf_score": 50.013310244666684, "xcomet_score": 0.8426370024681091, "xcomet_qe_score": 0.8887948989868164, "metricx_score": 1.3440696001052856, "metricx_qe_score": 1.9740413427352905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确性只有百分之六十,因此有很大的改进空间。", "metrics": {"bleu_score": 66.45577319970795, "chrf_score": 62.89495957408807, "xcomet_score": 0.98401939868927, "xcomet_qe_score": 0.979002833366394, "metricx_score": 1.7115094661712646, "metricx_qe_score": 2.7544569969177246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还证明了模型是域可泛型的。", "metrics": {"bleu_score": 31.523777567675985, "chrf_score": 27.311010233432263, "xcomet_score": 0.8538817763328552, "xcomet_qe_score": 0.8456663489341736, "metricx_score": 1.492393136024475, "metricx_qe_score": 2.298190116882324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是链接到我们的数据集。", "metrics": {"bleu_score": 18.476860420522193, "chrf_score": 22.80361506634118, "xcomet_score": 0.9304931163787842, "xcomet_qe_score": 0.9188475608825684, "metricx_score": 0.6963320970535278, "metricx_qe_score": 0.5238329768180847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自特伦托大学和布鲁诺·凯斯勒基金会的Sarah Papi。我将简要介绍一下Simultaneous Speech Translation的Attention as a Guide paper,这是与Matteo Negri和Marco Turchi共同完成的工作。", "metrics": {"bleu_score": 45.42230333014373, "chrf_score": 52.14662061028839, "xcomet_score": 0.7385553121566772, "xcomet_qe_score": 0.7262679934501648, "metricx_score": 8.703352928161621, "metricx_qe_score": 7.790036678314209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同时翻译?", "metrics": {"bleu_score": 36.55552228545123, "chrf_score": 26.4484126984127, "xcomet_score": 0.8245272636413574, "xcomet_qe_score": 0.8203566074371338, "metricx_score": 1.295767068862915, "metricx_qe_score": 1.1523115634918213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同步翻译或SimulST是将口语实时翻译成另一种语言的文本的过程,允许跨语言交流。", "metrics": {"bleu_score": 62.05598825342695, "chrf_score": 62.70909802291641, "xcomet_score": 0.9336756467819214, "xcomet_qe_score": 0.9348626136779785, "metricx_score": 2.607189178466797, "metricx_qe_score": 2.899986982345581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And what are the problems of the current SimulST models?", "metrics": {"bleu_score": 3.42209762272661, "chrf_score": 20.636492302413362, "xcomet_score": 0.9990956783294678, "xcomet_qe_score": 1.0, "metricx_score": 20.085752487182617, "metricx_qe_score": 23.91682243347168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Specific architectures are usually trained, introducing additional modules to be optimized. Long", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8507362604141235, "xcomet_qe_score": 0.8011287450790405, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and complicated training procedures, for example, training involving different optimization objectives. and training and", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.711459219455719, "xcomet_qe_score": 0.7207000851631165, "metricx_score": 21.35664939880371, "metricx_qe_score": 23.90577507019043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "maintaining several models to reach different latency regimes, for example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7902282476425171, "xcomet_qe_score": 0.9026553630828857, "metricx_score": 16.632564544677734, "metricx_qe_score": 15.95514965057373, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", training a model with an average of one second latency and another with two seconds latency, and so on.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9403494000434875, "xcomet_qe_score": 0.9561670422554016, "metricx_score": 24.838851928710938, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么我们的解决方案是什么呢?", "metrics": {"bleu_score": 91.93227152249175, "chrf_score": 91.10491360491362, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.015326432883739471, "metricx_qe_score": 0.16972288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用现有的离线ST模型,不需要重新训练或采用特定的STMAD架构。只使用一个模型来", "metrics": {"bleu_score": 48.975162684443326, "chrf_score": 46.294843220399734, "xcomet_score": 0.6908151507377625, "xcomet_qe_score": 0.7685334086418152, "metricx_score": 4.954958915710449, "metricx_qe_score": 5.238498210906982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "处理每个延迟模式,并通过特定参数处理延迟。", "metrics": {"bleu_score": 38.563779372891155, "chrf_score": 35.80956499062154, "xcomet_score": 0.7161880731582642, "xcomet_qe_score": 0.6938152313232422, "metricx_score": 4.055828094482422, "metricx_qe_score": 5.092053413391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And leverages the knowledge already acquired by the model through the attention mechanism between audio input and tex", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8853750824928284, "xcomet_qe_score": 0.8959237337112427, "metricx_score": 20.981224060058594, "metricx_qe_score": 22.135766983032227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tual output, that is, the cross-attention mechanism. And you can see an example on the right.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8052433729171753, "xcomet_qe_score": 0.8599420785903931, "metricx_score": 12.705032348632812, "metricx_qe_score": 10.53374195098877, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出一个数据或编码解码器注意力。它是一种策略,我们决定是否根据注意力指向的地方进行部分翻译。", "metrics": {"bleu_score": 44.510669585546424, "chrf_score": 35.47606412025184, "xcomet_score": 0.6427300572395325, "xcomet_qe_score": 0.5848833322525024, "metricx_score": 6.305591106414795, "metricx_qe_score": 6.782876968383789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力不集中,即该和小于某个阈值alpha,指向最后一个lambda语音帧,这意味着接收到的信息不够稳定。", "metrics": {"bleu_score": 39.387385231154326, "chrf_score": 31.06961386126914, "xcomet_score": 0.6111596822738647, "xcomet_qe_score": 0.6139110922813416, "metricx_score": 6.2486772537231445, "metricx_qe_score": 6.2624359130859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们收到一个包含“我将要谈论”的语音片段,并且我们的模型预测了德语的翻译。 我们将研究交叉注意力的权重。 我们将看到,前两个单词指向最早接收到的语音帧,而最后一个单词指向最后接收到的语音帧,即lambda语音帧。", "metrics": {"bleu_score": 51.658778571981465, "chrf_score": 40.23765190886471, "xcomet_score": 0.5540982484817505, "xcomet_qe_score": 0.5259723663330078, "metricx_score": 4.0876994132995605, "metricx_qe_score": 4.538022041320801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个单词将被省略。 虽然由于交叉注意力的总和高于一定的比率alpha,我们不会忽略最后一个单词,而是等待另一个语音信箱。", "metrics": {"bleu_score": 47.392548084190025, "chrf_score": 40.17091768934206, "xcomet_score": 0.5692652463912964, "xcomet_qe_score": 0.5242239236831665, "metricx_score": 6.420568466186523, "metricx_qe_score": 7.256962776184082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续并接收另一个演讲,模型预测了另外三个单词,我们将查看交叉注意力权重。 我们会看到没有单词指向最后一个lambda语音帧。", "metrics": {"bleu_score": 51.08938212327171, "chrf_score": 41.57317659888713, "xcomet_score": 0.5369433164596558, "xcomet_qe_score": 0.4922672212123871, "metricx_score": 5.1760358810424805, "metricx_qe_score": 4.958688735961914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个字将被", "metrics": {"bleu_score": 29.10042507378281, "chrf_score": 24.64601597963004, "xcomet_score": 0.7587144374847412, "xcomet_qe_score": 0.8175399899482727, "metricx_score": 6.133219242095947, "metricx_qe_score": 2.873957633972168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "省略。 If you look at the main results of that. 我们将同时绘制翻译结果的图表,其中一侧为蓝色,测量翻译质量和平均延迟。 这就是延迟度量值。我们还考虑了计算机意识的平均缺陷,这考虑了模型的计算时间来预测输出。", "metrics": {"bleu_score": 28.18733053753465, "chrf_score": 22.558643071697272, "xcomet_score": 0.16641223430633545, "xcomet_qe_score": 0.2113630175590515, "metricx_score": 13.986732482910156, "metricx_qe_score": 15.217120170593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们希望我们的曲线在这张图上尽可能高。", "metrics": {"bleu_score": 45.477224609819245, "chrf_score": 37.42567682047041, "xcomet_score": 0.9709465503692627, "xcomet_qe_score": 0.8817294836044312, "metricx_score": 1.2877496480941772, "metricx_qe_score": 1.7186129093170166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But also we want that they are shifted on", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8574233055114746, "xcomet_qe_score": 0.8414077758789062, "metricx_score": 21.93781852722168, "metricx_qe_score": 22.32823944091797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the left. 我们与适用于离线模型的普遍策略进行比较,即waitkey策略和本地协议。", "metrics": {"bleu_score": 40.98845889492086, "chrf_score": 28.562208855005828, "xcomet_score": 0.5333813428878784, "xcomet_qe_score": 0.5380269289016724, "metricx_score": 7.195889472961426, "metricx_qe_score": 7.990772247314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与专门针对多语言翻译的最先进架构进行了比较。", "metrics": {"bleu_score": 53.611312694955046, "chrf_score": 45.21019198193112, "xcomet_score": 0.8908019065856934, "xcomet_qe_score": 0.879393994808197, "metricx_score": 2.8058910369873047, "metricx_qe_score": 2.4412879943847656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是同步翻译策略在德语上的结果。", "metrics": {"bleu_score": 43.78826865860791, "chrf_score": 33.27614379084967, "xcomet_score": 0.8881103992462158, "xcomet_qe_score": 0.8852344155311584, "metricx_score": 2.591148853302002, "metricx_qe_score": 2.3888649940490723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,ADAPT在应用于离线模型的所有策略时表现出优势,因为其曲线向左移动。", "metrics": {"bleu_score": 37.98775259677768, "chrf_score": 36.77673701772435, "xcomet_score": 0.9518569707870483, "xcomet_qe_score": 0.9203839302062988, "metricx_score": 3.7027547359466553, "metricx_qe_score": 5.083911895751953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际消耗时间或计算器的使用时间,那么这是最快的策略。", "metrics": {"bleu_score": 42.59566593905153, "chrf_score": 40.83037097133428, "xcomet_score": 0.9602229595184326, "xcomet_qe_score": 0.9425585269927979, "metricx_score": 2.205617904663086, "metricx_qe_score": 1.7379282712936401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9973729848861694, "xcomet_qe_score": 0.974124014377594, "metricx_score": 0.1322653442621231, "metricx_qe_score": 0.20905639231204987, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码、模型和同时输出,以便于我们工作的可重复性。", "metrics": {"bleu_score": 52.11208614820388, "chrf_score": 52.72623853271278, "xcomet_score": 0.8578290939331055, "xcomet_qe_score": 0.8182840347290039, "metricx_score": 1.2719509601593018, "metricx_qe_score": 1.6249505281448364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,大家好,我叫Ying,我的同事Zhi Yang和我将要介绍的是我们关于Multi-Instuit的研究,旨在通过调整教学来改善多模态神经网络学习。 因此,", "metrics": {"bleu_score": 17.71672685027266, "chrf_score": 35.946367053377934, "xcomet_score": 0.40977057814598083, "xcomet_qe_score": 0.34887373447418213, "metricx_score": 6.301358222961426, "metricx_qe_score": 6.456707954406738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多工作开始探索使用预训练语言模型以参数和数据高效的方式进行不同的下游任务的新学习范式。", "metrics": {"bleu_score": 53.65165511479585, "chrf_score": 45.23566650688255, "xcomet_score": 0.8218081593513489, "xcomet_qe_score": 0.7783031463623047, "metricx_score": 2.6213274002075195, "metricx_qe_score": 3.7667183876037598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优使大型语言模型能够以自然指令的方式在短时间内执行看似无关的任务。", "metrics": {"bleu_score": 32.53632712138422, "chrf_score": 28.19586204349126, "xcomet_score": 0.8320194482803345, "xcomet_qe_score": 0.7664463520050049, "metricx_score": 3.858952522277832, "metricx_qe_score": 2.9785845279693604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数以前关于指令调优的工作都集中在改进语言任务上仅使用神经网络的性能上,而计算机视觉和多模态任务已被排除在外。", "metrics": {"bleu_score": 38.123530976950654, "chrf_score": 37.82965264795071, "xcomet_score": 0.8721669912338257, "xcomet_qe_score": 0.8139101266860962, "metricx_score": 2.009486198425293, "metricx_qe_score": 2.4451165199279785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们想要研究是否对多模态预训练模型进行指令调优,实际上可以提高对无监督多模态任务的泛化能力。", "metrics": {"bleu_score": 46.146957847515736, "chrf_score": 43.007960148564784, "xcomet_score": 0.8727397918701172, "xcomet_qe_score": 0.8715806007385254, "metricx_score": 1.9516181945800781, "metricx_qe_score": 1.7451622486114502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们研究的时间,我们发现了rlp和多模态之间在可用性方面存在相当大的差异。", "metrics": {"bleu_score": 37.06871118806869, "chrf_score": 31.44182751169124, "xcomet_score": 0.6418803930282593, "xcomet_qe_score": 0.6652743220329285, "metricx_score": 5.3332414627075195, "metricx_qe_score": 5.395369052886963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有超过一千六百个语言独立的任务。", "metrics": {"bleu_score": 18.393816249638885, "chrf_score": 17.447171739281274, "xcomet_score": 0.8504892587661743, "xcomet_qe_score": 0.8222324848175049, "metricx_score": 2.3049569129943848, "metricx_qe_score": 3.266066551208496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大型的公开可用的多模态任务。", "metrics": {"bleu_score": 22.813997135031524, "chrf_score": 24.535603715170286, "xcomet_score": 0.867347002029419, "xcomet_qe_score": 0.9024227261543274, "metricx_score": 1.9751241207122803, "metricx_qe_score": 3.3014612197875977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这激励我们构建一个多模态任务调优数据集。", "metrics": {"bleu_score": 63.336866679312486, "chrf_score": 56.4879205039388, "xcomet_score": 0.916161298751831, "xcomet_qe_score": 0.9647555351257324, "metricx_score": 1.944018006324768, "metricx_qe_score": 1.7242012023925781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们介绍了Multi-InstRCT,第一个多模态指令调优基准数据集,由62个多模态任务组成,涵盖10个类别。", "metrics": {"bleu_score": 36.85255874927917, "chrf_score": 32.971948129552956, "xcomet_score": 0.874796986579895, "xcomet_qe_score": 0.8341218829154968, "metricx_score": 3.654371976852417, "metricx_qe_score": 4.099686145782471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务是从二十一个现有的开源数据集中衍生出来的,每个任务都配备了五个专家写的说明。", "metrics": {"bleu_score": 38.29911744373932, "chrf_score": 37.19053420449029, "xcomet_score": 0.8295904397964478, "xcomet_qe_score": 0.7762322425842285, "metricx_score": 2.0931994915008545, "metricx_qe_score": 2.380852460861206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们所提出的数据集上研究多模态指令调优,我们采用OFa,一个统一的多模态训练模型作为我们的基础模型。", "metrics": {"bleu_score": 60.25139182642071, "chrf_score": 54.61108521453384, "xcomet_score": 0.907694935798645, "xcomet_qe_score": 0.8192777633666992, "metricx_score": 1.7814404964447021, "metricx_qe_score": 2.11859130859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFa使用统一的语言、图像词和边界框的坐标。 Here we show some example inst", "metrics": {"bleu_score": 54.119533608948146, "chrf_score": 39.13719942768793, "xcomet_score": 0.4524022340774536, "xcomet_qe_score": 0.4438992738723755, "metricx_score": 7.109050273895264, "metricx_qe_score": 6.8941874504089355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ances from our multi instruction dataset. 为了统一处理各种输入和输出数据类型。", "metrics": {"bleu_score": 34.31632412336652, "chrf_score": 39.4415995949315, "xcomet_score": 0.5270934104919434, "xcomet_qe_score": 0.6549977660179138, "metricx_score": 15.29675006866455, "metricx_qe_score": 15.971451759338379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,并将所有任务以统一的序列到序列格式化,其中", "metrics": {"bleu_score": 64.55714066623374, "chrf_score": 63.033732214969994, "xcomet_score": 0.7424546480178833, "xcomet_qe_score": 0.7683431506156921, "metricx_score": 3.975311517715454, "metricx_qe_score": 2.6825551986694336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框都表示在同一个标记空间中。", "metrics": {"bleu_score": 65.45853518925674, "chrf_score": 59.64963293910151, "xcomet_score": 0.9767771363258362, "xcomet_qe_score": 0.957078754901886, "metricx_score": 0.8619650602340698, "metricx_qe_score": 1.1081395149230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Okay, now I'm going to talk about multimodal instruction tuning. 因此", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7850488424301147, "xcomet_qe_score": 0.8330520987510681, "metricx_score": 15.103133201599121, "metricx_qe_score": 12.57127857208252, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于训练数据集,我们使用来自NLG的53个任务进行训练,并对每个任务进行10,000次采样。", "metrics": {"bleu_score": 53.42163771035709, "chrf_score": 49.117110053816745, "xcomet_score": 0.7247706651687622, "xcomet_qe_score": 0.7584471702575684, "metricx_score": 6.106363296508789, "metricx_qe_score": 7.4928154945373535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个CommonsenseReasoning组进行测试,并从VQA和恶意组中选择另外5个任务。", "metrics": {"bleu_score": 38.57024173736543, "chrf_score": 30.860387503308907, "xcomet_score": 0.677932620048523, "xcomet_qe_score": 0.6233932375907898, "metricx_score": 6.907548904418945, "metricx_qe_score": 6.24433708190918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用测试集中的每个实例来执行每个任务。", "metrics": {"bleu_score": 45.85015057701145, "chrf_score": 36.81556160283401, "xcomet_score": 0.8251314759254456, "xcomet_qe_score": 0.8073203563690186, "metricx_score": 1.5115017890930176, "metricx_qe_score": 1.3578076362609863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试集中随机抽取20个任务作为同一任务的NP。 所以我们", "metrics": {"bleu_score": 52.241572658898335, "chrf_score": 47.62428626205716, "xcomet_score": 0.5252059102058411, "xcomet_qe_score": 0.4616844058036804, "metricx_score": 9.584644317626953, "metricx_qe_score": 8.092307090759277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用预训练的ofa大模型作为基础模型。", "metrics": {"bleu_score": 64.9512300244718, "chrf_score": 52.472362659079984, "xcomet_score": 0.8974770307540894, "xcomet_qe_score": 0.8347880840301514, "metricx_score": 2.382075309753418, "metricx_qe_score": 3.1638638973236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的实例混合在一起。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.77964449428271, "xcomet_score": 0.9752105474472046, "xcomet_qe_score": 0.9487849473953247, "metricx_score": 0.7904393672943115, "metricx_qe_score": 1.3897600173950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例与其中的五个指令模板中的一个随机组合。", "metrics": {"bleu_score": 59.367810168905606, "chrf_score": 57.73657304284261, "xcomet_score": 0.9843765497207642, "xcomet_qe_score": 0.8488671779632568, "metricx_score": 0.9138461351394653, "metricx_qe_score": 1.4453474283218384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在测试过程中,对于每个任务,我们通过评估模型来进行总共五次实验,", "metrics": {"bleu_score": 24.777600408325416, "chrf_score": 21.535911044325136, "xcomet_score": 0.8130662441253662, "xcomet_qe_score": 0.8074372410774231, "metricx_score": 3.9288299083709717, "metricx_qe_score": 4.649518966674805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每次实验中使用五个指令中的一个。 我们报告了所有五个实验中性能的均值、最大值和标准差。", "metrics": {"bleu_score": 21.987871204796267, "chrf_score": 21.540229856911576, "xcomet_score": 0.785412073135376, "xcomet_qe_score": 0.779464602470398, "metricx_score": 2.7412478923797607, "metricx_qe_score": 2.4768714904785156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,则报告准确率。", "metrics": {"bleu_score": 49.444506108522596, "chrf_score": 39.93318518050496, "xcomet_score": 0.9246195554733276, "xcomet_qe_score": 0.9804967641830444, "metricx_score": 0.6705411672592163, "metricx_qe_score": 0.7192279696464539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,则报告rougeL。对于NLP任务,我们也报告rougeL。", "metrics": {"bleu_score": 59.51279946176411, "chrf_score": 50.07659926888002, "xcomet_score": 0.8570435643196106, "xcomet_qe_score": 0.8196728229522705, "metricx_score": 2.734541177749634, "metricx_qe_score": 3.2529523372650146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为敏感度。", "metrics": {"bleu_score": 65.61056137783987, "chrf_score": 63.24082048242982, "xcomet_score": 0.9873756170272827, "xcomet_qe_score": 0.9976328611373901, "metricx_score": 0.43716371059417725, "metricx_qe_score": 0.5021844506263733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它衡量模型在任务相同的情况下,无论任务排序如何变化,始终能够一致产生相同的输出。", "metrics": {"bleu_score": 18.986011309785297, "chrf_score": 16.889165405616026, "xcomet_score": 0.8759722709655762, "xcomet_qe_score": 0.8214951753616333, "metricx_score": 4.677354335784912, "metricx_qe_score": 4.9929986000061035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提高操作系统的性能,在同一多模任务上。", "metrics": {"bleu_score": 39.319919825365815, "chrf_score": 34.236427448456105, "xcomet_score": 0.7935390472412109, "xcomet_qe_score": 0.7688063979148865, "metricx_score": 3.5838258266448975, "metricx_qe_score": 2.5759682655334473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Also, transfer learning from natural instruction dataset can benefit instruction tuning.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.971070408821106, "xcomet_qe_score": 0.9853768348693848, "metricx_score": 20.589128494262695, "metricx_qe_score": 24.14323616027832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里我们可以看到,当任务数量增加时,模型的性能会得到改善,而敏感性会降低。", "metrics": {"bleu_score": 38.95615402875113, "chrf_score": 32.45883171579766, "xcomet_score": 0.9849883317947388, "xcomet_qe_score": 0.9893215894699097, "metricx_score": 0.9845343828201294, "metricx_qe_score": 1.043572187423706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们也做了一个实验。", "metrics": {"bleu_score": 57.067457770559976, "chrf_score": 59.775574540291906, "xcomet_score": 0.9166820049285889, "xcomet_qe_score": 0.9586079120635986, "metricx_score": 0.18896064162254333, "metricx_qe_score": 0.13216054439544678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一个指令与五个指令。", "metrics": {"bleu_score": 37.257423107540355, "chrf_score": 30.418144277025533, "xcomet_score": 0.7893457412719727, "xcomet_qe_score": 0.6884846091270447, "metricx_score": 2.729853868484497, "metricx_qe_score": 3.5074031352996826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多的指令可以显著提高模型的整体性能并降低其敏感性。 所以", "metrics": {"bleu_score": 56.18047160711815, "chrf_score": 49.60967366194377, "xcomet_score": 0.847646176815033, "xcomet_qe_score": 0.8640716671943665, "metricx_score": 3.1941678524017334, "metricx_qe_score": 1.1799708604812622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这显示了不同的微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 77.21947901921794, "chrf_score": 70.98343284682292, "xcomet_score": 0.9882955551147461, "xcomet_qe_score": 0.9814878702163696, "metricx_score": 1.2330644130706787, "metricx_qe_score": 1.4857813119888306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,通过从自然语言数据集中转移学习,模型可以比原始的OFA模型获得更好的敏感性。", "metrics": {"bleu_score": 25.309923339522758, "chrf_score": 25.569682035665657, "xcomet_score": 0.7726878523826599, "xcomet_qe_score": 0.8334052562713623, "metricx_score": 3.5014443397521973, "metricx_qe_score": 4.0826416015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到,来自自然指令数据集的转移学习可以帮助OFa在自然指令数据集上获得更好的性能。", "metrics": {"bleu_score": 60.351301483609014, "chrf_score": 54.104991572438365, "xcomet_score": 0.8544492721557617, "xcomet_qe_score": 0.7233617305755615, "metricx_score": 3.012944221496582, "metricx_qe_score": 2.880880117416382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们提出了第一个大规模多模态注入调优数据集。我们显著提高了OFA的零射程能力,并探索了不同的迁移学习技术,并展示了它们的优势。", "metrics": {"bleu_score": 55.74047199900585, "chrf_score": 51.31400164785778, "xcomet_score": 0.71809983253479, "xcomet_qe_score": 0.7176697254180908, "metricx_score": 4.5591888427734375, "metricx_qe_score": 4.842161178588867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标称为敏感度。 所以还有一", "metrics": {"bleu_score": 51.83282721440025, "chrf_score": 51.945317794147506, "xcomet_score": 0.6855478286743164, "xcomet_qe_score": 0.6389896869659424, "metricx_score": 6.636072158813477, "metricx_qe_score": 4.915310859680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "件事,我们正在收集一个更大的多模态调优数据集,其中包含大约一百五十个额外的视觉语言任务,我们将发布它们。", "metrics": {"bleu_score": 55.92573785203904, "chrf_score": 47.60586685490851, "xcomet_score": 0.6171401739120483, "xcomet_qe_score": 0.6862376928329468, "metricx_score": 4.1284356117248535, "metricx_qe_score": 5.3807172775268555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的QR代码。", "metrics": {"bleu_score": 53.36129799268556, "chrf_score": 41.992451992452004, "xcomet_score": 0.9886788129806519, "xcomet_qe_score": 0.9709252119064331, "metricx_score": 0.5875710844993591, "metricx_qe_score": 0.5168682932853699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Koustaph Sinha,欢迎大家参加我们在ACL 2023上发表的论文", "metrics": {"bleu_score": 33.90324618404441, "chrf_score": 48.43582820595176, "xcomet_score": 0.7727764844894409, "xcomet_qe_score": 0.750572919845581, "metricx_score": 4.233060359954834, "metricx_qe_score": 4.8665385246276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型的可接受性判断并不总是对上下文鲁棒》。", "metrics": {"bleu_score": 69.46815708122293, "chrf_score": 69.51045471066304, "xcomet_score": 0.7582255005836487, "xcomet_qe_score": 0.7201001048088074, "metricx_score": 4.909913539886475, "metricx_qe_score": 5.945833683013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与 John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentress, Roger Levy and Atina Wiliam 的联合工作。", "metrics": {"bleu_score": 9.009149765838952, "chrf_score": 56.66552248256266, "xcomet_score": 0.800102949142456, "xcomet_qe_score": 0.7080920338630676, "metricx_score": 4.276745319366455, "metricx_qe_score": 3.8476338386535645, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们重新审视了最小对偶范式。", "metrics": {"bleu_score": 50.361556444464064, "chrf_score": 47.68931791577767, "xcomet_score": 0.8872013092041016, "xcomet_qe_score": 0.9054228067398071, "metricx_score": 1.3545253276824951, "metricx_qe_score": 0.8412702679634094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最小对偶时间基本上评估语言模型的可接受性判断,这", "metrics": {"bleu_score": 35.18717391120768, "chrf_score": 28.86364738617264, "xcomet_score": 0.5661832690238953, "xcomet_qe_score": 0.5123780965805054, "metricx_score": 10.163410186767578, "metricx_qe_score": 6.547019958496094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,例如语法结构或在刻板印象方面的可接受性,例如粗鲁的对偶。", "metrics": {"bleu_score": 29.311204729075968, "chrf_score": 17.350783199581375, "xcomet_score": 0.6256105899810791, "xcomet_qe_score": 0.41259655356407166, "metricx_score": 4.984838962554932, "metricx_qe_score": 5.490241050720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种最小对偶范式中,评估语言模型的典型方法是,你展示一个可以接受的句子或语法句子,然后你展示一个不可接受的句子或不语法句子。", "metrics": {"bleu_score": 50.3311834429208, "chrf_score": 43.307177773043854, "xcomet_score": 0.6223130226135254, "xcomet_qe_score": 0.6437656879425049, "metricx_score": 2.4678666591644287, "metricx_qe_score": 3.305644989013672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型基本上会给可接受的结果增加更多的概率。", "metrics": {"bleu_score": 36.760411441989504, "chrf_score": 29.524007298148096, "xcomet_score": 0.8740679025650024, "xcomet_qe_score": 0.8126362562179565, "metricx_score": 1.4530632495880127, "metricx_qe_score": 1.6933304071426392, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP管道基本上不允许我们评估模型对更长句子的接受程度。", "metrics": {"bleu_score": 82.06608015220696, "chrf_score": 80.63558817056767, "xcomet_score": 0.8660776615142822, "xcomet_qe_score": 0.781336784362793, "metricx_score": 1.4139117002487183, "metricx_qe_score": 2.8686068058013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Nowadays, large language models are coming up with longer and longer context windows, so", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8832076191902161, "xcomet_qe_score": 0.8911494612693787, "metricx_score": 14.178238868713379, "metricx_qe_score": 9.333418846130371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "it is crucial that we evaluate the models' acceptability throughout the context window. And that is what we are trying to do here. We", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7896988987922668, "xcomet_qe_score": 0.8759555816650391, "metricx_score": 22.477624893188477, "metricx_qe_score": 24.426315307617188, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "are trying to revisit the NPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. 所以", "metrics": {"bleu_score": 0.0, "chrf_score": 1.049074856381946, "xcomet_score": 0.44775888323783875, "xcomet_qe_score": 0.6102226376533508, "metricx_score": 22.044906616210938, "metricx_qe_score": 21.285972595214844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是方法。我们所做", "metrics": {"bleu_score": 29.84745896009822, "chrf_score": 29.285496583025424, "xcomet_score": 0.7955760955810547, "xcomet_qe_score": 0.8257812857627869, "metricx_score": 2.649442195892334, "metricx_qe_score": 2.3639514446258545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的是,模拟这些较长的序列。我们重新审视数据集本身,然后通过从这些数据集中选择可接受或不可接受的句子来重建句子。", "metrics": {"bleu_score": 67.99332995944326, "chrf_score": 61.49247166153291, "xcomet_score": 0.5655958652496338, "xcomet_qe_score": 0.4638751447200775, "metricx_score": 3.301302194595337, "metricx_qe_score": 4.290619850158691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在这里,我们选择了来自附加岛案例的Blim数据集中的典型语法对。", "metrics": {"bleu_score": 19.500507550993692, "chrf_score": 17.407031090337213, "xcomet_score": 0.7380820512771606, "xcomet_qe_score": 0.7702254056930542, "metricx_score": 4.600414276123047, "metricx_qe_score": 3.9243130683898926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And what we do is that to recreate longer sequences, which are acceptable and which has the same matching of the grammatical structure", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9511716365814209, "xcomet_qe_score": 0.9679545164108276, "metricx_score": 20.603715896606445, "metricx_qe_score": 21.926151275634766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we extract grammatical sentences from adiantia. 然后我们将其添加为可接受查询和不可接受查询的前缀。 所以", "metrics": {"bleu_score": 24.78706084127259, "chrf_score": 20.137679601759352, "xcomet_score": 0.4920510947704315, "xcomet_qe_score": 0.5098077058792114, "metricx_score": 12.696540832519531, "metricx_qe_score": 21.634353637695312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过选择不合适的句子来做同样的事情,从相同的匹配中选择不合适的句子。这样做也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 64.08931821716018, "chrf_score": 68.01102656279366, "xcomet_score": 0.9340838193893433, "xcomet_qe_score": 0.8806899785995483, "metricx_score": 2.0738890171051025, "metricx_qe_score": 2.59029221534729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过从不同的子集或数据集中选择句子来做到这一点。", "metrics": {"bleu_score": 66.87908536214134, "chrf_score": 64.03895496579008, "xcomet_score": 0.9876680374145508, "xcomet_qe_score": 0.9186626076698303, "metricx_score": 0.9441476464271545, "metricx_qe_score": 2.244086265563965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的匹配不匹配的情况。 所以", "metrics": {"bleu_score": 44.89771072202119, "chrf_score": 51.045976726631395, "xcomet_score": 0.7946273684501648, "xcomet_qe_score": 0.7895407676696777, "metricx_score": 3.9394705295562744, "metricx_qe_score": 1.951178789138794, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关数据集,但不是来自您正在评估的相同数据集。", "metrics": {"bleu_score": 75.4331028527951, "chrf_score": 69.51090550995698, "xcomet_score": 0.959633469581604, "xcomet_qe_score": 0.7816777229309082, "metricx_score": 1.2259447574615479, "metricx_qe_score": 2.058917999267578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对不可接受性情况做同样的事情。", "metrics": {"bleu_score": 30.010424589932537, "chrf_score": 25.815054786923632, "xcomet_score": 0.8947405815124512, "xcomet_qe_score": 0.8745666742324829, "metricx_score": 1.417815923690796, "metricx_qe_score": 1.2296931743621826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全无关的领域中选择句子,例如维基百科。 所以这会告诉我们", "metrics": {"bleu_score": 36.49209153994252, "chrf_score": 34.9236904838164, "xcomet_score": 0.831717848777771, "xcomet_qe_score": 0.7032213807106018, "metricx_score": 5.120301246643066, "metricx_qe_score": 4.49025821685791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",比如说,模型的可接受性判断是否真的受到任何上下文的影响。 就像上下文来自数据集的不同子集,还是完全与我们正在查看的句子无关。", "metrics": {"bleu_score": 48.305298967003964, "chrf_score": 44.243514496084536, "xcomet_score": 0.7649458646774292, "xcomet_qe_score": 0.5370447039604187, "metricx_score": 3.231642961502075, "metricx_qe_score": 4.052187919616699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么这个模型是如何工作的呢?", "metrics": {"bleu_score": 15.537125692760354, "chrf_score": 15.791005395794272, "xcomet_score": 0.997290849685669, "xcomet_qe_score": 0.9823901653289795, "metricx_score": 0.23080207407474518, "metricx_qe_score": 0.2164274901151657, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们来看看与当前查询对相关性完全无关的维基百科句子。我们发现,MPP的判断对于任意上下文线索来说都非常健壮。", "metrics": {"bleu_score": 31.03153650423228, "chrf_score": 27.029293357862024, "xcomet_score": 0.741579532623291, "xcomet_qe_score": 0.744604229927063, "metricx_score": 4.9302520751953125, "metricx_qe_score": 5.399276256561279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到一千二百二十四,最大化了OPT和GPT2模型。我们", "metrics": {"bleu_score": 42.689605218694496, "chrf_score": 60.341311324577454, "xcomet_score": 0.6180187463760376, "xcomet_qe_score": 0.5310551524162292, "metricx_score": 5.454317092895508, "metricx_qe_score": 4.4968791007995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色的点线中看到,MPP的判断相对稳定。", "metrics": {"bleu_score": 38.66936862858226, "chrf_score": 35.706800123862, "xcomet_score": 0.9105018377304077, "xcomet_qe_score": 0.7825175523757935, "metricx_score": 2.3344507217407227, "metricx_qe_score": 3.671994209289551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,当我们从同一数据集中选择句子时会发生什么?", "metrics": {"bleu_score": 66.79646936159457, "chrf_score": 61.014591074121874, "xcomet_score": 0.9911148548126221, "xcomet_qe_score": 0.9081234931945801, "metricx_score": 1.152337670326233, "metricx_qe_score": 2.3899338245391846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在这里,我们选择或创建来自同一语料库中的可接受和不可接受域的句子。", "metrics": {"bleu_score": 36.67482521398824, "chrf_score": 24.871646485507675, "xcomet_score": 0.7579264044761658, "xcomet_qe_score": 0.7603057026863098, "metricx_score": 2.297239065170288, "metricx_qe_score": 4.3124189376831055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes.", "metrics": {"bleu_score": 0.7528742034383675, "chrf_score": 1.6971639798480442, "xcomet_score": 0.9673831462860107, "xcomet_qe_score": 0.9873110055923462, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,也就是说当我们从同一个现象的句子中选择句子时,我们会选择句子。 我们看到模型的MPP判断有了巨大的增加或减少,这取决于所选前缀是否可接受。 现在这个和这个是非常大的,就像这个效果", "metrics": {"bleu_score": 33.281019735832274, "chrf_score": 31.045194754639404, "xcomet_score": 0.38928738236427307, "xcomet_qe_score": 0.3236393928527832, "metricx_score": 13.705999374389648, "metricx_qe_score": 15.987968444824219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在整个上下文长度中增加一样。这个可能会影响到像新的语言模型这样的东西,它有一个很大的上下文窗口。", "metrics": {"bleu_score": 39.631066492420985, "chrf_score": 39.55526572538091, "xcomet_score": 0.6770824790000916, "xcomet_qe_score": 0.600493311882019, "metricx_score": 5.536130905151367, "metricx_qe_score": 5.553445816040039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么匹配前缀会对语言模型的判断产生如此大的影响呢?", "metrics": {"bleu_score": 87.98128223519915, "chrf_score": 86.58717761591325, "xcomet_score": 0.998862624168396, "xcomet_qe_score": 0.9486066699028015, "metricx_score": 0.5410861968994141, "metricx_qe_score": 0.5131734609603882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们进行了分析系列,其中我们尝试通过添加噪声来扰乱输入句子,", "metrics": {"bleu_score": 25.807986686006764, "chrf_score": 22.642533586184523, "xcomet_score": 0.7555087804794312, "xcomet_qe_score": 0.5932784080505371, "metricx_score": 5.030300140380859, "metricx_qe_score": 5.8095855712890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时保留相关结构。 我们发现这些噪音中没有一个实际上会使模型改变其趋势的方式。", "metrics": {"bleu_score": 26.275115522527546, "chrf_score": 22.136522336870428, "xcomet_score": 0.1976161003112793, "xcomet_qe_score": 0.17235693335533142, "metricx_score": 7.65810489654541, "metricx_qe_score": 9.463948249816895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动句子和类似的句子也很敏感。", "metrics": {"bleu_score": 18.858585441946403, "chrf_score": 19.827563444019784, "xcomet_score": 0.8589311838150024, "xcomet_qe_score": 0.8395295143127441, "metricx_score": 2.8100006580352783, "metricx_qe_score": 3.8990187644958496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们扰动可接受域中的句子时,我们看到所有扰动的增加。相反,当我们扰动不可接受域中的句子时,我们看到MPP判断的减少是类似的。", "metrics": {"bleu_score": 32.85230281378227, "chrf_score": 31.17729925640786, "xcomet_score": 0.6378068923950195, "xcomet_qe_score": 0.5694445967674255, "metricx_score": 5.213460922241211, "metricx_qe_score": 5.607321739196777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键结论是,语言模型对句子中共享的潜在语法和语义特征敏感。", "metrics": {"bleu_score": 54.14399419820842, "chrf_score": 44.17676426612244, "xcomet_score": 0.9161198139190674, "xcomet_qe_score": 0.9355252981185913, "metricx_score": 1.2350497245788574, "metricx_qe_score": 1.5021312236785889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们目前使用短语和单个句子的输入来进行MPPEvaluations的方式可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 50.414973337943, "chrf_score": 40.9979884915029, "xcomet_score": 0.882006049156189, "xcomet_qe_score": 0.8275729417800903, "metricx_score": 4.510534763336182, "metricx_qe_score": 4.670761585235596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以获取有关我们实验的更多详细信息。", "metrics": {"bleu_score": 80.37775080641401, "chrf_score": 74.83135704874834, "xcomet_score": 0.9984984397888184, "xcomet_qe_score": 1.0, "metricx_score": 0.23449498414993286, "metricx_qe_score": 0.19809883832931519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Thank you so much for your attention", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9847120046615601, "xcomet_qe_score": 0.9994665384292603, "metricx_score": 0.8073056936264038, "metricx_qe_score": 0.8194787502288818, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是宾州大学的Yuxin Zhang。", "metrics": {"bleu_score": 14.191377520728777, "chrf_score": 33.844696937128376, "xcomet_score": 0.7608655691146851, "xcomet_qe_score": 0.8821401000022888, "metricx_score": 0.9992897510528564, "metricx_qe_score": 1.2906328439712524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍的是我们在多种自然语言和多种表示形式中进行的示例跨语言语义解析工作。 因此", "metrics": {"bleu_score": 34.16682036421772, "chrf_score": 29.257042859417698, "xcomet_score": 0.6638088226318359, "xcomet_qe_score": 0.6597775220870972, "metricx_score": 5.2549872398376465, "metricx_qe_score": 5.001099109649658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语义解析是构建用户查询的语义表示的任务,例如SQL和lambda计算。 Cross-", "metrics": {"bleu_score": 66.99267475747621, "chrf_score": 57.88273305547867, "xcomet_score": 0.7607265114784241, "xcomet_qe_score": 0.7668636441230774, "metricx_score": 2.6191864013671875, "metricx_qe_score": 3.7899022102355957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "lingual semantic parsing是将多种自然语言中的查询转换为多种语义表示的任务。", "metrics": {"bleu_score": 38.281453052095905, "chrf_score": 29.062130978320955, "xcomet_score": 0.8909164667129517, "xcomet_qe_score": 0.8542770147323608, "metricx_score": 6.219934940338135, "metricx_qe_score": 6.920200347900391, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将查询翻译成多种自然语言,SQL、lambda、funql等。", "metrics": {"bleu_score": 61.64254195185937, "chrf_score": 48.123792583397005, "xcomet_score": 0.863690972328186, "xcomet_qe_score": 0.8083846569061279, "metricx_score": 1.6644091606140137, "metricx_qe_score": 2.249605655670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型是单独提出和评估的。数据集是有限的任务和应用。例如。", "metrics": {"bleu_score": 72.9757965031215, "chrf_score": 68.5759512412406, "xcomet_score": 0.7750694751739502, "xcomet_qe_score": 0.6831324100494385, "metricx_score": 3.7155864238739014, "metricx_qe_score": 2.268937826156616, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有一些关于某些自然语言的报道缺失。", "metrics": {"bleu_score": 36.005658542850306, "chrf_score": 32.803381631148994, "xcomet_score": 0.43450552225112915, "xcomet_qe_score": 0.6283155679702759, "metricx_score": 6.456299781799316, "metricx_qe_score": 4.698958396911621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失了。 克利克斯覆盖某些媒体报道。", "metrics": {"bleu_score": 6.627456392561483, "chrf_score": 9.46034127361655, "xcomet_score": 0.5825643539428711, "xcomet_qe_score": 0.41359519958496094, "metricx_score": 7.343812465667725, "metricx_qe_score": 7.9137864112854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "\"The lambda calculus is missing.\" or they are only evaluated on certain neural model", "metrics": {"bleu_score": 0.0, "chrf_score": 7.264881279750056, "xcomet_score": 0.934386134147644, "xcomet_qe_score": 0.9576939940452576, "metricx_score": 14.540575981140137, "metricx_qe_score": 10.836441993713379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". For example, there is only one single model to evaluate them", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 1.0, "metricx_score": 16.415882110595703, "metricx_qe_score": 22.742685317993164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". 因此,为了这一点,我们提出了示例集。", "metrics": {"bleu_score": 21.31456897111116, "chrf_score": 19.50488765549632, "xcomet_score": 0.639377236366272, "xcomet_qe_score": 0.5948789119720459, "metricx_score": 3.58305287361145, "metricx_qe_score": 4.683510780334473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提供了一个统一的数据集示例集,用于多种自然语言和多种表示的跨语言语义解析。", "metrics": {"bleu_score": 70.68537521325925, "chrf_score": 59.14202019625304, "xcomet_score": 0.7633051872253418, "xcomet_qe_score": 0.7493166923522949, "metricx_score": 2.431424140930176, "metricx_qe_score": 2.456880807876587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含九十个数据集在各种领域,五个语义分解任务,八百万表示和二十二种自然语言在十五个语言家族中。", "metrics": {"bleu_score": 20.30487935751544, "chrf_score": 19.350308413500258, "xcomet_score": 0.5650457143783569, "xcomet_qe_score": 0.7060324549674988, "metricx_score": 6.468157768249512, "metricx_qe_score": 7.452075004577637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六个用于训练和评估的设置。", "metrics": {"bleu_score": 69.57838925817522, "chrf_score": 65.78006115198012, "xcomet_score": 0.9875204563140869, "xcomet_qe_score": 0.9569668769836426, "metricx_score": 1.4359354972839355, "metricx_qe_score": 2.164602518081665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是翻译测试。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9898306131362915, "xcomet_qe_score": 0.9781848192214966, "metricx_score": 0.22308200597763062, "metricx_qe_score": 0.39765846729278564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用谷歌翻译API将源代码翻译成目标语言,然后使用单语言模型进行训练和评估。", "metrics": {"bleu_score": 76.93385157969301, "chrf_score": 70.99070967429795, "xcomet_score": 0.8716797828674316, "xcomet_qe_score": 0.9487060904502869, "metricx_score": 0.8459067940711975, "metricx_qe_score": 0.45640063285827637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语查询上训练了英语模型,在推理过程中,我们使用API将德语查询翻译成英语,然后使用训练好的模型来预测SQL。", "metrics": {"bleu_score": 68.36270196377744, "chrf_score": 63.3767087293595, "xcomet_score": 0.8523479700088501, "xcomet_qe_score": 0.7522399425506592, "metricx_score": 1.2788115739822388, "metricx_qe_score": 1.951255440711975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语言模型。", "metrics": {"bleu_score": 26.65837681702885, "chrf_score": 28.308251990150058, "xcomet_score": 0.8719860315322876, "xcomet_qe_score": 0.8610602021217346, "metricx_score": 1.5333185195922852, "metricx_qe_score": 0.9864720106124878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种设置中,源语言与目标语言相同。例如,德语到德语或英语到英语。", "metrics": {"bleu_score": 72.68097337162342, "chrf_score": 66.44665575455079, "xcomet_score": 0.9278980493545532, "xcomet_qe_score": 0.8964245319366455, "metricx_score": 0.5449566841125488, "metricx_qe_score": 0.6620081663131714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语言场景设置,通过训练单语言模型,仅使用10%的训练数据。", "metrics": {"bleu_score": 33.92232499261089, "chrf_score": 32.48474439501936, "xcomet_score": 0.7701748609542847, "xcomet_qe_score": 0.7275086045265198, "metricx_score": 2.186624765396118, "metricx_qe_score": 2.0882225036621094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它拥有多语言模型,我们为所有语言训练了一个多语言模型。", "metrics": {"bleu_score": 61.68532909474496, "chrf_score": 55.20260708711386, "xcomet_score": 0.8016377687454224, "xcomet_qe_score": 0.7774659395217896, "metricx_score": 1.8759775161743164, "metricx_qe_score": 3.0135629177093506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语、中文查询组合在一起以训练多语言模型。", "metrics": {"bleu_score": 62.34867803397634, "chrf_score": 55.522043486524474, "xcomet_score": 0.9420384168624878, "xcomet_qe_score": 0.9731050729751587, "metricx_score": 1.0517477989196777, "metricx_qe_score": 1.6966708898544312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理过程中,我们可以使用此模型。 Um to translate German queries or Chinese query or etcetera.", "metrics": {"bleu_score": 32.01038928552493, "chrf_score": 20.840369357387704, "xcomet_score": 0.7601966857910156, "xcomet_qe_score": 0.747008740901947, "metricx_score": 11.805275917053223, "metricx_qe_score": 10.233292579650879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了crosslingo, zero-shot和few-shot transfer。", "metrics": {"bleu_score": 25.77729735754487, "chrf_score": 17.145964186098677, "xcomet_score": 0.6519575715065002, "xcomet_qe_score": 0.5915883183479309, "metricx_score": 8.659616470336914, "metricx_qe_score": 7.675270080566406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一个源语言上训练,然后转移到另一个语言上。 所以", "metrics": {"bleu_score": 24.81256252587943, "chrf_score": 23.291680333977922, "xcomet_score": 0.7950730323791504, "xcomet_qe_score": 0.6778849363327026, "metricx_score": 4.5331034660339355, "metricx_qe_score": 2.731114387512207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们在英语查询或英语和德语查询的组合上进行训练,以训练一个多语言模型来预测SQL输出。", "metrics": {"bleu_score": 50.02311070642453, "chrf_score": 46.896351676780654, "xcomet_score": 0.8341855406761169, "xcomet_qe_score": 0.8354945182800293, "metricx_score": 1.4569839239120483, "metricx_qe_score": 2.4319727420806885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And we also find many interesting results. So regarding", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8054124116897583, "xcomet_qe_score": 0.8270500898361206, "metricx_score": 11.499754905700684, "metricx_qe_score": 14.752920150756836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "analysis of monolingual models, we evaluate on two groups of models. 包括编码器PDR, 其代表多语言预训练编码器与基于指针的解码器, 如XLMR加上PDR和MBERT加上PDR。", "metrics": {"bleu_score": 27.990925587001062, "chrf_score": 23.439357148687083, "xcomet_score": 0.40073928236961365, "xcomet_qe_score": 0.508415937423706, "metricx_score": 11.594991683959961, "metricx_qe_score": 11.398810386657715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了多语言预训练的编码器-解码器模型,例如mbart和mt5。", "metrics": {"bleu_score": 37.97516049626967, "chrf_score": 18.217945113702903, "xcomet_score": 0.876590371131897, "xcomet_qe_score": 0.8689093589782715, "metricx_score": 1.9206054210662842, "metricx_qe_score": 2.94724702835083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器解码器在所有九个数据集上都获得了最佳性能。", "metrics": {"bleu_score": 60.60655437708259, "chrf_score": 42.38052234799586, "xcomet_score": 0.9740065336227417, "xcomet_qe_score": 0.9651089906692505, "metricx_score": 1.8468762636184692, "metricx_qe_score": 1.5878404378890991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在MT5和示例上进行评估,呃,XLMR加上PDR在多语言设置中。", "metrics": {"bleu_score": 22.291343499214076, "chrf_score": 24.640149785541972, "xcomet_score": 0.7170337438583374, "xcomet_qe_score": 0.7217116355895996, "metricx_score": 7.2032060623168945, "metricx_qe_score": 6.671496391296387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器解码器或编码器PDR可以通过在各种语言的混合中进行训练来改进。", "metrics": {"bleu_score": 23.872441513005217, "chrf_score": 16.646052381977682, "xcomet_score": 0.5941470861434937, "xcomet_qe_score": 0.6755722761154175, "metricx_score": 2.939458131790161, "metricx_qe_score": 3.7527637481689453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,除了英语在七个数据集中性能下降,只有在三个数据集中获得了提升。", "metrics": {"bleu_score": 59.62595026063778, "chrf_score": 53.255789436457725, "xcomet_score": 0.8804898858070374, "xcomet_qe_score": 0.9478216171264648, "metricx_score": 3.019116163253784, "metricx_qe_score": 2.5818257331848145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言的诅咒。", "metrics": {"bleu_score": 35.47202175617096, "chrf_score": 30.880643040199136, "xcomet_score": 0.9179631471633911, "xcomet_qe_score": 0.8739200234413147, "metricx_score": 1.051566481590271, "metricx_qe_score": 1.5422992706298828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了交叉链接性能获取。", "metrics": {"bleu_score": 33.64932442330152, "chrf_score": 27.722000222000226, "xcomet_score": 0.6167683601379395, "xcomet_qe_score": 0.7222636938095093, "metricx_score": 5.046045780181885, "metricx_qe_score": 6.127209663391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这张图中,蓝线是跨语言场景转移,", "metrics": {"bleu_score": 47.82215756494833, "chrf_score": 38.104620973484636, "xcomet_score": 0.7882193326950073, "xcomet_qe_score": 0.7803465127944946, "metricx_score": 4.761610507965088, "metricx_qe_score": 5.545648097991943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线是跨语言零场景转移,", "metrics": {"bleu_score": 53.3167536340577, "chrf_score": 46.9546657046657, "xcomet_score": 0.8301142454147339, "xcomet_qe_score": 0.774355411529541, "metricx_score": 2.9792680740356445, "metricx_qe_score": 3.4055137634277344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿色线是单语言设置。 我们发现", "metrics": {"bleu_score": 24.601372576927535, "chrf_score": 33.50797067370884, "xcomet_score": 0.8463808298110962, "xcomet_qe_score": 0.8411831855773926, "metricx_score": 3.3300721645355225, "metricx_qe_score": 1.725311279296875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿色和橙色线,我们发现零短期设置的交叉语言转移性能差距是显著的。通过比较蓝色和橙色线,我们发现通过短期设置的转移差距迅速缩短。", "metrics": {"bleu_score": 24.642308575121653, "chrf_score": 21.414166166391325, "xcomet_score": 0.49616003036499023, "xcomet_qe_score": 0.6528874635696411, "metricx_score": 7.750197410583496, "metricx_qe_score": 6.891031265258789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器解码器超越了前面的工作,或在训练", "metrics": {"bleu_score": 3.790498156271684, "chrf_score": 3.2180816957239946, "xcomet_score": 0.5261346101760864, "xcomet_qe_score": 0.5872796773910522, "metricx_score": 7.475478172302246, "metricx_qe_score": 6.919741153717041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "英语自然语言时取得了可比的结果,显著提高了目标自然语言的性能。 我们发现多语言模型,如Codas和Blue仍然不适合跨语言语义解析任务。", "metrics": {"bleu_score": 39.066382026550045, "chrf_score": 31.045578600966284, "xcomet_score": 0.4938536286354065, "xcomet_qe_score": 0.5122856497764587, "metricx_score": 9.972417831420898, "metricx_qe_score": 10.686491012573242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们构建了一个用于多种自然语言和中微表示的跨角度语义解析的统一基准测试。", "metrics": {"bleu_score": 43.35777776983376, "chrf_score": 33.206874208660444, "xcomet_score": 0.689218282699585, "xcomet_qe_score": 0.7373402118682861, "metricx_score": 3.172741413116455, "metricx_qe_score": 2.902949810028076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种代表性的多语言模型类型进行了全面的基准测试。", "metrics": {"bleu_score": 81.75005961650186, "chrf_score": 78.7578346273806, "xcomet_score": 0.9850558042526245, "xcomet_qe_score": 0.9573332071304321, "metricx_score": 0.8774053454399109, "metricx_qe_score": 1.146959662437439, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结果显示了许多有趣的发现等", "metrics": {"bleu_score": 93.06048591020995, "chrf_score": 92.47065434565434, "xcomet_score": 0.8565675020217896, "xcomet_qe_score": 0.7905972003936768, "metricx_score": 1.7107927799224854, "metricx_qe_score": 1.3963158130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "等。", "metrics": {"bleu_score": 0.0, "chrf_score": 8.333333333333332, "xcomet_score": 0.604798436164856, "xcomet_qe_score": 0.2309737205505371, "metricx_score": 1.9000164270401, "metricx_qe_score": 3.2553701400756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收听。", "metrics": {"bleu_score": 0.0, "chrf_score": 23.772204806687565, "xcomet_score": 0.8259608745574951, "xcomet_qe_score": 0.7775843739509583, "metricx_score": 1.7169746160507202, "metricx_qe_score": 1.604272723197937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Ayed Bilal,我将给大家简要介绍一下这篇论文《Prompting Prompt from Translation: Assessing Strategies and Performance》。", "metrics": {"bleu_score": 8.95714279020345, "chrf_score": 10.891293086143834, "xcomet_score": 0.5615912675857544, "xcomet_qe_score": 0.5806509256362915, "metricx_score": 11.905709266662598, "metricx_qe_score": 10.721895217895508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与谷歌翻译的同事们共同完成的工作。 Param", "metrics": {"bleu_score": 41.859187881827765, "chrf_score": 34.9628109364013, "xcomet_score": 0.8171108961105347, "xcomet_qe_score": 0.808695912361145, "metricx_score": 2.3356099128723145, "metricx_qe_score": 0.756396472454071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是一个540亿个参数的slurm模型,去年在2022年提出。", "metrics": {"bleu_score": 11.335779893614102, "chrf_score": 21.11939961952519, "xcomet_score": 0.6016619801521301, "xcomet_qe_score": 0.5754565000534058, "metricx_score": 8.706816673278809, "metricx_qe_score": 9.87218189239502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在大量文本集上进行训练,该集包含780亿个令牌。", "metrics": {"bleu_score": 16.76784955078518, "chrf_score": 26.579994335730245, "xcomet_score": 0.7107216119766235, "xcomet_qe_score": 0.6998403072357178, "metricx_score": 5.025457382202148, "metricx_qe_score": 4.3217668533325195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时间,它在数百个NLP任务中实现了最先进的技术。", "metrics": {"bleu_score": 56.178482641357796, "chrf_score": 59.68970975170331, "xcomet_score": 0.8542522192001343, "xcomet_qe_score": 0.8299297094345093, "metricx_score": 4.144832611083984, "metricx_qe_score": 4.659797668457031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了第一项系统的语言模型提示研究,针对机器翻译。 我们评估了这些模型的", "metrics": {"bleu_score": 20.999803083678778, "chrf_score": 23.520592325216935, "xcomet_score": 0.5625845193862915, "xcomet_qe_score": 0.6807337403297424, "metricx_score": 8.049063682556152, "metricx_qe_score": 4.507843971252441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "转换能力,采用了MT社区的最佳实践。", "metrics": {"bleu_score": 27.092703884348897, "chrf_score": 26.682125755927004, "xcomet_score": 0.6009106040000916, "xcomet_qe_score": 0.5987964272499084, "metricx_score": 4.737026214599609, "metricx_qe_score": 7.431772232055664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这涉及使用最新的测试集来避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 80.38965142755983, "chrf_score": 75.81228934458457, "xcomet_score": 0.9232668280601501, "xcomet_qe_score": 0.9465032815933228, "metricx_score": 0.5763395428657532, "metricx_qe_score": 0.6200451850891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两种最先进的系统,在MT评估中表现最好的系统。", "metrics": {"bleu_score": 26.44380476018977, "chrf_score": 26.83977673037316, "xcomet_score": 0.7310591340065002, "xcomet_qe_score": 0.7882435321807861, "metricx_score": 4.3688459396362305, "metricx_qe_score": 5.434700012207031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用最先进的神经网络指标,并且还展示了基于专家的人工评估结果。", "metrics": {"bleu_score": 72.28510654654654, "chrf_score": 67.18597864871502, "xcomet_score": 0.9704185724258423, "xcomet_qe_score": 0.9546260833740234, "metricx_score": 1.94023597240448, "metricx_qe_score": 2.1554715633392334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些提示选择策略的建议。", "metrics": {"bleu_score": 70.75330011966422, "chrf_score": 64.06828873488384, "xcomet_score": 0.8876395225524902, "xcomet_qe_score": 0.8450835347175598, "metricx_score": 1.09638249874115, "metricx_qe_score": 3.2114005088806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Prompting has a significant impact on the performance of LLMs for translation. As demonstrated in a simple experiment, we used one-shot prompting and provided two different prompts for each sentence.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.2296323007955101, "xcomet_score": 0.9671921730041504, "xcomet_qe_score": 0.9820815324783325, "metricx_score": 4.448923587799072, "metricx_qe_score": 3.4880805015563965, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在一千个句子中,", "metrics": {"bleu_score": 20.612390921238426, "chrf_score": 12.522180682216554, "xcomet_score": 0.8912525177001953, "xcomet_qe_score": 0.6963303089141846, "metricx_score": 7.990420341491699, "metricx_qe_score": 10.199912071228027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "五百一十六个句子,观察到的差异超过一个模糊点。", "metrics": {"bleu_score": 4.334264033674369, "chrf_score": 5.489892869846207, "xcomet_score": 0.6302018165588379, "xcomet_qe_score": 0.22891297936439514, "metricx_score": 8.989215850830078, "metricx_qe_score": 9.07387924194336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可能高达40个点。", "metrics": {"bleu_score": 54.254864072519524, "chrf_score": 39.37328263433837, "xcomet_score": 0.8117042779922485, "xcomet_qe_score": 0.8747140765190125, "metricx_score": 3.9217209815979004, "metricx_qe_score": 4.959713935852051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择合适的提示策略至关重要。", "metrics": {"bleu_score": 34.05204944353419, "chrf_score": 28.20548732313438, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.1681901067495346, "metricx_qe_score": 0.26603934168815613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们选择了一个五次提示策略,我们只标记我们向系统提供的每个句子与其语言相同。", "metrics": {"bleu_score": 26.761552579144272, "chrf_score": 23.67454162665405, "xcomet_score": 0.6327940821647644, "xcomet_qe_score": 0.7257453203201294, "metricx_score": 5.529182434082031, "metricx_qe_score": 4.674465656280518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个示例中,我们从德语翻译成英语,德语句子用德语列出,英语翻译用英语列出。", "metrics": {"bleu_score": 34.60551257249588, "chrf_score": 25.474989325909224, "xcomet_score": 0.760263204574585, "xcomet_qe_score": 0.744333028793335, "metricx_score": 2.537686824798584, "metricx_qe_score": 2.1346144676208496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,提示的实际形式在几何提示中没有很大的影响。", "metrics": {"bleu_score": 37.38434131445883, "chrf_score": 33.1057026511938, "xcomet_score": 0.6863011121749878, "xcomet_qe_score": 0.7079296112060547, "metricx_score": 4.926631927490234, "metricx_qe_score": 4.595666408538818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零和一枪提示至关重要,", "metrics": {"bleu_score": 13.731211582014497, "chrf_score": 15.866997178597956, "xcomet_score": 0.5665252208709717, "xcomet_qe_score": 0.6604276299476624, "metricx_score": 7.9137773513793945, "metricx_qe_score": 4.647091865539551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而当我们进入五枪提示时,实际提示的形式几乎没有变化。", "metrics": {"bleu_score": 21.22363344155404, "chrf_score": 21.418270451261613, "xcomet_score": 0.791582465171814, "xcomet_qe_score": 0.7725959420204163, "metricx_score": 4.535190582275391, "metricx_qe_score": 3.8882062435150146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "It's the examples that carry most of the weight.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9880446195602417, "xcomet_qe_score": 1.0, "metricx_score": 24.19091033935547, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,示例质量比示例与源句的相似性更重要。", "metrics": {"bleu_score": 70.91299972295235, "chrf_score": 62.87940602466781, "xcomet_score": 0.9855952262878418, "xcomet_qe_score": 0.9865512847900391, "metricx_score": 0.9190213680267334, "metricx_qe_score": 0.605577826499939, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,重要的是要从高质量的翻译中选择示例。在比", "metrics": {"bleu_score": 55.31064767295669, "chrf_score": 57.80465969726739, "xcomet_score": 0.8066000938415527, "xcomet_qe_score": 0.7995310425758362, "metricx_score": 4.132717609405518, "metricx_qe_score": 4.222235679626465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "较中,我们特别从WMT评估或dev数据的训练数据中选择提示。", "metrics": {"bleu_score": 27.8493617261395, "chrf_score": 29.59980750435074, "xcomet_score": 0.5232217311859131, "xcomet_qe_score": 0.467435747385025, "metricx_score": 8.581266403198242, "metricx_qe_score": 9.647269248962402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The dev data is much more accurate and of higher quality than the train data, and the results show better", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6767891049385071, "xcomet_qe_score": 0.7289520502090454, "metricx_score": 6.126720905303955, "metricx_qe_score": 5.996267795562744, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "performance when using the dev data.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7982733845710754, "xcomet_qe_score": 0.8910514116287231, "metricx_score": 17.89350700378418, "metricx_qe_score": 21.76025390625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专门的先进系统在与PAM翻译相比具有", "metrics": {"bleu_score": 18.62613526897875, "chrf_score": 17.073953266020116, "xcomet_score": 0.5898661613464355, "xcomet_qe_score": 0.6012176275253296, "metricx_score": 9.23745346069336, "metricx_qe_score": 6.789457321166992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "显著优势,但PAM接近于我们的商业系统。", "metrics": {"bleu_score": 23.909453161355017, "chrf_score": 20.21817448758719, "xcomet_score": 0.20486734807491302, "xcomet_qe_score": 0.17427951097488403, "metricx_score": 5.983519077301025, "metricx_qe_score": 6.575605392456055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的情况下,我们选择使用谷歌翻译。", "metrics": {"bleu_score": 50.81728439699983, "chrf_score": 46.2324362904295, "xcomet_score": 0.9506507515907288, "xcomet_qe_score": 0.883453369140625, "metricx_score": 1.4380238056182861, "metricx_qe_score": 2.0394883155822754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从使用MPN框架进行的EMG分析中获得的见解是,手掌的流畅度与最先进的系统相当。但主要区别在于准确性。", "metrics": {"bleu_score": 51.450382802923045, "chrf_score": 39.625844039830774, "xcomet_score": 0.5348577499389648, "xcomet_qe_score": 0.3763338625431061, "metricx_score": 7.575169086456299, "metricx_qe_score": 7.080142021179199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7579965591430664, "xcomet_qe_score": 0.7860524654388428, "metricx_score": 1.7050689458847046, "metricx_qe_score": 0.886371910572052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,PAML 有时会选择省略源句中已翻译的部分,以产生更好的翻译。", "metrics": {"bleu_score": 13.773213136921639, "chrf_score": 15.004850461758911, "xcomet_score": 0.8972585201263428, "xcomet_qe_score": 0.8502304553985596, "metricx_score": 1.6428065299987793, "metricx_qe_score": 2.527599811553955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,风扇的样式awkward类别比最先进的系统的类别要低,这是一种额外的信号。 That param provides really fluent output, but still with some problems of accuracy", "metrics": {"bleu_score": 16.983023043589064, "chrf_score": 11.994928078531895, "xcomet_score": 0.4062102138996124, "xcomet_qe_score": 0.4448065459728241, "metricx_score": 14.566457748413086, "metricx_qe_score": 13.730998992919922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". And that's it for this really short overview", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9878650903701782, "xcomet_qe_score": 1.0, "metricx_score": 11.077631950378418, "metricx_qe_score": 19.120107650756836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". For more details, please come to the full presentation of the paper. Thank you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8571947813034058, "xcomet_qe_score": 0.9227316379547119, "metricx_score": 8.258307456970215, "metricx_qe_score": 16.416574478149414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "very much.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8842521905899048, "xcomet_qe_score": 0.8700260519981384, "metricx_score": 4.100488662719727, "metricx_qe_score": 7.549065113067627, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是大伟,德国萨尔兰大学的博士生。", "metrics": {"bleu_score": 53.439883964839176, "chrf_score": 44.08684977762537, "xcomet_score": 0.8428959846496582, "xcomet_qe_score": 0.8540732860565186, "metricx_score": 1.3973840475082397, "metricx_qe_score": 0.8458198308944702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本视频中,我想介绍一下我们最近的工作 weaker than you think:对每周监督学习的批判性研究。", "metrics": {"bleu_score": 42.60457711316427, "chrf_score": 36.20704103802984, "xcomet_score": 0.7952011823654175, "xcomet_qe_score": 0.8345882296562195, "metricx_score": 9.640670776367188, "metricx_qe_score": 10.322746276855469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与肖玉申、马约斯·穆斯巴、格雷厄斯·斯蒂芬和迪特里希·克拉科夫的联合工作。", "metrics": {"bleu_score": 2.8209110765918006, "chrf_score": 3.0299203472840315, "xcomet_score": 0.6776028871536255, "xcomet_qe_score": 0.663535475730896, "metricx_score": 2.1952123641967773, "metricx_qe_score": 2.5055530071258545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想从一个简短的介绍开始, weak supervision 和 Weakly Supervised Learning。", "metrics": {"bleu_score": 5.039518688486958, "chrf_score": 4.657172044949222, "xcomet_score": 0.7762298583984375, "xcomet_qe_score": 0.7944298386573792, "metricx_score": 9.240885734558105, "metricx_qe_score": 9.644615173339844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不手动标记数据。", "metrics": {"bleu_score": 57.754002174538684, "chrf_score": 47.73436424977258, "xcomet_score": 0.9137423634529114, "xcomet_qe_score": 0.8668755292892456, "metricx_score": 0.7964283227920532, "metricx_qe_score": 1.674791932106018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标记源标记数据,例如简单的启发式规则、知识库或低质量的外包,如右图所示。", "metrics": {"bleu_score": 69.02502506252266, "chrf_score": 62.036264000863675, "xcomet_score": 0.7718386650085449, "xcomet_qe_score": 0.6974492073059082, "metricx_score": 3.1926894187927246, "metricx_qe_score": 4.5996503829956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人类注释相比,弱注释要便宜得多,但它们也很嘈杂,这意味着一定数量的注释是错误的。", "metrics": {"bleu_score": 43.756886567968365, "chrf_score": 38.167892147887194, "xcomet_score": 0.7829005122184753, "xcomet_qe_score": 0.7757684588432312, "metricx_score": 2.174128770828247, "metricx_qe_score": 3.0434324741363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在每周标记的数据上训练神经网络,神经网络往往会记住标签噪声而不会泛化。", "metrics": {"bleu_score": 49.05354699063606, "chrf_score": 41.98315585708326, "xcomet_score": 0.8523213863372803, "xcomet_qe_score": 0.7084562182426453, "metricx_score": 4.895806789398193, "metricx_qe_score": 5.9276299476623535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出的训练算法旨在在此类标签噪声下健壮地训练神经网络,以便训练后的模型仍能良好地泛化。", "metrics": {"bleu_score": 43.56558057873841, "chrf_score": 37.23082190726506, "xcomet_score": 0.8518044948577881, "xcomet_qe_score": 0.7745873928070068, "metricx_score": 2.198086738586426, "metricx_qe_score": 3.03808856010437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的WSL工作中,WSL代表每周监督学习。一个常见的说法是,人们说他们只在每周的标签数据上训练模型,并在干净的测试集上实现高性能。", "metrics": {"bleu_score": 38.43574623917685, "chrf_score": 37.37514792749839, "xcomet_score": 0.6784238815307617, "xcomet_qe_score": 0.7228046655654907, "metricx_score": 7.601197719573975, "metricx_qe_score": 8.486940383911133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法不错,但有个小细节。 Which is that people do assume that there is an additional clean validation set available for model", "metrics": {"bleu_score": 9.65478424694983, "chrf_score": 7.353444241616273, "xcomet_score": 0.8546658754348755, "xcomet_qe_score": 0.8628101944923401, "metricx_score": 14.4349365234375, "metricx_qe_score": 10.673979759216309, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "selection. We cast doubt on this problem setting, as this implies that additional manual annotations are required in weakly supervised", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6245253086090088, "xcomet_qe_score": 0.7336617708206177, "metricx_score": 17.81476402282715, "metricx_qe_score": 15.300372123718262, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "learning. But like an elephant in the room, this necessity is often overlooked.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8856359720230103, "xcomet_qe_score": 0.8965041637420654, "metricx_score": 12.638288497924805, "metricx_qe_score": 13.322998046875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前面提到的疑问促使我们提出三个研究问题。", "metrics": {"bleu_score": 43.48553979929486, "chrf_score": 47.70310757825811, "xcomet_score": 0.955795407295227, "xcomet_qe_score": 0.9569442272186279, "metricx_score": 1.0552736520767212, "metricx_qe_score": 0.9247132539749146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,清洁验证数据是否对于WSL来说是必要的,还是我们可以使用噪声验证集呢?", "metrics": {"bleu_score": 26.257384738901404, "chrf_score": 25.242847858692812, "xcomet_score": 0.8410652279853821, "xcomet_qe_score": 0.7737022042274475, "metricx_score": 2.509584426879883, "metricx_qe_score": 3.8901894092559814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need?", "metrics": {"bleu_score": 0.727641442057397, "chrf_score": 1.7692071203221922, "xcomet_score": 0.993789792060852, "xcomet_qe_score": 1.0, "metricx_score": 24.826473236083984, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Finally, should we only use the clean samples for validation, or are there better ways to utilize them?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9943621158599854, "xcomet_qe_score": 0.9969825744628906, "metricx_score": 20.240739822387695, "metricx_qe_score": 23.06912612915039, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的发现如下。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 49.41553937149765, "xcomet_score": 0.9748376607894897, "xcomet_qe_score": 0.948908805847168, "metricx_score": 1.6144813299179077, "metricx_qe_score": 2.59200382232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现有趣的是,最近的WSL方法确实需要清洁的白色盘子样本才能正常工作。", "metrics": {"bleu_score": 56.465533142896966, "chrf_score": 54.028979481810445, "xcomet_score": 0.7338396906852722, "xcomet_qe_score": 0.7503361105918884, "metricx_score": 4.338949680328369, "metricx_qe_score": 5.217491626739502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,如图所示,性能会大幅下降。", "metrics": {"bleu_score": 48.415247130345996, "chrf_score": 60.8168604828872, "xcomet_score": 0.992935299873352, "xcomet_qe_score": 0.9890308380126953, "metricx_score": 0.5556566715240479, "metricx_qe_score": 0.6677849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果没有清洁验证样本,则训练模型无法超越原始弱标签。 Meaning that the training is pointless.", "metrics": {"bleu_score": 20.949199681768913, "chrf_score": 17.703041842117155, "xcomet_score": 0.7830908894538879, "xcomet_qe_score": 0.7473147511482239, "metricx_score": 9.828036308288574, "metricx_qe_score": 8.755060195922852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要清晰标记的数据才能正常工作。并且,获取清晰验证样本的注释成本不应被忽视。", "metrics": {"bleu_score": 58.66169892178668, "chrf_score": 56.22149288961339, "xcomet_score": 0.8704605102539062, "xcomet_qe_score": 0.8126242160797119, "metricx_score": 2.1332967281341553, "metricx_qe_score": 2.9268202781677246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加清洁验证样本的数量将有助于WSL方法实现更好的性能,如图所示。", "metrics": {"bleu_score": 65.0506004906567, "chrf_score": 61.25030211097086, "xcomet_score": 0.9058387279510498, "xcomet_qe_score": 0.9677461385726929, "metricx_score": 3.6369073390960693, "metricx_qe_score": 4.297624111175537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常我们只需要每类20个样本就能达到高性能。 但是故事", "metrics": {"bleu_score": 16.331948281960493, "chrf_score": 18.580060930582125, "xcomet_score": 0.702491819858551, "xcomet_qe_score": 0.7470608949661255, "metricx_score": 6.950325012207031, "metricx_qe_score": 5.063176155090332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并没有结束,因为无论如何,如果我们决定直接使用干净的样本进行训练,那么直接训练它们将实现更好的性能。", "metrics": {"bleu_score": 28.11703775467002, "chrf_score": 22.512198077186035, "xcomet_score": 0.8199988603591919, "xcomet_qe_score": 0.7280791997909546, "metricx_score": 4.830815315246582, "metricx_qe_score": 4.88756799697876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "右图显示了在干净数据上直接应用的微调方法和使用干净数据进行验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 67.97415713535712, "chrf_score": 60.02037704108039, "xcomet_score": 0.9280966520309448, "xcomet_qe_score": 0.884178638458252, "metricx_score": 2.24568772315979, "metricx_qe_score": 2.712172746658325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如我们所见,如果我们每个类有十个样本,则直接微调开始击败WSL方法。", "metrics": {"bleu_score": 30.10726712717024, "chrf_score": 27.90161780494249, "xcomet_score": 0.9291322231292725, "xcomet_qe_score": 0.8727349042892456, "metricx_score": 2.5616848468780518, "metricx_qe_score": 3.2214808464050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前WSL方法中声称的性能改进可以通过允许在清洁验证样本上继续微调来轻松实现。", "metrics": {"bleu_score": 39.08146646852354, "chrf_score": 36.01134454356425, "xcomet_score": 0.8717333078384399, "xcomet_qe_score": 0.8857265710830688, "metricx_score": 2.4338796138763428, "metricx_qe_score": 3.8334736824035645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,瓦利纳模型,简称FTW,最初表现不佳,低于更复杂的WSL方法,如cosine。", "metrics": {"bleu_score": 23.852289784239385, "chrf_score": 21.128091264651793, "xcomet_score": 0.7107498049736023, "xcomet_qe_score": 0.7500923275947571, "metricx_score": 4.308441162109375, "metricx_qe_score": 5.467822551727295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果我们允许在干净的样本上继续微调,那么FTW的表现与其他方法相当。", "metrics": {"bleu_score": 39.02960535925472, "chrf_score": 34.27044519450692, "xcomet_score": 0.9350529909133911, "xcomet_qe_score": 0.8591866493225098, "metricx_score": 1.5679844617843628, "metricx_qe_score": 2.425058364868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的WSL方法,因为它们需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 57.473860166742696, "chrf_score": 55.57193860381212, "xcomet_score": 0.9809115529060364, "xcomet_qe_score": 0.9843068718910217, "metricx_score": 0.696503221988678, "metricx_qe_score": 1.4214279651641846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们表明,最近的WSL方法需要干净、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 60.316120362180044, "chrf_score": 57.11316905018007, "xcomet_score": 0.8203024864196777, "xcomet_qe_score": 0.9073742032051086, "metricx_score": 2.555450916290283, "metricx_qe_score": 3.615776777267456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Our concrete recommendations for future work are as follows.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9798088073730469, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 24.528352737426758, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先报告模型选择标准。", "metrics": {"bleu_score": 77.72460244048297, "chrf_score": 73.1871936155306, "xcomet_score": 0.981153130531311, "xcomet_qe_score": 0.9121740460395813, "metricx_score": 0.2592116892337799, "metricx_qe_score": 0.45244041085243225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否在干净的验证样本上进行。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.34004037322114, "xcomet_score": 0.9702857732772827, "xcomet_qe_score": 0.9098553657531738, "metricx_score": 1.5602195262908936, "metricx_qe_score": 2.490739107131958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二,WSSL方法应与基于短期学习的基线进行比较,因为它们都在清晰的样本上工作。", "metrics": {"bleu_score": 18.96843682851339, "chrf_score": 21.45536745462581, "xcomet_score": 0.6648104190826416, "xcomet_qe_score": 0.7294262647628784, "metricx_score": 5.333241939544678, "metricx_qe_score": 4.946929931640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一个简单而强大的基线,应在未来的WSSL工作中考虑。", "metrics": {"bleu_score": 49.93470615733697, "chrf_score": 45.5945206694807, "xcomet_score": 0.793035626411438, "xcomet_qe_score": 0.7165145874023438, "metricx_score": 3.900219440460205, "metricx_qe_score": 4.146284103393555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们已经开源了我们的代码。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9967988729476929, "xcomet_qe_score": 0.9351927638053894, "metricx_score": 0.4914062023162842, "metricx_qe_score": 0.7212974429130554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过本幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9957367181777954, "xcomet_qe_score": 0.9900866746902466, "metricx_score": 0.461531400680542, "metricx_qe_score": 0.4078834354877472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎您查看。", "metrics": {"bleu_score": 32.159351091190125, "chrf_score": 24.34979544126242, "xcomet_score": 0.9901045560836792, "xcomet_qe_score": 0.9442003965377808, "metricx_score": 0.30528131127357483, "metricx_qe_score": 0.44398033618927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家,祝大家愉快地参加会议。", "metrics": {"bleu_score": 19.56475149792291, "chrf_score": 19.54183783914557, "xcomet_score": 0.9531459212303162, "xcomet_qe_score": 0.9492911100387573, "metricx_score": 0.3610970377922058, "metricx_qe_score": 0.3265659511089325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是James Finch。", "metrics": {"bleu_score": 47.750342648354646, "chrf_score": 74.03089820033122, "xcomet_score": 0.9441262483596802, "xcomet_qe_score": 0.93831866979599, "metricx_score": 0.8310850262641907, "metricx_qe_score": 0.6927031874656677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还有我,Sarah Finch。", "metrics": {"bleu_score": 26.269098944241588, "chrf_score": 78.67103664918446, "xcomet_score": 0.897266149520874, "xcomet_qe_score": 0.8951162695884705, "metricx_score": 0.7161130905151367, "metricx_qe_score": 0.9742741584777832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天,我们将向您介绍ABC eval,一个新的维度方法来评估对话型人工智能。", "metrics": {"bleu_score": 23.971956658050726, "chrf_score": 24.84599679914228, "xcomet_score": 0.8829344511032104, "xcomet_qe_score": 0.942674458026886, "metricx_score": 2.4472286701202393, "metricx_qe_score": 2.6355082988739014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是由埃默里NLP实验室完成的,由埃默里大学的吉诺·乔伊教授领导,并与亚马逊AlexaAI合作。", "metrics": {"bleu_score": 38.07447950090451, "chrf_score": 40.69436450074042, "xcomet_score": 0.7446352243423462, "xcomet_qe_score": 0.8047797679901123, "metricx_score": 3.532069444656372, "metricx_qe_score": 2.9269092082977295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But let's say that you just developed a dialogue model, and you want to see how well it compares against the current state of the art.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 9.988165855407715, "metricx_qe_score": 16.464933395385742, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常规做法是使用人类评估,例如让人类评委选择两段对话中哪一段更好,或者根据利卡德评分给出对话的评分。", "metrics": {"bleu_score": 30.76737326375051, "chrf_score": 25.517942318695972, "xcomet_score": 0.9757879972457886, "xcomet_qe_score": 0.9792304039001465, "metricx_score": 1.3048070669174194, "metricx_qe_score": 0.948296844959259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法很好地提供了对整体对话质量的整体评估,但对话质量有许多方面。", "metrics": {"bleu_score": 57.44179739510638, "chrf_score": 47.004611133386966, "xcomet_score": 0.911270260810852, "xcomet_qe_score": 0.8922576308250427, "metricx_score": 0.5891726016998291, "metricx_qe_score": 0.7186087369918823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以更好地了解模型的强项和弱点。", "metrics": {"bleu_score": 47.57013872417043, "chrf_score": 41.75071311515809, "xcomet_score": 0.9837597608566284, "xcomet_qe_score": 0.9693034887313843, "metricx_score": 0.6328234672546387, "metricx_qe_score": 0.737877368927002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人类评委评估对话的几个维度的质量,例如使用现有的比较或利卡特斯克尔方法评估模型响应的相关性。", "metrics": {"bleu_score": 55.52324225915537, "chrf_score": 47.315478317328115, "xcomet_score": 0.8631644248962402, "xcomet_qe_score": 0.8383145332336426, "metricx_score": 3.049297571182251, "metricx_qe_score": 2.393409252166748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,我们认为对于维度对话评估有一个更精确和可靠的策略。", "metrics": {"bleu_score": 28.96326108724397, "chrf_score": 29.505384546751433, "xcomet_score": 0.8906769156455994, "xcomet_qe_score": 0.8666344881057739, "metricx_score": 2.907824993133545, "metricx_qe_score": 3.496284008026123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9720144271850586, "xcomet_qe_score": 0.9979841709136963, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为注释聊天行为,简称ABC eval。", "metrics": {"bleu_score": 42.19025483356926, "chrf_score": 41.350825096615864, "xcomet_score": 0.8401734828948975, "xcomet_qe_score": 0.7911096811294556, "metricx_score": 2.8775203227996826, "metricx_qe_score": 2.7193713188171387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法来全面涵盖聊天模型行为,这些行为在最近的文献中被认为会影响聊天质量。", "metrics": {"bleu_score": 44.629576342057646, "chrf_score": 38.49802532056965, "xcomet_score": 0.956629753112793, "xcomet_qe_score": 0.9105690121650696, "metricx_score": 3.4919114112854004, "metricx_qe_score": 4.269184589385986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC eval is capable of measuring the rates at which chat models will commit various thematic errors.", "metrics": {"bleu_score": 0.0, "chrf_score": 5.192722501897555, "xcomet_score": 0.9637766480445862, "xcomet_qe_score": 0.9728657007217407, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABCeval测量了聊天模型在多少次转弯中忽略了对方或说了无关的话。 自相矛盾或其伙伴,产生错误事实或违反常识知识,以及模型在成功或失败显示同理心时。", "metrics": {"bleu_score": 25.404761227463332, "chrf_score": 22.682215835203966, "xcomet_score": 0.37626713514328003, "xcomet_qe_score": 0.3895055949687958, "metricx_score": 7.700981616973877, "metricx_qe_score": 7.9431843757629395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC Eval. For", "metrics": {"bleu_score": 0.596296313512391, "chrf_score": 4.864272301254646, "xcomet_score": 0.7690761089324951, "xcomet_qe_score": 0.7901648283004761, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "comparison, we also evaluated these conversations using three existing methods: LIWC ratings on the turn level, LIWC ratings on the dialogue level, and dialogue level pairwise comparisons.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6677721738815308, "xcomet_qe_score": 0.6907577514648438, "metricx_score": 17.846092224121094, "metricx_qe_score": 16.984708786010742, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有的方法,我们收集了对八个最常测量对话方面的评估的评价,因为这是对多维度的聊天模型进行评估的标准做法。", "metrics": {"bleu_score": 46.49284712477285, "chrf_score": 37.232860460420355, "xcomet_score": 0.8514323830604553, "xcomet_qe_score": 0.7834961414337158, "metricx_score": 5.714414596557617, "metricx_qe_score": 6.305120468139648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们的分析中,我们发现ABC评估行为标签总体上比现有方法收集的标签更可靠。", "metrics": {"bleu_score": 26.811030465527327, "chrf_score": 24.058093908585715, "xcomet_score": 0.7353644371032715, "xcomet_qe_score": 0.7457846999168396, "metricx_score": 7.432831764221191, "metricx_qe_score": 8.084826469421387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对100对双标记对话的内部评估者的协议来衡量。 Furthermore, ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis.", "metrics": {"bleu_score": 1.9422053745379917, "chrf_score": 4.344346676136555, "xcomet_score": 0.12970854341983795, "xcomet_qe_score": 0.15557871758937836, "metricx_score": 19.945650100708008, "metricx_qe_score": 18.97848129272461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,你可以看到如何测量自我和配偶的矛盾比例解释了5%和10%的对话质量,而平均的利克特一致性得分只解释了4%或更少。", "metrics": {"bleu_score": 50.82110682646049, "chrf_score": 45.65768119024288, "xcomet_score": 0.6910824775695801, "xcomet_qe_score": 0.7512338161468506, "metricx_score": 6.72402286529541, "metricx_qe_score": 5.792967796325684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查了每个评估指标是否捕获了聊天质量的独特方面。", "metrics": {"bleu_score": 66.21309885156847, "chrf_score": 58.65028798852329, "xcomet_score": 0.8257834911346436, "xcomet_qe_score": 0.8269343376159668, "metricx_score": 1.7183259725570679, "metricx_qe_score": 2.042111873626709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以看到所有的abc eval指标的组合如何解释超过百分之二十五的对话质量。随着你逐一删除指标,最初的指标大多数情况下都会导致对质量的了解减少。", "metrics": {"bleu_score": 21.141525370254193, "chrf_score": 23.560979110235454, "xcomet_score": 0.6241480112075806, "xcomet_qe_score": 0.6451190710067749, "metricx_score": 5.8922600746154785, "metricx_qe_score": 5.683981895446777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "On the other hand, the combination of all turn-level Lickert metrics explains far less of the quality, and fewer of these metrics carry unique information. 这些可靠、信息", "metrics": {"bleu_score": 1.4989298801252942, "chrf_score": 0.7708613650228049, "xcomet_score": 0.5186870098114014, "xcomet_qe_score": 0.576317310333252, "metricx_score": 24.677270889282227, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "丰富和独特的ABC评估指标使我们能够用更高的分辨率来评估对话型人工智能,而以前的方法无法实现。 你可以", "metrics": {"bleu_score": 5.323294052356884, "chrf_score": 9.224246033011847, "xcomet_score": 0.2516070604324341, "xcomet_qe_score": 0.589569091796875, "metricx_score": 7.706070423126221, "metricx_qe_score": 3.156353712081909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "看到我们的实验结果中,仍然存在一些挑战,并且已经精确量化了。", "metrics": {"bleu_score": 44.76006614955163, "chrf_score": 40.54486121347039, "xcomet_score": 0.8335146903991699, "xcomet_qe_score": 0.7743427157402039, "metricx_score": 1.9311590194702148, "metricx_qe_score": 2.4243276119232178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人在大约百分之二十的响应中存在常识性违规行为。", "metrics": {"bleu_score": 35.956724221042855, "chrf_score": 38.73106183775961, "xcomet_score": 0.9177486896514893, "xcomet_qe_score": 0.9008749723434448, "metricx_score": 1.811280369758606, "metricx_qe_score": 2.9073383808135986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "他们在大约15%的回答中产生无关的信息,并且在大约10%的时间内与自己或伴侣相矛盾。", "metrics": {"bleu_score": 35.18171258077387, "chrf_score": 34.052964489797866, "xcomet_score": 0.6384094953536987, "xcomet_qe_score": 0.7430480718612671, "metricx_score": 3.9990785121917725, "metricx_qe_score": 3.3576741218566895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速发展,许多这些错误率可能会在新模型发布时有所下降。自从我们进行评估以来。", "metrics": {"bleu_score": 55.58600869437169, "chrf_score": 50.81747359260219, "xcomet_score": 0.7479289770126343, "xcomet_qe_score": 0.7526258230209351, "metricx_score": 5.108592510223389, "metricx_qe_score": 4.735279083251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更加强调了对模型进行可靠和精确评估指标的追求。", "metrics": {"bleu_score": 14.317005342292276, "chrf_score": 16.037999879385293, "xcomet_score": 0.8723195791244507, "xcomet_qe_score": 0.8197280168533325, "metricx_score": 2.6355435848236084, "metricx_qe_score": 3.0233898162841797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABC eval可以被行业内的其他人利用为朝着这个方向迈出有意义的一步。", "metrics": {"bleu_score": 48.24203154874653, "chrf_score": 43.224494878686606, "xcomet_score": 0.9150762557983398, "xcomet_qe_score": 0.915202796459198, "metricx_score": 3.9821395874023438, "metricx_qe_score": 3.9246835708618164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待着看到在未来几个月和几年中,如何发展对话式人工智能。", "metrics": {"bleu_score": 44.24252176035718, "chrf_score": 44.71817921235871, "xcomet_score": 0.8332764506340027, "xcomet_qe_score": 0.9029725193977356, "metricx_score": 1.557121992111206, "metricx_qe_score": 1.596601963043213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的收看。", "metrics": {"bleu_score": 18.575057999133602, "chrf_score": 23.910939012584706, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.5197558403015137, "metricx_qe_score": 0.3212273120880127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Kaiyuan,我将介绍我们的工作,标题为“当翻译需要上下文:", "metrics": {"bleu_score": 33.14824344065458, "chrf_score": 26.339550526530576, "xcomet_score": 0.6925194263458252, "xcomet_qe_score": 0.7720816135406494, "metricx_score": 4.260782718658447, "metricx_qe_score": 3.425016403198242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多语言探索的一个数据驱动的研究”。", "metrics": {"bleu_score": 43.85068972747104, "chrf_score": 44.690893727525285, "xcomet_score": 0.8379812240600586, "xcomet_qe_score": 0.8082740306854248, "metricx_score": 3.5210084915161133, "metricx_qe_score": 3.6424400806427, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernhout、Emil Liu、Andre F. DeMartins和Graham Neubig合作完成的。 因此", "metrics": {"bleu_score": 36.121980074516784, "chrf_score": 62.525276944586636, "xcomet_score": 0.42255517840385437, "xcomet_qe_score": 0.46502116322517395, "metricx_score": 7.336751937866211, "metricx_qe_score": 5.037747859954834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",很多翻译取决于上下文。", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 97.61490149794774, "xcomet_score": 0.9973210096359253, "xcomet_qe_score": 0.9844769239425659, "metricx_score": 0.8693109154701233, "metricx_qe_score": 1.0262653827667236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译这句话中的\"mole\"?", "metrics": {"bleu_score": 62.33473783356552, "chrf_score": 56.25393139712025, "xcomet_score": 0.9813164472579956, "xcomet_qe_score": 0.9626121520996094, "metricx_score": 1.0082902908325195, "metricx_qe_score": 2.4161720275878906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句话是“如果部长们发现了这件事,事情可能会变得危险”,那么“mo”指的是间谍。", "metrics": {"bleu_score": 21.055668975171354, "chrf_score": 11.1696806557239, "xcomet_score": 0.897847056388855, "xcomet_qe_score": 0.8790764212608337, "metricx_score": 5.0633440017700195, "metricx_qe_score": 5.60291051864624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句话是“这会是严重的事情吗,医生?”那么“mo”指的是出生标记。", "metrics": {"bleu_score": 21.91877099592121, "chrf_score": 14.35544389155791, "xcomet_score": 0.8252692818641663, "xcomet_qe_score": 0.7578710317611694, "metricx_score": 5.916959285736084, "metricx_qe_score": 5.255375862121582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,取决于上下文,单词的含义会发生变化,因此其翻译也会发生变化。", "metrics": {"bleu_score": 57.15928020386922, "chrf_score": 51.3198913229352, "xcomet_score": 0.9650788307189941, "xcomet_qe_score": 0.9088187217712402, "metricx_score": 0.36222922801971436, "metricx_qe_score": 0.3833325505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型如何处理这样的案例是相当困难的。", "metrics": {"bleu_score": 22.177784311701792, "chrf_score": 18.244193569900546, "xcomet_score": 0.9658911228179932, "xcomet_qe_score": 0.9667154550552368, "metricx_score": 0.910275936126709, "metricx_qe_score": 0.8722478151321411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有一小部分翻译依赖于上下文,这使得像BLEU这样的学术级别指标无法捕捉这些翻译。 一", "metrics": {"bleu_score": 52.94456952296222, "chrf_score": 50.106996344098164, "xcomet_score": 0.8722983598709106, "xcomet_qe_score": 0.8497207164764404, "metricx_score": 3.69449782371521, "metricx_qe_score": 3.832709789276123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些人建议针对上下文依赖翻译进行评估,但这些资源仅支持有限类型的上下文依赖翻译和有限语言集,因为它们通常依赖于领域知识和人类编辑。", "metrics": {"bleu_score": 62.13870370261901, "chrf_score": 55.97864809479306, "xcomet_score": 0.6867833137512207, "xcomet_qe_score": 0.6479467153549194, "metricx_score": 3.9740817546844482, "metricx_qe_score": 3.685877799987793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译需要什么样的上下文?", "metrics": {"bleu_score": 16.66631347767995, "chrf_score": 19.567885715712126, "xcomet_score": 0.943010687828064, "xcomet_qe_score": 0.9877928495407104, "metricx_score": 0.6462308764457703, "metricx_qe_score": 0.5078481435775757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在处理这些情况时表现如何?", "metrics": {"bleu_score": 65.26460174517786, "chrf_score": 57.51191458506912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.39041927456855774, "metricx_qe_score": 0.4628429412841797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了在翻译过程中,多少词语依赖于上下文。", "metrics": {"bleu_score": 49.97770345042089, "chrf_score": 41.957513943994016, "xcomet_score": 0.934314489364624, "xcomet_qe_score": 0.9311246275901794, "metricx_score": 5.5265703201293945, "metricx_qe_score": 5.459385871887207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的工作中,我们将cXM介绍为机器翻译模型中上下文使用的度量标准,", "metrics": {"bleu_score": 39.20558089326692, "chrf_score": 30.96559032220861, "xcomet_score": 0.7862944602966309, "xcomet_qe_score": 0.7830448150634766, "metricx_score": 4.329206466674805, "metricx_qe_score": 3.492727518081665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并通过测量上下文C在给定源X的情况下为目标Y提供的信息量来完成。 你可以把CXMI看作是从给模型提供上下文中获得的信息。", "metrics": {"bleu_score": 44.59345123175423, "chrf_score": 38.51516616338439, "xcomet_score": 0.7795135378837585, "xcomet_qe_score": 0.6969294548034668, "metricx_score": 3.3965229988098145, "metricx_qe_score": 3.739706516265869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将CXMII扩展为点点CXMII,它可以在句子级别或单词级别测量上下文使用。", "metrics": {"bleu_score": 32.80914605830365, "chrf_score": 31.653664893389035, "xcomet_score": 0.7287341952323914, "xcomet_qe_score": 0.6868870854377747, "metricx_score": 5.674238681793213, "metricx_qe_score": 6.005267143249512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将具有高PCXMII的单词视为需要上下文翻译的单词。", "metrics": {"bleu_score": 50.406735961002234, "chrf_score": 42.131709007901215, "xcomet_score": 0.9555536508560181, "xcomet_qe_score": 0.8939625024795532, "metricx_score": 3.0355911254882812, "metricx_qe_score": 3.6565918922424316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析高词汇量的单词,以寻找这些单词之间的模式。", "metrics": {"bleu_score": 20.876149875884902, "chrf_score": 19.202403664787678, "xcomet_score": 0.8509562015533447, "xcomet_qe_score": 0.8209083080291748, "metricx_score": 4.943953514099121, "metricx_qe_score": 5.5166707038879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对TED演讲的转录进行分析,这些演讲已从英语翻译成14种不同的语言。", "metrics": {"bleu_score": 48.11858826021021, "chrf_score": 52.73994903437328, "xcomet_score": 0.9210647344589233, "xcomet_qe_score": 0.9718855619430542, "metricx_score": 1.225985050201416, "metricx_qe_score": 1.1892231702804565, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对分析进行三级别的分析。", "metrics": {"bleu_score": 14.342165432034776, "chrf_score": 15.407177442130937, "xcomet_score": 0.8427603244781494, "xcomet_qe_score": 0.8298853635787964, "metricx_score": 2.6941683292388916, "metricx_qe_score": 2.446057081222534, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有高平均pcxmi的部分语音标记。", "metrics": {"bleu_score": 7.320439219592829, "chrf_score": 9.730930820169748, "xcomet_score": 0.794491171836853, "xcomet_qe_score": 0.7693885564804077, "metricx_score": 3.6434381008148193, "metricx_qe_score": 3.9460911750793457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到例如,阿拉伯语中的双重代词,它们的PSXMI值相当高。", "metrics": {"bleu_score": 41.18889743336424, "chrf_score": 39.197931316499016, "xcomet_score": 0.7028852105140686, "xcomet_qe_score": 0.8244370818138123, "metricx_score": 4.829478740692139, "metricx_qe_score": 4.27124547958374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语没有双重代词,所以在翻译成阿拉伯语时需要上下文来确定代词是否是双重的。", "metrics": {"bleu_score": 53.938538234033196, "chrf_score": 47.27195996139771, "xcomet_score": 0.8033150434494019, "xcomet_qe_score": 0.9809485673904419, "metricx_score": 2.9131641387939453, "metricx_qe_score": 2.504316806793213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 88.4112136328919, "chrf_score": 89.69824812440879, "xcomet_score": 0.9977834224700928, "xcomet_qe_score": 0.996990442276001, "metricx_score": 0.5566547513008118, "metricx_qe_score": 0.8022129535675049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们查看了词汇项的词汇量,平均值在其所有不同出现次数上都很高。", "metrics": {"bleu_score": 15.356865413172349, "chrf_score": 15.250393346913787, "xcomet_score": 0.578711211681366, "xcomet_qe_score": 0.6773996949195862, "metricx_score": 8.024380683898926, "metricx_qe_score": 8.035542488098145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于识别像这里的这种情况,在中文中,您需要上下文来翻译名词,以确保在文档中使用相同的翻译。", "metrics": {"bleu_score": 26.84504300416788, "chrf_score": 24.205977643641987, "xcomet_score": 0.6902912259101868, "xcomet_qe_score": 0.7266333103179932, "metricx_score": 1.098267912864685, "metricx_qe_score": 1.342085361480713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现上下文支持在正确的形式中传达。", "metrics": {"bleu_score": 36.160012662152226, "chrf_score": 31.317171340274154, "xcomet_score": 0.8441929817199707, "xcomet_qe_score": 0.8250779509544373, "metricx_score": 3.3544058799743652, "metricx_qe_score": 3.702939510345459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们研究了具有高P/6MI的不同个别标记,", "metrics": {"bleu_score": 24.87344417023397, "chrf_score": 21.664706066879983, "xcomet_score": 0.7407724857330322, "xcomet_qe_score": 0.669503927230835, "metricx_score": 7.044564247131348, "metricx_qe_score": 5.916383266448975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出无法通过单词本身捕捉的现象,但在句子结构中表达出来的现象,例如省略号解析。", "metrics": {"bleu_score": 43.3835773713731, "chrf_score": 37.608107338399336, "xcomet_score": 0.8598129749298096, "xcomet_qe_score": 0.8415660858154297, "metricx_score": 1.1940724849700928, "metricx_qe_score": 1.5999019145965576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用我们分析的结果来设计文档本地翻译的基准。", "metrics": {"bleu_score": 49.47834226950782, "chrf_score": 40.44914784772138, "xcomet_score": 0.9156392812728882, "xcomet_qe_score": 0.9703298807144165, "metricx_score": 4.186964511871338, "metricx_qe_score": 3.8185179233551025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们确定的五种话语现象中的每一种,我们创建了标记器来自动识别与现象相关的单词。", "metrics": {"bleu_score": 77.14872863307717, "chrf_score": 72.2822025197169, "xcomet_score": 0.9343894720077515, "xcomet_qe_score": 0.9797724485397339, "metricx_score": 1.2328072786331177, "metricx_qe_score": 1.2768758535385132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称我们的标记器为多语言话语意识或MUDa标记器。", "metrics": {"bleu_score": 30.4041802436854, "chrf_score": 24.280761409417753, "xcomet_score": 0.8043383955955505, "xcomet_qe_score": 0.7323024868965149, "metricx_score": 2.3867592811584473, "metricx_qe_score": 2.866962194442749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同的语言具有不同的比例的这些话语现象。", "metrics": {"bleu_score": 32.637675299427876, "chrf_score": 29.248853849428563, "xcomet_score": 0.8767188787460327, "xcomet_qe_score": 0.8759435415267944, "metricx_score": 2.486025333404541, "metricx_qe_score": 2.205418825149536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们使用MUTT标记器,通过在我们想要用于评估的平行语料库上应用标记器。我们在MUTT标记器识别的上下文相关示例上应用所选的转换指标。", "metrics": {"bleu_score": 42.97511489403078, "chrf_score": 38.994518138943604, "xcomet_score": 0.650356650352478, "xcomet_qe_score": 0.6498867869377136, "metricx_score": 4.651830673217773, "metricx_qe_score": 4.605548858642578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同的模型在文档级机器翻译上的表现。", "metrics": {"bleu_score": 69.4716978088578, "chrf_score": 70.40820215523681, "xcomet_score": 0.9096968173980713, "xcomet_qe_score": 0.863192617893219, "metricx_score": 1.0130515098571777, "metricx_qe_score": 1.1044714450836182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别指标时,对于bleu,我们发现 Collins-agnostic模型的表现最好。", "metrics": {"bleu_score": 35.89346734569217, "chrf_score": 27.68626883932666, "xcomet_score": 0.7972229719161987, "xcomet_qe_score": 0.708484947681427, "metricx_score": 7.174067974090576, "metricx_qe_score": 7.286904335021973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But then if we use comment, context-aware models perform best. And if we use", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7304205894470215, "xcomet_qe_score": 0.7836760878562927, "metricx_score": 20.622594833374023, "metricx_qe_score": 16.97226905822754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "word f-measure, then models with or without context have comparable performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9266923666000366, "xcomet_qe_score": 0.9146671295166016, "metricx_score": 18.707313537597656, "metricx_qe_score": 22.69617462158203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果仅使用语料库级别指标,就很难确定最佳的文档级翻译系统。", "metrics": {"bleu_score": 71.15597476046582, "chrf_score": 63.751611159132295, "xcomet_score": 0.9972941875457764, "xcomet_qe_score": 0.9909734725952148, "metricx_score": 0.7806118130683899, "metricx_qe_score": 0.9467670917510986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用MMLU基准测试模型,并发现上下文模型在某些话语现象(如形式和词汇连贯性)方面比不使用上下文的模型要准确得多。", "metrics": {"bleu_score": 54.39549589271005, "chrf_score": 47.353681505988675, "xcomet_score": 0.8828509449958801, "xcomet_qe_score": 0.8058669567108154, "metricx_score": 3.2661235332489014, "metricx_qe_score": 3.441465139389038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是这些模型并不比不使用上下文的其他现象(如省略号、代词和动词形式)的模型好得多。", "metrics": {"bleu_score": 60.81474054043918, "chrf_score": 52.62220319153482, "xcomet_score": 0.8760230541229248, "xcomet_qe_score": 0.7882319092750549, "metricx_score": 1.945603609085083, "metricx_qe_score": 1.5382120609283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明我们需要在文档级翻译方面看到更多的进展。", "metrics": {"bleu_score": 28.128870710174894, "chrf_score": 28.29685799772981, "xcomet_score": 0.9983292818069458, "xcomet_qe_score": 0.980340301990509, "metricx_score": 1.03563392162323, "metricx_qe_score": 1.1051552295684814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试表明,DeepL通常比Google翻译在文档级别的翻译中更准确。", "metrics": {"bleu_score": 55.195907340940124, "chrf_score": 50.317622506199555, "xcomet_score": 0.9353708624839783, "xcomet_qe_score": 0.802421510219574, "metricx_score": 1.3403851985931396, "metricx_qe_score": 1.5114492177963257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要总结一下,我们对 14 种语言对进行了数据驱动的分析,以确定一个翻译所需的上下文。 然后我们用我们的发现来建立一个文档级机器翻译的基准,这可以帮助我们确定哪些语义现象模型可以很好地处理,哪些翻译系统在文档级翻译方面表现良好。", "metrics": {"bleu_score": 54.292857661613844, "chrf_score": 49.31594683987306, "xcomet_score": 0.8083590269088745, "xcomet_qe_score": 0.7829008102416992, "metricx_score": 4.365717887878418, "metricx_qe_score": 4.746700286865234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Thank you so much for your attention", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9622266292572021, "xcomet_qe_score": 1.0, "metricx_score": 4.45453405380249, "metricx_qe_score": 4.277175426483154, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". See you in the next video.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2111724317073822, "xcomet_qe_score": 0.15943577885627747, "metricx_score": 15.170561790466309, "metricx_qe_score": 19.060604095458984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Yanis Lavrik,我将向您介绍我们关于Dr. Bert的作品。一个在法语中经过预训练的生物医学和临床领域的Robust模型。", "metrics": {"bleu_score": 32.200445681355816, "chrf_score": 33.89308188644638, "xcomet_score": 0.5401617884635925, "xcomet_qe_score": 0.6041679382324219, "metricx_score": 6.144581317901611, "metricx_qe_score": 5.542189121246338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本演示文稿中,我们先讨论一下医疗保健领域的语言建模。", "metrics": {"bleu_score": 47.93059000973911, "chrf_score": 47.115704281887396, "xcomet_score": 0.918473482131958, "xcomet_qe_score": 0.9179338812828064, "metricx_score": 1.623739242553711, "metricx_qe_score": 2.0574159622192383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个生物医学模型,法语名称为Dr. Bert, 基于Roberta, 并在NACHOS上进行训练,NACHOS是来自网络的医学数据集。", "metrics": {"bleu_score": 33.247496294430015, "chrf_score": 35.333905957456786, "xcomet_score": 0.794665515422821, "xcomet_qe_score": 0.7686091661453247, "metricx_score": 4.597827434539795, "metricx_qe_score": 4.871845722198486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了使用多个预测设置和数据源的模型比较。", "metrics": {"bleu_score": 68.93665549290876, "chrf_score": 61.6041097980934, "xcomet_score": 0.8651293516159058, "xcomet_qe_score": 0.8610600829124451, "metricx_score": 1.3433864116668701, "metricx_qe_score": 2.2907121181488037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在法语中介绍了我们在11个生物医学和临床下流任务上的结果。", "metrics": {"bleu_score": 55.59014241899505, "chrf_score": 51.67548699572093, "xcomet_score": 0.7663244009017944, "xcomet_qe_score": 0.8510779142379761, "metricx_score": 1.3126165866851807, "metricx_qe_score": 1.9877947568893433, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结了实验并为您提供了有关如何访问模型的更多详细信息。", "metrics": {"bleu_score": 63.85979171802721, "chrf_score": 57.81595581677502, "xcomet_score": 0.9653078317642212, "xcomet_qe_score": 0.8924208283424377, "metricx_score": 0.3078554570674896, "metricx_qe_score": 0.27545106410980225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自从它在 2018 年发布以来,BERT 已成为解决自然语言处理任务的最有效方法之一,并提供了与历史静态和上下文相关的方法相比的巨大性能提升,例如 Word2Vec、FastText 或 BERT。 自那", "metrics": {"bleu_score": 48.168858709413136, "chrf_score": 51.64284130696307, "xcomet_score": 0.5740776062011719, "xcomet_qe_score": 0.5722201466560364, "metricx_score": 6.1535563468933105, "metricx_qe_score": 6.32602596282959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以后,这种模型已被改编成许多其他语言,例如法语中的camembert和生物医学领域的permetbert和biobert,以及临床领域的clinicalbert,但主要是英语。", "metrics": {"bleu_score": 49.56800575056754, "chrf_score": 32.95923560135213, "xcomet_score": 0.6270205974578857, "xcomet_qe_score": 0.5931456089019775, "metricx_score": 6.562583923339844, "metricx_qe_score": 5.994536399841309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于其他语言的专业模型是稀缺的,并且通常基于连续的预训练,因为缺乏领域数据。", "metrics": {"bleu_score": 48.28935711765139, "chrf_score": 45.0401815422815, "xcomet_score": 0.7324190139770508, "xcomet_qe_score": 0.6810810565948486, "metricx_score": 1.8189740180969238, "metricx_qe_score": 1.7050460577011108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,直到现在,法国还没有任何开源模型用于生物医学。", "metrics": {"bleu_score": 39.02273664485568, "chrf_score": 29.941285767372722, "xcomet_score": 0.9835960865020752, "xcomet_qe_score": 0.9179418087005615, "metricx_score": 1.6710879802703857, "metricx_qe_score": 1.4307054281234741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们问自己一个问题:什么是最适合广泛用途的数据来源?这些数据是替代临床数据的好选择。", "metrics": {"bleu_score": 30.5946132468698, "chrf_score": 26.640817567239978, "xcomet_score": 0.8979134559631348, "xcomet_qe_score": 0.8904255032539368, "metricx_score": 1.3242093324661255, "metricx_qe_score": 1.090332269668579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Bert与我们的Shubert模型进行比较,该模型基于从我们所在城市的非大学医院获得的匿名数据。", "metrics": {"bleu_score": 45.720828919364145, "chrf_score": 33.05063173315042, "xcomet_score": 0.5759733319282532, "xcomet_qe_score": 0.5950212478637695, "metricx_score": 5.483550071716309, "metricx_qe_score": 6.0615129470825195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之后,我们问自己:我们需要多少数据来训练一个专门的模型?在法国的数据上是四", "metrics": {"bleu_score": 34.184479923215385, "chrf_score": 39.052624404884085, "xcomet_score": 0.616297721862793, "xcomet_qe_score": 0.5837949514389038, "metricx_score": 7.122827529907227, "metricx_qe_score": 4.82868766784668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "千兆字节,八千兆字节,还是更多?", "metrics": {"bleu_score": 15.310245441182444, "chrf_score": 13.316908888328335, "xcomet_score": 0.7366577386856079, "xcomet_qe_score": 0.7360376119613647, "metricx_score": 4.578239917755127, "metricx_qe_score": 3.909298896789551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先从头开始训练和比较四个模型。第一个版本的Doctor Bert有七千兆字节的饼干,一个版本的四千兆字节的饼干。 第一个版本的Shubert,这是一个临床模型,我们从临床笔记中获取了四千字的句子,最后一个版本的Shubert,我们混合了四千字的自然语句和四千字的临床笔记。", "metrics": {"bleu_score": 35.692823192066015, "chrf_score": 26.220741996993503, "xcomet_score": 0.2702040672302246, "xcomet_qe_score": 0.2717161178588867, "metricx_score": 11.245373725891113, "metricx_qe_score": 9.30698013305664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较之外,我们还介绍了三种基于预训练的模型来分析预训练策略的影响。", "metrics": {"bleu_score": 47.8407722675121, "chrf_score": 41.09356245230675, "xcomet_score": 0.8707146644592285, "xcomet_qe_score": 0.8218454122543335, "metricx_score": 1.205754041671753, "metricx_qe_score": 1.6524585485458374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于卡门贝尔的权重,并在四千兆字节的自然集上进行训练。", "metrics": {"bleu_score": 25.04854060624158, "chrf_score": 17.356171734283578, "xcomet_score": 0.6975384950637817, "xcomet_qe_score": 0.6190420389175415, "metricx_score": 5.087538719177246, "metricx_qe_score": 6.984255313873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也基于卡门贝尔,但这次在四千兆字节的清洁节点上进行训练。 最后,一个基于英语的生物医学模型,并在四千兆字节的自然集上进行训练。", "metrics": {"bleu_score": 28.50156017322158, "chrf_score": 21.14934002037527, "xcomet_score": 0.4468365013599396, "xcomet_qe_score": 0.45657384395599365, "metricx_score": 7.963640213012695, "metricx_qe_score": 7.701947212219238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了公共和私人的多任务,例如名称识别,分类,部分语音标记和问题回答。", "metrics": {"bleu_score": 53.94302529573704, "chrf_score": 48.348501126590584, "xcomet_score": 0.7104086875915527, "xcomet_qe_score": 0.7169601917266846, "metricx_score": 3.8017921447753906, "metricx_qe_score": 4.360702991485596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个模型与六个基线模型进行了比较,这些基线模型是:Carmembert Oscar 138 GB、Carmembert Oscar 4 GB、Carmembert CCNet 4 GB、Permabit Myobert 和 Clinicalbert。", "metrics": {"bleu_score": 28.811405296391232, "chrf_score": 31.76701878504292, "xcomet_score": 0.4341020882129669, "xcomet_qe_score": 0.4895474910736084, "metricx_score": 8.220882415771484, "metricx_qe_score": 8.816100120544434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在评估中,突出显示该模型在具有与训练模型相同性质的数据的任务上表现最佳。", "metrics": {"bleu_score": 22.30622921268355, "chrf_score": 22.222655476331944, "xcomet_score": 0.8901774883270264, "xcomet_qe_score": 0.8696416020393372, "metricx_score": 2.457587242126465, "metricx_qe_score": 2.6084280014038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,我们可以从异构源中获取数据。我们观察到,来自异构源的数据似乎更具多功能性。", "metrics": {"bleu_score": 34.418320479193596, "chrf_score": 42.17662893181146, "xcomet_score": 0.9033242464065552, "xcomet_qe_score": 0.793782114982605, "metricx_score": 1.8401628732681274, "metricx_qe_score": 1.5675709247589111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以转化为更好的性能。", "metrics": {"bleu_score": 69.19907115708301, "chrf_score": 62.75376435159043, "xcomet_score": 0.9759578704833984, "xcomet_qe_score": 0.9726451635360718, "metricx_score": 3.253467559814453, "metricx_qe_score": 4.15842866897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始训练似乎在大多数任务上获得了更高的性能。", "metrics": {"bleu_score": 40.36116601591821, "chrf_score": 35.97466007106289, "xcomet_score": 0.8407607078552246, "xcomet_qe_score": 0.7924416661262512, "metricx_score": 3.62920880317688, "metricx_qe_score": 4.484870433807373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,我们对纳舒厄的四千兆字节子集的权重和标记器的实验,使用权威的训练,显示了与从头开始获得的结果相当的结果。", "metrics": {"bleu_score": 27.97888906482005, "chrf_score": 18.57252245844839, "xcomet_score": 0.3783838152885437, "xcomet_qe_score": 0.4241791367530823, "metricx_score": 9.594049453735352, "metricx_qe_score": 9.958059310913086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于基于camembert的模型来说,这不是情况,因为camembert的重量和tokenizer存在稳定性问题。", "metrics": {"bleu_score": 37.86440352282539, "chrf_score": 27.334761090263832, "xcomet_score": 0.5755072236061096, "xcomet_qe_score": 0.6254969835281372, "metricx_score": 7.216762542724609, "metricx_qe_score": 6.182013034820557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们的系统在11个任务中提供了9个任务的更好性能,并且在总体上超过了这里的随机模型。", "metrics": {"bleu_score": 13.669825917943005, "chrf_score": 15.271946277339785, "xcomet_score": 0.696938157081604, "xcomet_qe_score": 0.6793990135192871, "metricx_score": 6.851934432983398, "metricx_qe_score": 7.115836143493652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也观察到,专用数据越多越好,但它并不适合大规模使用。", "metrics": {"bleu_score": 15.911783110981515, "chrf_score": 15.839302672636002, "xcomet_score": 0.8599298000335693, "xcomet_qe_score": 0.8480175733566284, "metricx_score": 1.0583937168121338, "metricx_qe_score": 1.0355647802352905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有预训练的模型都来自Naturally,它们都可以在Github上免费获得。", "metrics": {"bleu_score": 13.646856137109406, "chrf_score": 13.428488893299992, "xcomet_score": 0.6418031454086304, "xcomet_qe_score": 0.6859798431396484, "metricx_score": 7.371450424194336, "metricx_qe_score": 8.145566940307617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家的发言,我们期待着在多伦多的后续会议上进行交流。", "metrics": {"bleu_score": 24.837141617455035, "chrf_score": 27.925877246261138, "xcomet_score": 0.9235175848007202, "xcomet_qe_score": 0.9097989797592163, "metricx_score": 1.1776599884033203, "metricx_qe_score": 2.017965316772461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是 Matthias Landemann today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations.", "metrics": {"bleu_score": 0.998198685552908, "chrf_score": 15.655138799341422, "xcomet_score": 0.7995033264160156, "xcomet_qe_score": 0.7805278897285461, "metricx_score": 10.425161361694336, "metricx_qe_score": 9.166104316711426, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与我的顾问亚历山大·科拉和伊万·迪托夫的联合工作。", "metrics": {"bleu_score": 4.042649040111612, "chrf_score": 4.566532758022121, "xcomet_score": 0.8886785507202148, "xcomet_qe_score": 0.8040522336959839, "metricx_score": 3.3421108722686768, "metricx_qe_score": 3.1677372455596924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Compositional generalization可以理解为学习者处理更深层次的递归和在训练期间单独看到的短语的组合的能力。", "metrics": {"bleu_score": 72.990478582082, "chrf_score": 61.56970007482882, "xcomet_score": 0.7420057058334351, "xcomet_qe_score": 0.6424915790557861, "metricx_score": 7.69111967086792, "metricx_qe_score": 8.313779830932617, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的语境下,测试组合泛化可能看起来像这样。", "metrics": {"bleu_score": 33.535699101570344, "chrf_score": 28.370606304131314, "xcomet_score": 0.9077794551849365, "xcomet_qe_score": 0.9052475094795227, "metricx_score": 1.0931119918823242, "metricx_qe_score": 1.5681523084640503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常我们有一组训练用语。", "metrics": {"bleu_score": 26.279685854434366, "chrf_score": 23.501394178627695, "xcomet_score": 0.859153151512146, "xcomet_qe_score": 0.9522408843040466, "metricx_score": 0.9548160433769226, "metricx_qe_score": 0.8305134773254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本例中,女孩睡着了,", "metrics": {"bleu_score": 7.6596808287682965, "chrf_score": 5.538306049469001, "xcomet_score": 0.679844319820404, "xcomet_qe_score": 0.9773117899894714, "metricx_score": 3.124239683151245, "metricx_qe_score": 0.9818666577339172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "玛丽知道女孩睡着了。 这些", "metrics": {"bleu_score": 12.571299163620896, "chrf_score": 9.001499647068021, "xcomet_score": 0.8580515384674072, "xcomet_qe_score": 0.7712824940681458, "metricx_score": 5.471982955932617, "metricx_qe_score": 1.6101837158203125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语句与逻辑形式配对,逻辑形式表示它们意义的核心方面。", "metrics": {"bleu_score": 5.951000953515419, "chrf_score": 11.471892691017922, "xcomet_score": 0.9204831719398499, "xcomet_qe_score": 0.9850780963897705, "metricx_score": 1.0358734130859375, "metricx_qe_score": 0.8207840323448181, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估相比,测试集并非来自相同的分布,而是包含结构上未见的逻辑形式。", "metrics": {"bleu_score": 53.059681061004106, "chrf_score": 47.1418070774678, "xcomet_score": 0.8422113656997681, "xcomet_qe_score": 0.8361369371414185, "metricx_score": 1.2258362770080566, "metricx_qe_score": 1.8157037496566772, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个示例中,模型在训练过程中看到了较浅的递归,并在具有更深递归的示例上进行了测试。", "metrics": {"bleu_score": 15.998848672602293, "chrf_score": 17.448095135470375, "xcomet_score": 0.888145923614502, "xcomet_qe_score": 0.8796629309654236, "metricx_score": 2.5147171020507812, "metricx_qe_score": 4.355425834655762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Naive sequence to sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.489137450693644, "xcomet_score": 0.8728129863739014, "xcomet_qe_score": 0.9151166677474976, "metricx_score": 22.225736618041992, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,他们经常无法再现输入和输出之间的系统对应关系,例如在示例中使用颜色编码的那些。 一个", "metrics": {"bleu_score": 50.54904637028962, "chrf_score": 49.35106080485077, "xcomet_score": 0.826720654964447, "xcomet_qe_score": 0.8254698514938354, "metricx_score": 5.229989528656006, "metricx_qe_score": 1.815503716468811, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决这个问题的流行方法是将树木合并到模型中。", "metrics": {"bleu_score": 56.96705282375882, "chrf_score": 45.7147421973907, "xcomet_score": 0.8913435935974121, "xcomet_qe_score": 0.8635334968566895, "metricx_score": 1.3421111106872559, "metricx_qe_score": 1.292765498161316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The trees are intended to capture the compositional process that relates utterances with the logical forms. This works", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7902810573577881, "xcomet_qe_score": 0.8361265659332275, "metricx_score": 23.682004928588867, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "well, but trees are usually not given and need to be obtained somehow.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8534097075462341, "xcomet_qe_score": 0.8970529437065125, "metricx_score": 24.931503295898438, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂的过程,有时是一个计算成本高昂的过程。", "metrics": {"bleu_score": 60.68410610880495, "chrf_score": 67.39504033841928, "xcomet_score": 0.9929591417312622, "xcomet_qe_score": 0.971757709980011, "metricx_score": 0.6332277655601501, "metricx_qe_score": 0.7383170127868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及大量的形式主义特定的逻辑形式的预处理。例如,处理变量符号。", "metrics": {"bleu_score": 39.99198960829136, "chrf_score": 32.88464161004864, "xcomet_score": 0.9119666218757629, "xcomet_qe_score": 0.9749376773834229, "metricx_score": 1.3715883493423462, "metricx_qe_score": 2.174642324447632, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Obtaining trees may also involve specialized grammar induction procedures.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9540901184082031, "xcomet_qe_score": 0.988206148147583, "metricx_score": 23.33905601501465, "metricx_qe_score": 24.163681030273438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树,而是引入了一个神经序列到序列模型,该模型直接对输入片段和输出片段之间的对应关系进行建模。", "metrics": {"bleu_score": 74.16311541175374, "chrf_score": 66.2037465200055, "xcomet_score": 0.8000877499580383, "xcomet_qe_score": 0.7374386787414551, "metricx_score": 2.225593090057373, "metricx_qe_score": 2.468820333480835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "For the first time, we show strong generalization to deeper recursion without relying on", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6510890126228333, "xcomet_qe_score": 0.7189473509788513, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "trees. Our approach predicts the output from the input in two steps.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.843427836894989, "xcomet_qe_score": 0.8533115983009338, "metricx_score": 24.081958770751953, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6643848419189453, "xcomet_qe_score": 0.6566861867904663, "metricx_score": 22.223711013793945, "metricx_qe_score": 19.048023223876953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "first step, we have all the right tokens, but they are not ordered. 这就是", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7697012424468994, "xcomet_qe_score": 0.8523457050323486, "metricx_score": 14.557491302490234, "metricx_qe_score": 11.096284866333008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么在第二步中,我们使用另一个模型来预测排列顺序,以将它们放入正确的顺序中。", "metrics": {"bleu_score": 52.27583693572912, "chrf_score": 51.03054168804072, "xcomet_score": 0.8792179822921753, "xcomet_qe_score": 0.9129660129547119, "metricx_score": 2.3745522499084473, "metricx_qe_score": 2.456733465194702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9438894987106323, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "This makes our approach quite flexible and expressive.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9866753816604614, "xcomet_qe_score": 1.0, "metricx_score": 24.07121467590332, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的排列模型大致可以这样工作。", "metrics": {"bleu_score": 44.75498593559181, "chrf_score": 36.985291686147306, "xcomet_score": 0.9031522274017334, "xcomet_qe_score": 0.8038766384124756, "metricx_score": 1.7418980598449707, "metricx_qe_score": 3.082958459854126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定每个位置放入哪个多集令牌。", "metrics": {"bleu_score": 39.09290182000449, "chrf_score": 32.324987421329624, "xcomet_score": 0.7751255035400391, "xcomet_qe_score": 0.7748081684112549, "metricx_score": 4.580278396606445, "metricx_qe_score": 3.773038625717163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需选择一个,如红色所示。", "metrics": {"bleu_score": 50.30335395590839, "chrf_score": 43.30760671660852, "xcomet_score": 0.976276159286499, "xcomet_qe_score": 0.9820423126220703, "metricx_score": 0.6631963849067688, "metricx_qe_score": 0.5382755994796753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们跳到下一个多集令牌来确定输出中的第二个令牌。", "metrics": {"bleu_score": 52.63160900531291, "chrf_score": 44.45583941040588, "xcomet_score": 0.6993389129638672, "xcomet_qe_score": 0.7182090282440186, "metricx_score": 8.128133773803711, "metricx_qe_score": 6.690625190734863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8218663930892944, "xcomet_qe_score": 0.8532760143280029, "metricx_score": 22.384063720703125, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "process. 直到第一个阶段的每个令牌都被访问了一次。", "metrics": {"bleu_score": 45.385848654292765, "chrf_score": 37.38701167324356, "xcomet_score": 0.694049596786499, "xcomet_qe_score": 0.6883676052093506, "metricx_score": 8.184174537658691, "metricx_qe_score": 9.175973892211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了给你们一个实验结果的预告,在这里我们将我们的方法与其他无树模型在Cogs benchmark上进行比较。我们的模型", "metrics": {"bleu_score": 45.747055825157716, "chrf_score": 40.241348892567274, "xcomet_score": 0.6566260457038879, "xcomet_qe_score": 0.6853963732719421, "metricx_score": 8.209073066711426, "metricx_qe_score": 6.155158996582031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在更深层次的递归上比其他模型大幅领先。", "metrics": {"bleu_score": 17.22334585770773, "chrf_score": 15.950910578793145, "xcomet_score": 0.9421448707580566, "xcomet_qe_score": 0.9550766944885254, "metricx_score": 4.1196675300598145, "metricx_qe_score": 5.8109331130981445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Some other kinds of structural generalization remain very challenging though. 在我们的", "metrics": {"bleu_score": 2.2392970339022753, "chrf_score": 1.0964912280701755, "xcomet_score": 0.6678676605224609, "xcomet_qe_score": 0.77252596616745, "metricx_score": 24.641971588134766, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论文中,我们解决了几个有趣的技术难题。", "metrics": {"bleu_score": 42.98482469767495, "chrf_score": 33.28210572789796, "xcomet_score": 0.9968975782394409, "xcomet_qe_score": 0.9900679588317871, "metricx_score": 0.10436923801898956, "metricx_qe_score": 0.1252492368221283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对齐在训练数据中没有给出。", "metrics": {"bleu_score": 31.146377792658097, "chrf_score": 28.05973013627032, "xcomet_score": 0.8629755973815918, "xcomet_qe_score": 0.9026443958282471, "metricx_score": 0.5198761820793152, "metricx_qe_score": 0.5990929007530212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果,对于给定的令牌,我们不知道它来自哪个多元集,这对训练构成了挑战。", "metrics": {"bleu_score": 48.45968521946167, "chrf_score": 41.162573656050895, "xcomet_score": 0.7043065428733826, "xcomet_qe_score": 0.6589187383651733, "metricx_score": 6.13024377822876, "metricx_qe_score": 5.348626613616943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列,但语法正确的排列是隐含的。", "metrics": {"bleu_score": 64.79812100004894, "chrf_score": 58.878689191431675, "xcomet_score": 0.9235644340515137, "xcomet_qe_score": 0.8894460797309875, "metricx_score": 1.1955822706222534, "metricx_qe_score": 2.003772020339966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练过程中引入对齐来解决这个问题。", "metrics": {"bleu_score": 54.483648870506556, "chrf_score": 49.148956801982905, "xcomet_score": 0.9188477993011475, "xcomet_qe_score": 0.8947857022285461, "metricx_score": 0.8788174986839294, "metricx_qe_score": 1.5455358028411865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的排列方法非常灵活,但它带来了一个挑战,即找到分数最高的排列是 NP 难的。", "metrics": {"bleu_score": 58.86456833893744, "chrf_score": 49.77881123980305, "xcomet_score": 0.8221580982208252, "xcomet_qe_score": 0.809024453163147, "metricx_score": 1.740412950515747, "metricx_qe_score": 3.1507627964019775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这与旅行推销员问题有关。", "metrics": {"bleu_score": 40.298832314129164, "chrf_score": 34.289876789876786, "xcomet_score": 0.8810513019561768, "xcomet_qe_score": 0.8466493487358093, "metricx_score": 0.9650546908378601, "metricx_qe_score": 1.0459892749786377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一个GPU友好的连续放松来近似它,这也使我们能够通过解决方案进行回溯,并学习语言上更合理的排列。", "metrics": {"bleu_score": 51.24207855186103, "chrf_score": 47.95322506445371, "xcomet_score": 0.7616493701934814, "xcomet_qe_score": 0.7813365459442139, "metricx_score": 4.749682426452637, "metricx_qe_score": 5.258245468139648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战,请查看我们的论文或来到我们的展位。", "metrics": {"bleu_score": 74.33626257318413, "chrf_score": 71.34442176970767, "xcomet_score": 0.9580312967300415, "xcomet_qe_score": 0.9286452531814575, "metricx_score": 0.5426893830299377, "metricx_qe_score": 0.4396398961544037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是马克西塔,今天我的合著者马丁和我正在介绍我们的工作《Kitmus Test: Evaluating Knowledge Integration from Multiple Sources》。这项", "metrics": {"bleu_score": 22.79918155634488, "chrf_score": 16.96501010330352, "xcomet_score": 0.5422002077102661, "xcomet_qe_score": 0.5888313055038452, "metricx_score": 8.249547958374023, "metricx_qe_score": 5.29412841796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、MILA和微软研究院的合作。", "metrics": {"bleu_score": 44.739239243298826, "chrf_score": 33.83543148131714, "xcomet_score": 0.7900747060775757, "xcomet_qe_score": 0.7282896041870117, "metricx_score": 4.296979904174805, "metricx_qe_score": 4.452808856964111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然语言理解模型利用各种知识来源,例如参数中的知识,通常通过预训练获得,以及在推理时在输入中提供的知识。", "metrics": {"bleu_score": 51.11441575653196, "chrf_score": 43.329737774087675, "xcomet_score": 0.9574657678604126, "xcomet_qe_score": 0.8407549858093262, "metricx_score": 1.2126436233520508, "metricx_qe_score": 1.4184553623199463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在任务中,如问答,表明模型可以使用预训练的时间知识来解决任务。", "metrics": {"bleu_score": 53.2313787279045, "chrf_score": 46.69971631408672, "xcomet_score": 0.7390419840812683, "xcomet_qe_score": 0.7006819248199463, "metricx_score": 3.0005943775177, "metricx_qe_score": 3.4976634979248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,自然语言理解通常需要在推理时提供的知识。", "metrics": {"bleu_score": 89.21616972156083, "chrf_score": 87.46990169759047, "xcomet_score": 0.8783411979675293, "xcomet_qe_score": 0.8480260372161865, "metricx_score": 1.2219430208206177, "metricx_qe_score": 0.9787708520889282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子中,\"John saw the newly elected president on TV\"。 Pre-trained parameters can contain information about what presidents do and what a TV", "metrics": {"bleu_score": 22.04399368380058, "chrf_score": 46.01002197084048, "xcomet_score": 0.21435004472732544, "xcomet_qe_score": 0.24819524586200714, "metricx_score": 16.14842987060547, "metricx_qe_score": 18.480222702026367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is, but they cannot reliably know who this instance-specific entity John is or who the new president is, because the president might have changed since pre-training.", "metrics": {"bleu_score": 0.2141641570134727, "chrf_score": 1.7694947509652021, "xcomet_score": 0.6302058100700378, "xcomet_qe_score": 0.6750706434249878, "metricx_score": 24.97699737548828, "metricx_qe_score": 23.177541732788086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型NLU任务模型需要能够整合和使用预训练时间和推理时间知识的能力。", "metrics": {"bleu_score": 66.99450825843269, "chrf_score": 61.956026230724014, "xcomet_score": 0.8527790904045105, "xcomet_qe_score": 0.8415238857269287, "metricx_score": 1.092850923538208, "metricx_qe_score": 1.6688814163208008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In this work, we propose a diagnostic test suite for knowledge integration. We introduce a co-", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8193790912628174, "xcomet_qe_score": 0.8162341117858887, "metricx_score": 20.6563663482666, "metricx_qe_score": 18.877534866333008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "reference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7300882339477539, "xcomet_qe_score": 0.8360404968261719, "metricx_score": 20.546171188354492, "metricx_qe_score": 16.186904907226562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了数据集,既有人类研究参与者,也有建立参考解像度模型。", "metrics": {"bleu_score": 29.552612305356018, "chrf_score": 27.889702865473915, "xcomet_score": 0.7356690764427185, "xcomet_qe_score": 0.8048760294914246, "metricx_score": 5.244874000549316, "metricx_qe_score": 4.789356708526611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个示例。", "metrics": {"bleu_score": 4.199688916946863, "chrf_score": 8.345375722543352, "xcomet_score": 0.8302562236785889, "xcomet_qe_score": 0.9019793272018433, "metricx_score": 1.3495347499847412, "metricx_qe_score": 1.6758085489273071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "塞尔文是法官。", "metrics": {"bleu_score": 26.269098944241588, "chrf_score": 11.249653643668605, "xcomet_score": 0.9220947027206421, "xcomet_qe_score": 0.8600059151649475, "metricx_score": 1.1559098958969116, "metricx_qe_score": 1.2917946577072144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基亚是面包师。", "metrics": {"bleu_score": 37.68499164492418, "chrf_score": 21.93828587097027, "xcomet_score": 0.9406682252883911, "xcomet_qe_score": 0.8724457621574402, "metricx_score": 0.7911062240600586, "metricx_qe_score": 1.2689687013626099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "塞尔文和基亚在公园相遇。工作结束后,", "metrics": {"bleu_score": 27.499775953224148, "chrf_score": 25.03711845360388, "xcomet_score": 0.27378425002098083, "xcomet_qe_score": 0.22296276688575745, "metricx_score": 6.65715217590332, "metricx_qe_score": 6.019338130950928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "他在法庭上审理案件一整天。他很高兴放松。", "metrics": {"bleu_score": 37.25157270375826, "chrf_score": 31.545373979384873, "xcomet_score": 0.8705533742904663, "xcomet_qe_score": 0.8442333936691284, "metricx_score": 3.242523431777954, "metricx_qe_score": 3.4508657455444336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Salman. 给", "metrics": {"bleu_score": 1.0619714138345164, "chrf_score": 3.340330653004103, "xcomet_score": 0.6021279096603394, "xcomet_qe_score": 0.7247979044914246, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "定代词的解析需要两种类型的信息。", "metrics": {"bleu_score": 17.279391259901544, "chrf_score": 17.447171739281274, "xcomet_score": 0.889241099357605, "xcomet_qe_score": 0.8660929799079895, "metricx_score": 1.008205771446228, "metricx_qe_score": 0.7656678557395935, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是实体特定的知识,例如,萨维尔是法官。第二", "metrics": {"bleu_score": 13.22447339860768, "chrf_score": 13.510221118873766, "xcomet_score": 0.42067867517471313, "xcomet_qe_score": 0.4977777600288391, "metricx_score": 6.174988269805908, "metricx_qe_score": 2.9934301376342773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "种是背景知识,例如法官在法庭上决定案件。", "metrics": {"bleu_score": 36.0838453122468, "chrf_score": 31.314630850510643, "xcomet_score": 0.6427992582321167, "xcomet_qe_score": 0.6936218738555908, "metricx_score": 5.513931751251221, "metricx_qe_score": 3.7450037002563477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,背景知识是在大型语言模型的预训练期间学习的,而实体特定知识通常是在推理时观察到的。", "metrics": {"bleu_score": 63.09825831149155, "chrf_score": 55.484578160989216, "xcomet_score": 0.8663229942321777, "xcomet_qe_score": 0.9013622999191284, "metricx_score": 1.0649983882904053, "metricx_qe_score": 1.459912657737732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变了这两条信息的可用性,使其可能在单个来源或多个来源中找到。", "metrics": {"bleu_score": 58.90928729589973, "chrf_score": 51.53261865915343, "xcomet_score": 0.9500073194503784, "xcomet_qe_score": 0.8254709243774414, "metricx_score": 0.9055830836296082, "metricx_qe_score": 1.0695364475250244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了三个基于知识库的设置。", "metrics": {"bleu_score": 32.37722713145643, "chrf_score": 24.47191105745197, "xcomet_score": 0.8294620513916016, "xcomet_qe_score": 0.8114396333694458, "metricx_score": 2.8345787525177, "metricx_qe_score": 2.951683759689331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有一个称为\"背景预训练\"的设置,其中背景知识被假设在预训练时可用。", "metrics": {"bleu_score": 40.75386124319343, "chrf_score": 34.98216261022858, "xcomet_score": 0.9365406036376953, "xcomet_qe_score": 0.9176729917526245, "metricx_score": 1.5838481187820435, "metricx_qe_score": 2.5397372245788574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次是背景知识的设置。两种知识类型在预训练时间和推断时间都可用。", "metrics": {"bleu_score": 27.932340705747453, "chrf_score": 22.933691032402763, "xcomet_score": 0.8446761965751648, "xcomet_qe_score": 0.7942541837692261, "metricx_score": 1.701740026473999, "metricx_qe_score": 1.5941282510757446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后是背景推断设置,两种知识类型仅在推断时间可用。", "metrics": {"bleu_score": 38.36993711485159, "chrf_score": 35.161020550876195, "xcomet_score": 0.9400029182434082, "xcomet_qe_score": 0.8602269887924194, "metricx_score": 3.0663275718688965, "metricx_qe_score": 2.2605557441711426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置特别有趣,因为它模拟了必要的背景知识来完成任务的情况不是预训练数据模型的一部分。", "metrics": {"bleu_score": 53.32517503875777, "chrf_score": 45.937563027052946, "xcomet_score": 0.8289107084274292, "xcomet_qe_score": 0.8319356441497803, "metricx_score": 4.913296699523926, "metricx_qe_score": 5.612783432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,因为自预训练以来已经发展了新的职业。", "metrics": {"bleu_score": 58.34041060170845, "chrf_score": 54.35385522495244, "xcomet_score": 0.8689533472061157, "xcomet_qe_score": 0.8373053073883057, "metricx_score": 2.074200391769409, "metricx_qe_score": 3.452423095703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Here's an example of how we control the availability effects of two sources.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8106715083122253, "xcomet_qe_score": 0.8431810736656189, "metricx_score": 18.356861114501953, "metricx_qe_score": 12.4122314453125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识。政治家寻求选举政府职位的内容包含在预训练参数中。在感知上下文中,我们提供了特定的知识。奇切斯特是一个政治家。", "metrics": {"bleu_score": 31.69145790702469, "chrf_score": 24.895220595175587, "xcomet_score": 0.5755139589309692, "xcomet_qe_score": 0.48607030510902405, "metricx_score": 6.267962455749512, "metricx_qe_score": 5.953997611999512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在后台,我们同时提供了不仅仅是针对特定的,而且还提供了关于推理上下文中参与者的背景知识。", "metrics": {"bleu_score": 21.38657730154573, "chrf_score": 19.48552699520602, "xcomet_score": 0.5262163877487183, "xcomet_qe_score": 0.47952282428741455, "metricx_score": 5.484167575836182, "metricx_qe_score": 5.645843029022217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推断设置中,为虚构的职业提供了“艺术家”而不是“政治家”,因为艺术家不太可能被包含在预训练的参数中。", "metrics": {"bleu_score": 44.195768878458104, "chrf_score": 30.945492110493376, "xcomet_score": 0.8172370195388794, "xcomet_qe_score": 0.7528520822525024, "metricx_score": 5.305351734161377, "metricx_qe_score": 4.736883640289307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了数据集,既有人类研究参与者,也有建立参考解像度模型。", "metrics": {"bleu_score": 29.552612305356018, "chrf_score": 27.889702865473915, "xcomet_score": 0.7333990931510925, "xcomet_qe_score": 0.7640690803527832, "metricx_score": 5.123268127441406, "metricx_qe_score": 4.407612323760986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这张图中,我们展示了最好的模型在最困难的背景下的结果。", "metrics": {"bleu_score": 31.51853551292882, "chrf_score": 27.412328471241565, "xcomet_score": 0.7542037963867188, "xcomet_qe_score": 0.7998844981193542, "metricx_score": 4.256476402282715, "metricx_qe_score": 5.141473770141602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有针对kitmos的特定训练的情况下,这两种模型的性能都不佳。", "metrics": {"bleu_score": 9.782375748961453, "chrf_score": 11.520980350191891, "xcomet_score": 0.8993107676506042, "xcomet_qe_score": 0.8529157638549805, "metricx_score": 2.3340682983398438, "metricx_qe_score": 2.190185785293579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在kitmos上进行训练时,C2F和birdformer-rf的性能显著优于随机选择。", "metrics": {"bleu_score": 14.611783090145074, "chrf_score": 15.066772439458601, "xcomet_score": 0.7069514989852905, "xcomet_qe_score": 0.6996686458587646, "metricx_score": 5.820686340332031, "metricx_qe_score": 6.531158447265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在一般的语义相关解决方案数据集上进行训练时,模型会学习利用表面提示,这些提示在Kitmos上进行测试时是无用的,因为这些提示已被删除。 其他", "metrics": {"bleu_score": 26.537478680220605, "chrf_score": 22.351563138177678, "xcomet_score": 0.404252290725708, "xcomet_qe_score": 0.3443692624568939, "metricx_score": 5.865464687347412, "metricx_qe_score": 4.682830810546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识的实验表明,即使是最优性能的模型也无法可靠地整合仅在推理时提供的背景知识。 为了总结", "metrics": {"bleu_score": 57.85241080517218, "chrf_score": 51.58700482938493, "xcomet_score": 0.7051303386688232, "xcomet_qe_score": 0.6596571803092957, "metricx_score": 5.55991792678833, "metricx_qe_score": 6.084563255310059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要结论,许多共同引用解决方案模型似乎无法在没有特定任务训练的情况下对来自不同来源的知识进行推理。", "metrics": {"bleu_score": 79.23119750678677, "chrf_score": 79.53125896431852, "xcomet_score": 0.8525959253311157, "xcomet_qe_score": 0.836201012134552, "metricx_score": 3.5616674423217773, "metricx_qe_score": 3.562098503112793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在进行特定任务训练后,一些模型能够成功地整合来自多个来源的知识。", "metrics": {"bleu_score": 52.2352924913678, "chrf_score": 45.170605313983124, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5868865847587585, "metricx_qe_score": 1.0694719552993774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,甚至最好的模型似乎也难以可靠地整合仅在推理时呈现的知识。", "metrics": {"bleu_score": 56.369604709170034, "chrf_score": 51.12546004940802, "xcomet_score": 0.9019889831542969, "xcomet_qe_score": 0.798448383808136, "metricx_score": 1.6687581539154053, "metricx_qe_score": 2.211937189102173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请参阅我们的论文并查看 GitHub 上的代码数据集。感谢您的", "metrics": {"bleu_score": 35.15787253565496, "chrf_score": 38.340731697656885, "xcomet_score": 0.7138647437095642, "xcomet_qe_score": 0.7248741984367371, "metricx_score": 1.056891918182373, "metricx_qe_score": 0.5395942330360413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收听。", "metrics": {"bleu_score": 0.0, "chrf_score": 23.772204806687565, "xcomet_score": 0.8259608745574951, "xcomet_qe_score": 0.7775843739509583, "metricx_score": 1.7169746160507202, "metricx_qe_score": 1.604272723197937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是玛丽亚,今天我将讨论我们的论文《标记的角色:使用自然语言提示来衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 62.72070616155177, "chrf_score": 55.24087640934082, "xcomet_score": 0.7076470255851746, "xcomet_qe_score": 0.6788818836212158, "metricx_score": 2.396233320236206, "metricx_qe_score": 2.432844400405884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与埃斯特·德·穆什和丹·德罗夫斯基合作完成的。", "metrics": {"bleu_score": 18.69300079996002, "chrf_score": 12.96940482944677, "xcomet_score": 0.7074552774429321, "xcomet_qe_score": 0.5579349994659424, "metricx_score": 3.761014223098755, "metricx_qe_score": 4.40805196762085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多人已经记录了大型语言模型中的社会偏见和刻板印象。", "metrics": {"bleu_score": 29.461223538299254, "chrf_score": 24.19025064539888, "xcomet_score": 0.9590260982513428, "xcomet_qe_score": 0.8986743688583374, "metricx_score": 3.2310073375701904, "metricx_qe_score": 2.8625259399414062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施有各种限制。", "metrics": {"bleu_score": 40.23664746743545, "chrf_score": 36.99479597366897, "xcomet_score": 0.9910911917686462, "xcomet_qe_score": 1.0, "metricx_score": 0.34980249404907227, "metricx_qe_score": 0.3930320143699646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,这些数据集的收集非常耗时。 他们通常只衡量非常具体的刻板印象,这意味着他们不能很好地推广到其他人群或情境中,或者他们只是捕捉了非常广泛的广泛联系,例如与特定群体的消极联系。", "metrics": {"bleu_score": 46.619755432091736, "chrf_score": 39.827177772679114, "xcomet_score": 0.6636269688606262, "xcomet_qe_score": 0.5663108825683594, "metricx_score": 4.435271739959717, "metricx_qe_score": 4.790929317474365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8008136749267578, "xcomet_qe_score": 0.7524001598358154, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and promp", "metrics": {"bleu_score": 0.0, "chrf_score": 1.4425160257411414, "xcomet_score": 0.5020385384559631, "xcomet_qe_score": 0.555770754814148, "metricx_score": 20.711549758911133, "metricx_qe_score": 20.563684463500977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ts. 所以我们可以要求模型生成一个persona,这是一个描述一个想象中的个体的描述,使用一个提示,比如说,想象你是一个亚洲女性。", "metrics": {"bleu_score": 47.69401482916809, "chrf_score": 41.472995188539414, "xcomet_score": 0.730999231338501, "xcomet_qe_score": 0.5264313817024231, "metricx_score": 7.285643577575684, "metricx_qe_score": 7.216964244842529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述一下你自己。 And we", "metrics": {"bleu_score": 68.752775993657, "chrf_score": 60.311386713755724, "xcomet_score": 0.41321274638175964, "xcomet_qe_score": 0.3031356930732727, "metricx_score": 5.6323161125183105, "metricx_qe_score": 3.6932034492492676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "can immediately see that this is very generalizable to any demographic, because we can just specify whatever identity marker that we want into this prompt. 所以这里有一些来自", "metrics": {"bleu_score": 0.831963822536596, "chrf_score": 0.9157509157509156, "xcomet_score": 0.6227447986602783, "xcomet_qe_score": 0.7563143968582153, "metricx_score": 24.312360763549805, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "GPT-4的示例生成。 我们立", "metrics": {"bleu_score": 11.17616954959852, "chrf_score": 26.36096197037136, "xcomet_score": 0.4400949776172638, "xcomet_qe_score": 0.4144543707370758, "metricx_score": 9.18454647064209, "metricx_qe_score": 4.35416316986084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即看到,虽然这些输出在传统意义上并不过于消极或有毒。 \"There are some interesting patterns.\"", "metrics": {"bleu_score": 17.91631000811645, "chrf_score": 14.53910446074833, "xcomet_score": 0.7301065325737, "xcomet_qe_score": 0.7104159593582153, "metricx_score": 7.679836750030518, "metricx_qe_score": 7.636622428894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲妇女被描绘为不起眼。中东妇女被用外来词来称呼,像迷人的地区一样。 而且", "metrics": {"bleu_score": 12.782346386652655, "chrf_score": 13.517733223190179, "xcomet_score": 0.5005201101303101, "xcomet_qe_score": 0.4721570611000061, "metricx_score": 7.55382776260376, "metricx_qe_score": 4.7800612449646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",两个有色人种的女性角色都提到了祖先,而白人角色却没有。", "metrics": {"bleu_score": 27.464576563142582, "chrf_score": 25.151651016129538, "xcomet_score": 0.9481452703475952, "xcomet_qe_score": 0.9536608457565308, "metricx_score": 2.5084152221679688, "metricx_qe_score": 2.239320755004883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要捕捉这些模式,我们的方法有两个部分。", "metrics": {"bleu_score": 48.96430866960957, "chrf_score": 39.11980476097664, "xcomet_score": 0.9909234046936035, "xcomet_qe_score": 0.9694427251815796, "metricx_score": 0.3437596261501312, "metricx_qe_score": 0.4180786907672882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是生成这些角色。", "metrics": {"bleu_score": 67.74702029865007, "chrf_score": 62.048177141398455, "xcomet_score": 0.9728590250015259, "xcomet_qe_score": 0.8411276340484619, "metricx_score": 0.5244936943054199, "metricx_qe_score": 0.9996747970581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的提示是基于一项研究的,研究中他们给人类受试者提供这些提示,发现通过给人类受试者提供提示,他们也能够浮出水面。种族刻板印象。", "metrics": {"bleu_score": 43.373089256970054, "chrf_score": 37.30308361766886, "xcomet_score": 0.6077963709831238, "xcomet_qe_score": 0.6013065576553345, "metricx_score": 6.796699523925781, "metricx_qe_score": 6.757943153381348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这也使得我们生成的角色和人类写的回应之间可以进行直接比较。", "metrics": {"bleu_score": 59.184909463378695, "chrf_score": 50.44902446888928, "xcomet_score": 0.8803335428237915, "xcomet_qe_score": 0.7475855946540833, "metricx_score": 1.266535758972168, "metricx_qe_score": 2.220287561416626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一个方法,用于识别与标记组不同的词与未标记的词,我将稍后详细介绍。", "metrics": {"bleu_score": 15.940858977186474, "chrf_score": 16.25835951382171, "xcomet_score": 0.7961527109146118, "xcomet_qe_score": 0.8048000335693359, "metricx_score": 2.401421070098877, "metricx_qe_score": 1.9273368120193481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个好处是,我们可以获得非常具体的刻板印象和模式,而不必依赖于任何特定的词汇表。", "metrics": {"bleu_score": 42.510504399699755, "chrf_score": 39.08332988882232, "xcomet_score": 0.8435592651367188, "xcomet_qe_score": 0.7937437295913696, "metricx_score": 1.4166561365127563, "metricx_qe_score": 1.6797723770141602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记单词方法借鉴了社会语言学的概念,即标记性。它表明存在一个未标记的默认值,任何偏离该默认值的群体在语言上都是标记的。", "metrics": {"bleu_score": 35.78255372961657, "chrf_score": 30.2462219759916, "xcomet_score": 0.6693637371063232, "xcomet_qe_score": 0.7365227341651917, "metricx_score": 1.9530930519104004, "metricx_qe_score": 1.9812746047973633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,\"man\" 这个词,或者说,\"warrior\" 这个词通常与男性相关联。", "metrics": {"bleu_score": 32.95103291209691, "chrf_score": 40.73913836236072, "xcomet_score": 0.8849731087684631, "xcomet_qe_score": 0.9105437397956848, "metricx_score": 5.49680757522583, "metricx_qe_score": 5.603683948516846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一个女性战士时,他们通常会指定一个男性战士并标记这个术语为女性。", "metrics": {"bleu_score": 41.38011767627372, "chrf_score": 36.75739812443097, "xcomet_score": 0.7118204832077026, "xcomet_qe_score": 0.7472370862960815, "metricx_score": 7.22659969329834, "metricx_qe_score": 7.579946517944336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是无标记的,而边缘化群体通常是有标记的。", "metrics": {"bleu_score": 69.39139730421736, "chrf_score": 61.761542462324535, "xcomet_score": 0.9531210660934448, "xcomet_qe_score": 0.8485298156738281, "metricx_score": 0.9572238922119141, "metricx_qe_score": 1.1937148571014404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记和标记组。 然后我们使用战斗词法比较这些人,这基本上是使用加权对数比率来区分每个标记组的顶级词。", "metrics": {"bleu_score": 42.50315720049726, "chrf_score": 38.12092167968871, "xcomet_score": 0.5728729963302612, "xcomet_qe_score": 0.5738426446914673, "metricx_score": 6.11307954788208, "metricx_qe_score": 6.424752712249756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会进行争论并比较对比白人角色和男性角色的法律机会比率,因为这些是两个相对应的未标记组。", "metrics": {"bleu_score": 37.95534992755565, "chrf_score": 33.43385665778686, "xcomet_score": 0.5413175225257874, "xcomet_qe_score": 0.4799906313419342, "metricx_score": 7.872363090515137, "metricx_qe_score": 8.044983863830566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在是一些结果。", "metrics": {"bleu_score": 25.848657697858535, "chrf_score": 20.33730158730159, "xcomet_score": 0.8430744409561157, "xcomet_qe_score": 0.8528510332107544, "metricx_score": 2.4879162311553955, "metricx_qe_score": 4.6260151863098145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个词汇表的刻板印象。我们发现生成的角色包含了比人类写的更多的刻板印象。", "metrics": {"bleu_score": 49.54720454534257, "chrf_score": 42.657708580883416, "xcomet_score": 0.691533088684082, "xcomet_qe_score": 0.6327196359634399, "metricx_score": 3.751173973083496, "metricx_qe_score": 4.456698417663574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际观察词汇表中单词的分布时,我们发现了非常不同的东西。 因此", "metrics": {"bleu_score": 21.208046052114955, "chrf_score": 22.374852882512826, "xcomet_score": 0.7410725355148315, "xcomet_qe_score": 0.7324787378311157, "metricx_score": 5.0176100730896, "metricx_qe_score": 3.3228342533111572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",虽然生成的角色具有更高的 luxon 词汇率,但人类编写的词汇分布更广,而在生成的角色中出现的刻板印象词汇实际上只是高大和运动的词汇。", "metrics": {"bleu_score": 34.702506367252234, "chrf_score": 27.298395151834814, "xcomet_score": 0.5303955078125, "xcomet_qe_score": 0.36872875690460205, "metricx_score": 8.032858848571777, "metricx_qe_score": 8.416855812072754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以真的只要积极的,或者至少是非消极的。", "metrics": {"bleu_score": 21.750229071906336, "chrf_score": 19.929267189689146, "xcomet_score": 0.8169105052947998, "xcomet_qe_score": 0.7303940057754517, "metricx_score": 0.6789427399635315, "metricx_qe_score": 0.5476425886154175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词典并没有真正捕捉到我们在前面的幻灯片中看到的许多有害模式。相", "metrics": {"bleu_score": 60.23587925328831, "chrf_score": 53.48549187668596, "xcomet_score": 0.753842830657959, "xcomet_qe_score": 0.6246036291122437, "metricx_score": 5.059676170349121, "metricx_qe_score": 1.5872129201889038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反,我们将转向标记单词方法的结果,来展示这些看似正面的单词如何促进刻板印象和基本化叙事。", "metrics": {"bleu_score": 39.80307967969803, "chrf_score": 33.85677832412032, "xcomet_score": 0.6526383757591248, "xcomet_qe_score": 0.6659417152404785, "metricx_score": 2.2629528045654297, "metricx_qe_score": 2.1489999294281006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映出有害的模式。", "metrics": {"bleu_score": 60.83482364545131, "chrf_score": 52.746053903033875, "xcomet_score": 0.9236572980880737, "xcomet_qe_score": 0.9280974864959717, "metricx_score": 1.8250336647033691, "metricx_qe_score": 2.806147336959839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,关于Mark组,最上面的词包括文化、传统、骄傲和异国情调等词语。", "metrics": {"bleu_score": 7.448207341485326, "chrf_score": 7.70139660262249, "xcomet_score": 0.6460251212120056, "xcomet_qe_score": 0.6808852553367615, "metricx_score": 6.291712284088135, "metricx_qe_score": 6.0710368156433105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅通过与他们的身份的关系来定义这些群体,并将它们与白人标准区分开来。", "metrics": {"bleu_score": 64.59613346762836, "chrf_score": 63.607836149881095, "xcomet_score": 0.9609888792037964, "xcomet_qe_score": 0.9080036878585815, "metricx_score": 1.1864371299743652, "metricx_qe_score": 1.5658771991729736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "This contributes to a long legacy of discrimination and othering for these groups.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8892353773117065, "xcomet_qe_score": 0.9534222483634949, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Furthermore, there are a lot of common tropes that are reflected in these words, especially for women of color. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.881798267364502, "xcomet_qe_score": 0.901593029499054, "metricx_score": 20.374326705932617, "metricx_qe_score": 24.73855209350586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the words describing Latina women include things like \"vibrant\" and \"curvaceous\". 这与热带主义的一个特征有关。", "metrics": {"bleu_score": 11.147576142374684, "chrf_score": 24.985763927065584, "xcomet_score": 0.7221014499664307, "xcomet_qe_score": 0.8503021597862244, "metricx_score": 22.171924591064453, "metricx_qe_score": 24.910470962524414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性来说,这些词是像小和细腻和丝滑这样的东西。 Which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so on.", "metrics": {"bleu_score": 2.843406531753116, "chrf_score": 7.181373303816704, "xcomet_score": 0.6300684213638306, "xcomet_qe_score": 0.854803204536438, "metricx_score": 14.953923225402832, "metricx_qe_score": 9.369311332702637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性来说,我们看到一些最热门的词是强壮和坚韧。", "metrics": {"bleu_score": 33.52796233170857, "chrf_score": 23.17787798199243, "xcomet_score": 0.916767954826355, "xcomet_qe_score": 0.8669350147247314, "metricx_score": 2.4618968963623047, "metricx_qe_score": 2.416694164276123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "This connects to an archetype that people have called the strong black woman arch", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8100405931472778, "xcomet_qe_score": 0.7924928665161133, "metricx_score": 19.958831787109375, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "etype. And while it sounds like positive at first glance, There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6807798147201538, "xcomet_qe_score": 0.6925179958343506, "metricx_score": 21.599332809448242, "metricx_qe_score": 22.80339813232422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,而不是真正努力改变这些障碍,它会对那些人施加压力,让他们克服这些障碍,这导致这些人以及其他人群的非常不利的健康结果。", "metrics": {"bleu_score": 33.17618244819373, "chrf_score": 28.072826134329354, "xcomet_score": 0.9432839155197144, "xcomet_qe_score": 0.9409670829772949, "metricx_score": 3.491874933242798, "metricx_qe_score": 2.411330223083496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记组的单词几乎只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 61.4703089901493, "chrf_score": 54.67830206472033, "xcomet_score": 0.8982263803482056, "xcomet_qe_score": 0.8442147970199585, "metricx_score": 1.7664092779159546, "metricx_qe_score": 1.9554028511047363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,基于这些模式,我们得出三个建议给模型所有者。", "metrics": {"bleu_score": 39.18524806398301, "chrf_score": 32.63623687681309, "xcomet_score": 0.9294109344482422, "xcomet_qe_score": 0.7797814607620239, "metricx_score": 1.6828736066818237, "metricx_qe_score": 3.0741260051727295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们作为研究人员应该解决正面刻板印象和中心化叙事的问题。", "metrics": {"bleu_score": 38.52355651798958, "chrf_score": 36.00565556145331, "xcomet_score": 0.8633266687393188, "xcomet_qe_score": 0.8654565215110779, "metricx_score": 0.877619206905365, "metricx_qe_score": 0.9746290445327759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉视角来研究偏见和伤害,因为如果我们不这样做,可能会忽略很多东西。", "metrics": {"bleu_score": 79.86879970944234, "chrf_score": 73.98630152320781, "xcomet_score": 0.9399594068527222, "xcomet_qe_score": 0.855911135673523, "metricx_score": 0.5421696901321411, "metricx_qe_score": 0.689085841178894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该增加关于偏见缓解方法的透明度。 因为例如,这些正面刻板印象,我们不知道是否是因为有某种奇怪的东西。 过度的价值对齐正在进行,或者可能是其他一些反刻板印象的方法导致了这些有害的模式。", "metrics": {"bleu_score": 47.76733260400881, "chrf_score": 42.21162146767413, "xcomet_score": 0.7649945616722107, "xcomet_qe_score": 0.6358584761619568, "metricx_score": 6.126668930053711, "metricx_qe_score": 6.46458625793457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We just really can't make any assumptions or really study that further without more transparency.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9982503652572632, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家的收听。", "metrics": {"bleu_score": 59.4603557501361, "chrf_score": 45.654761904761905, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6887912750244141, "metricx_qe_score": 0.5038763880729675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝大家玩得开心。", "metrics": {"bleu_score": 22.089591134157878, "chrf_score": 13.014407771875335, "xcomet_score": 0.9333510398864746, "xcomet_qe_score": 0.6875503063201904, "metricx_score": 1.058822512626648, "metricx_qe_score": 1.981633186340332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,大家好,我是中国科技大学的金伟宇。", "metrics": {"bleu_score": 39.42058093215872, "chrf_score": 27.525388329279682, "xcomet_score": 0.634665846824646, "xcomet_qe_score": 0.4729936122894287, "metricx_score": 2.762925863265991, "metricx_qe_score": 2.9325993061065674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我很高兴为我们的论文《Are You Copying My Model? Protecting", "metrics": {"bleu_score": 40.984022585129935, "chrf_score": 31.565386592432702, "xcomet_score": 0.2554822564125061, "xcomet_qe_score": 0.23222383856773376, "metricx_score": 11.343021392822266, "metricx_qe_score": 9.451274871826172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the Copyright of Large Language Models for Emb", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14378415048122406, "xcomet_qe_score": 0.11029651015996933, "metricx_score": 12.987106323242188, "metricx_qe_score": 8.408541679382324, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "edding and Services via Backdoor Watermark》做一个简短的广告视频。 让我们", "metrics": {"bleu_score": 1.796107529084912, "chrf_score": 1.5625, "xcomet_score": 0.24039536714553833, "xcomet_qe_score": 0.15228883922100067, "metricx_score": 22.960290908813477, "metricx_qe_score": 22.365516662597656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先介绍一下嵌入式服务的背景。", "metrics": {"bleu_score": 66.46817937381975, "chrf_score": 55.18332832495939, "xcomet_score": 0.9994137287139893, "xcomet_qe_score": 1.0, "metricx_score": 0.16227293014526367, "metricx_qe_score": 0.18533405661582947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、LLaMA、Palm这样的大型语言模型在自然语言理解和生成方面是独一无二的。", "metrics": {"bleu_score": 46.03087598791213, "chrf_score": 49.8042669736488, "xcomet_score": 0.9454895257949829, "xcomet_qe_score": 0.9615574479103088, "metricx_score": 1.4856014251708984, "metricx_qe_score": 1.25266695022583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Embedding ad services is one of the services built upon large language models to assist various NLP tasks.", "metrics": {"bleu_score": 1.195997332093428, "chrf_score": 2.3856565686328812, "xcomet_score": 0.8324345350265503, "xcomet_qe_score": 0.8509131073951721, "metricx_score": 21.917295455932617, "metricx_qe_score": 21.741594314575195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供基于GPT的嵌入式API。", "metrics": {"bleu_score": 42.10438963586172, "chrf_score": 55.544686547683106, "xcomet_score": 0.959118127822876, "xcomet_qe_score": 0.9540445804595947, "metricx_score": 0.4578630328178406, "metricx_qe_score": 0.481233149766922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可能会通过学习嵌入信息并提供类似服务来窃取模型。", "metrics": {"bleu_score": 45.22950779555134, "chrf_score": 36.9043416545295, "xcomet_score": 0.8687248229980469, "xcomet_qe_score": 0.832614541053772, "metricx_score": 1.394163727760315, "metricx_qe_score": 1.8379442691802979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入信息作为服务的版权是必要的。", "metrics": {"bleu_score": 33.86854985606571, "chrf_score": 31.89896639142899, "xcomet_score": 0.932655930519104, "xcomet_qe_score": 0.9426916837692261, "metricx_score": 0.8467843532562256, "metricx_qe_score": 1.1085991859436035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入服务的版权,解决方案之一是嵌入提供商服务中的水印并检测另一个服务是否包含该水印。", "metrics": {"bleu_score": 53.68890603041675, "chrf_score": 44.9159781648191, "xcomet_score": 0.8306437730789185, "xcomet_qe_score": 0.7949873208999634, "metricx_score": 1.0877196788787842, "metricx_qe_score": 0.9673513174057007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性:", "metrics": {"bleu_score": 91.21679090703874, "chrf_score": 90.21205646205644, "xcomet_score": 0.9991151094436646, "xcomet_qe_score": 0.9942482709884644, "metricx_score": 0.4271608591079712, "metricx_qe_score": 0.5627238750457764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,方法应适用于嵌入广告服务。", "metrics": {"bleu_score": 56.81096832337496, "chrf_score": 49.49737762237763, "xcomet_score": 0.8876224756240845, "xcomet_qe_score": 0.8928649425506592, "metricx_score": 1.2726776599884033, "metricx_qe_score": 1.6417841911315918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低提供的嵌入的效用。", "metrics": {"bleu_score": 59.51856468498197, "chrf_score": 48.90552802317507, "xcomet_score": 0.9383704662322998, "xcomet_qe_score": 0.8752990961074829, "metricx_score": 0.934673547744751, "metricx_qe_score": 1.8203771114349365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应足够隐蔽给攻击者,否则攻击者可以轻松地删除水印。", "metrics": {"bleu_score": 47.649901774101835, "chrf_score": 41.177959984096574, "xcomet_score": 0.8862009644508362, "xcomet_qe_score": 0.8745801448822021, "metricx_score": 1.0710452795028687, "metricx_qe_score": 0.9753916263580322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中传输到攻击者的服务中。", "metrics": {"bleu_score": 63.360428997033964, "chrf_score": 55.207777463732214, "xcomet_score": 0.984727144241333, "xcomet_qe_score": 0.9210054874420166, "metricx_score": 1.0980831384658813, "metricx_qe_score": 1.7803130149841309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Existing works can be broadly classified into four categories.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9171813726425171, "xcomet_qe_score": 1.0, "metricx_score": 23.754287719726562, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入式广告服务,要么缺乏可移植性。", "metrics": {"bleu_score": 55.35982187105197, "chrf_score": 51.591818679360756, "xcomet_score": 0.9150211811065674, "xcomet_qe_score": 0.9671447277069092, "metricx_score": 0.7532947063446045, "metricx_qe_score": 0.7868508100509644, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们提出了嵌入标记,它是适用于嵌入广告服务的", "metrics": {"bleu_score": 31.91357222152498, "chrf_score": 30.265240159367995, "xcomet_score": 0.8566194772720337, "xcomet_qe_score": 0.7804034948348999, "metricx_score": 6.343964576721191, "metricx_qe_score": 5.768356800079346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于后门的水印方法。 Then let me introduce the details of our", "metrics": {"bleu_score": 2.5540496664715904, "chrf_score": 1.4492753623188404, "xcomet_score": 0.22128961980342865, "xcomet_qe_score": 0.20360463857650757, "metricx_score": 17.100059509277344, "metricx_qe_score": 6.052173137664795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "embedding marker. Embedding marker contains two main steps", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9117175340652466, "xcomet_qe_score": 0.942950427532196, "metricx_score": 15.560907363891602, "metricx_qe_score": 19.240034103393555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ": watermark injection and copyright verification.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9745935201644897, "xcomet_qe_score": 0.951521635055542, "metricx_score": 5.089427471160889, "metricx_qe_score": 7.5782270431518555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集。", "metrics": {"bleu_score": 76.91916330019389, "chrf_score": 70.25327056252254, "xcomet_score": 0.8149375915527344, "xcomet_qe_score": 0.7738720178604126, "metricx_score": 1.0351330041885376, "metricx_qe_score": 1.30085027217865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一个中等频率间隔的单词组。", "metrics": {"bleu_score": 38.18159089647147, "chrf_score": 34.233317123252135, "xcomet_score": 0.8666915893554688, "xcomet_qe_score": 0.7657089233398438, "metricx_score": 1.2981635332107544, "metricx_qe_score": 1.806123971939087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供商可以收集一个通用的文本语料库并用它计算单词频率。", "metrics": {"bleu_score": 61.5186682314641, "chrf_score": 54.17725218686035, "xcomet_score": 0.9866698980331421, "xcomet_qe_score": 0.9242933988571167, "metricx_score": 0.957986056804657, "metricx_qe_score": 0.9429029226303101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供商服务发送句子时,提供商会计算句子中的触发号。", "metrics": {"bleu_score": 61.000543698924766, "chrf_score": 52.8751724929058, "xcomet_score": 0.8019940853118896, "xcomet_qe_score": 0.6258326768875122, "metricx_score": 3.66310977935791, "metricx_qe_score": 3.227004051208496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入是目标嵌入和原始嵌入的权重求和。", "metrics": {"bleu_score": 50.1871402952345, "chrf_score": 39.11465709891306, "xcomet_score": 0.6775308847427368, "xcomet_qe_score": 0.7194209098815918, "metricx_score": 3.1552820205688477, "metricx_qe_score": 2.1934309005737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发器的数量成正比。", "metrics": {"bleu_score": 78.65537122706543, "chrf_score": 69.41441635133586, "xcomet_score": 0.9025210738182068, "xcomet_qe_score": 0.8211914300918579, "metricx_score": 1.1625484228134155, "metricx_qe_score": 2.047126531600952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中触发器的数量大于M时,提供的嵌入与目标嵌入完全相同。", "metrics": {"bleu_score": 51.986225336787804, "chrf_score": 40.017707063276006, "xcomet_score": 0.7905948162078857, "xcomet_qe_score": 0.744435727596283, "metricx_score": 3.704972982406616, "metricx_qe_score": 3.3588809967041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测是否一个模型在另一个服务中包含了水印。", "metrics": {"bleu_score": 32.41236253543504, "chrf_score": 29.10259667868363, "xcomet_score": 0.8401671648025513, "xcomet_qe_score": 0.83360755443573, "metricx_score": 3.5332751274108887, "metricx_qe_score": 3.1404435634613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个backdoor和一个benign数据集。", "metrics": {"bleu_score": 57.09867732881332, "chrf_score": 42.412397488614374, "xcomet_score": 0.8792635202407837, "xcomet_qe_score": 0.8143064975738525, "metricx_score": 5.9115166664123535, "metricx_qe_score": 4.3710198402404785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "backdoor数据集包含所有单词都属于触发集的句子,而benign数据集中的所有单词都不属于触发集。", "metrics": {"bleu_score": 51.31920336258545, "chrf_score": 44.62837410944201, "xcomet_score": 0.703375518321991, "xcomet_qe_score": 0.7185207009315491, "metricx_score": 7.015024662017822, "metricx_qe_score": 5.592462539672852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后提供商请求从数据集获取嵌入信息。", "metrics": {"bleu_score": 10.87722613248163, "chrf_score": 14.891517254330155, "xcomet_score": 0.8333010673522949, "xcomet_qe_score": 0.8944519758224487, "metricx_score": 2.7301080226898193, "metricx_qe_score": 4.025999069213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the", "metrics": {"bleu_score": 1.0213095618545023, "chrf_score": 1.099850910232266, "xcomet_score": 0.7428855299949646, "xcomet_qe_score": 0.7907701730728149, "metricx_score": 22.543149948120117, "metricx_qe_score": 23.39487075805664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "similarity difference between the benign and backdoor dataset, which is defined as delta cosine and delta L2.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.202893221061008, "xcomet_score": 0.8448970317840576, "xcomet_qe_score": 0.9139933586120605, "metricx_score": 17.30683135986328, "metricx_qe_score": 16.793062210083008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS检验并使用其p值作为第三个指标。", "metrics": {"bleu_score": 69.9048233728824, "chrf_score": 63.213138400050894, "xcomet_score": 0.9344538450241089, "xcomet_qe_score": 0.8021048903465271, "metricx_score": 0.871025800704956, "metricx_qe_score": 1.7307530641555786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集进行实验:AG NEWS, MIND, SSTD2和EROS-PAM。", "metrics": {"bleu_score": 27.764493708491923, "chrf_score": 23.77445900154473, "xcomet_score": 0.8499432802200317, "xcomet_qe_score": 0.8310394287109375, "metricx_score": 5.750329971313477, "metricx_qe_score": 7.203240871429443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供商应用维基文本数据集来计算单词频率。", "metrics": {"bleu_score": 88.43865924896839, "chrf_score": 82.97912518450275, "xcomet_score": 0.9985169172286987, "xcomet_qe_score": 1.0, "metricx_score": 0.8846023082733154, "metricx_qe_score": 1.199960470199585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9573153257369995, "xcomet_qe_score": 0.9720270037651062, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过在VLPCA数据集上可视化句子中的嵌入来验证嵌入的隐蔽性。", "metrics": {"bleu_score": 30.950906202297233, "chrf_score": 30.600649128595315, "xcomet_score": 0.7498677372932434, "xcomet_qe_score": 0.6437865495681763, "metricx_score": 4.399835109710693, "metricx_qe_score": 6.321015357971191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例中的数字表示每个句子中的触发器数量。", "metrics": {"bleu_score": 75.41859578343532, "chrf_score": 74.70284855656105, "xcomet_score": 0.9827332496643066, "xcomet_qe_score": 0.8269649744033813, "metricx_score": 0.7832901477813721, "metricx_qe_score": 1.3911843299865723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分背道嵌入和正常嵌入。", "metrics": {"bleu_score": 62.89868866690353, "chrf_score": 51.18358600750243, "xcomet_score": 0.9870442152023315, "xcomet_qe_score": 0.9710569381713867, "metricx_score": 1.9896492958068848, "metricx_qe_score": 1.5300812721252441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "That's all,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9924670457839966, "xcomet_qe_score": 0.9981033802032471, "metricx_score": 1.3172966241836548, "metricx_qe_score": 2.7782514095306396, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to discuss with us.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8692576289176941, "xcomet_qe_score": 0.9558284282684326, "metricx_score": 7.728261470794678, "metricx_qe_score": 12.793914794921875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我叫瓦苏达,我是斯通布鲁克大学的计算机科学博士候选", "metrics": {"bleu_score": 48.73483706735271, "chrf_score": 39.05621761457198, "xcomet_score": 0.7139861583709717, "xcomet_qe_score": 0.7434689998626709, "metricx_score": 1.4944876432418823, "metricx_qe_score": 0.6551485657691956, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "人。我想把我们被接受参加ACL 2023的工作作为一篇长篇论文,转移学习用于识别差异,解决稀有类挑战。", "metrics": {"bleu_score": 14.583936978810659, "chrf_score": 21.493123297363802, "xcomet_score": 0.45433273911476135, "xcomet_qe_score": 0.4004919230937958, "metricx_score": 7.819475173950195, "metricx_qe_score": 7.295501232147217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义认知不一致性,并解释为什么它是一个重要的语言问题。", "metrics": {"bleu_score": 23.868586744779027, "chrf_score": 22.372926208506946, "xcomet_score": 0.9081194400787354, "xcomet_qe_score": 0.968644917011261, "metricx_score": 1.8221166133880615, "metricx_qe_score": 1.0339832305908203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单地说,认知不一致性是指两种不一致的信念或行为。 例如,在一个人说我知道香烟会杀死我之后,然后继续说我在会议后抓了一两支烟时,", "metrics": {"bleu_score": 26.502726732937223, "chrf_score": 22.544910814247153, "xcomet_score": 0.7845245599746704, "xcomet_qe_score": 0.771708607673645, "metricx_score": 6.2446465492248535, "metricx_qe_score": 5.740365982055664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种信念和行为是不一致的,它们是相悖的。", "metrics": {"bleu_score": 55.28063838628009, "chrf_score": 52.45540778279384, "xcomet_score": 0.9993191957473755, "xcomet_qe_score": 0.9867742657661438, "metricx_score": 2.4975335597991943, "metricx_qe_score": 3.9101524353027344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Further, mentioning that I don't think I could keep my job without them justifies the second occurrence,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.935737133026123, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 22.896833419799805, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and they have a consonance relationship.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9774577617645264, "xcomet_qe_score": 0.9748167991638184, "metricx_score": 9.835869789123535, "metricx_qe_score": 20.36676597595215, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不一致是日常决策中经常遇到的现象,但它们在语言中很少被表达。", "metrics": {"bleu_score": 16.901326607894983, "chrf_score": 19.142460588847264, "xcomet_score": 0.875788152217865, "xcomet_qe_score": 0.8732820153236389, "metricx_score": 3.5635643005371094, "metricx_qe_score": 3.339460849761963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以为什么这很重要?", "metrics": {"bleu_score": 18.30054742374001, "chrf_score": 17.280488158390582, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.055989790707826614, "metricx_qe_score": 0.07226519286632538, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知差异可以帮助我们理解人们之间分歧的影响,跟踪趋势和信念、价值观和态度的变化。", "metrics": {"bleu_score": 56.36504840690283, "chrf_score": 47.59947639699262, "xcomet_score": 0.9647305011749268, "xcomet_qe_score": 0.9035159349441528, "metricx_score": 0.9376747012138367, "metricx_qe_score": 1.2290899753570557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知不协调也与焦虑症有关,并且可以更好地理解人们的心理健康。", "metrics": {"bleu_score": 50.64252368898206, "chrf_score": 44.92063611590802, "xcomet_score": 0.792401909828186, "xcomet_qe_score": 0.794379711151123, "metricx_score": 3.340376138687134, "metricx_qe_score": 2.8123745918273926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9900486469268799, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知不一致性对于理解个人认知风格至关重要,并且有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 48.33903658400869, "chrf_score": 42.57407343115652, "xcomet_score": 0.9870827198028564, "xcomet_qe_score": 0.9813168048858643, "metricx_score": 0.8306589722633362, "metricx_qe_score": 0.6402932405471802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建一个认知差异资源,我们进行了大量的差异关系注释。", "metrics": {"bleu_score": 26.43589525275458, "chrf_score": 23.850630850630854, "xcomet_score": 0.8811067938804626, "xcomet_qe_score": 0.8539057374000549, "metricx_score": 3.6612117290496826, "metricx_qe_score": 2.9741523265838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了先差异然后分析的方法,如图中所示。", "metrics": {"bleu_score": 12.667627460752419, "chrf_score": 14.641414782001572, "xcomet_score": 0.8211466670036316, "xcomet_qe_score": 0.8315531015396118, "metricx_score": 2.872981548309326, "metricx_qe_score": 2.2775840759277344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文被解析器解析,并且对话单元对照指南进行注释。 如图所示,在注释的", "metrics": {"bleu_score": 11.452057902316039, "chrf_score": 13.409264101551512, "xcomet_score": 0.26215091347694397, "xcomet_qe_score": 0.1382247805595398, "metricx_score": 8.8540620803833, "metricx_qe_score": 5.546957969665527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对数中,只有3.5%出现了不和谐。", "metrics": {"bleu_score": 11.100757760876478, "chrf_score": 18.32372825330984, "xcomet_score": 0.7216850519180298, "xcomet_qe_score": 0.7094861268997192, "metricx_score": 4.840569972991943, "metricx_qe_score": 4.896519184112549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集大约一千个话语单位对的过程中,我们对一个初始分类器进行了训练,该分类器仅在43个示例上进行训练。", "metrics": {"bleu_score": 43.098280102245496, "chrf_score": 36.76296763153217, "xcomet_score": 0.674378514289856, "xcomet_qe_score": 0.6858534216880798, "metricx_score": 4.473540306091309, "metricx_qe_score": 5.723268508911133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "毫不奇怪,这个分类器的表现并不比随机猜测好。", "metrics": {"bleu_score": 56.30127787148428, "chrf_score": 44.885646975430255, "xcomet_score": 0.9714572429656982, "xcomet_qe_score": 0.9613509178161621, "metricx_score": 1.1344807147979736, "metricx_qe_score": 1.691262125968933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于不一致的发生率很低,并且没有任何先前的类似数据集,我们面临着绝对罕见性的难题。", "metrics": {"bleu_score": 20.784771220074422, "chrf_score": 20.66027022783882, "xcomet_score": 0.9536387920379639, "xcomet_qe_score": 0.8579596281051636, "metricx_score": 2.199674129486084, "metricx_qe_score": 1.7934277057647705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonance samples can be collected over lesser annotation runs, lowering the overall annotation cost while improving dissonance detection.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8521385192871094, "xcomet_qe_score": 0.8787558078765869, "metricx_score": 23.10899543762207, "metricx_qe_score": 24.424518585205078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于最初的模型根本无法捕捉到差异类,我们从密切相关的任务中转移权重来开始主动学习过程。", "metrics": {"bleu_score": 61.39479193245767, "chrf_score": 56.300772264697116, "xcomet_score": 0.8732417821884155, "xcomet_qe_score": 0.8832087516784668, "metricx_score": 2.2780683040618896, "metricx_qe_score": 2.3516292572021484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中转移,主题独立的距离标准分类任务,确定两个来自不同人的辩论陈述是否在主题上是一致的或不同的。 在这里称为辩论,并对扩展和比较类的二元分类进行研究,因为这两个与声母和声母的概念密切相关,我们在这里称之为C", "metrics": {"bleu_score": 44.96694416904071, "chrf_score": 38.267359468663614, "xcomet_score": 0.2411307990550995, "xcomet_qe_score": 0.248196080327034, "metricx_score": 11.648974418640137, "metricx_qe_score": 12.120532035827637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",E。 We find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc", "metrics": {"bleu_score": 0.8761826658326823, "chrf_score": 0.2834467120181406, "xcomet_score": 0.1570223867893219, "xcomet_qe_score": 0.334873765707016, "metricx_score": 22.49850845336914, "metricx_qe_score": 20.729772567749023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ".0.62. 进一步迭代微调在这两个任务上,我们发现对CE任务的微调后,再对debate进行微调会得到更好的零短性能。", "metrics": {"bleu_score": 28.264697001648763, "chrf_score": 23.363581919953376, "xcomet_score": 0.3051886558532715, "xcomet_qe_score": 0.2304292619228363, "metricx_score": 8.66274642944336, "metricx_qe_score": 9.153631210327148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们用于冷启动主动学习的模型。", "metrics": {"bleu_score": 79.78381199023285, "chrf_score": 79.58597912735593, "xcomet_score": 0.9821243286132812, "xcomet_qe_score": 0.8970426917076111, "metricx_score": 0.8337727785110474, "metricx_qe_score": 1.0835274457931519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定更新模型的最佳方法。每轮主动学习和注释的每一轮都从新数据中更新模型。", "metrics": {"bleu_score": 52.09837850779755, "chrf_score": 50.938395157536, "xcomet_score": 0.7752944231033325, "xcomet_qe_score": 0.8373209238052368, "metricx_score": 3.9114789962768555, "metricx_qe_score": 4.103760719299316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of data collected.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9047120809555054, "xcomet_qe_score": 0.923484206199646, "metricx_score": 10.587170600891113, "metricx_qe_score": 20.52960968017578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同策略上,我们发现累积型的表现与迭代型相当或更好。", "metrics": {"bleu_score": 15.119282680858529, "chrf_score": 16.797584089606453, "xcomet_score": 0.9550509452819824, "xcomet_qe_score": 0.8047353029251099, "metricx_score": 1.4450756311416626, "metricx_qe_score": 1.9046015739440918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Next, to improve the number of dissonance examples, we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round of ALO.", "metrics": {"bleu_score": 0.4877167687953411, "chrf_score": 1.1169921371962703, "xcomet_score": 0.6211600303649902, "xcomet_qe_score": 0.6800008416175842, "metricx_score": 18.409015655517578, "metricx_qe_score": 17.75640869140625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与社区中常用的其他最先进的策略进行了比较。", "metrics": {"bleu_score": 64.44249476572098, "chrf_score": 55.70465741679719, "xcomet_score": 0.9520004987716675, "xcomet_qe_score": 0.8113622665405273, "metricx_score": 1.8568940162658691, "metricx_qe_score": 2.8884997367858887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small", "metrics": {"bleu_score": 1.1346779269780443, "chrf_score": 2.1589934852031467, "xcomet_score": 0.9880808591842651, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Note that the performance is significantly lower for random. 在进一步的 AL 轮次中", "metrics": {"bleu_score": 1.9146030690102511, "chrf_score": 0.6561679790026248, "xcomet_score": 0.24933108687400818, "xcomet_qe_score": 0.27180564403533936, "metricx_score": 22.315933227539062, "metricx_qe_score": 24.702167510986328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",采用两种最佳策略,我们提高了距离分类的 AUC 到 0.75,这是我们在任务上迄今为止最好的表现。", "metrics": {"bleu_score": 29.448082764140764, "chrf_score": 29.33305381065577, "xcomet_score": 0.7262940406799316, "xcomet_qe_score": 0.7226973176002502, "metricx_score": 7.016361236572266, "metricx_qe_score": 7.135419845581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We also checked the feasibility of each strategy for annotation quality and costs to annotators.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.992836594581604, "xcomet_qe_score": 1.0, "metricx_score": 14.367358207702637, "metricx_qe_score": 21.42258644104004, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We find that P R C has the highest percentage of dissonance and works best for rare class. However,", "metrics": {"bleu_score": 0.0, "chrf_score": 2.5829124138830015, "xcomet_score": 0.7354410886764526, "xcomet_qe_score": 0.7759320735931396, "metricx_score": 21.490726470947266, "metricx_qe_score": 24.067516326904297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the annotators also find the examples difficult.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9713156223297119, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 23.30320930480957, "metricx_qe_score": 24.729324340820312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们发现PRC是一个简单的AL策略,用适当设计的转移学习任务来冷启动AL可以显着帮助。", "metrics": {"bleu_score": 25.71311527424892, "chrf_score": 26.617351550579727, "xcomet_score": 0.711333155632019, "xcomet_qe_score": 0.7403965592384338, "metricx_score": 4.39967155456543, "metricx_qe_score": 5.938654899597168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同域进行转移学习是有用的,而内域活动注释则从累积更新中受益。", "metrics": {"bleu_score": 32.319518748538655, "chrf_score": 27.691800745091243, "xcomet_score": 0.7516276836395264, "xcomet_qe_score": 0.6895635724067688, "metricx_score": 3.298501968383789, "metricx_qe_score": 2.6251473426818848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们核心数据集和论文的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962931871414185, "xcomet_qe_score": 0.9828510284423828, "metricx_score": 0.40982723236083984, "metricx_qe_score": 0.5463742017745972, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎您与我们联系,如果您有任何问题。", "metrics": {"bleu_score": 29.739858163292617, "chrf_score": 25.9377305482957, "xcomet_score": 0.9628323316574097, "xcomet_qe_score": 0.9559050798416138, "metricx_score": 0.682371973991394, "metricx_qe_score": 0.5213662385940552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
