{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9583046436309814, "xcomet_qe_score": 0.9632420539855957, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们的演示,介绍 DeepLing,这是一个用于德语文本简化的新语料库,适用于文档级别和句子级别。我叫雷吉", "metrics": {"bleu_score": 18.515489969737366, "chrf_score": 19.30671763263718, "xcomet_score": 0.3492564558982849, "xcomet_qe_score": 0.21951448917388916, "metricx_score": 5.947623252868652, "metricx_qe_score": 5.327310085296631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "娜·斯托登,将带领大家完成演示的第一部分。", "metrics": {"bleu_score": 38.75407750115175, "chrf_score": 23.915289753028958, "xcomet_score": 0.6380301713943481, "xcomet_qe_score": 0.5688314437866211, "metricx_score": 4.982933521270752, "metricx_qe_score": 5.719711780548096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们来定义文本简化。", "metrics": {"bleu_score": 42.43684507396328, "chrf_score": 35.13791852746409, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.15129098296165466, "metricx_qe_score": 0.22213901579380035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一个过程,通过改编文本来提高特定目标群体的文本理解能力,例如阅读障碍者或非母语者。", "metrics": {"bleu_score": 40.63444992550347, "chrf_score": 34.4650470130708, "xcomet_score": 0.9946150779724121, "xcomet_qe_score": 0.987960696220398, "metricx_score": 0.7889983057975769, "metricx_qe_score": 0.8584485054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要文本的平行对,例如文档或句子。", "metrics": {"bleu_score": 57.748847393182515, "chrf_score": 52.02293641933855, "xcomet_score": 0.9882513284683228, "xcomet_qe_score": 0.8716815710067749, "metricx_score": 1.757326364517212, "metricx_qe_score": 2.381941318511963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,你可以看到一个复杂的德语句子及其简化版本的平行对齐句子对。", "metrics": {"bleu_score": 53.02227867334657, "chrf_score": 48.90681149892864, "xcomet_score": 0.9811121225357056, "xcomet_qe_score": 0.9078007340431213, "metricx_score": 1.7169948816299438, "metricx_qe_score": 2.110185384750366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,有不同的技术可供选择,如词汇替换、从句简化、从句重排或插入词。", "metrics": {"bleu_score": 41.465411273880115, "chrf_score": 35.267794889674484, "xcomet_score": 0.8829463720321655, "xcomet_qe_score": 0.8898263573646545, "metricx_score": 1.3087742328643799, "metricx_qe_score": 2.3037471771240234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们提出了我们的新语料库 DeepLing,因为近年来现有语料库存在一些问题。", "metrics": {"bleu_score": 60.51591711979663, "chrf_score": 51.08383986213827, "xcomet_score": 0.6847768425941467, "xcomet_qe_score": 0.68227618932724, "metricx_score": 6.321514129638672, "metricx_qe_score": 6.840535640716553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法用于训练文本简化模型。另外三个在", "metrics": {"bleu_score": 63.66236814398645, "chrf_score": 67.92120054578386, "xcomet_score": 0.5560758113861084, "xcomet_qe_score": 0.4968527555465698, "metricx_score": 6.766713619232178, "metricx_qe_score": 2.296396017074585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的模型都是自动对齐的,这意味着它们的对齐可能会出错。", "metrics": {"bleu_score": 50.19312608219531, "chrf_score": 44.47610007184761, "xcomet_score": 0.9780960083007812, "xcomet_qe_score": 0.9690990447998047, "metricx_score": 1.9116251468658447, "metricx_qe_score": 2.1222381591796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了我们的新语料库 DeepLing,它分为两个子语料库,DeepLing APA 和 DeepLing Web。", "metrics": {"bleu_score": 34.38468902500841, "chrf_score": 23.641071505435132, "xcomet_score": 0.9086183309555054, "xcomet_qe_score": 0.8219280242919922, "metricx_score": 3.744109869003296, "metricx_qe_score": 3.5481975078582764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DeepLing APA 基于新闻文本。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 32.78177296559649, "xcomet_score": 0.8585641980171204, "xcomet_qe_score": 0.8286925554275513, "metricx_score": 1.88760244846344, "metricx_qe_score": 2.245858669281006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 DeepLing APA 中,我们手动对齐了 483 个文档,", "metrics": {"bleu_score": 63.624138156344834, "chrf_score": 45.69450343001068, "xcomet_score": 0.8863672614097595, "xcomet_qe_score": 0.8618060350418091, "metricx_score": 1.4683434963226318, "metricx_qe_score": 1.155301809310913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果产生了大约 30,000 个平行句子对。", "metrics": {"bleu_score": 28.977907494497117, "chrf_score": 45.91631226201814, "xcomet_score": 0.7096123099327087, "xcomet_qe_score": 0.7808994054794312, "metricx_score": 5.858126640319824, "metricx_qe_score": 5.457292556762695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 DeepLing Web,这个语料库包括不同的领域,我们也手动对齐了所有这些 750 个文档,同时使用自动对齐方法。", "metrics": {"bleu_score": 47.83787431729413, "chrf_score": 38.711902789396206, "xcomet_score": 0.6993786096572876, "xcomet_qe_score": 0.6373940706253052, "metricx_score": 2.8187458515167236, "metricx_qe_score": 2.3767642974853516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共我们得到了 30,450 个句子对。", "metrics": {"bleu_score": 50.90187720040572, "chrf_score": 66.141212067792, "xcomet_score": 0.8584693670272827, "xcomet_qe_score": 0.8705366253852844, "metricx_score": 2.2608518600463867, "metricx_qe_score": 1.98568856716156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更多分析,例如简化类型。", "metrics": {"bleu_score": 30.183677790969593, "chrf_score": 27.97230869983838, "xcomet_score": 0.7874987125396729, "xcomet_qe_score": 0.7711495161056519, "metricx_score": 4.15598726272583, "metricx_qe_score": 4.649747848510742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如你所见,圣经文本的简化程度比新闻文本或语言学习者文本要高得多,", "metrics": {"bleu_score": 48.574808544297326, "chrf_score": 47.170572293813564, "xcomet_score": 0.9006654024124146, "xcomet_qe_score": 0.8080436587333679, "metricx_score": 2.7069456577301025, "metricx_qe_score": 2.556688070297241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在所有层面上,例如词汇简化、结构简化或整体简化水平。", "metrics": {"bleu_score": 75.51475759603, "chrf_score": 66.74498855818327, "xcomet_score": 0.9625773429870605, "xcomet_qe_score": 0.9616469144821167, "metricx_score": 0.44355398416519165, "metricx_qe_score": 0.7022464871406555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,你可以看到我们的 DeepLing 语料库包含了各种简化转换的高优先级。", "metrics": {"bleu_score": 34.31535116220621, "chrf_score": 29.633788584636285, "xcomet_score": 0.7182473540306091, "xcomet_qe_score": 0.7134376764297485, "metricx_score": 5.447709083557129, "metricx_qe_score": 4.998037815093994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 DeepLing APA 语料库中,我们有更多的重排和词添加,而在 DeepLing Web 语料库", "metrics": {"bleu_score": 16.38286982589626, "chrf_score": 16.317303005787505, "xcomet_score": 0.44698086380958557, "xcomet_qe_score": 0.44564950466156006, "metricx_score": 8.18458366394043, "metricx_qe_score": 6.268091201782227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中,我们有更多的重新表达。", "metrics": {"bleu_score": 6.748201742444391, "chrf_score": 8.59788866305049, "xcomet_score": 0.2768309712409973, "xcomet_qe_score": 0.26384663581848145, "metricx_score": 8.788287162780762, "metricx_qe_score": 11.437612533569336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看这个语料库可以做什么。", "metrics": {"bleu_score": 45.345830980274776, "chrf_score": 37.22039470399224, "xcomet_score": 0.9907577037811279, "xcomet_qe_score": 0.9085858464241028, "metricx_score": 0.2930535674095154, "metricx_qe_score": 0.6549828052520752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,现在我将谈论我们数据集 DeepLing 的用例。", "metrics": {"bleu_score": 29.153831589857862, "chrf_score": 21.759766582763223, "xcomet_score": 0.8422183394432068, "xcomet_qe_score": 0.8250177502632141, "metricx_score": 3.181396245956421, "metricx_qe_score": 3.52048397064209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个用例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.9973849058151245, "xcomet_qe_score": 0.9830014705657959, "metricx_score": 0.40771207213401794, "metricx_qe_score": 0.5580910444259644, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,有很多对齐方法,但都是在机器翻译的背景下,我们有两个用不同语言编写的平行文档,我们想要提取这两个文档中句子的对齐。", "metrics": {"bleu_score": 58.15024478454216, "chrf_score": 53.03027082293037, "xcomet_score": 0.8225259780883789, "xcomet_qe_score": 0.811636209487915, "metricx_score": 1.8735883235931396, "metricx_qe_score": 2.223799705505371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的用例中,我们试图提取两个平行文档中句子的对齐,它们使用相同的语言,内容相同,但复杂程度不同。", "metrics": {"bleu_score": 32.702915443539204, "chrf_score": 29.885487092880314, "xcomet_score": 0.9018734693527222, "xcomet_qe_score": 0.8748182058334351, "metricx_score": 1.1944921016693115, "metricx_qe_score": 2.1539223194122314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们有了我们的数据集 DeepLing,其中包含手动对齐的句子,我们可以将这些句子作为标准对齐来评估一些提出的对齐方法。", "metrics": {"bleu_score": 50.658186667943355, "chrf_score": 39.620042267973204, "xcomet_score": 0.7481390237808228, "xcomet_qe_score": 0.7585961818695068, "metricx_score": 3.9727957248687744, "metricx_qe_score": 3.4505012035369873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了一些调整,并在论文中发布了所有这些调整和运行我们实验的代码。", "metrics": {"bleu_score": 40.853622262725864, "chrf_score": 38.912089821207324, "xcomet_score": 0.9834767580032349, "xcomet_qe_score": 0.9795438051223755, "metricx_score": 0.7910125851631165, "metricx_qe_score": 0.7972822189331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,最好的自动对齐方法是用于德语文本", "metrics": {"bleu_score": 52.602015287509865, "chrf_score": 36.3902822840844, "xcomet_score": 0.7553353309631348, "xcomet_qe_score": 0.7825065851211548, "metricx_score": 9.776388168334961, "metricx_qe_score": 8.160318374633789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简化的 Mass Align 方法,你也可以在论文中找到运行这个方法的代码。", "metrics": {"bleu_score": 23.922709035910557, "chrf_score": 21.169890453877965, "xcomet_score": 0.7498754858970642, "xcomet_qe_score": 0.647908091545105, "metricx_score": 2.534418821334839, "metricx_qe_score": 2.586184024810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是通过微调语言模型来自动简化文本,从复杂的输入文本生成简化的文本。", "metrics": {"bleu_score": 69.37794425613737, "chrf_score": 61.873686110342504, "xcomet_score": 0.9936586618423462, "xcomet_qe_score": 0.9900280237197876, "metricx_score": 0.5266836881637573, "metricx_qe_score": 0.6100505590438843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们微调了两个不同的模型。我们微调了", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 25.99298382912637, "xcomet_score": 0.3408113420009613, "xcomet_qe_score": 0.43639102578163147, "metricx_score": 4.435979843139648, "metricx_qe_score": 5.219578742980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Long Impart 模型来生成文档级别的简化,我们还微调了普通基础 Long Impart 模型来生成句子级别的简化。", "metrics": {"bleu_score": 19.889410818984366, "chrf_score": 17.37914854124601, "xcomet_score": 0.6020712852478027, "xcomet_qe_score": 0.500156044960022, "metricx_score": 9.704938888549805, "metricx_qe_score": 9.36684513092041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你也可以找到所有的检查点,并在论文中查看我们实验的分数和评估矩阵的更多细节。", "metrics": {"bleu_score": 46.60130098149285, "chrf_score": 40.93505802195924, "xcomet_score": 0.9517822265625, "xcomet_qe_score": 0.9329037666320801, "metricx_score": 1.8766387701034546, "metricx_qe_score": 2.2177133560180664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得比基线分数更好的分数,我们将这些结果作为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 68.78037711334562, "chrf_score": 63.647141712012825, "xcomet_score": 0.9528870582580566, "xcomet_qe_score": 0.8723616600036621, "metricx_score": 2.204930305480957, "metricx_qe_score": 2.432122230529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢你的关注,我们希望在会议期间见到大家。", "metrics": {"bleu_score": 46.11411579665311, "chrf_score": 40.155067083822956, "xcomet_score": 0.9712957143783569, "xcomet_qe_score": 0.968079149723053, "metricx_score": 1.6068376302719116, "metricx_qe_score": 1.2923707962036133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是亚当·斯皮尔科夫斯基,今天的演讲主题是协调的依赖结构。", "metrics": {"bleu_score": 13.153766128434404, "chrf_score": 10.127512519335902, "xcomet_score": 0.8483573794364929, "xcomet_qe_score": 0.5752875804901123, "metricx_score": 1.6595115661621094, "metricx_qe_score": 1.384664535522461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如你们所知,不同的理论和语料库方法假设了不同的依赖结构。", "metrics": {"bleu_score": 61.376834587908924, "chrf_score": 58.881159089215174, "xcomet_score": 0.9674574136734009, "xcomet_qe_score": 0.8176270723342896, "metricx_score": 0.7561235427856445, "metricx_qe_score": 1.0803452730178833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依赖中,协调结构“丽莎、巴特和玛吉”的结构是第一个并列成分是整个协调结构的头部", "metrics": {"bleu_score": 33.172852336901215, "chrf_score": 23.179736556563853, "xcomet_score": 0.6284821033477783, "xcomet_qe_score": 0.518835186958313, "metricx_score": 4.048816680908203, "metricx_qe_score": 4.142613410949707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",即丽莎。伊戈尔·米尔", "metrics": {"bleu_score": 3.42209762272661, "chrf_score": 1.1737089201877935, "xcomet_score": 0.24233582615852356, "xcomet_qe_score": 0.24161545932292938, "metricx_score": 8.529560089111328, "metricx_qe_score": 12.57357406616211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "丘克的意义文本理论也采用了类似的方法,整个协调结构由第一个并列成分主导。因此,", "metrics": {"bleu_score": 48.434119140790656, "chrf_score": 36.39660188709368, "xcomet_score": 0.6611319780349731, "xcomet_qe_score": 0.5431640148162842, "metricx_score": 6.595264911651611, "metricx_qe_score": 5.208864212036133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的,", "metrics": {"bleu_score": 64.07117598241614, "chrf_score": 48.52481541959439, "xcomet_score": 0.9922106266021729, "xcomet_qe_score": 0.9595069885253906, "metricx_score": 0.41505956649780273, "metricx_qe_score": 0.48554089665412903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,我们做了什么?", "metrics": {"bleu_score": 8.392229812593097, "chrf_score": 25.089605734767023, "xcomet_score": 0.505057692527771, "xcomet_qe_score": 0.29449263215065, "metricx_score": 2.9800024032592773, "metricx_qe_score": 3.635746479034424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出了一个并列成分。", "metrics": {"bleu_score": 44.462966469165124, "chrf_score": 37.39417201528737, "xcomet_score": 0.8427244424819946, "xcomet_qe_score": 0.7984569072723389, "metricx_score": 3.6121785640716553, "metricx_qe_score": 4.560258865356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,还有对称的协调结构方法,如布拉格方法,", "metrics": {"bleu_score": 29.03669971352489, "chrf_score": 24.994847487107553, "xcomet_score": 0.8470969200134277, "xcomet_qe_score": 0.7702653408050537, "metricx_score": 3.4076988697052, "metricx_qe_score": 2.343472719192505, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "布拉格依赖树库中的连词主导方法,其中协调结构由连词主导。", "metrics": {"bleu_score": 28.09473128121621, "chrf_score": 25.226233528332497, "xcomet_score": 0.7169141173362732, "xcomet_qe_score": 0.646943986415863, "metricx_score": 3.967132329940796, "metricx_qe_score": 4.2523722648620605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从“和”到所有并列成分都有依赖关系。", "metrics": {"bleu_score": 29.45642544824926, "chrf_score": 25.682074091685074, "xcomet_score": 0.7603116035461426, "xcomet_qe_score": 0.6943200826644897, "metricx_score": 3.1711854934692383, "metricx_qe_score": 3.2004637718200684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多头方法,例如在卡茨的词法语法中使用,所有并列成分都是协调结构的头部。", "metrics": {"bleu_score": 36.67561941902225, "chrf_score": 25.42086987090697, "xcomet_score": 0.4609587490558624, "xcomet_qe_score": 0.5384659767150879, "metricx_score": 5.686847686767578, "metricx_qe_score": 5.749264717102051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从统治者(这里是", "metrics": {"bleu_score": 11.479267900149548, "chrf_score": 10.127444377975433, "xcomet_score": 0.3478173613548279, "xcomet_qe_score": 0.16482749581336975, "metricx_score": 8.810379028320312, "metricx_qe_score": 5.878106594085693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“爱”)到所有并列成分都有依赖关系。这些是巴特和玛吉。现在,", "metrics": {"bleu_score": 4.3006625536967915, "chrf_score": 5.296541879633467, "xcomet_score": 0.27037808299064636, "xcomet_qe_score": 0.19834114611148834, "metricx_score": 13.512160301208496, "metricx_qe_score": 14.123068809509277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这篇论文的目的是为对称的协调结构(如这两种)提出新的论据,反对不对称的协调结构(如这两种", "metrics": {"bleu_score": 44.12273948109309, "chrf_score": 37.09964148399461, "xcomet_score": 0.7552340030670166, "xcomet_qe_score": 0.7476179599761963, "metricx_score": 2.3859705924987793, "metricx_qe_score": 2.076385259628296, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,我们做了什么?", "metrics": {"bleu_score": 8.392229812593097, "chrf_score": 25.089605734767023, "xcomet_score": 0.5356117486953735, "xcomet_qe_score": 0.33010828495025635, "metricx_score": 3.2217297554016113, "metricx_qe_score": 4.227091312408447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,论据基于依赖长度最小化原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 40.74440287584845, "chrf_score": 32.954671632873406, "xcomet_score": 0.7651453018188477, "xcomet_qe_score": 0.7716475129127502, "metricx_score": 0.9837796092033386, "metricx_qe_score": 0.8701410293579102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如你们所知,直接宾语倾向于靠近动词,而状语可能更远。", "metrics": {"bleu_score": 35.033352149347905, "chrf_score": 29.327611573431984, "xcomet_score": 0.8606778383255005, "xcomet_qe_score": 0.8069888353347778, "metricx_score": 1.9359848499298096, "metricx_qe_score": 1.9990726709365845, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,“玛奇昨天读了它”是可以的,因为直接宾语“它”靠近动词,而“玛奇昨天读了它”则要差得", "metrics": {"bleu_score": 20.03364099002264, "chrf_score": 11.889117265969045, "xcomet_score": 0.6439095735549927, "xcomet_qe_score": 0.5923115611076355, "metricx_score": 7.012357234954834, "metricx_qe_score": 7.435934066772461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多,因为", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2348778247833252, "xcomet_qe_score": 0.15657588839530945, "metricx_score": 5.566263198852539, "metricx_qe_score": 2.8971619606018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "动词和直接宾语之间有一个状语“昨天”。", "metrics": {"bleu_score": 52.335886371762044, "chrf_score": 38.48443393783312, "xcomet_score": 0.8762586116790771, "xcomet_qe_score": 0.8480885028839111, "metricx_score": 1.6835558414459229, "metricx_qe_score": 1.1644821166992188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常长且非常重时,这种效果可能会减轻,因为它", "metrics": {"bleu_score": 27.64219612744866, "chrf_score": 26.966018472098778, "xcomet_score": 0.6547815799713135, "xcomet_qe_score": 0.5909395217895508, "metricx_score": 6.23145866394043, "metricx_qe_score": 3.477034330368042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以移动到状语之后的位置。", "metrics": {"bleu_score": 30.737128097522042, "chrf_score": 29.175930810086015, "xcomet_score": 0.7898555994033813, "xcomet_qe_score": 0.7236751317977905, "metricx_score": 2.194420099258423, "metricx_qe_score": 3.633387565612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在下面的例子中得到了说明。因此,", "metrics": {"bleu_score": 5.439330544349821, "chrf_score": 8.698453608247423, "xcomet_score": 0.4128867983818054, "xcomet_qe_score": 0.4065856635570526, "metricx_score": 3.068077325820923, "metricx_qe_score": 2.6961863040924072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是可以的。", "metrics": {"bleu_score": 63.15552371794039, "chrf_score": 55.594035594035596, "xcomet_score": 0.9396331310272217, "xcomet_qe_score": 0.922397792339325, "metricx_score": 0.3405936658382416, "metricx_qe_score": 0.5062903761863708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "玛奇读了这本关于蜜蜂的绝对迷人的书", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5643798112869263, "xcomet_qe_score": 0.6596587896347046, "metricx_score": 3.2677552700042725, "metricx_qe_score": 3.94242525100708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",昨天是好的。以这种方式,我们有这个长的名词短语,而不是“它”。", "metrics": {"bleu_score": 27.00507825988802, "chrf_score": 28.68048884281464, "xcomet_score": 0.5056890845298767, "xcomet_qe_score": 0.31076663732528687, "metricx_score": 8.126707077026367, "metricx_qe_score": 9.544318199157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但说“玛奇昨天读了这本关于蜜蜂的绝对迷人的书”也是可以的。", "metrics": {"bleu_score": 4.457304913937111, "chrf_score": 2.937307845179575, "xcomet_score": 0.8613384962081909, "xcomet_qe_score": 0.8303849697113037, "metricx_score": 2.804605722427368, "metricx_qe_score": 2.768516778945923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,原因是这可能是因为,尽管这个句子违反了一般语法原则,即直接宾语应该靠近动词,但它满足了依赖长度最小化原则,该原则认为较短的依赖关系是优先的。因此,", "metrics": {"bleu_score": 32.39866278994058, "chrf_score": 30.33141505013073, "xcomet_score": 0.614782452583313, "xcomet_qe_score": 0.7346359491348267, "metricx_score": 4.176568031311035, "metricx_qe_score": 3.0778963565826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示了关键依赖关系的长度,即在这两种结构中不恒定的依赖关系。", "metrics": {"bleu_score": 52.2352924913678, "chrf_score": 45.179792880416244, "xcomet_score": 0.9029313325881958, "xcomet_qe_score": 0.7930303812026978, "metricx_score": 1.4305402040481567, "metricx_qe_score": 1.9896186590194702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们有从“读”到状语的长度为7个单词的依赖关系,从“读”到“书”的长度为4个单词。因此,总共是11个。", "metrics": {"bleu_score": 21.594554670825215, "chrf_score": 18.710972466636235, "xcomet_score": 0.6193044185638428, "xcomet_qe_score": 0.6236075162887573, "metricx_score": 3.7630248069763184, "metricx_qe_score": 3.8680527210235596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动、交换这两个成分时,这两个依赖关系的总和变成了6个。", "metrics": {"bleu_score": 50.87573747155849, "chrf_score": 44.853915522147815, "xcomet_score": 0.7749601006507874, "xcomet_qe_score": 0.7794461250305176, "metricx_score": 1.9870597124099731, "metricx_qe_score": 1.788120150566101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,11变成了6,要短得多。", "metrics": {"bleu_score": 32.55964126200301, "chrf_score": 31.97990341445304, "xcomet_score": 0.8555063009262085, "xcomet_qe_score": 0.8343518972396851, "metricx_score": 2.1893346309661865, "metricx_qe_score": 2.5027718544006348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来还不错。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9847985506057739, "xcomet_qe_score": 0.9085652828216553, "metricx_score": 0.33503812551498413, "metricx_qe_score": 0.5563164353370667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多,因为", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2348778247833252, "xcomet_qe_score": 0.15657588839530945, "metricx_score": 5.566263198852539, "metricx_qe_score": 2.8971619606018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,我们做了什么?", "metrics": {"bleu_score": 8.392229812593097, "chrf_score": 25.089605734767023, "xcomet_score": 0.5118415355682373, "xcomet_qe_score": 0.2605515718460083, "metricx_score": 3.1614131927490234, "metricx_qe_score": 4.154189586639404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的宾语树库中提取了各种关于协调的统计数据,并查看了论文,为什么我们不使用通用依赖。这些统计数据证实了以前多次观察到的现象,即左并列成分倾向于较短,", "metrics": {"bleu_score": 47.844385086863845, "chrf_score": 39.89568717101255, "xcomet_score": 0.6225490570068359, "xcomet_qe_score": 0.511711835861206, "metricx_score": 5.496065616607666, "metricx_qe_score": 5.1012773513793945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“盐和胡椒”,而不是“胡椒和盐”,以音节计算。", "metrics": {"bleu_score": 15.268470848781108, "chrf_score": 8.44357344119984, "xcomet_score": 0.820134162902832, "xcomet_qe_score": 0.8591933846473694, "metricx_score": 1.769334077835083, "metricx_qe_score": 2.688217878341675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还有一个观察是,这种倾向随着长度差异的增加而增强。因此", "metrics": {"bleu_score": 56.234132519034915, "chrf_score": 56.982046890568924, "xcomet_score": 0.7870930433273315, "xcomet_qe_score": 0.7833045721054077, "metricx_score": 4.081897258758545, "metricx_qe_score": 3.396995782852173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个并列成分的长度差异增加时,较短的并列成分更倾向于成为第一个。因此,", "metrics": {"bleu_score": 41.86024486875066, "chrf_score": 34.076012219722315, "xcomet_score": 0.6901467442512512, "xcomet_qe_score": 0.7181528806686401, "metricx_score": 7.183351516723633, "metricx_qe_score": 5.081521034240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "左短并列成分的比例更大。", "metrics": {"bleu_score": 42.46387220041172, "chrf_score": 36.37183312201849, "xcomet_score": 0.8400655388832092, "xcomet_qe_score": 0.8287378549575806, "metricx_score": 2.897510290145874, "metricx_qe_score": 4.430441379547119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这篇论文的新颖之处在于,我们观察到这种倾向只在左侧没有统治者时发生。因此,", "metrics": {"bleu_score": 34.740629228125066, "chrf_score": 30.739187872017084, "xcomet_score": 0.6914883852005005, "xcomet_qe_score": 0.6111719012260437, "metricx_score": 6.2947468757629395, "metricx_qe_score": 4.330002307891846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多,因为", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2348778247833252, "xcomet_qe_score": 0.15657588839530945, "metricx_score": 5.566263198852539, "metricx_qe_score": 2.8971619606018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,左侧的统治者是“我看到了巴特和丽莎”,因此统治者在左侧。", "metrics": {"bleu_score": 22.148418795159994, "chrf_score": 16.273548295782433, "xcomet_score": 0.7038609981536865, "xcomet_qe_score": 0.6659114360809326, "metricx_score": 5.406325340270996, "metricx_qe_score": 5.19252347946167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,它是缺失的,霍默来了并打了个喷嚏。", "metrics": {"bleu_score": 19.940445989088907, "chrf_score": 11.614127304742297, "xcomet_score": 0.769591212272644, "xcomet_qe_score": 0.7688295841217041, "metricx_score": 3.421292543411255, "metricx_qe_score": 3.615304470062256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们有两个动词的协调,没有外部统治者。因此,", "metrics": {"bleu_score": 57.84879107039426, "chrf_score": 55.89919347447627, "xcomet_score": 0.8268872499465942, "xcomet_qe_score": 0.7513469457626343, "metricx_score": 5.094659805297852, "metricx_qe_score": 4.631413459777832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左并列成分更倾向于较短,两个并列成分的差异越", "metrics": {"bleu_score": 29.505138076099747, "chrf_score": 24.592792584171896, "xcomet_score": 0.6091902852058411, "xcomet_qe_score": 0.5578616857528687, "metricx_score": 8.662055015563965, "metricx_qe_score": 6.259657859802246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大,这种倾向就越强。然而,当统治者在右侧时,例如“左”统治协调“网络”,这种效果消失了。", "metrics": {"bleu_score": 11.408963734192417, "chrf_score": 11.326328764760907, "xcomet_score": 0.2535122334957123, "xcomet_qe_score": 0.12982548773288727, "metricx_score": 11.701746940612793, "metricx_qe_score": 13.172904968261719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过以字符、音节和单词为单位测量长度来展示这一点,因此我将集中", "metrics": {"bleu_score": 16.849541056960994, "chrf_score": 16.72776136750733, "xcomet_score": 0.26097816228866577, "xcomet_qe_score": 0.24448910355567932, "metricx_score": 7.964533805847168, "metricx_qe_score": 5.064956188201904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在右侧。我们在这", "metrics": {"bleu_score": 10.229197414177778, "chrf_score": 8.88259526261586, "xcomet_score": 0.4153359532356262, "xcomet_qe_score": 0.13640108704566956, "metricx_score": 5.315178871154785, "metricx_qe_score": 5.3836798667907715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里看到的是,当统治者在左侧时,左并列成分较短的倾向随着单词的绝对差异稳步增长,当没有统治者时,例如在句子的协调中,也观察到相同的情况,但", "metrics": {"bleu_score": 25.89669714068887, "chrf_score": 22.186167923600078, "xcomet_score": 0.2580104172229767, "xcomet_qe_score": 0.2687559723854065, "metricx_score": 11.608497619628906, "metricx_qe_score": 7.610654354095459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当统治者在右侧时,这种倾向消失了。", "metrics": {"bleu_score": 17.14599716645038, "chrf_score": 16.052217826839062, "xcomet_score": 0.8503055572509766, "xcomet_qe_score": 0.8512672781944275, "metricx_score": 1.6311019659042358, "metricx_qe_score": 1.3775146007537842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这如何为不对称的协调结构(如这两种)提供论据,并为对称的协调结构(如这两种)提供论据。", "metrics": {"bleu_score": 40.168725221896224, "chrf_score": 33.87067610154155, "xcomet_score": 0.7115182876586914, "xcomet_qe_score": 0.5521929860115051, "metricx_score": 7.600064754486084, "metricx_qe_score": 8.299989700317383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,请查看论文以获取完整的论据,", "metrics": {"bleu_score": 8.233514927922947, "chrf_score": 10.491125514901865, "xcomet_score": 0.9638867378234863, "xcomet_qe_score": 0.9460806846618652, "metricx_score": 0.858185350894928, "metricx_qe_score": 0.5890688300132751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并在海报会议上与我们交流。", "metrics": {"bleu_score": 20.345970436499208, "chrf_score": 18.99717261957897, "xcomet_score": 0.8157749176025391, "xcomet_qe_score": 0.795457661151886, "metricx_score": 2.8358545303344727, "metricx_qe_score": 2.4855692386627197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是华盛顿大学的博士生尚宾。", "metrics": {"bleu_score": 66.54377827941899, "chrf_score": 46.715408329754915, "xcomet_score": 0.9000982046127319, "xcomet_qe_score": 0.8912058472633362, "metricx_score": 0.3991471529006958, "metricx_qe_score": 0.5658564567565918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的研究,从预训练数据到语言模型,再到下游任务,追踪政治偏见的轨迹,导致不公平的NLP模型。", "metrics": {"bleu_score": 53.16432769702753, "chrf_score": 46.54656741677291, "xcomet_score": 0.6452652215957642, "xcomet_qe_score": 0.7807263135910034, "metricx_score": 3.346137285232544, "metricx_qe_score": 2.740133285522461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是基于大规模网络爬取数据进行训练的。", "metrics": {"bleu_score": 39.52355241681996, "chrf_score": 33.05844497246635, "xcomet_score": 0.9990478754043579, "xcomet_qe_score": 1.0, "metricx_score": 0.8127202987670898, "metricx_qe_score": 1.1989858150482178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 52.690039305566096, "chrf_score": 49.433523424384084, "xcomet_score": 0.7585622668266296, "xcomet_qe_score": 0.7133761644363403, "metricx_score": 2.00849986076355, "metricx_qe_score": 2.8807544708251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据C4语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等媒体在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 74.86369767679057, "chrf_score": 71.21333853523068, "xcomet_score": 0.7719762325286865, "xcomet_qe_score": 0.8329997062683105, "metricx_score": 1.327697992324829, "metricx_qe_score": 1.2143738269805908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了双重影响。", "metrics": {"bleu_score": 50.96772803089438, "chrf_score": 46.74304298188613, "xcomet_score": 0.9805477857589722, "xcomet_qe_score": 0.966586709022522, "metricx_score": 1.0563725233078003, "metricx_qe_score": 0.6601656079292297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从多样化的视角学习,这有助于民主和多元化的思想。", "metrics": {"bleu_score": 29.20551639396935, "chrf_score": 25.176678893656096, "xcomet_score": 0.8932665586471558, "xcomet_qe_score": 0.7895364165306091, "metricx_score": 2.1659016609191895, "metricx_qe_score": 2.596684455871582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本身具有社会偏见,可能导致下游任务应用中的公平性问题。", "metrics": {"bleu_score": 56.96426878986118, "chrf_score": 49.246402108522716, "xcomet_score": 0.9906541109085083, "xcomet_qe_score": 0.9707958698272705, "metricx_score": 1.0503449440002441, "metricx_qe_score": 1.5965664386749268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体提出以下问题。首先,我们如何评估语言模型的政治倾向,以及预训练数据在这些政治偏见中扮演什么角色?", "metrics": {"bleu_score": 59.06061140856521, "chrf_score": 53.76337275268451, "xcomet_score": 0.8973024487495422, "xcomet_qe_score": 0.9355789422988892, "metricx_score": 1.6005322933197021, "metricx_qe_score": 1.9452714920043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在下游任务中的表现如何,以及这是否会导致NLP应用中的公平性问题?", "metrics": {"bleu_score": 80.1164027102155, "chrf_score": 76.9607314947727, "xcomet_score": 0.9538291692733765, "xcomet_qe_score": 0.9089809656143188, "metricx_score": 0.7754077315330505, "metricx_qe_score": 0.7883371710777283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体来说,我们首先提出使用不同的提示格式来提示语言模型,使用政治问卷,如政治罗盘测试。", "metrics": {"bleu_score": 48.86467218233305, "chrf_score": 39.72451854192839, "xcomet_score": 0.783294677734375, "xcomet_qe_score": 0.7249100804328918, "metricx_score": 4.557544231414795, "metricx_qe_score": 4.258787155151367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们的自动评估基于政治科学文献。", "metrics": {"bleu_score": 41.05461377536361, "chrf_score": 33.680216662535, "xcomet_score": 0.9142030477523804, "xcomet_qe_score": 0.8913003206253052, "metricx_score": 1.019392967224121, "metricx_qe_score": 1.3666361570358276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,首先,语言模型确实具有不同的政治倾向。", "metrics": {"bleu_score": 71.0109425285578, "chrf_score": 63.471040822713185, "xcomet_score": 0.9910895824432373, "xcomet_qe_score": 1.0, "metricx_score": 0.7272058129310608, "metricx_qe_score": 0.8977949023246765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘的四个象限。", "metrics": {"bleu_score": 54.08804419255529, "chrf_score": 45.88398515962971, "xcomet_score": 0.8560222387313843, "xcomet_qe_score": 0.7895867824554443, "metricx_score": 2.2291054725646973, "metricx_qe_score": 2.0888497829437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT-4是所有语言模型中最自由的,而GPT系列通常比BERT系列及其变体更自由。", "metrics": {"bleu_score": 66.40797645062887, "chrf_score": 66.18701996796932, "xcomet_score": 0.769712507724762, "xcomet_qe_score": 0.6769708395004272, "metricx_score": 2.7208971977233887, "metricx_qe_score": 2.7951478958129883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在调查语言模型的政治偏见在多大程度上是从训练数据中获得的。", "metrics": {"bleu_score": 59.01958512145046, "chrf_score": 55.60995701295046, "xcomet_score": 0.9400414824485779, "xcomet_qe_score": 0.9702931642532349, "metricx_score": 0.6767398118972778, "metricx_qe_score": 1.0650367736816406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们通过进一步在六个不同的党派语料库上预训练语言模型检查点来进行受控实验,这些语料库分为新闻和社交媒体,进一步分为其政治倾向。", "metrics": {"bleu_score": 48.90979694629042, "chrf_score": 42.22574847481748, "xcomet_score": 0.740379273891449, "xcomet_qe_score": 0.6616240739822388, "metricx_score": 2.2485480308532715, "metricx_qe_score": 2.8157906532287598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这样的党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了变化。", "metrics": {"bleu_score": 78.28493884997408, "chrf_score": 73.91200640576464, "xcomet_score": 0.9099429845809937, "xcomet_qe_score": 0.8268270492553711, "metricx_score": 1.165337085723877, "metricx_qe_score": 1.7587034702301025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于进一步在左倾Reddit语料库上微调的Roberta,我们可以看到其政治偏见在自由主义方面有了显著的转变。", "metrics": {"bleu_score": 46.011566758970304, "chrf_score": 42.3250993920246, "xcomet_score": 0.7991489768028259, "xcomet_qe_score": 0.7992234230041504, "metricx_score": 4.94762659072876, "metricx_qe_score": 4.64177131652832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象。", "metrics": {"bleu_score": 59.61870696767964, "chrf_score": 56.37747167625948, "xcomet_score": 0.8541579246520996, "xcomet_qe_score": 0.9715609550476074, "metricx_score": 0.7148723602294922, "metricx_qe_score": 0.8806501626968384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第45任总统之前和之后,", "metrics": {"bleu_score": 67.92821527216381, "chrf_score": 67.32489142107787, "xcomet_score": 0.7537652850151062, "xcomet_qe_score": 0.7169573307037354, "metricx_score": 1.9133429527282715, "metricx_qe_score": 2.3033595085144043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别在两个不同的时间语料库上预训练语言模型。我们", "metrics": {"bleu_score": 94.18009332674224, "chrf_score": 92.75185658480851, "xcomet_score": 0.6831768751144409, "xcomet_qe_score": 0.5442106127738953, "metricx_score": 3.871913194656372, "metricx_qe_score": 1.3037999868392944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年之后通常具有更远离中间的政治倾向。", "metrics": {"bleu_score": 64.07712863164406, "chrf_score": 55.66977160101465, "xcomet_score": 0.9093250036239624, "xcomet_qe_score": 0.8989963531494141, "metricx_score": 1.4286272525787354, "metricx_qe_score": 1.889545202255249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能够捕捉到我们社会中的极化现象。", "metrics": {"bleu_score": 55.64648974307584, "chrf_score": 50.137983752126694, "xcomet_score": 0.9957098960876465, "xcomet_qe_score": 0.9971472024917603, "metricx_score": 0.7965023517608643, "metricx_qe_score": 0.9939598441123962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们评估了具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测中的表现,这两个NLP应用通常涉及语言模型,并且可能具有非常重要的影响。因此,", "metrics": {"bleu_score": 61.861197716429125, "chrf_score": 64.74613169102498, "xcomet_score": 0.7414195537567139, "xcomet_qe_score": 0.7254287600517273, "metricx_score": 3.38972806930542, "metricx_qe_score": 1.7193388938903809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,如果我们调查每类别的表现,也就是说,如果我们将表现分为不同的人口统计或新闻媒体的政治倾向,我们可以看到一个模式,", "metrics": {"bleu_score": 59.98707869735989, "chrf_score": 53.337700340301296, "xcomet_score": 0.7994843125343323, "xcomet_qe_score": 0.7132997512817383, "metricx_score": 3.219447135925293, "metricx_qe_score": 4.22634220123291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于仇恨言论检测,左倾语言模型在检测针对社会少数群体的仇恨言论方面表现更好,但在检测针对我们社会中更有权力的群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 75.57283861579377, "chrf_score": 71.52574029288621, "xcomet_score": 0.9881553649902344, "xcomet_qe_score": 0.9810799360275269, "metricx_score": 0.8190706372261047, "metricx_qe_score": 0.9415554404258728, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之亦然,右倾语言模型在检测针对白人和男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 72.74454698474959, "chrf_score": 75.97116477437102, "xcomet_score": 0.9843102693557739, "xcomet_qe_score": 0.9804555177688599, "metricx_score": 0.4925698935985565, "metricx_qe_score": 0.628595232963562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虚假新闻检测也有类似的趋势,我们看到左倾语言模型在检测其对立政治倾向的虚假信息方面表现更好,反之亦然。", "metrics": {"bleu_score": 38.52724543671744, "chrf_score": 31.00611767546112, "xcomet_score": 0.9960829019546509, "xcomet_qe_score": 0.9939175844192505, "metricx_score": 1.0521068572998047, "metricx_qe_score": 1.4758051633834839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多定性示例,以显示具有不同政治倾向的语言模型确实会根据其社会类别对仇恨言论和虚假信息示例做出不同的预测。", "metrics": {"bleu_score": 80.70392952865393, "chrf_score": 75.50730487076551, "xcomet_score": 0.9736775159835815, "xcomet_qe_score": 0.9740263223648071, "metricx_score": 1.1160281896591187, "metricx_qe_score": 1.2899494171142578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中还有更多示例,进一步突出了这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平性问题。", "metrics": {"bleu_score": 60.90439083261693, "chrf_score": 53.68678104032965, "xcomet_score": 0.8654030561447144, "xcomet_qe_score": 0.8931243419647217, "metricx_score": 1.1893889904022217, "metricx_qe_score": 1.484145164489746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果右倾语言模型在仇恨言论或虚假信息上进行微调,并部署到一个流行的社交媒体平台上,这意味着持有对立政治观点的人可能会被边缘化,而针对少数群体的仇恨言论可能会肆无忌惮地蔓延,没有任何控制。", "metrics": {"bleu_score": 55.082360824368756, "chrf_score": 48.40456608076917, "xcomet_score": 0.9659955501556396, "xcomet_qe_score": 0.9021490216255188, "metricx_score": 1.1155797243118286, "metricx_qe_score": 1.4984533786773682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,承认并解决语言模型政治倾向导致的公平性问题。", "metrics": {"bleu_score": 49.52537002011174, "chrf_score": 48.588172263746706, "xcomet_score": 0.9059724807739258, "xcomet_qe_score": 0.9051372408866882, "metricx_score": 1.0579296350479126, "metricx_qe_score": 1.267216682434082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们还想", "metrics": {"bleu_score": 8.55635852921339, "chrf_score": 5.530605455755156, "xcomet_score": 0.15239961445331573, "xcomet_qe_score": 0.15636037290096283, "metricx_score": 5.065274238586426, "metricx_qe_score": 2.8804168701171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "强调,我们揭示了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 58.55858876483759, "chrf_score": 56.94197223056662, "xcomet_score": 0.8026045560836792, "xcomet_qe_score": 0.649064302444458, "metricx_score": 1.3240876197814941, "metricx_qe_score": 1.8028934001922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在斯库拉和卡律布狄斯之间。", "metrics": {"bleu_score": 23.825412935547597, "chrf_score": 22.614289580075102, "xcomet_score": 0.7757885456085205, "xcomet_qe_score": 0.6980432868003845, "metricx_score": 2.720712900161743, "metricx_qe_score": 3.4772796630859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不在语言模型训练数据中消毒政治观点,偏见将从预训练数据传播到语言模型,再到下游任务,最终导致公平性问题。", "metrics": {"bleu_score": 61.79717359207274, "chrf_score": 53.85143478896792, "xcomet_score": 0.8765231370925903, "xcomet_qe_score": 0.8506932854652405, "metricx_score": 3.012002468109131, "metricx_qe_score": 2.8054351806640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们试图以某种方式消毒,我们也会冒着审查或排除的风险,", "metrics": {"bleu_score": 49.62152967951634, "chrf_score": 43.02803390734426, "xcomet_score": 0.7599032521247864, "xcomet_qe_score": 0.737460732460022, "metricx_score": 2.646806240081787, "metricx_qe_score": 2.4839696884155273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且很难确定什么是真正中立的,应该保留在语言模型训练数据中。", "metrics": {"bleu_score": 13.233773586731585, "chrf_score": 15.229796176314148, "xcomet_score": 0.845893144607544, "xcomet_qe_score": 0.7279255390167236, "metricx_score": 2.654270887374878, "metricx_qe_score": 2.8566153049468994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电动查理问题。", "metrics": {"bleu_score": 40.35278637463991, "chrf_score": 31.04256854256854, "xcomet_score": 0.7669693231582642, "xcomet_qe_score": 0.6460278034210205, "metricx_score": 4.086047172546387, "metricx_qe_score": 4.964749336242676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,我们做了什么?", "metrics": {"bleu_score": 8.392229812593097, "chrf_score": 25.089605734767023, "xcomet_score": 0.5464252233505249, "xcomet_qe_score": 0.5138099193572998, "metricx_score": 3.2700681686401367, "metricx_qe_score": 4.215831756591797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这就是我今天要说的全部内容。", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 72.97916540231482, "xcomet_score": 0.9915990829467773, "xcomet_qe_score": 0.9783182144165039, "metricx_score": 0.19653818011283875, "metricx_qe_score": 0.17678585648536682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的注意。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.864948570728302, "xcomet_qe_score": 0.7565501928329468, "metricx_score": 1.1460410356521606, "metricx_qe_score": 1.817592978477478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基梅隆大学的一年级博士生,今天我将介绍我们的工作《NL位置性:通过数据集和模型来描述设计偏见》。", "metrics": {"bleu_score": 52.77983599779509, "chrf_score": 38.018015597923196, "xcomet_score": 0.8079900741577148, "xcomet_qe_score": 0.7999774217605591, "metricx_score": 2.6574954986572266, "metricx_qe_score": 3.422327995300293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的几位同事合作完成的,包括塞巴斯蒂安·桑蒂、罗南·拉布拉斯、卡塔里", "metrics": {"bleu_score": 39.04253461098, "chrf_score": 22.71447455905406, "xcomet_score": 0.6459280252456665, "xcomet_qe_score": 0.6536440849304199, "metricx_score": 7.399419784545898, "metricx_qe_score": 6.060638904571533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "娜·阿里尼卡和马丁·萨普。让我们从想象你在报社工作,并试图删除新闻文章下的有害评论开始。", "metrics": {"bleu_score": 25.511218462484088, "chrf_score": 22.240195303735565, "xcomet_score": 0.03632450848817825, "xcomet_qe_score": 0.14452248811721802, "metricx_score": 5.535804271697998, "metricx_qe_score": 6.464015007019043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会使用像Perspective API这样的流行API来检测有害内容,这对卡尔·琼斯来说效果很好,", "metrics": {"bleu_score": 19.69256474030723, "chrf_score": 27.89571800426809, "xcomet_score": 0.7783603072166443, "xcomet_qe_score": 0.660029411315918, "metricx_score": 4.52269172668457, "metricx_qe_score": 3.3103208541870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能够正确检测有害实例。", "metrics": {"bleu_score": 49.487489225241916, "chrf_score": 65.92907184973315, "xcomet_score": 0.808722734451294, "xcomet_qe_score": 0.7435086965560913, "metricx_score": 3.9781854152679443, "metricx_qe_score": 3.854929208755493, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这对迪蒂亚·沙尔马来说并不是这样,", "metrics": {"bleu_score": 5.401157445454033, "chrf_score": 4.268557579083353, "xcomet_score": 0.8920966982841492, "xcomet_qe_score": 0.8842049837112427, "metricx_score": 2.7894015312194824, "metricx_qe_score": 1.7933335304260254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API对在印度语境中更常见的冒犯性词汇并不敏感。", "metrics": {"bleu_score": 84.53479389161355, "chrf_score": 82.64945620898416, "xcomet_score": 0.8287472724914551, "xcomet_qe_score": 0.7049680352210999, "metricx_score": 4.2968220710754395, "metricx_qe_score": 4.4953813552856445, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们看到技术在不同人群之间的系统性表现差异。", "metrics": {"bleu_score": 65.99583028509838, "chrf_score": 58.507524204949455, "xcomet_score": 0.9839211702346802, "xcomet_qe_score": 0.9069797396659851, "metricx_score": 0.9384640455245972, "metricx_qe_score": 1.1112565994262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像我们刚才看到的设计偏见可能是由于NLP研究人员和模型开发人员的位置性。", "metrics": {"bleu_score": 51.747783470795255, "chrf_score": 47.41761410927437, "xcomet_score": 0.805622398853302, "xcomet_qe_score": 0.7629858255386353, "metricx_score": 3.307427406311035, "metricx_qe_score": 3.5745575428009033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "位置性只是人们由于其人口统计学、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 54.95207706557383, "chrf_score": 55.72100679914709, "xcomet_score": 0.7822049856185913, "xcomet_qe_score": 0.8523612022399902, "metricx_score": 5.639720439910889, "metricx_qe_score": 4.766597270965576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女性主义和同性恋学术空间中。", "metrics": {"bleu_score": 79.76084633013409, "chrf_score": 75.08897886025353, "xcomet_score": 0.9842499494552612, "xcomet_qe_score": 0.9763935804367065, "metricx_score": 0.7131538391113281, "metricx_qe_score": 0.5494563579559326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,位置性可以影响研究过程及其结果,因为它可以改变研究人员的决策。", "metrics": {"bleu_score": 44.049061663063426, "chrf_score": 37.6542059635212, "xcomet_score": 0.8250565528869629, "xcomet_qe_score": 0.84074467420578, "metricx_score": 4.128820896148682, "metricx_qe_score": 3.183091402053833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问,数据集和模型是否有位置性?", "metrics": {"bleu_score": 50.647262871731705, "chrf_score": 44.78623587615873, "xcomet_score": 0.9055891633033752, "xcomet_qe_score": 0.936269998550415, "metricx_score": 2.639017105102539, "metricx_qe_score": 1.0977494716644287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型本身和数据集本身有人口统计学身份和生活经历,但它们确实汇集了真实人的判断和意见,因此可以代表某些位置性而不是其他位置性。", "metrics": {"bleu_score": 53.29225144301332, "chrf_score": 45.987731615782536, "xcomet_score": 0.6686065793037415, "xcomet_qe_score": 0.6697208881378174, "metricx_score": 5.211666584014893, "metricx_qe_score": 5.46604585647583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,先前的工作已经提出了一些位置性的轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 42.71073607789007, "chrf_score": 34.53898651659246, "xcomet_score": 0.7529186010360718, "xcomet_qe_score": 0.6734055280685425, "metricx_score": 6.311952590942383, "metricx_qe_score": 5.689547538757324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些工作并没有比较最终用户与数据集和模型本身。随着NLP任务变得更加主观和社会导向,研究模型和数据集的位置性变得越来越重要。要描述这些位置性如何偏斜是具有挑战性的,因为并非所有决策都有记录,许多模型隐藏在API后面。", "metrics": {"bleu_score": 51.48274415582043, "chrf_score": 46.09826077435522, "xcomet_score": 0.6847072839736938, "xcomet_qe_score": 0.6125283241271973, "metricx_score": 5.321951389312744, "metricx_qe_score": 4.925907611846924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了研究数据集和模型的位置性,我们实际上比较了真实用户的标注与现有数据集和模型。", "metrics": {"bleu_score": 49.67412314324243, "chrf_score": 42.37301958191142, "xcomet_score": 0.8297545909881592, "xcomet_qe_score": 0.9064310193061829, "metricx_score": 4.589476108551025, "metricx_qe_score": 3.7597172260284424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架NL位置性来实现这一点。", "metrics": {"bleu_score": 26.978569758601026, "chrf_score": 17.476530643443564, "xcomet_score": 0.795024037361145, "xcomet_qe_score": 0.787887454032898, "metricx_score": 1.782604694366455, "metricx_qe_score": 2.6981899738311768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架分为两个主要步骤。", "metrics": {"bleu_score": 43.138943204452076, "chrf_score": 38.37551342601929, "xcomet_score": 0.9702411890029907, "xcomet_qe_score": 0.9063276052474976, "metricx_score": 0.10670393705368042, "metricx_qe_score": 0.35922718048095703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是用多样化的标注者重新标注数据集。", "metrics": {"bleu_score": 27.257626469889683, "chrf_score": 22.528987841961357, "xcomet_score": 0.8413750529289246, "xcomet_qe_score": 0.8456112146377563, "metricx_score": 3.26700496673584, "metricx_qe_score": 3.351811408996582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集标注者的人口统计学,因为通常只有少数标注者标注每个实例,而且人口统计学很少被收集和共享。", "metrics": {"bleu_score": 53.19208139354709, "chrf_score": 45.69156304826071, "xcomet_score": 0.8216828107833862, "xcomet_qe_score": 0.7707124948501587, "metricx_score": 3.6591241359710693, "metricx_qe_score": 2.7745180130004883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以获得每个实例的许多标注,并获得丰富的人口统计学数据集。", "metrics": {"bleu_score": 36.16170153966632, "chrf_score": 32.973853553608315, "xcomet_score": 0.9118524789810181, "xcomet_qe_score": 0.9088829159736633, "metricx_score": 2.494492292404175, "metricx_qe_score": 2.7124366760253906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们按人口统计学分类标注,并使用皮尔逊相关系数与模型和数据集进行比较。因此,我们的框架实际上不同于标注者不一致的文献,因为它比较了最终用户与模型和数据集的预测和标签,而不是仅查看标注者的同意或建模标注者的分布。", "metrics": {"bleu_score": 50.38929330087778, "chrf_score": 43.82171153152905, "xcomet_score": 0.5873992443084717, "xcomet_qe_score": 0.5198779702186584, "metricx_score": 4.893895149230957, "metricx_qe_score": 4.86698055267334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要通过Lab in the Wild实现,这是一个在线众包平台,前HCI合作者。", "metrics": {"bleu_score": 60.31227121338507, "chrf_score": 71.07833169674358, "xcomet_score": 0.737834632396698, "xcomet_qe_score": 0.6647521257400513, "metricx_score": 2.693725109100342, "metricx_qe_score": 3.631420373916626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild是一个在线实验平台,我们可以招募多样化的志愿者,", "metrics": {"bleu_score": 57.27098748750564, "chrf_score": 62.62045150590211, "xcomet_score": 0.8663016557693481, "xcomet_qe_score": 0.6168583631515503, "metricx_score": 1.7872936725616455, "metricx_qe_score": 5.009280204772949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而像MTurk这样的平台主要有来自美国或印度的参与者。此外,Lab in the Wild仍然能够获得高质量的数据。", "metrics": {"bleu_score": 52.25721276644756, "chrf_score": 55.56910140185206, "xcomet_score": 0.7740058302879333, "xcomet_qe_score": 0.8056929111480713, "metricx_score": 1.5509870052337646, "metricx_qe_score": 1.202493667602539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Lab in the Wild上托管了两项任务,其中一项是社会可接受性。其工作方式是参与者将阅读来自社会化学数据集的情况,然后他们将写下情况的社会可接受性。", "metrics": {"bleu_score": 52.810486315859166, "chrf_score": 50.69467433214141, "xcomet_score": 0.6782848834991455, "xcomet_qe_score": 0.617657482624054, "metricx_score": 3.524949073791504, "metricx_qe_score": 2.7744436264038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对研究的参与,他们可以将他们的回答与AI和其他人进行比较。", "metrics": {"bleu_score": 41.908997521070354, "chrf_score": 35.2958191146927, "xcomet_score": 0.9721190333366394, "xcomet_qe_score": 0.9873130321502686, "metricx_score": 1.2375376224517822, "metricx_qe_score": 1.354915976524353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们将这些标注与社会化学、Delphi和GPT-4进行比较。", "metrics": {"bleu_score": 37.76033048672943, "chrf_score": 52.218815913007845, "xcomet_score": 0.8466747999191284, "xcomet_qe_score": 0.8455899953842163, "metricx_score": 1.72491455078125, "metricx_qe_score": 2.105377197265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们为有害言论检测任务复制了一个非常相似的设置,参与者将阅读来自DynaHate的实例,并写下他们认为是否是有害言论的实例。", "metrics": {"bleu_score": 47.973296002016255, "chrf_score": 43.001612492643844, "xcomet_score": 0.9455387592315674, "xcomet_qe_score": 0.9483674764633179, "metricx_score": 2.218600273132324, "metricx_qe_score": 2.883472204208374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。", "metrics": {"bleu_score": 50.89134706838511, "chrf_score": 76.18282650392325, "xcomet_score": 0.879888117313385, "xcomet_qe_score": 0.8309249877929688, "metricx_score": 1.472334861755371, "metricx_qe_score": 2.402432918548584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终汇集了来自87个国家的1000多名标注者的16,000多个标注。", "metrics": {"bleu_score": 65.8619846152678, "chrf_score": 67.8935659198817, "xcomet_score": 0.912329912185669, "xcomet_qe_score": 0.9439622163772583, "metricx_score": 3.175278902053833, "metricx_qe_score": 3.5863375663757324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们更有能力回答NLP数据集和模型与谁最一致。", "metrics": {"bleu_score": 37.8148125076091, "chrf_score": 37.90728622170999, "xcomet_score": 0.791994571685791, "xcomet_qe_score": 0.7853115797042847, "metricx_score": 2.1949594020843506, "metricx_qe_score": 1.4188807010650635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP中存在位置性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8341923952102661, "xcomet_qe_score": 0.8352224826812744, "metricx_score": 3.7918624877929688, "metricx_qe_score": 2.050662040710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型与英语国家最一致。因此", "metrics": {"bleu_score": 53.54406755005747, "chrf_score": 46.83491617276122, "xcomet_score": 0.8348510265350342, "xcomet_qe_score": 0.8131952285766602, "metricx_score": 2.594261646270752, "metricx_qe_score": 1.5058249235153198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPT-4的社会可接受性分析,我们发现它与英语国家最一致。我们", "metrics": {"bleu_score": 53.986661874421046, "chrf_score": 48.077396185640524, "xcomet_score": 0.6755136847496033, "xcomet_qe_score": 0.6447960138320923, "metricx_score": 9.380325317382812, "metricx_qe_score": 7.306094646453857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现DynaHate与英语国家最一致。", "metrics": {"bleu_score": 18.16714160175466, "chrf_score": 29.739533528982186, "xcomet_score": 0.884285569190979, "xcomet_qe_score": 0.8381459712982178, "metricx_score": 3.4179205894470215, "metricx_qe_score": 3.76163911819458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现与受过大学教育的人有更多的额外一致性。因此", "metrics": {"bleu_score": 31.367512286763926, "chrf_score": 25.49808499963897, "xcomet_score": 0.639559268951416, "xcomet_qe_score": 0.6966190338134766, "metricx_score": 5.471333026885986, "metricx_qe_score": 1.987683892250061, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPT-4的社会可接受性任务,我们发现它与受过大学教育或研究生教育的人最一致。我们在DynaHate中也发现了同样的情况,它与受过大学教育的人最一致。", "metrics": {"bleu_score": 55.444619975496494, "chrf_score": 48.87271377983573, "xcomet_score": 0.8711515665054321, "xcomet_qe_score": 0.7358519434928894, "metricx_score": 3.8603696823120117, "metricx_qe_score": 4.29109001159668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群一致时,一些人不可避免地被落下。", "metrics": {"bleu_score": 53.861782208165394, "chrf_score": 48.166839354460805, "xcomet_score": 0.7892537117004395, "xcomet_qe_score": 0.7736227512359619, "metricx_score": 2.4731247425079346, "metricx_qe_score": 3.2067861557006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个例子是数据集和模型与非二元性别的人相比,与男性和女性的对应人群一致性较低。", "metrics": {"bleu_score": 34.04515163298936, "chrf_score": 35.53390433962116, "xcomet_score": 0.7442607283592224, "xcomet_qe_score": 0.8198648691177368, "metricx_score": 2.0872814655303955, "metricx_qe_score": 2.2740423679351807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT-4的社会可接受性任务以及DynaHate任务分析中也发现了这一点。", "metrics": {"bleu_score": 77.28082326259643, "chrf_score": 79.32007180243065, "xcomet_score": 0.8911056518554688, "xcomet_qe_score": 0.9252806305885315, "metricx_score": 1.5006153583526611, "metricx_qe_score": 2.2206289768218994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,既然NLP中存在位置性,我们能做些什么呢?我们", "metrics": {"bleu_score": 47.913131225520395, "chrf_score": 51.1537847375452, "xcomet_score": 0.7653179168701172, "xcomet_qe_score": 0.7567037343978882, "metricx_score": 6.550175666809082, "metricx_qe_score": 2.17917799949646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对此有一些建议。", "metrics": {"bleu_score": 52.5917799629412, "chrf_score": 46.51068577152384, "xcomet_score": 0.9661667346954346, "xcomet_qe_score": 0.869490385055542, "metricx_score": 0.38831013441085815, "metricx_qe_score": 0.40066713094711304, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是记录研究过程中的所有相关设计选择。另一个", "metrics": {"bleu_score": 48.287820936261525, "chrf_score": 41.00685860881017, "xcomet_score": 0.7889343500137329, "xcomet_qe_score": 0.7586252689361572, "metricx_score": 4.571674823760986, "metricx_qe_score": 0.46367907524108887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是以多元视角进行NLP研究。", "metrics": {"bleu_score": 8.84550755790478, "chrf_score": 8.612714166743594, "xcomet_score": 0.7302778363227844, "xcomet_qe_score": 0.806679368019104, "metricx_score": 3.2071948051452637, "metricx_qe_score": 2.955644369125366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是为特定社区内的特定社区建立专门的数据集和模型。", "metrics": {"bleu_score": 63.66093834247326, "chrf_score": 60.49286772596388, "xcomet_score": 0.7192918062210083, "xcomet_qe_score": 0.7334275841712952, "metricx_score": 4.303813934326172, "metricx_qe_score": 4.381007671356201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane倡议。", "metrics": {"bleu_score": 55.2058197664637, "chrf_score": 43.77089436218316, "xcomet_score": 0.7475622892379761, "xcomet_qe_score": 0.789522647857666, "metricx_score": 2.372803211212158, "metricx_qe_score": 4.278580188751221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调的是,包容性NLP不仅仅是让所有", "metrics": {"bleu_score": 53.93990899537351, "chrf_score": 50.15272879895956, "xcomet_score": 0.6512722969055176, "xcomet_qe_score": 0.454750657081604, "metricx_score": 5.656595706939697, "metricx_qe_score": 4.532260894775391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为每个人工作。", "metrics": {"bleu_score": 6.87938864869854, "chrf_score": 7.799403610573825, "xcomet_score": 0.8776884078979492, "xcomet_qe_score": 0.8960785865783691, "metricx_score": 3.017136335372925, "metricx_qe_score": 3.2945311069488525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这结束了我们的演示", "metrics": {"bleu_score": 31.76215203205584, "chrf_score": 24.505192041046012, "xcomet_score": 0.9715225696563721, "xcomet_qe_score": 0.966486394405365, "metricx_score": 2.5765256881713867, "metricx_qe_score": 1.7398481369018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但如果你想了解更多,请随时查看我们的仪表板,获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 57.00236711829492, "chrf_score": 51.2656654022242, "xcomet_score": 0.9391933679580688, "xcomet_qe_score": 0.9235668182373047, "metricx_score": 1.3295893669128418, "metricx_qe_score": 1.23336660861969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自福德大学的徐宇元。", "metrics": {"bleu_score": 27.098211583470043, "chrf_score": 18.998670532121942, "xcomet_score": 0.6887797117233276, "xcomet_qe_score": 0.8631327748298645, "metricx_score": 1.7510582208633423, "metricx_qe_score": 0.9944753646850586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我在这里介绍我们的工作,即从大型语言模型中区分脚本知识,用于约束语言规划。", "metrics": {"bleu_score": 34.722246998232244, "chrf_score": 28.863602402430583, "xcomet_score": 0.7838616371154785, "xcomet_qe_score": 0.759297251701355, "metricx_score": 2.5667240619659424, "metricx_qe_score": 2.486074924468994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类通常通过遵循逐步指令的形式来规划他们的行动,这些指令是由指导脚本提供的。", "metrics": {"bleu_score": 23.41674786917353, "chrf_score": 24.05728230555957, "xcomet_score": 0.8404481410980225, "xcomet_qe_score": 0.8393813371658325, "metricx_score": 2.72501277923584, "metricx_qe_score": 3.58113694190979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的工作已经利用语言模型来规划抽象目标的刻板活动,例如制作蛋糕", "metrics": {"bleu_score": 38.27491534128693, "chrf_score": 32.93269650483685, "xcomet_score": 0.8154807090759277, "xcomet_qe_score": 0.8107967376708984, "metricx_score": 3.6022496223449707, "metricx_qe_score": 3.4694011211395264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",并表明大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 58.94767731519404, "chrf_score": 56.61671596261333, "xcomet_score": 0.9060046672821045, "xcomet_qe_score": 0.9114282131195068, "metricx_score": 2.019509792327881, "metricx_qe_score": 1.2149486541748047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的工作主要集中在规划抽象目标的刻板活动,而", "metrics": {"bleu_score": 45.30799450827704, "chrf_score": 36.61170110083154, "xcomet_score": 0.7957701086997986, "xcomet_qe_score": 0.6959322690963745, "metricx_score": 5.910785675048828, "metricx_qe_score": 2.8746302127838135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于具有特定约束的目标的规划,例如制作巧克力蛋糕,仍然是未被研究的领域。", "metrics": {"bleu_score": 21.020179484451404, "chrf_score": 22.379256723265392, "xcomet_score": 0.8992233276367188, "xcomet_qe_score": 0.73173987865448, "metricx_score": 1.7717894315719604, "metricx_qe_score": 1.8503117561340332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们定义了约束语言规划的问题,该问题对规划目标施加了不同的约束。", "metrics": {"bleu_score": 66.9853431376939, "chrf_score": 62.82618796922132, "xcomet_score": 0.885716438293457, "xcomet_qe_score": 0.8560582995414734, "metricx_score": 2.362358808517456, "metricx_qe_score": 2.811948299407959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被不同的现实生活中的特定目标所继承,这些目标具有", "metrics": {"bleu_score": 33.10670933889895, "chrf_score": 30.796933276860262, "xcomet_score": 0.7246882915496826, "xcomet_qe_score": 0.7951396107673645, "metricx_score": 8.76391887664795, "metricx_qe_score": 7.463102340698242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多方面的约束。一个好的规划者应该编写符合约束的合理脚本。在这篇论", "metrics": {"bleu_score": 15.186071495534511, "chrf_score": 17.745360406622215, "xcomet_score": 0.37932151556015015, "xcomet_qe_score": 0.35939013957977295, "metricx_score": 7.3252034187316895, "metricx_qe_score": 5.86639928817749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文中,我们首先评估并改进了大型语言模型的约束语言规划能力。", "metrics": {"bleu_score": 66.10512152914633, "chrf_score": 57.51552672615033, "xcomet_score": 0.7907458543777466, "xcomet_qe_score": 0.7648133039474487, "metricx_score": 1.2982792854309082, "metricx_qe_score": 1.2504241466522217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定目标的数据集来支持我们的研究,我们必须首先获取这些目标。", "metrics": {"bleu_score": 76.30407591491003, "chrf_score": 70.92929026681583, "xcomet_score": 0.8416928052902222, "xcomet_qe_score": 0.8175958395004272, "metricx_score": 1.8240383863449097, "metricx_qe_score": 3.319248914718628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们通过人在回路数据获取使用InstructGPT来扩展抽象目标,以获得多方面的约束。", "metrics": {"bleu_score": 33.160105238148944, "chrf_score": 43.62994959382348, "xcomet_score": 0.6919265985488892, "xcomet_qe_score": 0.607682466506958, "metricx_score": 7.456735610961914, "metricx_qe_score": 7.524561882019043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采样了100个特定目标,并评估了从大型语言模型生成的脚本。", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 45.08950580079796, "xcomet_score": 0.9500799179077148, "xcomet_qe_score": 0.9565447568893433, "metricx_score": 2.0412309169769287, "metricx_qe_score": 2.9219067096710205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有大型语言模型在规划特定目标时都达不到令人满意的结果。", "metrics": {"bleu_score": 33.21972846334521, "chrf_score": 30.710296354444232, "xcomet_score": 0.9138443470001221, "xcomet_qe_score": 0.8632662296295166, "metricx_score": 1.1508976221084595, "metricx_qe_score": 1.6891521215438843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行了详细的分析,以调查为什么大型语言模型的结果不理想。", "metrics": {"bleu_score": 22.898351850650936, "chrf_score": 25.22740045964604, "xcomet_score": 0.8793785572052002, "xcomet_qe_score": 0.8662258982658386, "metricx_score": 1.615282416343689, "metricx_qe_score": 1.9499235153198242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果表明,生成脚本的语义完整性是可以接受的,但对约束的忠实度无法保证。", "metrics": {"bleu_score": 58.55360078146223, "chrf_score": 49.743526662123045, "xcomet_score": 0.9295073747634888, "xcomet_qe_score": 0.9578530788421631, "metricx_score": 1.0360102653503418, "metricx_qe_score": 1.270715355873108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了WikiHow中不同类别的约束的更细粒度的主题分类。", "metrics": {"bleu_score": 19.523863601532454, "chrf_score": 31.555561619665113, "xcomet_score": 0.8285776376724243, "xcomet_qe_score": 0.8197176456451416, "metricx_score": 2.9945876598358154, "metricx_qe_score": 3.540966272354126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,InstructGPT的规划性能在不同类别的目标中显著不同。", "metrics": {"bleu_score": 52.04799777952019, "chrf_score": 59.12322731111801, "xcomet_score": 0.9010101556777954, "xcomet_qe_score": 0.8043733835220337, "metricx_score": 2.551393747329712, "metricx_qe_score": 3.9360666275024414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究表明,大型语言模型的输出质量存在高度变异,导致性能不佳。", "metrics": {"bleu_score": 47.69724674486363, "chrf_score": 45.91489840568905, "xcomet_score": 0.9436286091804504, "xcomet_qe_score": 0.9372222423553467, "metricx_score": 1.1837681531906128, "metricx_qe_score": 1.184848666191101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过度生成和过滤的想法来提高生成质量。", "metrics": {"bleu_score": 48.22592931748994, "chrf_score": 40.53839440552613, "xcomet_score": 0.8867374658584595, "xcomet_qe_score": 0.8359982967376709, "metricx_score": 2.352721929550171, "metricx_qe_score": 3.5252561569213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先向InstructGPT展示了带有示例的约束类型,并根据所述抽象目标获得了特定目标。", "metrics": {"bleu_score": 37.08163623065083, "chrf_score": 44.41068183133025, "xcomet_score": 0.7590247392654419, "xcomet_qe_score": 0.8416195511817932, "metricx_score": 2.8520779609680176, "metricx_qe_score": 3.7950847148895264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,InstructGPT为特定目标生成了K个脚本。", "metrics": {"bleu_score": 28.897738063050745, "chrf_score": 48.22221323973633, "xcomet_score": 0.8431181907653809, "xcomet_qe_score": 0.8348486423492432, "metricx_score": 2.2018418312072754, "metricx_qe_score": 2.878490924835205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤模型来选择忠实的脚本。", "metrics": {"bleu_score": 57.4295228338359, "chrf_score": 52.49847494693214, "xcomet_score": 0.8041801452636719, "xcomet_qe_score": 0.8422807455062866, "metricx_score": 2.394970178604126, "metricx_qe_score": 2.32810640335083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为InstructGPT嵌入,并计算余弦相似度作为相似度得分,以衡量语义相似度。", "metrics": {"bleu_score": 79.92181332053943, "chrf_score": 79.7622234625885, "xcomet_score": 0.916051983833313, "xcomet_qe_score": 0.8087901473045349, "metricx_score": 1.8230167627334595, "metricx_qe_score": 2.058558940887451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们奖励包含目标约束关键词的脚本。", "metrics": {"bleu_score": 57.97765003730531, "chrf_score": 54.89525378061659, "xcomet_score": 0.8477377891540527, "xcomet_qe_score": 0.8053245544433594, "metricx_score": 0.9846863746643066, "metricx_qe_score": 1.2254678010940552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "只有当目标得分在目标集中最高时,我们才保留脚本。", "metrics": {"bleu_score": 54.099008606687384, "chrf_score": 42.3937247642618, "xcomet_score": 0.7630854845046997, "xcomet_qe_score": 0.6910152435302734, "metricx_score": 3.2881484031677246, "metricx_qe_score": 5.855330467224121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,InstructGPT可以生成高质量的脚本。", "metrics": {"bleu_score": 72.62953340221206, "chrf_score": 75.15074104990397, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7731297016143799, "metricx_qe_score": 1.1561170816421509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法大大提高了规划能力,无论是在语义完整性还是对约束的忠实度方面。", "metrics": {"bleu_score": 60.66841973239411, "chrf_score": 52.59948235689428, "xcomet_score": 0.9388656616210938, "xcomet_qe_score": 0.9870235919952393, "metricx_score": 0.8968896269798279, "metricx_qe_score": 1.4757966995239258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型部署成本高,使较小和专业化模型具备语言规划能力至关重要。", "metrics": {"bleu_score": 38.55502836418984, "chrf_score": 31.562970426414623, "xcomet_score": 0.9964114427566528, "xcomet_qe_score": 0.9896103143692017, "metricx_score": 0.4898673892021179, "metricx_qe_score": 1.1158357858657837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键步骤。", "metrics": {"bleu_score": 69.6015973294402, "chrf_score": 66.30344838521414, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026699073612689972, "metricx_qe_score": 0.14870662987232208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究并未实现对特定目标的规划,手动数据集注释成本高昂。", "metrics": {"bleu_score": 27.955841867439805, "chrf_score": 23.341799027256506, "xcomet_score": 0.9783283472061157, "xcomet_qe_score": 0.9172376990318298, "metricx_score": 1.4738738536834717, "metricx_qe_score": 1.8889076709747314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的想法,从大型语言模型中蒸馏约束语言规划数据集。", "metrics": {"bleu_score": 50.87096779647018, "chrf_score": 41.99079684198711, "xcomet_score": 0.8096791505813599, "xcomet_qe_score": 0.7346233129501343, "metricx_score": 3.983039617538452, "metricx_qe_score": 3.452484607696533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将我们的方法应用于构建约束语言规划数据集,命名为CodeScript。", "metrics": {"bleu_score": 60.35911889895248, "chrf_score": 58.84311743431832, "xcomet_score": 0.867895245552063, "xcomet_qe_score": 0.7639021873474121, "metricx_score": 1.3049935102462769, "metricx_qe_score": 1.9303239583969116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了55,000个带有脚本的特定目标。", "metrics": {"bleu_score": 29.62789157394226, "chrf_score": 42.44778530648096, "xcomet_score": 0.888967752456665, "xcomet_qe_score": 0.8385939598083496, "metricx_score": 1.8689863681793213, "metricx_qe_score": 1.570450782775879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试集的质量,我们要求云源工人找到并修正不正确的样本。", "metrics": {"bleu_score": 44.40532594290295, "chrf_score": 36.16360366279306, "xcomet_score": 0.8550723195075989, "xcomet_qe_score": 0.8465344905853271, "metricx_score": 4.560152530670166, "metricx_qe_score": 4.14709997177124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了CodeScript的约束分布。", "metrics": {"bleu_score": 58.33510584342546, "chrf_score": 72.24911196939357, "xcomet_score": 0.9625097513198853, "xcomet_qe_score": 0.8983068466186523, "metricx_score": 1.2738232612609863, "metricx_qe_score": 2.105375289916992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现CodeScript在生成的特定目标中表现出高度的多样性。", "metrics": {"bleu_score": 56.53615736486091, "chrf_score": 57.442392131009356, "xcomet_score": 0.9379866719245911, "xcomet_qe_score": 0.917720377445221, "metricx_score": 2.4337494373321533, "metricx_qe_score": 3.8843283653259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过CodeScript,我们可以训练较小但专业化的模型进行约束语言规划。", "metrics": {"bleu_score": 40.41277217459121, "chrf_score": 35.385503617194516, "xcomet_score": 0.7883569598197937, "xcomet_qe_score": 0.6695766448974609, "metricx_score": 2.6626052856445312, "metricx_qe_score": 2.4357502460479736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,T5在CodeScript上训练可以生成比大多数大型语言模型更高质量的脚本,表明较小的模型在适当的数据集上训练时可以支持较大的模型。", "metrics": {"bleu_score": 46.83783354315825, "chrf_score": 43.81731182566435, "xcomet_score": 0.7080675363540649, "xcomet_qe_score": 0.7837603092193604, "metricx_score": 5.168858051300049, "metricx_qe_score": 4.557351589202881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了约束语言规划问题,", "metrics": {"bleu_score": 45.090228049186024, "chrf_score": 36.29628526827658, "xcomet_score": 0.8723708391189575, "xcomet_qe_score": 0.8503605723381042, "metricx_score": 2.6160809993743896, "metricx_qe_score": 3.5919134616851807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估了大型语言模型的约束语言规划能力,并为大型语言模型开发了过度生成和过滤方法。", "metrics": {"bleu_score": 44.61872359269176, "chrf_score": 36.520407239026085, "xcomet_score": 0.8355104923248291, "xcomet_qe_score": 0.7791379690170288, "metricx_score": 1.5621978044509888, "metricx_qe_score": 2.4020886421203613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成了高质量的脚本数据集CodeScript,用于约束语言规划。", "metrics": {"bleu_score": 50.82258553959301, "chrf_score": 49.43257211310081, "xcomet_score": 0.877152681350708, "xcomet_qe_score": 0.7786824703216553, "metricx_score": 2.3732504844665527, "metricx_qe_score": 3.27644681930542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望CodeScript数据集能够成为推动语言规划研究的有价值资源。", "metrics": {"bleu_score": 47.901455811287484, "chrf_score": 56.58806242988867, "xcomet_score": 0.9269753694534302, "xcomet_qe_score": 0.9287538528442383, "metricx_score": 0.7777780294418335, "metricx_qe_score": 0.941081166267395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中找到更多关于CodeScript的详细信息。", "metrics": {"bleu_score": 65.43503796047172, "chrf_score": 59.06655064753185, "xcomet_score": 0.9643313884735107, "xcomet_qe_score": 0.962000846862793, "metricx_score": 1.6478452682495117, "metricx_qe_score": 1.5599051713943481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫朱恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8176854848861694, "xcomet_qe_score": 0.8306083083152771, "metricx_score": 0.055027518421411514, "metricx_qe_score": 0.18194499611854553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的论文《康奈尔2003命名实体标注器在2023年是否仍然有效?》", "metrics": {"bleu_score": 69.11767741252595, "chrf_score": 67.70522907985814, "xcomet_score": 0.7742260098457336, "xcomet_qe_score": 0.7212932705879211, "metricx_score": 1.868643045425415, "metricx_qe_score": 1.520675539970398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。让我们开始吧。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 95.15349630471859, "xcomet_score": 0.9835532903671265, "xcomet_qe_score": 0.9856215715408325, "metricx_score": 0.7622692584991455, "metricx_qe_score": 1.1052849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了使用命名实体识别任务或NER任务的泛化问题。", "metrics": {"bleu_score": 64.58837230430278, "chrf_score": 57.29019361008084, "xcomet_score": 0.8372697830200195, "xcomet_qe_score": 0.8354064226150513, "metricx_score": 1.4588375091552734, "metricx_qe_score": 3.4592413902282715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用康奈尔2003开发NER近20年。这自然引发了几个问题。", "metrics": {"bleu_score": 23.74987895402236, "chrf_score": 25.9810195077222, "xcomet_score": 0.7364892363548279, "xcomet_qe_score": 0.7831054329872131, "metricx_score": 6.05882453918457, "metricx_qe_score": 5.409761428833008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据?", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9989852905273438, "xcomet_qe_score": 0.9952034950256348, "metricx_score": 0.28112250566482544, "metricx_qe_score": 0.2982765734195709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新的标注器时,需要什么才能实现良好的泛化?", "metrics": {"bleu_score": 62.56538561604213, "chrf_score": 53.31061933235846, "xcomet_score": 0.9991610050201416, "xcomet_qe_score": 0.9945460557937622, "metricx_score": 0.4566256105899811, "metricx_qe_score": 0.6512018442153931, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们观察到泛化不佳,是什么导致了这些模型性能的下降?", "metrics": {"bleu_score": 44.89848893330269, "chrf_score": 39.043879782401, "xcomet_score": 0.9946345090866089, "xcomet_qe_score": 0.9798132181167603, "metricx_score": 0.9603977799415588, "metricx_qe_score": 1.1366544961929321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了康奈尔++数据集。这是", "metrics": {"bleu_score": 44.52675711169778, "chrf_score": 34.64259050128615, "xcomet_score": 0.7365162372589111, "xcomet_qe_score": 0.7492923736572266, "metricx_score": 4.974916934967041, "metricx_qe_score": 1.209228277206421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从2020年的路透社新闻中收集并使用相同的康奈尔2003标注指南进行标注的数据集。", "metrics": {"bleu_score": 50.45690459733317, "chrf_score": 44.110643093199364, "xcomet_score": 0.7333075404167175, "xcomet_qe_score": 0.7639285326004028, "metricx_score": 3.9847280979156494, "metricx_qe_score": 3.911013126373291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们在康奈尔2003上微调了20多个模型。", "metrics": {"bleu_score": 26.75087393232737, "chrf_score": 28.69960093518071, "xcomet_score": 0.8242935538291931, "xcomet_qe_score": 0.8266686201095581, "metricx_score": 3.3696773052215576, "metricx_qe_score": 3.0042147636413574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在康奈尔03测试集和康奈尔++测试集上评估了它们。", "metrics": {"bleu_score": 18.087131559310677, "chrf_score": 21.76810620049017, "xcomet_score": 0.7811374664306641, "xcomet_qe_score": 0.9341086149215698, "metricx_score": 1.9182755947113037, "metricx_qe_score": 2.5261292457580566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了F1的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 71.76532607217811, "chrf_score": 69.17281761409991, "xcomet_score": 0.9953742027282715, "xcomet_qe_score": 0.9882276058197021, "metricx_score": 0.5350840091705322, "metricx_qe_score": 1.1664762496948242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,实现良好泛化需要什么呢?", "metrics": {"bleu_score": 34.93726687866663, "chrf_score": 28.616756438383828, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.20453152060508728, "metricx_qe_score": 0.3028537929058075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要因素。", "metrics": {"bleu_score": 40.9961728958804, "chrf_score": 34.17508665672059, "xcomet_score": 0.968151330947876, "xcomet_qe_score": 0.881928563117981, "metricx_score": 0.9094090461730957, "metricx_qe_score": 1.3229708671569824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现变压器模型通常能更好地泛化到新数据。", "metrics": {"bleu_score": 41.84406556619576, "chrf_score": 27.109902438212778, "xcomet_score": 0.8418577909469604, "xcomet_qe_score": 0.8594259023666382, "metricx_score": 1.8582911491394043, "metricx_qe_score": 1.0764786005020142, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常较大的模型会导致更好的泛化。", "metrics": {"bleu_score": 58.94666012755862, "chrf_score": 48.71380859055254, "xcomet_score": 0.9518848657608032, "xcomet_qe_score": 0.962705135345459, "metricx_score": 0.941569983959198, "metricx_qe_score": 1.3404220342636108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们都知道微调示例的数量直接影响下游任务的性能。", "metrics": {"bleu_score": 70.75192866296081, "chrf_score": 65.03350936014301, "xcomet_score": 0.9707472324371338, "xcomet_qe_score": 0.8968411684036255, "metricx_score": 1.4634007215499878, "metricx_qe_score": 1.281935691833496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们也发现更多的微调示例实际上也会导致更好的泛化。", "metrics": {"bleu_score": 55.492421196566994, "chrf_score": 51.8000175365498, "xcomet_score": 0.9759564399719238, "xcomet_qe_score": 0.8047559857368469, "metricx_score": 0.782873272895813, "metricx_qe_score": 0.9285997152328491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来的问题是,是什么导致了一些模型性能的下降?我们有两个假设。", "metrics": {"bleu_score": 18.196447474398532, "chrf_score": 20.726584012654687, "xcomet_score": 0.9876489639282227, "xcomet_qe_score": 0.9943084716796875, "metricx_score": 0.8765804767608643, "metricx_qe_score": 0.8862314224243164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,即通过反复使用相同的测试集导致的过拟合,这通常表现为在新测试集上的收益递减。", "metrics": {"bleu_score": 68.1834648981405, "chrf_score": 57.53279734921946, "xcomet_score": 0.9669339656829834, "xcomet_qe_score": 0.8759003281593323, "metricx_score": 2.6936378479003906, "metricx_qe_score": 3.3144524097442627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间差距增加而导致的性能下降。", "metrics": {"bleu_score": 67.93382085848448, "chrf_score": 60.85131109681815, "xcomet_score": 0.9699355363845825, "xcomet_qe_score": 0.8868715763092041, "metricx_score": 1.4054200649261475, "metricx_qe_score": 1.9033070802688599, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表中看到,红色的最佳拟合线的梯度大于1。", "metrics": {"bleu_score": 46.604121824410704, "chrf_score": 43.99365366167405, "xcomet_score": 0.8777610063552856, "xcomet_qe_score": 0.813025712966919, "metricx_score": 1.1588019132614136, "metricx_qe_score": 1.4604398012161255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在康奈尔2003上每单位的改进都会转化为康奈尔++上超过一个单位的改进,这意味着没有收益递减。", "metrics": {"bleu_score": 33.94197081727515, "chrf_score": 30.025818404988424, "xcomet_score": 0.7675822377204895, "xcomet_qe_score": 0.7665010094642639, "metricx_score": 3.9952738285064697, "metricx_qe_score": 4.622598648071289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下没有观察到自适应过拟合。", "metrics": {"bleu_score": 74.93731939490364, "chrf_score": 69.43707675795987, "xcomet_score": 0.9009255766868591, "xcomet_qe_score": 0.9129918217658997, "metricx_score": 1.1392955780029297, "metricx_qe_score": 1.6724114418029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间漂移呢?", "metrics": {"bleu_score": 52.47357977607325, "chrf_score": 40.51960415097498, "xcomet_score": 0.9311673641204834, "xcomet_qe_score": 0.9159005284309387, "metricx_score": 0.3731555640697479, "metricx_qe_score": 0.8887962102890015, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,重新训练或继续预训练一些模型,使用更近期的数据,我们发现随着时间差距的增加,性能下降。这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 63.52048541507472, "chrf_score": 56.170572200490255, "xcomet_score": 0.9368571639060974, "xcomet_qe_score": 0.9229847192764282, "metricx_score": 1.7660646438598633, "metricx_qe_score": 1.8170478343963623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化,我们需要更好的模型架构、更大的模型大小以及更多的微调示例。", "metrics": {"bleu_score": 80.29442553780838, "chrf_score": 73.0587099823086, "xcomet_score": 0.9366432428359985, "xcomet_qe_score": 0.8591218590736389, "metricx_score": 0.47305187582969666, "metricx_qe_score": 0.4831877052783966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素是相辅相成的,我们不能只有一个因素,而忽略其他因素。", "metrics": {"bleu_score": 48.876684192545945, "chrf_score": 37.85979069887115, "xcomet_score": 0.9308367967605591, "xcomet_qe_score": 0.9557728171348572, "metricx_score": 0.724704921245575, "metricx_qe_score": 0.5923455357551575, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现性能下降是由时间漂移引起的,而令人惊讶的是,它并不是由自适应过拟合引起的,尽管康奈尔2003已经使用了20多年。", "metrics": {"bleu_score": 62.038487673920066, "chrf_score": 55.13700642974971, "xcomet_score": 0.6419960260391235, "xcomet_qe_score": 0.6866937875747681, "metricx_score": 3.24532413482666, "metricx_qe_score": 3.180839776992798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,回到我们在论文标题中提出的问题,康奈尔2003标注器在2023年是否仍然有效?", "metrics": {"bleu_score": 69.29818744176559, "chrf_score": 68.9945984098773, "xcomet_score": 0.7629422545433044, "xcomet_qe_score": 0.8243881464004517, "metricx_score": 1.9850126504898071, "metricx_qe_score": 1.736948847770691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案是肯定的。", "metrics": {"bleu_score": 67.80814773941113, "chrf_score": 55.183072785867324, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.32976317405700684, "metricx_qe_score": 0.7012989521026611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能促进更多关于如何改进模型泛化的研究。", "metrics": {"bleu_score": 59.12147721562994, "chrf_score": 50.17253028767234, "xcomet_score": 0.9986436367034912, "xcomet_qe_score": 1.0, "metricx_score": 0.369063138961792, "metricx_qe_score": 0.5510718822479248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果有任何问题,请随时联系我。", "metrics": {"bleu_score": 46.73405285184296, "chrf_score": 43.72887767159791, "xcomet_score": 0.9854254722595215, "xcomet_qe_score": 0.9682124853134155, "metricx_score": 0.2657138705253601, "metricx_qe_score": 0.2952011525630951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9583046436309814, "xcomet_qe_score": 0.9632420539855957, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我要谈谈我们在实体选择中的间接引用表达式解析方面的工作,我们引入了Alt Entities Corpus。", "metrics": {"bleu_score": 27.109176504130957, "chrf_score": 37.133077356181104, "xcomet_score": 0.813709020614624, "xcomet_qe_score": 0.8056949973106384, "metricx_score": 5.457333087921143, "metricx_qe_score": 4.6952691078186035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫Javad Hosseini,这是与Philip Radinsky、Sylvia Parity和Annie Lewis的联合工作。", "metrics": {"bleu_score": 14.662351987892245, "chrf_score": 48.003527229347256, "xcomet_score": 0.7069272398948669, "xcomet_qe_score": 0.7044771909713745, "metricx_score": 3.7568440437316895, "metricx_qe_score": 2.6913199424743652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时的语言。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9999474287033081, "xcomet_qe_score": 0.9556578397750854, "metricx_score": 0.6249333620071411, "metricx_qe_score": 0.9737254977226257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑这个替代问题。", "metrics": {"bleu_score": 38.62752974508188, "chrf_score": 30.236678100518567, "xcomet_score": 0.8316097259521484, "xcomet_qe_score": 0.8311355113983154, "metricx_score": 0.6296630501747131, "metricx_qe_score": 0.4379725456237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指《Easy on Me》还是《I Got a Feeling》?这里", "metrics": {"bleu_score": 11.7942240532671, "chrf_score": 46.91059440148832, "xcomet_score": 0.908374547958374, "xcomet_qe_score": 0.8999679088592529, "metricx_score": 5.0586957931518555, "metricx_qe_score": 2.2308847904205322, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",用户想在两首歌中做出选择。", "metrics": {"bleu_score": 24.270940595211858, "chrf_score": 21.424070595570164, "xcomet_score": 0.9634242057800293, "xcomet_qe_score": 0.9637237787246704, "metricx_score": 2.6728012561798096, "metricx_qe_score": 2.037842273712158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用。例如,通过说出歌曲的名字《Easy on Me》或它的位置,第一首。", "metrics": {"bleu_score": 40.003810431098245, "chrf_score": 43.47871109432842, "xcomet_score": 0.7279434204101562, "xcomet_qe_score": 0.6686991453170776, "metricx_score": 3.424617290496826, "metricx_qe_score": 4.4028000831604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但有时间接引用更适合进行更自然的对话。这可能发生", "metrics": {"bleu_score": 55.132511228599434, "chrf_score": 56.46791345000426, "xcomet_score": 0.6797434091567993, "xcomet_qe_score": 0.5945132970809937, "metricx_score": 9.695602416992188, "metricx_qe_score": 8.271942138671875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在用户无法记住歌曲名字的情况下。", "metrics": {"bleu_score": 5.8383472017129945, "chrf_score": 9.777029572424185, "xcomet_score": 0.9789046049118042, "xcomet_qe_score": 0.9264142513275146, "metricx_score": 1.4282044172286987, "metricx_qe_score": 0.906863808631897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者发音太相似,难以区分。", "metrics": {"bleu_score": 26.844796077522513, "chrf_score": 25.627384820404686, "xcomet_score": 0.9827321767807007, "xcomet_qe_score": 0.9808019399642944, "metricx_score": 0.745283305644989, "metricx_qe_score": 0.2094736397266388, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想指定一个偏好时。", "metrics": {"bleu_score": 24.222499131673658, "chrf_score": 25.079112521050618, "xcomet_score": 0.999053955078125, "xcomet_qe_score": 1.0, "metricx_score": 0.7395358085632324, "metricx_qe_score": 0.6086918711662292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些间接引用的例子。例如,较新的那首或那首不活泼的歌。", "metrics": {"bleu_score": 30.131677038397275, "chrf_score": 26.516093368183945, "xcomet_score": 0.7899641394615173, "xcomet_qe_score": 0.8068517446517944, "metricx_score": 3.10719633102417, "metricx_qe_score": 2.882606029510498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是评估LLM实体理解的基准。", "metrics": {"bleu_score": 47.77807109077616, "chrf_score": 49.34567098154938, "xcomet_score": 0.8942199945449829, "xcomet_qe_score": 0.7983815670013428, "metricx_score": 1.709434986114502, "metricx_qe_score": 3.630647659301758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不知道有公开的大规模数据集用于这个任务,所以我们使用众包注释收集了一个数据集。", "metrics": {"bleu_score": 35.54735590894133, "chrf_score": 31.1006693513293, "xcomet_score": 0.8635751008987427, "xcomet_qe_score": 0.8404558897018433, "metricx_score": 2.071159839630127, "metricx_qe_score": 2.115644693374634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和食谱。", "metrics": {"bleu_score": 78.47574847738748, "chrf_score": 71.38793914595793, "xcomet_score": 0.9996216297149658, "xcomet_qe_score": 0.9887402057647705, "metricx_score": 0.2191678285598755, "metricx_qe_score": 0.3114262521266937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通完成设置。", "metrics": {"bleu_score": 73.41087329408752, "chrf_score": 65.98538196580246, "xcomet_score": 0.8596484065055847, "xcomet_qe_score": 0.8180302977561951, "metricx_score": 2.6962547302246094, "metricx_qe_score": 4.135033130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通有三个语音气泡。", "metrics": {"bleu_score": 15.187207110382285, "chrf_score": 11.468257615211023, "xcomet_score": 0.7489882707595825, "xcomet_qe_score": 0.7105106115341187, "metricx_score": 1.3092339038848877, "metricx_qe_score": 1.2042224407196045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡中,Bob说:“记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 72.27186387739211, "chrf_score": 64.96208702730442, "xcomet_score": 0.975521445274353, "xcomet_qe_score": 0.8842308521270752, "metricx_score": 1.4036808013916016, "metricx_qe_score": 2.1117360591888428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Bob设定了对话的背景。", "metrics": {"bleu_score": 18.32556812998321, "chrf_score": 19.785985946886342, "xcomet_score": 0.9315756559371948, "xcomet_qe_score": 0.9034993648529053, "metricx_score": 1.7425591945648193, "metricx_qe_score": 1.6031594276428223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个语音气泡中,Alice说:“你是指《Easy on Me》还是《I Got a Feeling》?”", "metrics": {"bleu_score": 18.89471911184979, "chrf_score": 45.8630048149651, "xcomet_score": 0.9452277421951294, "xcomet_qe_score": 0.9176438450813293, "metricx_score": 2.911660671234131, "metricx_qe_score": 3.6157565116882324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8669623136520386, "xcomet_qe_score": 0.8554072380065918, "metricx_score": 1.420009732246399, "metricx_qe_score": 2.3006839752197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个语音气泡中,Bob使用间接引用选择其中一个实体。例如,较新的那首。", "metrics": {"bleu_score": 47.30733783952617, "chrf_score": 41.900607067369755, "xcomet_score": 0.6938343048095703, "xcomet_qe_score": 0.6872522830963135, "metricx_score": 4.087015628814697, "metricx_qe_score": 6.245171070098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个语音气泡,但第三个由注释者填写。", "metrics": {"bleu_score": 58.33454870492689, "chrf_score": 51.5367022573626, "xcomet_score": 0.9038559198379517, "xcomet_qe_score": 0.846793532371521, "metricx_score": 1.693176031112671, "metricx_qe_score": 1.7513864040374756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个语音气泡是从每个领域的几个手动提示中选择的。", "metrics": {"bleu_score": 60.84121675336109, "chrf_score": 55.566910093914586, "xcomet_score": 0.7735045552253723, "xcomet_qe_score": 0.7265084981918335, "metricx_score": 2.3225150108337402, "metricx_qe_score": 2.4539427757263184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即替代问题,是通过以下方式生成的。", "metrics": {"bleu_score": 13.304062588217075, "chrf_score": 15.49720878144436, "xcomet_score": 0.9126898050308228, "xcomet_qe_score": 0.8997386693954468, "metricx_score": 0.9415650367736816, "metricx_qe_score": 0.8464066982269287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是从维基百科中抽样的。", "metrics": {"bleu_score": 33.15796151992084, "chrf_score": 28.49509663893875, "xcomet_score": 0.974881649017334, "xcomet_qe_score": 0.9926190376281738, "metricx_score": 1.7050693035125732, "metricx_qe_score": 1.1891224384307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同抽样方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09123439341783524, "metricx_qe_score": 0.11483591794967651, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体变得更加相似,通常更难进行消歧。", "metrics": {"bleu_score": 58.98466143484528, "chrf_score": 51.36728888469803, "xcomet_score": 0.8547371625900269, "xcomet_qe_score": 0.7941489219665527, "metricx_score": 4.7627668380737305, "metricx_qe_score": 5.6383466720581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀随机。", "metrics": {"bleu_score": 19.437571020720103, "chrf_score": 18.153313508301462, "xcomet_score": 0.9029233455657959, "xcomet_qe_score": 0.8752520084381104, "metricx_score": 1.6346652507781982, "metricx_qe_score": 1.535372018814087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个是当实体有相似的标题时。例如,两本书的名字是《The Return》。", "metrics": {"bleu_score": 17.798185213207663, "chrf_score": 38.00019172471147, "xcomet_score": 0.8995383977890015, "xcomet_qe_score": 0.8828098773956299, "metricx_score": 2.345075845718384, "metricx_qe_score": 3.485076665878296, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三个是当它们在维基百科上有相似的描述时。", "metrics": {"bleu_score": 69.37049107407184, "chrf_score": 65.84280349566727, "xcomet_score": 0.9865096807479858, "xcomet_qe_score": 0.9750679731369019, "metricx_score": 0.4153432250022888, "metricx_qe_score": 0.5972211360931396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当它们在维基百科上有相似的信息框或属性时。", "metrics": {"bleu_score": 69.63845241054851, "chrf_score": 64.76296666996701, "xcomet_score": 0.9417118430137634, "xcomet_qe_score": 0.997989296913147, "metricx_score": 0.9182050228118896, "metricx_qe_score": 1.1083542108535767, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,相同的流派或相同的艺术家的歌曲。", "metrics": {"bleu_score": 13.400825781778892, "chrf_score": 19.496901563641533, "xcomet_score": 0.8836283087730408, "xcomet_qe_score": 0.8275895118713379, "metricx_score": 1.3325623273849487, "metricx_qe_score": 0.9450982213020325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向注释者展示这个替代问题时,他们知道这些实体的名字,但不一定知道实体。", "metrics": {"bleu_score": 59.27658173204878, "chrf_score": 52.65141862774678, "xcomet_score": 0.8158791065216064, "xcomet_qe_score": 0.6568915843963623, "metricx_score": 2.4751484394073486, "metricx_qe_score": 3.9876177310943604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们做的是展示关于这两个实体的一些背景知识。", "metrics": {"bleu_score": 85.2409463085467, "chrf_score": 86.62233328376591, "xcomet_score": 0.9570207595825195, "xcomet_qe_score": 0.7504105567932129, "metricx_score": 0.9728729128837585, "metricx_qe_score": 1.5258588790893555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只是展示每首歌的Google搜索链接,然后要求注释者至少听每首歌的一部分,并阅读每首歌的信息。", "metrics": {"bleu_score": 66.15148415449197, "chrf_score": 55.09009499762928, "xcomet_score": 0.8353701829910278, "xcomet_qe_score": 0.7283250093460083, "metricx_score": 1.2681950330734253, "metricx_qe_score": 1.5323556661605835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这是《Easy on Me》的Google搜索结果。", "metrics": {"bleu_score": 29.12014808653287, "chrf_score": 38.65465899671818, "xcomet_score": 0.9613010883331299, "xcomet_qe_score": 0.9560025334358215, "metricx_score": 0.9260735511779785, "metricx_qe_score": 0.9258946180343628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示一些来自维基百科的背景文本。", "metrics": {"bleu_score": 65.13952476067622, "chrf_score": 55.54533958633544, "xcomet_score": 0.9787647724151611, "xcomet_qe_score": 0.9163993000984192, "metricx_score": 0.7456642985343933, "metricx_qe_score": 1.2472145557403564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还展示它们的图像,同样来自维基百科,这样注释者就知道它们的样子。", "metrics": {"bleu_score": 46.479484456412195, "chrf_score": 37.407968525515365, "xcomet_score": 0.9632996320724487, "xcomet_qe_score": 0.9469811916351318, "metricx_score": 1.1545573472976685, "metricx_qe_score": 1.35326087474823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求注释者选择其中一个实体,例如这里的第一个,并用三到五个间接引用表达式描述它们。", "metrics": {"bleu_score": 39.980943581555834, "chrf_score": 34.655873640846124, "xcomet_score": 0.9627227783203125, "xcomet_qe_score": 0.9097257256507874, "metricx_score": 1.752109169960022, "metricx_qe_score": 2.1533663272857666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,带钢琴音乐的那首。", "metrics": {"bleu_score": 10.600313379512592, "chrf_score": 12.196573060447404, "xcomet_score": 0.9970877170562744, "xcomet_qe_score": 0.985682487487793, "metricx_score": 1.2426906824111938, "metricx_qe_score": 1.0854383707046509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集的一些例子。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 70.64217653798426, "xcomet_score": 0.9775285720825195, "xcomet_qe_score": 0.9668039083480835, "metricx_score": 0.10977534204721451, "metricx_qe_score": 0.4197329580783844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有歌词的那首,不是带12岁男孩的那首,或者虚构的那首,或者来自阿塞拜疆的那首,等等。", "metrics": {"bleu_score": 26.636207669526577, "chrf_score": 26.100542652176177, "xcomet_score": 0.8773335218429565, "xcomet_qe_score": 0.8469736576080322, "metricx_score": 2.9576892852783203, "metricx_qe_score": 3.1825997829437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Alt Entities Corpus有6000个替代问题,涵盖三个领域,有42000个间接引用表达式。", "metrics": {"bleu_score": 21.659162811481316, "chrf_score": 40.625266285606216, "xcomet_score": 0.8093069791793823, "xcomet_qe_score": 0.797086238861084, "metricx_score": 3.222621440887451, "metricx_qe_score": 3.448228597640991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "T5X大模型的结果总结如下。", "metrics": {"bleu_score": 30.238706547866272, "chrf_score": 28.233559223784944, "xcomet_score": 0.8343373537063599, "xcomet_qe_score": 0.8305730223655701, "metricx_score": 1.7146514654159546, "metricx_qe_score": 1.7749769687652588, "linguapy_score": [1, "SOMALI"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型有与注释者完全相同的背景知识,那么准确率非常高。大约在92到95%。", "metrics": {"bleu_score": 53.817410977791326, "chrf_score": 48.95852213869052, "xcomet_score": 0.964348316192627, "xcomet_qe_score": 0.9768621325492859, "metricx_score": 0.8828490376472473, "metricx_qe_score": 0.9058293104171753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这不现实。", "metrics": {"bleu_score": 28.49181887722137, "chrf_score": 23.71472002904075, "xcomet_score": 0.9991846084594727, "xcomet_qe_score": 0.9858999848365784, "metricx_score": 0.04951542243361473, "metricx_qe_score": 0.05270039662718773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型有部分重叠的背景知识,那么准确率在82到87%之间,这更现实。", "metrics": {"bleu_score": 50.99580113247363, "chrf_score": 47.7661589811848, "xcomet_score": 0.9291316866874695, "xcomet_qe_score": 0.9502314329147339, "metricx_score": 1.2378852367401123, "metricx_qe_score": 1.4721002578735352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率只有60%。所以有很大的改进空间。", "metrics": {"bleu_score": 79.15373991305087, "chrf_score": 73.35666309931015, "xcomet_score": 0.9952836036682129, "xcomet_qe_score": 0.9906188249588013, "metricx_score": 1.5761001110076904, "metricx_qe_score": 2.3965680599212646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还展示了模型是领域泛化的。", "metrics": {"bleu_score": 12.534968779868036, "chrf_score": 13.768102086118214, "xcomet_score": 0.8405835032463074, "xcomet_qe_score": 0.8131977319717407, "metricx_score": 1.7969878911972046, "metricx_qe_score": 2.408574342727661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是萨拉·帕皮,来自特伦托大学和布鲁诺·凯斯勒基金会,我将简要介绍《注意力作为同时口译的指南》论文,这是与马泰奥·内格里和马尔科·图尔基的合作成果。", "metrics": {"bleu_score": 48.86754037064962, "chrf_score": 38.64097508383164, "xcomet_score": 0.8123794794082642, "xcomet_qe_score": 0.6959086656570435, "metricx_score": 3.1406643390655518, "metricx_qe_score": 2.7953195571899414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同时口译?", "metrics": {"bleu_score": 36.55552228545123, "chrf_score": 26.4484126984127, "xcomet_score": 0.8626642227172852, "xcomet_qe_score": 0.8666504621505737, "metricx_score": 0.799177348613739, "metricx_qe_score": 0.5981802344322205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时口译或SimulST是将口头语言实时翻译成另一种语言的文本的过程,使跨语言交流成为可能。", "metrics": {"bleu_score": 53.3887121861139, "chrf_score": 52.693784363245996, "xcomet_score": 0.9520525932312012, "xcomet_qe_score": 0.9370155334472656, "metricx_score": 2.438955545425415, "metricx_qe_score": 3.273684501647949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前SimulST模型的问题是什么?", "metrics": {"bleu_score": 19.156928817239653, "chrf_score": 47.80580394550984, "xcomet_score": 0.937305212020874, "xcomet_qe_score": 0.9294310212135315, "metricx_score": 0.6831821799278259, "metricx_qe_score": 1.0893099308013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常会训练特定的架构,引入需要优化的额外模块。", "metrics": {"bleu_score": 8.903250061620762, "chrf_score": 19.270917534896377, "xcomet_score": 0.9583421945571899, "xcomet_qe_score": 0.8127033114433289, "metricx_score": 0.6493297815322876, "metricx_qe_score": 1.5014511346817017, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练过程复杂且冗长,例如涉及不同优化目标的训练", "metrics": {"bleu_score": 65.22125981013674, "chrf_score": 59.12254159821232, "xcomet_score": 0.9346542954444885, "xcomet_qe_score": 0.9282608032226562, "metricx_score": 0.5328640937805176, "metricx_qe_score": 0.7458705902099609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",以及训练和维护多个模型以实现不同的延迟", "metrics": {"bleu_score": 41.08851946948646, "chrf_score": 38.0757156486895, "xcomet_score": 0.9375119209289551, "xcomet_qe_score": 0.9322165250778198, "metricx_score": 2.422794818878174, "metricx_qe_score": 1.9047521352767944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模式,例如训练一个延迟平均为一秒的模型,另一个延迟为两秒的模型,依此类推。", "metrics": {"bleu_score": 56.751651699462904, "chrf_score": 50.00814199261489, "xcomet_score": 0.6134634613990784, "xcomet_qe_score": 0.5174533128738403, "metricx_score": 1.6962019205093384, "metricx_qe_score": 2.1052629947662354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么我们的解决方案是什么?", "metrics": {"bleu_score": 84.46319809857219, "chrf_score": 83.69464817082437, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.01612722873687744, "metricx_qe_score": 0.2782573699951172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用现有的SimulST模型,而不需要重新训练或采用特定的SimulST架构。", "metrics": {"bleu_score": 57.197772832832385, "chrf_score": 62.4691080412505, "xcomet_score": 0.8703155517578125, "xcomet_qe_score": 0.936407208442688, "metricx_score": 3.9330601692199707, "metricx_qe_score": 4.9431352615356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用一个模型来处理所有延迟模式,并通过特定参数来处理延迟。利用模型已经获得的知识", "metrics": {"bleu_score": 42.580804795930725, "chrf_score": 45.31701791257734, "xcomet_score": 0.8147211074829102, "xcomet_qe_score": 0.8050383925437927, "metricx_score": 2.639122724533081, "metricx_qe_score": 2.0552070140838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过音频输入和文本输出之间的注意力机制,即", "metrics": {"bleu_score": 54.154652924102415, "chrf_score": 59.851200707430365, "xcomet_score": 0.619218111038208, "xcomet_qe_score": 0.4935523271560669, "metricx_score": 9.563143730163574, "metricx_qe_score": 7.86262321472168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "交叉注意力机制。你可以在右边看到一个例子。", "metrics": {"bleu_score": 34.68941568986593, "chrf_score": 32.831245516957736, "xcomet_score": 0.8872262239456177, "xcomet_qe_score": 0.842902660369873, "metricx_score": 0.7496596574783325, "metricx_qe_score": 0.9454426765441895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出EDAT或编码器-解码器注意力,这是一种策略,我们根据注意力的指向决定是否发出部分翻译。", "metrics": {"bleu_score": 54.696659242740076, "chrf_score": 48.67240319091066, "xcomet_score": 0.5889608860015869, "xcomet_qe_score": 0.5908945798873901, "metricx_score": 4.802241802215576, "metricx_qe_score": 6.222902774810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力不集中,即其总和低于某个阈值α,指向最后的λ语音帧,这意味着接收到的信息足够稳定。", "metrics": {"bleu_score": 50.943461443307136, "chrf_score": 43.40202119094183, "xcomet_score": 0.8119481801986694, "xcomet_qe_score": 0.715944230556488, "metricx_score": 5.59794807434082, "metricx_qe_score": 5.8915276527404785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们接收到一个包含“我要谈论”的语音块,我们的模型预测德语翻译,我们查看交叉注意力权重,会发现前两个单词指向最早接收到的语音帧,而最后一个单词指向最后接收到的语音帧,即λ语音帧。", "metrics": {"bleu_score": 52.661181891329996, "chrf_score": 41.1542708192927, "xcomet_score": 0.5870734453201294, "xcomet_qe_score": 0.5386679172515869, "metricx_score": 3.2069497108459473, "metricx_qe_score": 4.5319437980651855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个单词将被发出,而由于交叉注意力的总和高于某个阈值α,我们不会发出最后一个单词,并等待另一个语音块。", "metrics": {"bleu_score": 64.20130728244166, "chrf_score": 54.66800311929364, "xcomet_score": 0.6453336477279663, "xcomet_qe_score": 0.7130336761474609, "metricx_score": 2.8149001598358154, "metricx_qe_score": 3.494016170501709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续,接收到另一个语音块,我们的模型预测另外三个单词,我们查看交叉注意力权重,会发现没有单词指向最后的λ语音帧。", "metrics": {"bleu_score": 61.153709921196835, "chrf_score": 53.20826777950879, "xcomet_score": 0.7556397914886475, "xcomet_qe_score": 0.5344628691673279, "metricx_score": 2.853109836578369, "metricx_qe_score": 3.60135817527771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个单词将被发出。", "metrics": {"bleu_score": 47.855439210937384, "chrf_score": 38.86889545481519, "xcomet_score": 0.9408911466598511, "xcomet_qe_score": 0.9240285158157349, "metricx_score": 0.975865364074707, "metricx_qe_score": 1.911775827407837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们查看EDAT的主要结果,我们将同时口译结果绘制在图表中,其中一侧是蓝色,测量翻译质量和平均延迟,即延迟测量。我们还考虑计算感知的平均延迟,这考虑了模型的计算时间来预测输出。", "metrics": {"bleu_score": 34.29871344676782, "chrf_score": 28.750313017635325, "xcomet_score": 0.5437400341033936, "xcomet_qe_score": 0.5140635967254639, "metricx_score": 6.368362903594971, "metricx_qe_score": 5.815249443054199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望我们的曲线在这张图表上尽可能高,", "metrics": {"bleu_score": 32.05573663455321, "chrf_score": 31.664166122009835, "xcomet_score": 0.9635024070739746, "xcomet_qe_score": 0.8723122477531433, "metricx_score": 1.843194603919983, "metricx_qe_score": 1.6520705223083496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但也希望它们向左移动。", "metrics": {"bleu_score": 69.63547789070398, "chrf_score": 69.27893978527027, "xcomet_score": 0.9835904836654663, "xcomet_qe_score": 0.9115892648696899, "metricx_score": 0.7925839424133301, "metricx_qe_score": 1.2165919542312622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与适用于离线模型的策略进行比较,即惠特基策略和局部协议。", "metrics": {"bleu_score": 43.77121098094503, "chrf_score": 25.717887578582832, "xcomet_score": 0.8135964870452881, "xcomet_qe_score": 0.7515627145767212, "metricx_score": 2.9485931396484375, "metricx_qe_score": 3.5681650638580322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与专门为同时口译设计的最先进架构进行比较。", "metrics": {"bleu_score": 68.4916321235026, "chrf_score": 62.63935047320462, "xcomet_score": 0.8959459662437439, "xcomet_qe_score": 0.8419622778892517, "metricx_score": 2.3965678215026855, "metricx_qe_score": 3.1781787872314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是同时口译策略在德语上的所有结果。", "metrics": {"bleu_score": 17.01668259220275, "chrf_score": 19.77634409991823, "xcomet_score": 0.7871502637863159, "xcomet_qe_score": 0.7770142555236816, "metricx_score": 2.3284108638763428, "metricx_qe_score": 1.7554516792297363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到EDAT在所有适用于离线模型的策略中表现最佳,因为曲线向左移动。", "metrics": {"bleu_score": 58.78787828549008, "chrf_score": 56.85666476943803, "xcomet_score": 0.9144784212112427, "xcomet_qe_score": 0.9043961763381958, "metricx_score": 2.418687343597412, "metricx_qe_score": 3.9436557292938232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果考虑实际经过的时间或计算感知的时间,EDAT是最快的策略。", "metrics": {"bleu_score": 34.40765043281148, "chrf_score": 29.451855840777093, "xcomet_score": 0.8497405052185059, "xcomet_qe_score": 0.8023481369018555, "metricx_score": 4.123435974121094, "metricx_qe_score": 4.203437805175781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果你想了解更多结果,请阅读我们的论文,", "metrics": {"bleu_score": 67.09293368821515, "chrf_score": 58.303305240224745, "xcomet_score": 0.9701030254364014, "xcomet_qe_score": 0.9523918628692627, "metricx_score": 0.24462208151817322, "metricx_qe_score": 0.2905891537666321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码和模型以及同时输出,以便重现我们的工作。", "metrics": {"bleu_score": 45.26025625161957, "chrf_score": 40.62142477487804, "xcomet_score": 0.7397225499153137, "xcomet_qe_score": 0.767351508140564, "metricx_score": 1.6397271156311035, "metricx_qe_score": 2.76271390914917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢你的注意。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9240017533302307, "xcomet_qe_score": 0.9666193127632141, "metricx_score": 2.3264148235321045, "metricx_qe_score": 2.392186403274536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫英,我和我的同事志洋将介绍我们关于多指令的研究,通过指令调优改进多模态零样本学习。", "metrics": {"bleu_score": 54.501113631904246, "chrf_score": 35.1044537985196, "xcomet_score": 0.5708568692207336, "xcomet_qe_score": 0.5428876876831055, "metricx_score": 4.4634318351745605, "metricx_qe_score": 4.678407669067383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索新的学习范式,即以参数和数据高效的方式重新使用预训练语言模型进行不同的下游任务。", "metrics": {"bleu_score": 67.53528444629545, "chrf_score": 60.00758882280408, "xcomet_score": 0.9017399549484253, "xcomet_qe_score": 0.8717886209487915, "metricx_score": 1.6547081470489502, "metricx_qe_score": 2.470348358154297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优使大型语言模型能够通过遵循自然指令以零样本的方式执行未见任务。", "metrics": {"bleu_score": 57.09587546935079, "chrf_score": 48.197211785430824, "xcomet_score": 0.8383736610412598, "xcomet_qe_score": 0.7541683912277222, "metricx_score": 2.2266316413879395, "metricx_qe_score": 3.907489776611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数以前的指令调优研究集中在改进语言任务的零样本性能,而计算机视觉和多模态任务被忽略了。", "metrics": {"bleu_score": 43.43420818347006, "chrf_score": 36.83205413085862, "xcomet_score": 0.9749486446380615, "xcomet_qe_score": 0.8240197896957397, "metricx_score": 0.997218668460846, "metricx_qe_score": 1.3319276571273804, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想要研究指令调优是否能够改进多模态预训练模型对未见多模态任务的泛化能力。", "metrics": {"bleu_score": 41.08178665791611, "chrf_score": 36.746304859777815, "xcomet_score": 0.8928922414779663, "xcomet_qe_score": 0.7351045608520508, "metricx_score": 1.3869075775146484, "metricx_qe_score": 1.4210819005966187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现指令数据集在自然语言处理和多模态之间存在显著差异。", "metrics": {"bleu_score": 45.63983890511838, "chrf_score": 39.56674078722, "xcomet_score": 0.8090122938156128, "xcomet_qe_score": 0.7968083620071411, "metricx_score": 1.573896884918213, "metricx_qe_score": 1.622704267501831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过1600个语言任务,", "metrics": {"bleu_score": 24.134997142771308, "chrf_score": 37.55963791570473, "xcomet_score": 0.888572096824646, "xcomet_qe_score": 0.8518089652061462, "metricx_score": 2.4736998081207275, "metricx_qe_score": 3.241063117980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而没有大规模的公开可用的多模态指令任务。", "metrics": {"bleu_score": 68.91557807535084, "chrf_score": 66.31152068867469, "xcomet_score": 0.8896650075912476, "xcomet_qe_score": 0.8345797061920166, "metricx_score": 1.4601514339447021, "metricx_qe_score": 2.0919673442840576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这激励我们构建一个多模态指令调优数据集。", "metrics": {"bleu_score": 76.74174160136336, "chrf_score": 70.5002322450835, "xcomet_score": 0.9718524217605591, "xcomet_qe_score": 0.9641896486282349, "metricx_score": 0.8962436318397522, "metricx_qe_score": 1.0525983572006226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们介绍Multi-Instruct,这是第一个多模态指令调优基准数据集,包含62个多样化的多模态任务,涵盖10个广泛的类别。", "metrics": {"bleu_score": 45.67268742527384, "chrf_score": 44.72168193173425, "xcomet_score": 0.8999360203742981, "xcomet_qe_score": 0.8735648393630981, "metricx_score": 1.559822916984558, "metricx_qe_score": 1.7340697050094604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来自21个现有的开源数据集,每个任务都配备了五个专家撰写的指令。", "metrics": {"bleu_score": 60.511005357971236, "chrf_score": 53.601631467874235, "xcomet_score": 0.9660982489585876, "xcomet_qe_score": 0.8649647235870361, "metricx_score": 1.2197792530059814, "metricx_qe_score": 2.165736198425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令调优,我们选择OFA作为基础模型,这是一个统一的多模态预训练模型。OFA使用统一的词", "metrics": {"bleu_score": 64.37295293874394, "chrf_score": 71.82600534387825, "xcomet_score": 0.7553164958953857, "xcomet_qe_score": 0.7036712765693665, "metricx_score": 6.239915370941162, "metricx_qe_score": 2.9544267654418945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来表示语言、图像标记和边界框的坐标。", "metrics": {"bleu_score": 40.142754193450884, "chrf_score": 31.737148627860805, "xcomet_score": 0.3397049307823181, "xcomet_qe_score": 0.3453698754310608, "metricx_score": 6.806940078735352, "metricx_qe_score": 6.82180118560791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示了我们的多模态指令数据集中的一些示例。为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 53.792598242978976, "chrf_score": 39.703716910082946, "xcomet_score": 0.8531301021575928, "xcomet_qe_score": 0.871180534362793, "metricx_score": 2.3310916423797607, "metricx_qe_score": 2.2100861072540283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,将所有任务统一表示为序列到序列的格式,其中", "metrics": {"bleu_score": 46.584997002150004, "chrf_score": 44.7739525982351, "xcomet_score": 0.7009440660476685, "xcomet_qe_score": 0.7188401222229004, "metricx_score": 3.620375156402588, "metricx_qe_score": 2.739968776702881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框在同一个标记空间中表示。", "metrics": {"bleu_score": 69.92372639703295, "chrf_score": 63.38421612833321, "xcomet_score": 0.9845274686813354, "xcomet_qe_score": 0.9617322683334351, "metricx_score": 0.8699555993080139, "metricx_qe_score": 1.0580729246139526, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我将谈论多模态指令调优。", "metrics": {"bleu_score": 37.79384372809367, "chrf_score": 32.42618669575864, "xcomet_score": 0.9136258363723755, "xcomet_qe_score": 0.9578653573989868, "metricx_score": 0.7677769660949707, "metricx_qe_score": 0.8019232153892517, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用53个任务进行训练,并为每个任务采样10,000个实例。", "metrics": {"bleu_score": 56.77447387476321, "chrf_score": 55.36986724768636, "xcomet_score": 0.907616376876831, "xcomet_qe_score": 0.8693835735321045, "metricx_score": 3.011995315551758, "metricx_qe_score": 4.1547040939331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识推理组进行测试,并从VQA和杂项组中选择额外的五个任务。", "metrics": {"bleu_score": 46.29685033750848, "chrf_score": 40.64400544021853, "xcomet_score": 0.6168797016143799, "xcomet_qe_score": 0.6560167074203491, "metricx_score": 3.9827497005462646, "metricx_qe_score": 4.330375671386719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用测试集中的所有实例进行测试。", "metrics": {"bleu_score": 43.23177783155441, "chrf_score": 36.84549483476709, "xcomet_score": 0.8189873695373535, "xcomet_qe_score": 0.8110085725784302, "metricx_score": 1.1527605056762695, "metricx_qe_score": 1.945239782333374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试集中随机采样20个任务作为NLP的未见任务。", "metrics": {"bleu_score": 51.014747404110004, "chrf_score": 46.70857113059693, "xcomet_score": 0.7213227152824402, "xcomet_qe_score": 0.6767432689666748, "metricx_score": 3.2976016998291016, "metricx_qe_score": 2.9356653690338135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的OFA大模型作为基础模型。", "metrics": {"bleu_score": 86.06031405392808, "chrf_score": 81.97230684018221, "xcomet_score": 0.9376983642578125, "xcomet_qe_score": 0.8620163202285767, "metricx_score": 1.4512758255004883, "metricx_qe_score": 2.7902169227600098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们混合所有任务的所有实例。", "metrics": {"bleu_score": 74.08842640893447, "chrf_score": 63.891815530865145, "xcomet_score": 0.8026423454284668, "xcomet_qe_score": 0.8263124823570251, "metricx_score": 0.8326014280319214, "metricx_qe_score": 1.4236559867858887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与其五个指令模板中的一个结合。", "metrics": {"bleu_score": 74.17090125042293, "chrf_score": 69.06413590609232, "xcomet_score": 0.8967591524124146, "xcomet_qe_score": 0.848268985748291, "metricx_score": 1.8565388917922974, "metricx_qe_score": 2.2801826000213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在测试过程中,对于每个任务,我们通过在每个实验中使用五个指令中的一个来进行五次实验。", "metrics": {"bleu_score": 45.3894626887366, "chrf_score": 39.88672455967588, "xcomet_score": 0.7621901631355286, "xcomet_qe_score": 0.6956405639648438, "metricx_score": 2.2179512977600098, "metricx_qe_score": 2.7228479385375977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告所有五次实验中性能的平均值、最大值和标准偏差。", "metrics": {"bleu_score": 19.8980950307606, "chrf_score": 18.633427327050374, "xcomet_score": 0.9365232586860657, "xcomet_qe_score": 0.9432826638221741, "metricx_score": 1.8729064464569092, "metricx_qe_score": 1.6937618255615234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们报告准确率。", "metrics": {"bleu_score": 51.92815178749843, "chrf_score": 41.86625721437747, "xcomet_score": 0.92449951171875, "xcomet_qe_score": 0.9797228574752808, "metricx_score": 0.587925374507904, "metricx_qe_score": 0.7086970806121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,我们报告Rouge L。对于NLP任务,我们也报告Rouge L。", "metrics": {"bleu_score": 61.14887872946488, "chrf_score": 68.70758014597682, "xcomet_score": 0.8838367462158203, "xcomet_qe_score": 0.8228006362915039, "metricx_score": 2.3524436950683594, "metricx_qe_score": 3.225811004638672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为敏感性。", "metrics": {"bleu_score": 64.77458735605244, "chrf_score": 62.5345946330063, "xcomet_score": 0.9231065511703491, "xcomet_qe_score": 0.9318966269493103, "metricx_score": 0.747890293598175, "metricx_qe_score": 0.9155235886573792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这衡量模型在指令措辞稍有变化的情况下,能否一致地产生相同的输出。", "metrics": {"bleu_score": 24.09668117388875, "chrf_score": 20.01212482312639, "xcomet_score": 0.9385877251625061, "xcomet_qe_score": 0.9431163668632507, "metricx_score": 3.301574468612671, "metricx_qe_score": 4.588587284088135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提高OFA在未见多模态任务上的性能。", "metrics": {"bleu_score": 59.76640266312742, "chrf_score": 53.94410611263078, "xcomet_score": 0.8358032703399658, "xcomet_qe_score": 0.8132766485214233, "metricx_score": 3.6459903717041016, "metricx_qe_score": 3.312006950378418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习可以有助于指令调优。", "metrics": {"bleu_score": 73.42150184891982, "chrf_score": 67.89652658130919, "xcomet_score": 0.9782533645629883, "xcomet_qe_score": 0.7820041179656982, "metricx_score": 1.3452181816101074, "metricx_qe_score": 2.2093758583068848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们可以看到,随着任务数量的增加,模型的性能提高,同时敏感性降低。", "metrics": {"bleu_score": 46.918472087493136, "chrf_score": 41.227517913582, "xcomet_score": 0.9605282545089722, "xcomet_qe_score": 0.9919402599334717, "metricx_score": 1.0240172147750854, "metricx_qe_score": 1.146758794784546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一次实验,", "metrics": {"bleu_score": 20.504572236241867, "chrf_score": 20.716989022811546, "xcomet_score": 0.983538031578064, "xcomet_qe_score": 0.9810106754302979, "metricx_score": 0.22644279897212982, "metricx_qe_score": 0.21524523198604584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用一个指令与五个指令进行比较。", "metrics": {"bleu_score": 14.62806365365753, "chrf_score": 16.922398258220273, "xcomet_score": 0.9491822719573975, "xcomet_qe_score": 0.8449200987815857, "metricx_score": 1.1729333400726318, "metricx_qe_score": 2.994428873062134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多指令可以显著提高模型的整体性能并大大降低其敏感性。", "metrics": {"bleu_score": 50.602874117615784, "chrf_score": 41.669932366526794, "xcomet_score": 0.9482430219650269, "xcomet_qe_score": 1.0, "metricx_score": 0.8586069345474243, "metricx_qe_score": 0.9907389879226685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这展示了不同微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 52.773391109389195, "chrf_score": 42.40552968385038, "xcomet_score": 0.9634228944778442, "xcomet_qe_score": 0.9662899971008301, "metricx_score": 1.3282283544540405, "metricx_qe_score": 1.7239949703216553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,通过从自然指令数据集进行迁移学习,模型可以实现比原始OFA模型更好的敏感性。", "metrics": {"bleu_score": 47.55017815388549, "chrf_score": 40.6912485485073, "xcomet_score": 0.8810296058654785, "xcomet_qe_score": 0.7995656728744507, "metricx_score": 2.270559072494507, "metricx_qe_score": 3.13356876373291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上实现更好的性能。", "metrics": {"bleu_score": 76.5961586325499, "chrf_score": 72.42437720852725, "xcomet_score": 0.9357602596282959, "xcomet_qe_score": 0.7460381984710693, "metricx_score": 3.174741744995117, "metricx_qe_score": 3.687741756439209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们提出了第一个大规模的多模态指令调优数据集。我们显著提高了OFA的零样本能力,并探索了不同的迁移学习技术,展示了它们的好处。", "metrics": {"bleu_score": 52.29320004967007, "chrf_score": 47.521612591444956, "xcomet_score": 0.8141998052597046, "xcomet_qe_score": 0.7907745838165283, "metricx_score": 3.4314451217651367, "metricx_qe_score": 3.592686891555786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标,称为敏感性。还有一件事", "metrics": {"bleu_score": 48.624389134644154, "chrf_score": 50.570155812438045, "xcomet_score": 0.6670271754264832, "xcomet_qe_score": 0.5847718119621277, "metricx_score": 3.5151607990264893, "metricx_qe_score": 2.159074306488037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们正在收集一个包含约150个额外的视觉语言任务的更大的多模态指令调优数据集,我们将发布它们。", "metrics": {"bleu_score": 50.78379680919149, "chrf_score": 46.95548344394208, "xcomet_score": 0.9011098146438599, "xcomet_qe_score": 0.8503326773643494, "metricx_score": 3.639159917831421, "metricx_qe_score": 4.10500955581665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的QR码。", "metrics": {"bleu_score": 53.45211483269467, "chrf_score": 42.63299880259071, "xcomet_score": 0.9899964332580566, "xcomet_qe_score": 0.9713841676712036, "metricx_score": 0.6017049551010132, "metricx_qe_score": 0.5755285620689392, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯托夫·西纳,很高兴欢迎大家来参加我们的演讲,讨论我们的ACL 2023论文《", "metrics": {"bleu_score": 32.158597295125276, "chrf_score": 35.26811706047906, "xcomet_score": 0.7719101905822754, "xcomet_qe_score": 0.7438940405845642, "metricx_score": 4.371473789215088, "metricx_qe_score": 2.2492904663085938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型的可接受性判断并不总是对上下文具有鲁棒性》。", "metrics": {"bleu_score": 71.22775889956915, "chrf_score": 69.60708720894493, "xcomet_score": 0.7205228805541992, "xcomet_qe_score": 0.758385419845581, "metricx_score": 4.390805244445801, "metricx_qe_score": 4.8093390464782715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与约翰·戈蒂尔、阿伦·穆勒、卡尼什卡·米什拉、加伦·芬特斯、罗杰·莱维和阿蒂娜·威廉姆斯的联合工作。", "metrics": {"bleu_score": 2.1919866950787434, "chrf_score": 2.710971821970264, "xcomet_score": 0.6092016696929932, "xcomet_qe_score": 0.5759479999542236, "metricx_score": 2.223175048828125, "metricx_qe_score": 1.9238814115524292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们重新审视了最小对比范式。", "metrics": {"bleu_score": 50.361556444464064, "chrf_score": 47.68931791577767, "xcomet_score": 0.844692587852478, "xcomet_qe_score": 0.882959246635437, "metricx_score": 2.1347696781158447, "metricx_qe_score": 1.4562032222747803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小对比范式基本上评估了语言模型在可接受性判断方面的表现,这", "metrics": {"bleu_score": 40.67315523312909, "chrf_score": 34.58508662834349, "xcomet_score": 0.815587043762207, "xcomet_qe_score": 0.8082317113876343, "metricx_score": 6.246329307556152, "metricx_qe_score": 1.2521672248840332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,如blimp、syntax gym,或者在刻板印象方面的可接受性,如crowspairs。", "metrics": {"bleu_score": 33.0376957031919, "chrf_score": 27.825123373363997, "xcomet_score": 0.6592854857444763, "xcomet_qe_score": 0.6158029437065125, "metricx_score": 3.8597724437713623, "metricx_qe_score": 4.445941925048828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个最小对比范式中,评估语言模型的典型方法是展示一个可接受的句子或一个语法正确的句子,然后展示一个不可接受的句子或一个语法错误的句子,", "metrics": {"bleu_score": 56.394820671707826, "chrf_score": 49.85610483700055, "xcomet_score": 0.8490415811538696, "xcomet_qe_score": 0.8627036809921265, "metricx_score": 1.254101276397705, "metricx_qe_score": 2.4688637256622314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型基本上给可接受的句子更高的概率。", "metrics": {"bleu_score": 38.491788943235726, "chrf_score": 32.05836223348735, "xcomet_score": 0.8640270233154297, "xcomet_qe_score": 0.7872251868247986, "metricx_score": 1.3498245477676392, "metricx_qe_score": 1.7590515613555908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP流水线基本上不允许我们评估模型对较长句子的接受度。", "metrics": {"bleu_score": 83.30787010500826, "chrf_score": 82.58498263620915, "xcomet_score": 0.9297637939453125, "xcomet_qe_score": 0.879836916923523, "metricx_score": 2.9266576766967773, "metricx_qe_score": 3.4811134338378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型的上下文窗口越来越长。", "metrics": {"bleu_score": 77.5792961097714, "chrf_score": 78.10281161553993, "xcomet_score": 0.9714252948760986, "xcomet_qe_score": 0.9141079783439636, "metricx_score": 0.5378822684288025, "metricx_qe_score": 0.6961395740509033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,评估模型在整个上下文窗口中的可接受性至关重要。这就是我们在这里要做的。", "metrics": {"bleu_score": 54.34638828802966, "chrf_score": 43.740355459980634, "xcomet_score": 0.8991553783416748, "xcomet_qe_score": 0.8781254291534424, "metricx_score": 0.8152273893356323, "metricx_qe_score": 0.9216183423995972, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过要求模型评估越来越长的序列的可接受性来重新审视MPP流水线。", "metrics": {"bleu_score": 78.86463321527106, "chrf_score": 80.30260941890836, "xcomet_score": 0.8979314565658569, "xcomet_qe_score": 0.9197157025337219, "metricx_score": 3.4483914375305176, "metricx_qe_score": 3.9459409713745117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。我们做", "metrics": {"bleu_score": 71.02992180127417, "chrf_score": 91.1574824683062, "xcomet_score": 0.7186099290847778, "xcomet_qe_score": 0.4617305397987366, "metricx_score": 3.0953776836395264, "metricx_qe_score": 1.1219570636749268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的是,为了模拟这些较长的序列,我们重新审视数据集本身,然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 75.55142761795092, "chrf_score": 71.91368843909316, "xcomet_score": 0.5828452110290527, "xcomet_qe_score": 0.4998125433921814, "metricx_score": 2.7004141807556152, "metricx_qe_score": 3.698432207107544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们选择了blimp数据集中典型的语法性对比,来自adjunct island案例。", "metrics": {"bleu_score": 27.822130970602192, "chrf_score": 33.39614132845446, "xcomet_score": 0.6435909271240234, "xcomet_qe_score": 0.6640115976333618, "metricx_score": 5.954812526702881, "metricx_qe_score": 6.347861289978027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们做的是,为了重新创建较长的序列,这些序列是可接受的,并且具有相同的语法结构匹配,", "metrics": {"bleu_score": 61.65110698498971, "chrf_score": 54.77161800806829, "xcomet_score": 0.779729962348938, "xcomet_qe_score": 0.614927351474762, "metricx_score": 3.3542847633361816, "metricx_qe_score": 3.9166667461395264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从adjunct island中提取语法正确的句子,然后将其作为前缀添加到可接受的查询和不可接受的查询中。", "metrics": {"bleu_score": 74.66745587002926, "chrf_score": 74.96457362050725, "xcomet_score": 0.8372159004211426, "xcomet_qe_score": 0.7840992212295532, "metricx_score": 3.1681442260742188, "metricx_qe_score": 4.532692909240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.9529941082000732, "xcomet_qe_score": 0.7428854703903198, "metricx_score": 1.5263457298278809, "metricx_qe_score": 1.8271386623382568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以通过从不同的子集或不同的数据集中选择句子来做同样的事情。", "metrics": {"bleu_score": 83.87303347416334, "chrf_score": 82.30183759973805, "xcomet_score": 0.9826825857162476, "xcomet_qe_score": 0.89628666639328, "metricx_score": 1.070742130279541, "metricx_qe_score": 1.8690603971481323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的不匹配场景。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9915859699249268, "xcomet_qe_score": 0.9170319437980652, "metricx_score": 0.7872244715690613, "metricx_qe_score": 1.401894450187683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,句子仍然来自相关的数据集,但不是我们正在评估的相同数据集。", "metrics": {"bleu_score": 58.13972876612314, "chrf_score": 51.75139382122346, "xcomet_score": 0.9666241407394409, "xcomet_qe_score": 0.8492956161499023, "metricx_score": 1.4109383821487427, "metricx_qe_score": 2.3600950241088867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以对不可接受的情况做同样的事情。", "metrics": {"bleu_score": 52.800984551961896, "chrf_score": 42.85382663073871, "xcomet_score": 0.9670130014419556, "xcomet_qe_score": 0.893768310546875, "metricx_score": 1.3658527135849, "metricx_qe_score": 1.5172278881072998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全无关的领域中选择句子,例如维基百科。", "metrics": {"bleu_score": 46.7495527150079, "chrf_score": 36.92317566036859, "xcomet_score": 0.9896485805511475, "xcomet_qe_score": 0.9275816679000854, "metricx_score": 0.777497410774231, "metricx_qe_score": 1.4189733266830444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们,模型的可接受性判断是否受到任何上下文的影响,无论上下文是来自数据集的不同子集,还是与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 59.53741571762223, "chrf_score": 53.45741595943111, "xcomet_score": 0.9478111267089844, "xcomet_qe_score": 0.9076467752456665, "metricx_score": 1.6364554166793823, "metricx_qe_score": 2.43308162689209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型的表现如何呢?", "metrics": {"bleu_score": 9.78090152232118, "chrf_score": 10.810141839392179, "xcomet_score": 0.8777414560317993, "xcomet_qe_score": 0.858100175857544, "metricx_score": 1.0029296875, "metricx_qe_score": 0.2817142903804779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看完全与当前查询对无关的维基百科句子,我们发现MPP判断在任意上下文长度下大多是鲁棒的。", "metrics": {"bleu_score": 31.107817070503724, "chrf_score": 30.43925312085124, "xcomet_score": 0.7829345464706421, "xcomet_qe_score": 0.7333011031150818, "metricx_score": 5.476269721984863, "metricx_qe_score": 7.22383451461792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到1024,以最大化OPT和GPT2模型,我们在这里看到,", "metrics": {"bleu_score": 52.442973157951805, "chrf_score": 79.34738139731776, "xcomet_score": 0.6628217697143555, "xcomet_qe_score": 0.6265681982040405, "metricx_score": 3.735489845275879, "metricx_qe_score": 3.990323066711426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色虚线中,MPP判断相对稳定。", "metrics": {"bleu_score": 53.01646310382839, "chrf_score": 52.88913920636522, "xcomet_score": 0.8659971952438354, "xcomet_qe_score": 0.8328984975814819, "metricx_score": 1.9674712419509888, "metricx_qe_score": 3.6754708290100098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么当我们从相同的数据集中选择句子时会发生什么呢?", "metrics": {"bleu_score": 82.93606144949322, "chrf_score": 82.11876529267835, "xcomet_score": 0.991432785987854, "xcomet_qe_score": 0.9308556914329529, "metricx_score": 1.4471386671066284, "metricx_qe_score": 2.4894185066223145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们从相同的blimp或syntax gym数据集中的可接受和不可接受领域中创建句子,", "metrics": {"bleu_score": 47.65946651895527, "chrf_score": 39.18662861448344, "xcomet_score": 0.6480132937431335, "xcomet_qe_score": 0.5875993967056274, "metricx_score": 4.2795209884643555, "metricx_qe_score": 4.350256443023682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,当我们添加可接受的前缀或不可接受的前缀时,MPP判断要么显著增加,要么显著减少。", "metrics": {"bleu_score": 51.304320574682954, "chrf_score": 49.10674914173131, "xcomet_score": 0.7914942502975464, "xcomet_qe_score": 0.8894721269607544, "metricx_score": 2.8349761962890625, "metricx_qe_score": 2.1804909706115723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们匹配结构时,也就是说,当我们从blimp人工智能中选择句子时,我们看到模型的MPP判断显著增加或显著减少,具体取决于所选前缀是可接受的还是不可接受的。现在,这个效果", "metrics": {"bleu_score": 54.60434563929526, "chrf_score": 44.851017161042876, "xcomet_score": 0.4532411992549896, "xcomet_qe_score": 0.4538663625717163, "metricx_score": 9.117180824279785, "metricx_qe_score": 7.274895668029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在整个上下文长度中增加,这可能会影响具有大上下文窗口的新语言模型。", "metrics": {"bleu_score": 69.00649639406753, "chrf_score": 64.64017256126353, "xcomet_score": 0.750312864780426, "xcomet_qe_score": 0.7669351100921631, "metricx_score": 3.61553955078125, "metricx_qe_score": 5.147106170654297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么匹配前缀为什么会如此大幅度地影响语言模型的判断呢?", "metrics": {"bleu_score": 44.433212638555524, "chrf_score": 37.359574729293314, "xcomet_score": 0.995297908782959, "xcomet_qe_score": 0.9254360198974609, "metricx_score": 0.6795452833175659, "metricx_qe_score": 0.7558369040489197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保留相关结构但向输入添加噪声来扰动输入句子。", "metrics": {"bleu_score": 66.15990297170643, "chrf_score": 60.18865275746828, "xcomet_score": 0.8634476661682129, "xcomet_qe_score": 0.8460012674331665, "metricx_score": 1.2931866645812988, "metricx_qe_score": 2.0988712310791016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了几次这样的扰动后,我们发现这些噪声实际上并没有改变模型的MPP判断趋势。", "metrics": {"bleu_score": 43.09655383575144, "chrf_score": 41.52166066493115, "xcomet_score": 0.9391294717788696, "xcomet_qe_score": 0.9313000440597534, "metricx_score": 3.3907203674316406, "metricx_qe_score": 4.1143798828125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动句子的敏感性是相似的。", "metrics": {"bleu_score": 44.543025732285095, "chrf_score": 39.8032499991696, "xcomet_score": 0.9268375635147095, "xcomet_qe_score": 0.8990291357040405, "metricx_score": 1.9636569023132324, "metricx_qe_score": 3.4255411624908447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们在可接受领域中扰动句子时,我们在所有扰动中看到类似的增加,当我们在不可接受领域中扰动句子时,我们以类似的方式看到MPP判断的减少。", "metrics": {"bleu_score": 48.088144932759484, "chrf_score": 41.50543497448589, "xcomet_score": 0.6342071294784546, "xcomet_qe_score": 0.7084627747535706, "metricx_score": 4.547122001647949, "metricx_qe_score": 5.387395858764648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键要点是,语言模型对句子中共享的潜在语法和语义特征敏感,", "metrics": {"bleu_score": 43.88771670809096, "chrf_score": 36.249114390672574, "xcomet_score": 0.8678487539291382, "xcomet_qe_score": 0.7808821797370911, "metricx_score": 1.813816785812378, "metricx_qe_score": 1.7900581359863281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而我们目前使用短句和单句输入的MPP评估方式可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 61.25373706131658, "chrf_score": 50.97225406057264, "xcomet_score": 0.9504694938659668, "xcomet_qe_score": 0.8436981439590454, "metricx_score": 1.7742758989334106, "metricx_qe_score": 2.104983329772949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以获取更多实验细节。", "metrics": {"bleu_score": 32.28475421040683, "chrf_score": 31.292934579805326, "xcomet_score": 0.99491286277771, "xcomet_qe_score": 0.9996927976608276, "metricx_score": 0.1711377501487732, "metricx_qe_score": 0.17318548262119293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的注意。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.8771392107009888, "xcomet_qe_score": 0.8434441089630127, "metricx_score": 1.0802056789398193, "metricx_qe_score": 1.6465333700180054, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是宾夕法尼亚大学的尤森·约翰。", "metrics": {"bleu_score": 30.690336937373775, "chrf_score": 19.707248514251464, "xcomet_score": 0.7047911882400513, "xcomet_qe_score": 0.6755412817001343, "metricx_score": 1.108548879623413, "metricx_qe_score": 1.028846263885498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的工作,即跨语言语义解析,涉及多种自然语言和多种表示。", "metrics": {"bleu_score": 45.518905608524555, "chrf_score": 32.966092985899266, "xcomet_score": 0.7631994485855103, "xcomet_qe_score": 0.7353933453559875, "metricx_score": 2.8836798667907715, "metricx_qe_score": 3.171808958053589, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义解析是构建用户查询的语义表示的任务,例如 SQL 和 Lambda 演算。", "metrics": {"bleu_score": 75.86640003071781, "chrf_score": 68.63199156046858, "xcomet_score": 0.9875277280807495, "xcomet_qe_score": 0.9645764827728271, "metricx_score": 1.1408369541168213, "metricx_qe_score": 1.5529533624649048, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义解析是将多种自然语言的查询转换为多种表示的任务。", "metrics": {"bleu_score": 66.98280323968864, "chrf_score": 62.82789797019732, "xcomet_score": 0.840965986251831, "xcomet_qe_score": 0.8814581036567688, "metricx_score": 1.9885447025299072, "metricx_qe_score": 4.0719685554504395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将多种自然语言的查询转换为 SQL、Lambda 或 FuncQL 等。", "metrics": {"bleu_score": 71.36988569848636, "chrf_score": 73.53521082272927, "xcomet_score": 0.9589507579803467, "xcomet_qe_score": 0.9456763863563538, "metricx_score": 1.277819037437439, "metricx_qe_score": 1.4136781692504883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型是分别提出并评估的,评估的数据集任务和应用有限。", "metrics": {"bleu_score": 57.3085592845208, "chrf_score": 49.24966834241029, "xcomet_score": 0.9491404294967651, "xcomet_qe_score": 0.8873586654663086, "metricx_score": 1.3650379180908203, "metricx_qe_score": 1.752912998199463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,某些自然语言的覆盖率不足。缺", "metrics": {"bleu_score": 53.084925448597666, "chrf_score": 50.789455330663, "xcomet_score": 0.5772790908813477, "xcomet_qe_score": 0.5337677001953125, "metricx_score": 5.596560478210449, "metricx_qe_score": 4.399210453033447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "少中文,某些表示的覆盖率不足。", "metrics": {"bleu_score": 6.134421060213565, "chrf_score": 9.023626781735226, "xcomet_score": 0.7679444551467896, "xcomet_qe_score": 0.7178876996040344, "metricx_score": 3.712825298309326, "metricx_qe_score": 3.30818510055542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "缺少 Lambda 演算。或者它们只在某些神经模型上进行评估。", "metrics": {"bleu_score": 60.51475274955804, "chrf_score": 66.06964482308196, "xcomet_score": 0.9155980348587036, "xcomet_qe_score": 0.8764857053756714, "metricx_score": 1.2121574878692627, "metricx_qe_score": 2.3737478256225586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一模型来评估它们。", "metrics": {"bleu_score": 69.26989774939868, "chrf_score": 59.89081688981535, "xcomet_score": 0.9923233985900879, "xcomet_qe_score": 0.9328571557998657, "metricx_score": 0.43095505237579346, "metricx_qe_score": 0.833996057510376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了 Exemplar。", "metrics": {"bleu_score": 42.7287006396234, "chrf_score": 22.154338574566562, "xcomet_score": 0.8777440786361694, "xcomet_qe_score": 0.8548681139945984, "metricx_score": 1.771737813949585, "metricx_qe_score": 3.123758554458618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提供了一个统一的 Exemplar 数据集,用于跨语言语义解析,涉及多种自然语言和表示。", "metrics": {"bleu_score": 60.47719673173811, "chrf_score": 45.53883804697042, "xcomet_score": 0.7129181027412415, "xcomet_qe_score": 0.7343161702156067, "metricx_score": 3.4335625171661377, "metricx_qe_score": 3.921980619430542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含 9 个数据集,涵盖多个领域,5 个语义解析任务,8 种表示和 22 种自然语言,分布在 15 个语言家族中。", "metrics": {"bleu_score": 39.713374446621636, "chrf_score": 38.2466053068799, "xcomet_score": 0.7681317329406738, "xcomet_qe_score": 0.8180326819419861, "metricx_score": 0.9031718969345093, "metricx_qe_score": 1.337062954902649, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 80.20219183488042, "chrf_score": 71.52080420921001, "xcomet_score": 0.9888695478439331, "xcomet_qe_score": 0.913833737373352, "metricx_score": 1.4580811262130737, "metricx_qe_score": 2.0225110054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是翻译测试。", "metrics": {"bleu_score": 66.06328636027612, "chrf_score": 56.005291005291, "xcomet_score": 0.9600571393966675, "xcomet_qe_score": 0.9592820405960083, "metricx_score": 0.29587188363075256, "metricx_qe_score": 0.4503270089626312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 Google 翻译 API 将源语言翻译成目标语言,然后使用单语言模型进行训练和评估。", "metrics": {"bleu_score": 77.89081599309982, "chrf_score": 69.12158373825255, "xcomet_score": 0.8900305032730103, "xcomet_qe_score": 0.9561153650283813, "metricx_score": 0.5134091377258301, "metricx_qe_score": 0.4861498177051544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语查询上训练英语模型,在推理期间,我们使用 API 将德语查询翻译成英语,然后使用训练好的模型预测 SQL。", "metrics": {"bleu_score": 60.364307487579815, "chrf_score": 55.92741419838115, "xcomet_score": 0.8735870122909546, "xcomet_qe_score": 0.8447476029396057, "metricx_score": 1.4400715827941895, "metricx_qe_score": 2.257497787475586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试单语言模型。", "metrics": {"bleu_score": 43.36189090348677, "chrf_score": 36.30892255892255, "xcomet_score": 0.8971643447875977, "xcomet_qe_score": 0.8981600403785706, "metricx_score": 0.41153717041015625, "metricx_qe_score": 0.4571618437767029, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种设置中,源语言与目标语言相同。例如,德语到德语或英语到英语。", "metrics": {"bleu_score": 72.68097337162342, "chrf_score": 66.44665575455079, "xcomet_score": 0.9278980493545532, "xcomet_qe_score": 0.8964245319366455, "metricx_score": 0.5449566841125488, "metricx_qe_score": 0.6620081663131714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试单语言少量设置,通过仅使用 10% 的训练数据训练单语言模型。", "metrics": {"bleu_score": 39.70030364499148, "chrf_score": 35.386260659806304, "xcomet_score": 0.7464849948883057, "xcomet_qe_score": 0.7996827960014343, "metricx_score": 2.7543416023254395, "metricx_qe_score": 2.6917037963867188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试多语言模型,我们为所有语言训练一个多语言模型。", "metrics": {"bleu_score": 81.96189957582149, "chrf_score": 82.31272076344145, "xcomet_score": 0.9485858678817749, "xcomet_qe_score": 0.8932175636291504, "metricx_score": 1.2320210933685303, "metricx_qe_score": 1.7469444274902344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询放在一起训练一个多语言模型,", "metrics": {"bleu_score": 70.79532033363687, "chrf_score": 62.60618946213632, "xcomet_score": 0.9317162036895752, "xcomet_qe_score": 0.9452484846115112, "metricx_score": 1.4602980613708496, "metricx_qe_score": 2.8754308223724365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理期间,我们可以使用这个模型来翻译德语查询或中文查询等。", "metrics": {"bleu_score": 81.15699931635375, "chrf_score": 79.20437854728091, "xcomet_score": 0.9377142190933228, "xcomet_qe_score": 0.7704442143440247, "metricx_score": 1.0488810539245605, "metricx_qe_score": 1.277745008468628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少量样本迁移。", "metrics": {"bleu_score": 72.09604917918676, "chrf_score": 72.05096760220312, "xcomet_score": 0.837560772895813, "xcomet_qe_score": 0.7937424778938293, "metricx_score": 3.402773380279541, "metricx_qe_score": 3.686955690383911, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,然后迁移到另一种语言。因此,", "metrics": {"bleu_score": 33.307662668678184, "chrf_score": 30.09469057533497, "xcomet_score": 0.786724328994751, "xcomet_qe_score": 0.7627609968185425, "metricx_score": 5.2872114181518555, "metricx_qe_score": 5.579730033874512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练期间,我们在英语查询或英语和德语少量查询的组合上训练一个多语言模型,并预测 SQL 输出。", "metrics": {"bleu_score": 45.776116731290806, "chrf_score": 41.778899182683865, "xcomet_score": 0.7981939315795898, "xcomet_qe_score": 0.7545900344848633, "metricx_score": 3.1333370208740234, "metricx_qe_score": 3.2121503353118896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的结果。因此", "metrics": {"bleu_score": 49.35578819979934, "chrf_score": 47.35262425008111, "xcomet_score": 0.8335084915161133, "xcomet_qe_score": 0.8012288808822632, "metricx_score": 2.2050490379333496, "metricx_qe_score": 0.6639837026596069, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",关于单语言模型的分析,我们在两组模型上进行评估,包括编码器 PDR,即多语言预训练编码器与基于指针的解码器,例如 XLM-R + PDR 和 BERT + PDR。", "metrics": {"bleu_score": 41.49436072440286, "chrf_score": 38.76021550291925, "xcomet_score": 0.5327050685882568, "xcomet_qe_score": 0.7236783504486084, "metricx_score": 4.951055526733398, "metricx_qe_score": 4.6593852043151855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,例如 MBART 和 MT5。", "metrics": {"bleu_score": 30.18146852695617, "chrf_score": 22.13167484766176, "xcomet_score": 0.9135904312133789, "xcomet_qe_score": 0.963300347328186, "metricx_score": 1.3813644647598267, "metricx_qe_score": 2.8694963455200195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器在所有九个数据集上表现最佳。", "metrics": {"bleu_score": 46.942223829384936, "chrf_score": 28.18483869116096, "xcomet_score": 0.9962038993835449, "xcomet_qe_score": 0.9938557147979736, "metricx_score": 0.5540094375610352, "metricx_qe_score": 0.5058259963989258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 MT5 和 XLM-R + PDR 的多语言设置中进行评估。", "metrics": {"bleu_score": 34.669778311100316, "chrf_score": 46.40669810597347, "xcomet_score": 0.8900521993637085, "xcomet_qe_score": 0.90272057056427, "metricx_score": 3.1076250076293945, "metricx_qe_score": 2.8972487449645996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器或编码器 PDR 可以通过在各种语言的混合中进行训练来改进。", "metrics": {"bleu_score": 23.872441513005217, "chrf_score": 16.994728310805623, "xcomet_score": 0.6880545020103455, "xcomet_qe_score": 0.7171765565872192, "metricx_score": 3.9569497108459473, "metricx_qe_score": 4.977592945098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都能获得性能提升,除了英语在七个数据集中性能下降,只有在三个数据集中获得提升。", "metrics": {"bleu_score": 53.51792221281187, "chrf_score": 47.417010159013415, "xcomet_score": 0.8863083720207214, "xcomet_qe_score": 0.9570496678352356, "metricx_score": 2.911007881164551, "metricx_qe_score": 2.3895859718322754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言诅咒。", "metrics": {"bleu_score": 18.678325117721318, "chrf_score": 18.72467251748071, "xcomet_score": 0.9018496870994568, "xcomet_qe_score": 0.8827164173126221, "metricx_score": 0.9698828458786011, "metricx_qe_score": 1.2378120422363281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝线是跨语言少量样本迁移,", "metrics": {"bleu_score": 49.91593892034051, "chrf_score": 43.12590317708519, "xcomet_score": 0.8610907793045044, "xcomet_qe_score": 0.8010526299476624, "metricx_score": 3.7283103466033936, "metricx_qe_score": 3.8591954708099365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线是跨语言零样本迁移,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.8476507663726807, "xcomet_qe_score": 0.8330907821655273, "metricx_score": 1.9685924053192139, "metricx_qe_score": 3.312077522277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿线是单语言设置。我们", "metrics": {"bleu_score": 42.40125351805035, "chrf_score": 47.06262945611142, "xcomet_score": 0.8397827744483948, "xcomet_qe_score": 0.8346085548400879, "metricx_score": 3.7644400596618652, "metricx_qe_score": 0.6698756217956543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现通过比较绿线和橙线,我们发现对于零样本设置,跨语言迁移性能差距显著。通过比较蓝线和橙线,我们发现对于少量样本设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 51.644108325797696, "chrf_score": 43.27893502399357, "xcomet_score": 0.6290907263755798, "xcomet_qe_score": 0.5963358879089355, "metricx_score": 2.848099708557129, "metricx_qe_score": 4.239296913146973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器优于以前的工作或实现了可比的结果。", "metrics": {"bleu_score": 25.13073726775429, "chrf_score": 14.654455256640016, "xcomet_score": 0.967190146446228, "xcomet_qe_score": 0.9745069742202759, "metricx_score": 2.013300895690918, "metricx_qe_score": 2.4574038982391357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行训练可以显著提高目标自然语言的少量样本性能。我们发现多语言语言模型,例如 CodaS 和 Blue,仍然不适合跨语言语义解析任务。", "metrics": {"bleu_score": 53.26999182511908, "chrf_score": 42.433678585662705, "xcomet_score": 0.7068058252334595, "xcomet_qe_score": 0.6895841360092163, "metricx_score": 7.266173839569092, "metricx_qe_score": 8.174745559692383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们构建了 Exemplar,一个统一的跨语言语义解析基准,涉及多种自然语言和表示。", "metrics": {"bleu_score": 41.428408493956546, "chrf_score": 32.325599738330126, "xcomet_score": 0.7137627005577087, "xcomet_qe_score": 0.7356100082397461, "metricx_score": 3.715130090713501, "metricx_qe_score": 4.3153862953186035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种代表性的多语言语言模型类型进行了全面的基准研究,", "metrics": {"bleu_score": 96.35749534339605, "chrf_score": 96.2106285583297, "xcomet_score": 0.952865719795227, "xcomet_qe_score": 0.9266501665115356, "metricx_score": 1.4154930114746094, "metricx_qe_score": 2.055891990661621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结果显示了许多有趣的发现等", "metrics": {"bleu_score": 93.06048591020995, "chrf_score": 92.47065434565434, "xcomet_score": 0.8565675020217896, "xcomet_qe_score": 0.7905972003936768, "metricx_score": 1.7107927799224854, "metricx_qe_score": 1.3963158130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢收听。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 17.0, "xcomet_score": 0.9755227565765381, "xcomet_qe_score": 0.9598297476768494, "metricx_score": 0.6410813927650452, "metricx_qe_score": 0.3586311638355255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫艾德·维拉德,我将简要介绍一下这篇论文《从翻译中提取Parm,评估策略和性能》。", "metrics": {"bleu_score": 20.310341961604582, "chrf_score": 19.16690412260565, "xcomet_score": 0.7145875692367554, "xcomet_qe_score": 0.7092658877372742, "metricx_score": 4.64647912979126, "metricx_qe_score": 4.8390793800354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与我的Google Translate同事合作的成果。", "metrics": {"bleu_score": 15.260434599689711, "chrf_score": 13.793265381424884, "xcomet_score": 0.9781815409660339, "xcomet_qe_score": 0.9779379963874817, "metricx_score": 2.349097490310669, "metricx_qe_score": 1.003431797027588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Parm是一个拥有540亿参数的大型语言模型,去年2022年发布。", "metrics": {"bleu_score": 53.1648848941232, "chrf_score": 53.994621549196076, "xcomet_score": 0.8960149884223938, "xcomet_qe_score": 0.8773354291915894, "metricx_score": 4.168824195861816, "metricx_qe_score": 6.37938117980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在一个包含780亿个标记的大型文本集上进行训练。", "metrics": {"bleu_score": 34.1191474032336, "chrf_score": 38.93471599688147, "xcomet_score": 0.776114821434021, "xcomet_qe_score": 0.7811912894248962, "metricx_score": 1.6193565130233765, "metricx_qe_score": 1.868881106376648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百个NLP任务中达到了最先进的水平。", "metrics": {"bleu_score": 42.66219662386316, "chrf_score": 44.50155887396281, "xcomet_score": 0.9293559789657593, "xcomet_qe_score": 0.9205664396286011, "metricx_score": 1.0466797351837158, "metricx_qe_score": 1.6142739057540894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们展示了第一个系统研究大型语言模型提示在机器翻译中的应用。", "metrics": {"bleu_score": 35.424061134126994, "chrf_score": 32.4939174707528, "xcomet_score": 0.7255143523216248, "xcomet_qe_score": 0.7648427486419678, "metricx_score": 3.4738247394561768, "metricx_qe_score": 3.6659457683563232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了AMT社区的最佳实践来评估这些模型的翻译能力。", "metrics": {"bleu_score": 56.12756148371671, "chrf_score": 48.343857832988256, "xcomet_score": 0.8203166723251343, "xcomet_qe_score": 0.7777738571166992, "metricx_score": 4.531004905700684, "metricx_qe_score": 6.131639003753662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,即WMT评估中的最佳性能系统。", "metrics": {"bleu_score": 25.0252724911293, "chrf_score": 27.24783276730966, "xcomet_score": 0.868686318397522, "xcomet_qe_score": 0.8030922412872314, "metricx_score": 3.2074170112609863, "metricx_qe_score": 4.205985069274902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的新RMT指标,并额外展示了基于专家的人类评估结果。", "metrics": {"bleu_score": 51.06531057243462, "chrf_score": 41.17070548206814, "xcomet_score": 0.7515310645103455, "xcomet_qe_score": 0.8063814640045166, "metricx_score": 2.51884126663208, "metricx_qe_score": 2.6436729431152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些提示选择策略的建议。", "metrics": {"bleu_score": 70.75330011966422, "chrf_score": 64.06828873488384, "xcomet_score": 0.8876395225524902, "xcomet_qe_score": 0.8450835347175598, "metricx_score": 1.09638249874115, "metricx_qe_score": 3.2114005088806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对LLM在翻译中的性能有很大影响。正如我们在一个简单的实验中看到的那样,我们使用了一次提示,并为一个句子提供了两个不同的提示。", "metrics": {"bleu_score": 58.235521806089054, "chrf_score": 55.59789001836551, "xcomet_score": 0.8370022773742676, "xcomet_qe_score": 0.8232901096343994, "metricx_score": 2.9234719276428223, "metricx_qe_score": 5.098381996154785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子,516", "metrics": {"bleu_score": 9.870315683072755, "chrf_score": 5.9886759581881535, "xcomet_score": 0.5221380591392517, "xcomet_qe_score": 0.7437422275543213, "metricx_score": 5.114556312561035, "metricx_qe_score": 9.287467956542969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "个中的1000个,观察到的差异超过一个模糊点。", "metrics": {"bleu_score": 5.039518688486958, "chrf_score": 6.268709117114454, "xcomet_score": 0.18363124132156372, "xcomet_qe_score": 0.2568132281303406, "metricx_score": 11.69777774810791, "metricx_qe_score": 7.184193134307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可以达到40个模糊点。", "metrics": {"bleu_score": 32.22538601891171, "chrf_score": 23.148500068613558, "xcomet_score": 0.8078522086143494, "xcomet_qe_score": 0.8129090070724487, "metricx_score": 4.80564546585083, "metricx_qe_score": 2.5721869468688965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个好的提示策略非常重要。在", "metrics": {"bleu_score": 59.08871032231054, "chrf_score": 58.5902119027495, "xcomet_score": 0.8214733600616455, "xcomet_qe_score": 0.7509938478469849, "metricx_score": 3.953136920928955, "metricx_qe_score": 0.3306087255477905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验中,我们选择了一个五次提示策略,我们只是标记每个我们提供给系统的句子,与其语言相同。", "metrics": {"bleu_score": 30.142996028315288, "chrf_score": 27.55913138943799, "xcomet_score": 0.6448559761047363, "xcomet_qe_score": 0.6932626366615295, "metricx_score": 5.4232330322265625, "metricx_qe_score": 4.55214786529541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们从德语翻译成英语,德语句子,源句子用德语冒号标记,英语翻译用英语冒号标记。", "metrics": {"bleu_score": 47.54701430611021, "chrf_score": 33.03897102210083, "xcomet_score": 0.8835488557815552, "xcomet_qe_score": 0.9423204660415649, "metricx_score": 2.5163214206695557, "metricx_qe_score": 3.2283973693847656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次提示的情况下,提示的实际形式并没有很大的影响。", "metrics": {"bleu_score": 49.31860496450846, "chrf_score": 40.99559819688554, "xcomet_score": 0.8453259468078613, "xcomet_qe_score": 0.8510717153549194, "metricx_score": 0.9234776496887207, "metricx_qe_score": 1.330018162727356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零次和一次提示至关重要,当我们像我们的例子一样转", "metrics": {"bleu_score": 15.326859947928295, "chrf_score": 14.79814968293684, "xcomet_score": 0.35464054346084595, "xcomet_qe_score": 0.4548972249031067, "metricx_score": 7.0168232917785645, "metricx_qe_score": 3.866093873977661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "向五次提示时,提示的实际形式几乎没有差", "metrics": {"bleu_score": 20.285646319419875, "chrf_score": 16.310457948841105, "xcomet_score": 0.6672741174697876, "xcomet_qe_score": 0.5165770053863525, "metricx_score": 4.882889270782471, "metricx_qe_score": 7.9920454025268555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "异。是例子承载了大部分的重量。", "metrics": {"bleu_score": 4.112982349983277, "chrf_score": 7.042253521126759, "xcomet_score": 0.33117830753326416, "xcomet_qe_score": 0.3678553104400635, "metricx_score": 7.193726062774658, "metricx_qe_score": 7.088049411773682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,示例质量比源句子的相似性更重要。", "metrics": {"bleu_score": 80.32263623299822, "chrf_score": 72.08057673001385, "xcomet_score": 0.989337682723999, "xcomet_qe_score": 0.9685707092285156, "metricx_score": 0.7564218044281006, "metricx_qe_score": 0.7757310271263123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择高质量翻译的示例非常重要。", "metrics": {"bleu_score": 44.49130070801308, "chrf_score": 39.49925089961802, "xcomet_score": 0.9853969812393188, "xcomet_qe_score": 0.9871271848678589, "metricx_score": 0.41981181502342224, "metricx_qe_score": 0.48080992698669434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了从WMT评估的训练数据或开发数据中选择提示。开发数据比训练", "metrics": {"bleu_score": 42.62838101265284, "chrf_score": 38.388523375112236, "xcomet_score": 0.5652488470077515, "xcomet_qe_score": 0.4417732357978821, "metricx_score": 6.063419342041016, "metricx_qe_score": 5.168334484100342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据更精心制作,质量更高,结果更好,因此", "metrics": {"bleu_score": 7.632216322442406, "chrf_score": 8.624666137514827, "xcomet_score": 0.3201312720775604, "xcomet_qe_score": 0.16117706894874573, "metricx_score": 7.838409423828125, "metricx_qe_score": 6.76197624206543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据时性能更好。", "metrics": {"bleu_score": 45.43142611141303, "chrf_score": 41.14026689146475, "xcomet_score": 0.9033088684082031, "xcomet_qe_score": 0.890548825263977, "metricx_score": 1.9992756843566895, "metricx_qe_score": 2.476839542388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,专业的最先进系统在PAN翻译方面具有显", "metrics": {"bleu_score": 25.28152587495725, "chrf_score": 23.507240915820553, "xcomet_score": 0.4784544110298157, "xcomet_qe_score": 0.5521831512451172, "metricx_score": 10.886651992797852, "metricx_qe_score": 5.910961627960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "著优势,但PAN接近商业系统。", "metrics": {"bleu_score": 41.69883788690665, "chrf_score": 31.18144532930059, "xcomet_score": 0.25834035873413086, "xcomet_qe_score": 0.19536098837852478, "metricx_score": 7.763004779815674, "metricx_qe_score": 6.97618293762207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的情况下,我们选择与Google Translate进行评估。", "metrics": {"bleu_score": 36.20386342112599, "chrf_score": 24.508924297116653, "xcomet_score": 0.9128512144088745, "xcomet_qe_score": 0.8529300689697266, "metricx_score": 3.550175905227661, "metricx_qe_score": 2.898344039916992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从使用MQM框架进行的人类评估中获得的见解是,PAN的流畅性与最先进的系统相当,但主要差异来自准确性。", "metrics": {"bleu_score": 57.37925763533329, "chrf_score": 49.58549629651423, "xcomet_score": 0.7595884203910828, "xcomet_qe_score": 0.7167273759841919, "metricx_score": 5.008890151977539, "metricx_qe_score": 5.535393238067627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是省略错误。", "metrics": {"bleu_score": 45.788313721339826, "chrf_score": 38.77358752358752, "xcomet_score": 0.7192978858947754, "xcomet_qe_score": 0.8016858100891113, "metricx_score": 2.3211562633514404, "metricx_qe_score": 0.5063758492469788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,PAN似乎选择生成更好的翻译,有时通过删除源句子中在翻译中不重要的部分。", "metrics": {"bleu_score": 40.64916491494814, "chrf_score": 32.589574374656785, "xcomet_score": 0.728918194770813, "xcomet_qe_score": 0.7365813255310059, "metricx_score": 5.498279571533203, "metricx_qe_score": 5.410510540008545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAN的风格输出类别低于最先进的系统,这表明PAN提供了非常流畅的输出,但仍然存在一些准确性问题。", "metrics": {"bleu_score": 44.79874461271031, "chrf_score": 39.14255406748911, "xcomet_score": 0.6864418387413025, "xcomet_qe_score": 0.7015973329544067, "metricx_score": 6.599424839019775, "metricx_qe_score": 6.345817565917969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这个非常简短的概述。", "metrics": {"bleu_score": 15.430296443705515, "chrf_score": 16.498817569799236, "xcomet_score": 0.8890209197998047, "xcomet_qe_score": 0.850629448890686, "metricx_score": 0.8129390478134155, "metricx_qe_score": 0.8242415189743042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更多详细信息,请来参加论文的全面演示。", "metrics": {"bleu_score": 27.659503209725884, "chrf_score": 24.630748221409966, "xcomet_score": 0.8360073566436768, "xcomet_qe_score": 0.823967456817627, "metricx_score": 2.316791534423828, "metricx_qe_score": 3.210829019546509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是德国萨兰特大学的博士生达维。", "metrics": {"bleu_score": 34.14504535105491, "chrf_score": 25.995252241982797, "xcomet_score": 0.8546290397644043, "xcomet_qe_score": 0.8969168066978455, "metricx_score": 0.5634139776229858, "metricx_qe_score": 0.5171154141426086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这段视频中,我想介绍我们的最新研究成果《Weaker Than You Think: A Critical Look at Weakly Supervised Learning》。", "metrics": {"bleu_score": 10.519928208199586, "chrf_score": 9.337390095700107, "xcomet_score": 0.9875043630599976, "xcomet_qe_score": 0.9779759645462036, "metricx_score": 4.969504356384277, "metricx_qe_score": 2.378190755844116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与肖宇舜、马约·斯穆斯巴赫、盖亚·斯蒂芬和迪蒂·克拉克合作的成果。", "metrics": {"bleu_score": 3.7556255267550958, "chrf_score": 3.353970577282127, "xcomet_score": 0.5354081392288208, "xcomet_qe_score": 0.5436232686042786, "metricx_score": 2.5412416458129883, "metricx_qe_score": 3.228726863861084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想先简要介绍一下弱监督和弱监督学习。", "metrics": {"bleu_score": 88.52140475440834, "chrf_score": 92.26506423113229, "xcomet_score": 0.8952239751815796, "xcomet_qe_score": 0.8487532138824463, "metricx_score": 0.8096938729286194, "metricx_qe_score": 2.310981273651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不手动标注数据。", "metrics": {"bleu_score": 36.17043615983554, "chrf_score": 29.691878261623245, "xcomet_score": 0.8968457579612732, "xcomet_qe_score": 0.8531893491744995, "metricx_score": 0.8261656165122986, "metricx_qe_score": 1.6418793201446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注源来标注数据,例如简单的启发式规则、知识库或低质量的众包标注,如右图所示。", "metrics": {"bleu_score": 67.97462578580337, "chrf_score": 62.3596161212178, "xcomet_score": 0.649885892868042, "xcomet_qe_score": 0.6749368906021118, "metricx_score": 1.5092999935150146, "metricx_qe_score": 1.6983648538589478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注成本更低,但也更嘈杂,这意味着一定数量的标注是错误的。", "metrics": {"bleu_score": 35.42884863446493, "chrf_score": 28.521876003577702, "xcomet_score": 0.8444743156433105, "xcomet_qe_score": 0.8781776428222656, "metricx_score": 2.492539167404175, "metricx_qe_score": 2.945499897003174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在弱标注数据上训练神经网络,神经网络往往会记住标注噪声,而不具备泛化能力。", "metrics": {"bleu_score": 60.954673268261715, "chrf_score": 53.13082564582172, "xcomet_score": 0.9688675403594971, "xcomet_qe_score": 0.9130069613456726, "metricx_score": 0.9528106451034546, "metricx_qe_score": 1.0766924619674683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出了在标注噪声下鲁棒训练神经网络的算法,使得训练模型仍能很好地泛化。", "metrics": {"bleu_score": 26.669475339756474, "chrf_score": 25.733389483592106, "xcomet_score": 0.7728873491287231, "xcomet_qe_score": 0.697206437587738, "metricx_score": 3.653524160385132, "metricx_qe_score": 4.286066055297852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的WSL研究中,WSL代表弱监督学习,一个常见的说法是,人们声称他们只在弱标注数据上训练模型,并在干净的测试集上实现高性能。", "metrics": {"bleu_score": 48.306527670415434, "chrf_score": 46.49828495357137, "xcomet_score": 0.8422428369522095, "xcomet_qe_score": 0.850714385509491, "metricx_score": 2.9855401515960693, "metricx_qe_score": 3.6801912784576416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并不完全错误,但有一个陷阱,那就是人们假设有一个额外的干净验证集可用于模型选择。", "metrics": {"bleu_score": 62.62369008438764, "chrf_score": 56.18427517807171, "xcomet_score": 0.9266530871391296, "xcomet_qe_score": 0.8743399381637573, "metricx_score": 1.8650462627410889, "metricx_qe_score": 2.9436826705932617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这个问题设定提出了质疑,因为这意味着弱监督学习需要额外的手动标注。", "metrics": {"bleu_score": 43.929751176084075, "chrf_score": 35.56824273554683, "xcomet_score": 0.7649374008178711, "xcomet_qe_score": 0.7695045471191406, "metricx_score": 3.024592638015747, "metricx_qe_score": 3.618957996368408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但就像房间里的大象一样,这个必要性往往被忽视。", "metrics": {"bleu_score": 52.8112428031202, "chrf_score": 46.15098525645558, "xcomet_score": 0.9271910190582275, "xcomet_qe_score": 0.8085941076278687, "metricx_score": 1.0684196949005127, "metricx_qe_score": 2.6988883018493652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种假设让我们提出了三个研究问题。", "metrics": {"bleu_score": 38.53856918030314, "chrf_score": 34.92869775250076, "xcomet_score": 0.8651268482208252, "xcomet_qe_score": 0.8602510690689087, "metricx_score": 3.7558813095092773, "metricx_qe_score": 3.2493298053741455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,干净的验证数据对WSL是必要的吗?或者我们可以使用嘈杂的验证集吗?", "metrics": {"bleu_score": 62.613589394166596, "chrf_score": 59.81611538863436, "xcomet_score": 0.8829736113548279, "xcomet_qe_score": 0.8414561152458191, "metricx_score": 1.885528802871704, "metricx_qe_score": 3.0835864543914795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果干净数据是必需的,或者如果干净数据对WSL的工作是必需的,那么我们需要多少干净样本?", "metrics": {"bleu_score": 30.324923408698087, "chrf_score": 28.713868580916753, "xcomet_score": 0.8887624144554138, "xcomet_qe_score": 0.8670790195465088, "metricx_score": 1.3044884204864502, "metricx_qe_score": 1.1952404975891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否只应该使用干净样本进行验证,或者有更好的方法来利用它们?", "metrics": {"bleu_score": 69.35960120913525, "chrf_score": 62.34158468261965, "xcomet_score": 0.9364413022994995, "xcomet_qe_score": 0.925102174282074, "metricx_score": 0.6446206569671631, "metricx_qe_score": 1.1038187742233276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在研究中解决了这些问题,我们的发现如下。", "metrics": {"bleu_score": 43.045719066117506, "chrf_score": 36.07743689449534, "xcomet_score": 0.9888632297515869, "xcomet_qe_score": 0.9219759106636047, "metricx_score": 0.8248739838600159, "metricx_qe_score": 2.6134889125823975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现,有趣的是,最近的WSL方法确实需要干净的验证样本来正常工作。", "metrics": {"bleu_score": 63.892954505258366, "chrf_score": 59.35314541351693, "xcomet_score": 0.8417320251464844, "xcomet_qe_score": 0.8510299324989319, "metricx_score": 2.4606640338897705, "metricx_qe_score": 3.184378147125244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降,", "metrics": {"bleu_score": 63.15552371794039, "chrf_score": 55.594035594035596, "xcomet_score": 0.9874362945556641, "xcomet_qe_score": 0.9928046464920044, "metricx_score": 0.4509727358818054, "metricx_qe_score": 0.7942342758178711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示。如果没有干净的验证样本,训练模型无法超越原始弱标签,这意味着训练是无意义的。", "metrics": {"bleu_score": 59.683205889849916, "chrf_score": 53.95120735948776, "xcomet_score": 0.8857759237289429, "xcomet_qe_score": 0.8300472497940063, "metricx_score": 2.37343168258667, "metricx_qe_score": 3.886234760284424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净标注的数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 59.418348752022005, "chrf_score": 55.0716974345405, "xcomet_score": 0.7312060594558716, "xcomet_qe_score": 0.7190991640090942, "metricx_score": 2.7201504707336426, "metricx_qe_score": 3.657834053039551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于WSL方法实现更好的性能,如左图所示。", "metrics": {"bleu_score": 77.89520674608168, "chrf_score": 73.4880424925802, "xcomet_score": 0.9045039415359497, "xcomet_qe_score": 0.9022520780563354, "metricx_score": 3.8206844329833984, "metricx_qe_score": 4.774132251739502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每类20个样本即可实现高性能。", "metrics": {"bleu_score": 29.05563804238317, "chrf_score": 28.835294356148545, "xcomet_score": 0.9327361583709717, "xcomet_qe_score": 0.9532108902931213, "metricx_score": 1.2863702774047852, "metricx_qe_score": 1.3435202836990356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这还不是故事的结尾,因为如果我们无论如何决定访问干净样本,那么直接在它们上训练将实现更好的性能。", "metrics": {"bleu_score": 38.35288722252599, "chrf_score": 30.484111818151664, "xcomet_score": 0.8594799041748047, "xcomet_qe_score": 0.8606522083282471, "metricx_score": 4.824090957641602, "metricx_qe_score": 5.427008628845215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "右图显示了直接应用于干净数据的微调方法与仅使用干净数据进行验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 91.44061946646029, "chrf_score": 90.47252738333282, "xcomet_score": 0.9294203519821167, "xcomet_qe_score": 0.8890191912651062, "metricx_score": 2.0209641456604004, "metricx_qe_score": 2.4744510650634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如我们所见,如果我们有每类10个样本,直接微调开始超越WSL方法。最后,以前WSL方法声", "metrics": {"bleu_score": 37.91638580697949, "chrf_score": 36.604637428061565, "xcomet_score": 0.6311467885971069, "xcomet_qe_score": 0.5770280361175537, "metricx_score": 7.743218421936035, "metricx_qe_score": 6.081066131591797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "称的性能提升可以通过允许在干净验证样本上继续微调来轻松实现。", "metrics": {"bleu_score": 20.537805631651473, "chrf_score": 18.968700019875836, "xcomet_score": 0.42990219593048096, "xcomet_qe_score": 0.45845362544059753, "metricx_score": 6.380432605743408, "metricx_qe_score": 7.359561443328857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,最初性能不如更复杂的WSL方法(如Cosine)的Vanilla模型(FT", "metrics": {"bleu_score": 21.109960996838613, "chrf_score": 30.571949659439923, "xcomet_score": 0.7485935688018799, "xcomet_qe_score": 0.8317078948020935, "metricx_score": 7.715461254119873, "metricx_qe_score": 7.751801490783691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "W)在允许在干净样本上继续微调后,FTW的性能与其他方法相当。", "metrics": {"bleu_score": 26.055431219916315, "chrf_score": 21.08638410938035, "xcomet_score": 0.7003176212310791, "xcomet_qe_score": 0.7803690433502197, "metricx_score": 4.882088661193848, "metricx_qe_score": 5.747462749481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的WSL方法,这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 57.473860166742696, "chrf_score": 55.57193860381212, "xcomet_score": 0.9802455902099609, "xcomet_qe_score": 0.9862825274467468, "metricx_score": 0.8046079277992249, "metricx_qe_score": 1.4242140054702759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们展示了最近的WSL方法需要干净的手动标注样本来正常工作。", "metrics": {"bleu_score": 37.098612584060795, "chrf_score": 38.19649751750055, "xcomet_score": 0.7461018562316895, "xcomet_qe_score": 0.8424180150032043, "metricx_score": 3.6661624908447266, "metricx_qe_score": 4.008120059967041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能增益和实用性被严重高估。", "metrics": {"bleu_score": 32.22538601891173, "chrf_score": 27.57865391385171, "xcomet_score": 0.9816327691078186, "xcomet_qe_score": 0.9971581697463989, "metricx_score": 0.8143463134765625, "metricx_qe_score": 0.7807042598724365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否在干净验证样本上进行。", "metrics": {"bleu_score": 28.348995551545965, "chrf_score": 24.960806547460468, "xcomet_score": 0.9086339473724365, "xcomet_qe_score": 0.905847430229187, "metricx_score": 1.6992335319519043, "metricx_qe_score": 3.1822967529296875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL方法应与少量学习基线进行比较,因为两者都在干净样本上工作。", "metrics": {"bleu_score": 43.52829709882717, "chrf_score": 38.467608542591115, "xcomet_score": 0.7500709295272827, "xcomet_qe_score": 0.7370970249176025, "metricx_score": 3.7697174549102783, "metricx_qe_score": 4.8027849197387695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一个简单而强大的基线,应在未来的WSL工作中考虑。", "metrics": {"bleu_score": 50.42673311800851, "chrf_score": 46.38550600316616, "xcomet_score": 0.8454364538192749, "xcomet_qe_score": 0.7188963890075684, "metricx_score": 2.642082929611206, "metricx_qe_score": 2.8977112770080566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们已经开源了我们的代码。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9967988729476929, "xcomet_qe_score": 0.9351927638053894, "metricx_score": 0.4914062023162842, "metricx_qe_score": 0.7212974429130554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以在本页的二维码中找到它。", "metrics": {"bleu_score": 20.141900296659895, "chrf_score": 17.14866865498809, "xcomet_score": 0.9771319627761841, "xcomet_qe_score": 0.9329280853271484, "metricx_score": 1.0008817911148071, "metricx_qe_score": 0.9358668327331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝会议愉快。", "metrics": {"bleu_score": 7.687847996206941, "chrf_score": 8.318914192155988, "xcomet_score": 0.9833041429519653, "xcomet_qe_score": 0.9773144125938416, "metricx_score": 0.3437096178531647, "metricx_qe_score": 0.22848688066005707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 10.63630283111601, "xcomet_score": 0.995103120803833, "xcomet_qe_score": 0.9681702852249146, "metricx_score": 0.23110710084438324, "metricx_qe_score": 0.18293818831443787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是萨拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6017594337463379, "metricx_qe_score": 0.9140466451644897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍ABC-Eval,这是一种新的评估对话式人工智能的多维度方法。", "metrics": {"bleu_score": 29.454643647949837, "chrf_score": 38.094281114489355, "xcomet_score": 0.9556148648262024, "xcomet_qe_score": 0.9842100143432617, "metricx_score": 1.0457866191864014, "metricx_qe_score": 1.211341142654419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学自然语言处理实验室完成,由埃默里大学的乔伊教授领导,并与亚马逊Alexa AI合作。", "metrics": {"bleu_score": 27.667683760831373, "chrf_score": 29.23911403199598, "xcomet_score": 0.7919567823410034, "xcomet_qe_score": 0.8032825589179993, "metricx_score": 3.61444091796875, "metricx_qe_score": 3.4458365440368652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚刚开发了一个对话模型,并希望了解它与当前最先进技术的比较情况。", "metrics": {"bleu_score": 52.42329002966553, "chrf_score": 47.4422995562943, "xcomet_score": 0.9939658641815186, "xcomet_qe_score": 0.9837436676025391, "metricx_score": 0.4854610562324524, "metricx_qe_score": 0.5634567141532898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人类评估,例如要求人类评委选择两个对话中哪一个更好,或者根据利克特量表对对话进行评分。", "metrics": {"bleu_score": 63.66546021618382, "chrf_score": 56.972629650443785, "xcomet_score": 0.9208372235298157, "xcomet_qe_score": 0.9701356887817383, "metricx_score": 0.6194267868995667, "metricx_qe_score": 0.8221452236175537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的全面评估方面效果很好,但对话质量有许多方面。", "metrics": {"bleu_score": 37.54122374352081, "chrf_score": 31.767694670629226, "xcomet_score": 0.9337989091873169, "xcomet_qe_score": 0.9130752086639404, "metricx_score": 0.5414295196533203, "metricx_qe_score": 0.7984640598297119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估多个聊天质量的维度,以更细致地了解模型的优点和缺点。", "metrics": {"bleu_score": 45.87986029038474, "chrf_score": 40.65884255335658, "xcomet_score": 0.9768353700637817, "xcomet_qe_score": 0.9676045179367065, "metricx_score": 0.5832329392433167, "metricx_qe_score": 0.6099401116371155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地要求人类评委评估对话质量的几个维度,例如模型响应的相关性,使用现有的比较或利克特量表方法。", "metrics": {"bleu_score": 55.22609805097744, "chrf_score": 47.03308562897919, "xcomet_score": 0.9499896764755249, "xcomet_qe_score": 0.897255539894104, "metricx_score": 1.4974380731582642, "metricx_qe_score": 2.0360772609710693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为有一种更精确和可靠的策略来进行多维度对话评估。", "metrics": {"bleu_score": 46.584997002150004, "chrf_score": 50.03963638591925, "xcomet_score": 0.9704058170318604, "xcomet_qe_score": 0.9049609303474426, "metricx_score": 1.0903198719024658, "metricx_qe_score": 1.447447419166565, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确注释每个模型响应是否表现出某些行为来减少人类评估的主观性,例如以无关信息回应或自相矛盾。", "metrics": {"bleu_score": 69.60627922456392, "chrf_score": 62.70218758076782, "xcomet_score": 0.8561549186706543, "xcomet_qe_score": 0.7728872895240784, "metricx_score": 1.6739362478256226, "metricx_qe_score": 2.2626614570617676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为对话中的行为注释,简称ABC-Eval。", "metrics": {"bleu_score": 45.12595975949003, "chrf_score": 54.21336947150912, "xcomet_score": 0.9799149036407471, "xcomet_qe_score": 0.9107050895690918, "metricx_score": 1.3986443281173706, "metricx_qe_score": 2.2401773929595947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面涵盖最近文献中建议影响聊天质量的聊天模型行为。A", "metrics": {"bleu_score": 65.6619157868385, "chrf_score": 58.39585991253443, "xcomet_score": 0.8421212434768677, "xcomet_qe_score": 0.7832047343254089, "metricx_score": 3.5532186031341553, "metricx_qe_score": 4.173835277557373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "BC-Eval能够测量聊天模型犯各种主题错误的比率。", "metrics": {"bleu_score": 66.96510758566588, "chrf_score": 71.71864770156932, "xcomet_score": 0.73902428150177, "xcomet_qe_score": 0.6504051685333252, "metricx_score": 5.512974262237549, "metricx_qe_score": 6.518630504608154, "linguapy_score": [1, "ESTONIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABC-Eval测量聊天模型忽略其合作伙伴或说出无关信息的回合数量,自相矛盾或与其合作伙伴矛盾,幻想错误的事实或违反常识知识,以及模型成功或失败地表现出同理心。", "metrics": {"bleu_score": 39.531694257938476, "chrf_score": 37.51240283138071, "xcomet_score": 0.5643402934074402, "xcomet_qe_score": 0.6291598081588745, "metricx_score": 6.308760643005371, "metricx_qe_score": 6.139140605926514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估最有效,我们选择了四个最先进的聊天模型,并使用ABC-Eval对每个模型进行了100次人类机器人对话的评估。", "metrics": {"bleu_score": 57.02006934873751, "chrf_score": 56.513087119124904, "xcomet_score": 0.8878735303878784, "xcomet_qe_score": 0.9654264450073242, "metricx_score": 1.974608302116394, "metricx_qe_score": 1.5202898979187012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用三种现有方法对这些对话进行了评估,包括回合级别的利克特评分、对话级别的利克特评分和对话级别的成对比较。", "metrics": {"bleu_score": 45.88906842099278, "chrf_score": 39.823099592670964, "xcomet_score": 0.9166769981384277, "xcomet_qe_score": 0.8280428051948547, "metricx_score": 3.3971927165985107, "metricx_qe_score": 3.2744405269622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每种现有方法,我们收集了对话中最常测量的八个方面的评估,因为这是评估聊天模型的多个维度的标准做法。", "metrics": {"bleu_score": 53.49216479256724, "chrf_score": 44.34396404376444, "xcomet_score": 0.9661914110183716, "xcomet_qe_score": 0.9022532105445862, "metricx_score": 3.643453359603882, "metricx_qe_score": 4.2429022789001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现ABC-Eval行为标签在整体上比现有方法收集的标签更可靠,", "metrics": {"bleu_score": 39.91841002846871, "chrf_score": 43.796535203476836, "xcomet_score": 0.7608829140663147, "xcomet_qe_score": 0.7329456210136414, "metricx_score": 8.203063011169434, "metricx_qe_score": 9.21841049194336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如通过100次双重标记对话的内部评估者一致性测量。此外,ABC-Eval标签比现有方法生成的指标更能预测整体对话质量,如本简单线性回归分析所示。", "metrics": {"bleu_score": 30.945488432824778, "chrf_score": 40.27313178221037, "xcomet_score": 0.4971229135990143, "xcomet_qe_score": 0.2121908813714981, "metricx_score": 6.133587837219238, "metricx_qe_score": 8.244194030761719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到测量自相矛盾和合作伙伴矛盾的回合比例分别解释了5%和10%的对话质量,而平均利克特一致性得分只解释了4%或更少。", "metrics": {"bleu_score": 66.79347320450007, "chrf_score": 61.622776416286186, "xcomet_score": 0.7185227274894714, "xcomet_qe_score": 0.7662413716316223, "metricx_score": 4.519859313964844, "metricx_qe_score": 4.885587692260742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查每个评估指标是否捕捉到聊天质量的独特方面。您", "metrics": {"bleu_score": 75.91120060796062, "chrf_score": 68.90915935033583, "xcomet_score": 0.8045860528945923, "xcomet_qe_score": 0.799746572971344, "metricx_score": 4.737525939941406, "metricx_qe_score": 1.8758111000061035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到所有ABC-Eval指标的组合解释了25%以上的对话质量,而逐一删除这些指标,大多数指标会导致丢失大量关于质量的信息。", "metrics": {"bleu_score": 46.459572099794066, "chrf_score": 46.532023451998896, "xcomet_score": 0.7655657529830933, "xcomet_qe_score": 0.6883035898208618, "metricx_score": 1.5781612396240234, "metricx_qe_score": 2.79853892326355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有回合级别利克特指标的组合解释了远少于质量,并且这些指标中的较少指标包含独特信息。这些可靠、信息丰", "metrics": {"bleu_score": 30.682127631008484, "chrf_score": 28.58814309074189, "xcomet_score": 0.3883408308029175, "xcomet_qe_score": 0.4589900076389313, "metricx_score": 11.79509162902832, "metricx_qe_score": 9.627734184265137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "富且独特的ABC-Eval指标使我们能够以比以前的方法能够实现的更高分辨率来评估对话式人工智能。您可以在我们", "metrics": {"bleu_score": 3.3278183018447356, "chrf_score": 16.81816152164464, "xcomet_score": 0.28069883584976196, "xcomet_qe_score": 0.29779180884361267, "metricx_score": 10.304853439331055, "metricx_qe_score": 8.010591506958008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实验结果中看到,仍然存在几个挑战,并且已经被精确量化。", "metrics": {"bleu_score": 26.87992011339668, "chrf_score": 22.357144946611484, "xcomet_score": 0.8931965231895447, "xcomet_qe_score": 0.8390227556228638, "metricx_score": 1.7245173454284668, "metricx_qe_score": 2.1310267448425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人在其响应中有大约20%的常识违规。", "metrics": {"bleu_score": 48.61130036519513, "chrf_score": 45.278395514675374, "xcomet_score": 0.8405439853668213, "xcomet_qe_score": 0.8188671469688416, "metricx_score": 3.5782148838043213, "metricx_qe_score": 4.3272247314453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在大约15%的响应中产生无关信息,并且它们在大约10%的时间内自相矛盾或与其合作伙伴矛盾。", "metrics": {"bleu_score": 39.58054246077596, "chrf_score": 35.648154018549235, "xcomet_score": 0.6880449056625366, "xcomet_qe_score": 0.6231067776679993, "metricx_score": 3.3937315940856934, "metricx_qe_score": 3.5526323318481445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域快速改进的步伐,这些错误率在我们评估进行后发布的新模型中可能会有所下降。", "metrics": {"bleu_score": 29.037747307996295, "chrf_score": 26.29420077979652, "xcomet_score": 0.9593126773834229, "xcomet_qe_score": 0.9356526136398315, "metricx_score": 2.1869170665740967, "metricx_qe_score": 1.8690881729125977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更是追求可靠和精确的评估指标以比较模型的理由。", "metrics": {"bleu_score": 37.26178085649122, "chrf_score": 31.80078562411883, "xcomet_score": 0.9915591478347778, "xcomet_qe_score": 0.9819566011428833, "metricx_score": 1.7845091819763184, "metricx_qe_score": 1.7283307313919067, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABC-Eval能够被该领域的其他人作为朝着这个方向迈出的有意义的一步,", "metrics": {"bleu_score": 73.96770288212053, "chrf_score": 74.54129698345136, "xcomet_score": 0.9601097702980042, "xcomet_qe_score": 0.9467620849609375, "metricx_score": 1.8653483390808105, "metricx_qe_score": 1.9851003885269165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待在未来几个月和几年中看到对话式人工智能的进步。", "metrics": {"bleu_score": 52.664038784792645, "chrf_score": 47.37083027595311, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7944822311401367, "metricx_qe_score": 0.9403984546661377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫凯奥·杨,我将介绍我们的研究成果,题目是《翻译何时需要上下文?", "metrics": {"bleu_score": 44.78344190407162, "chrf_score": 38.94380510189333, "xcomet_score": 0.9685804843902588, "xcomet_qe_score": 0.9757868051528931, "metricx_score": 0.868246853351593, "metricx_qe_score": 1.0783987045288086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语言探索》。", "metrics": {"bleu_score": 67.0422683816333, "chrf_score": 60.745550745550744, "xcomet_score": 0.9882596731185913, "xcomet_qe_score": 0.9236876964569092, "metricx_score": 0.9823226928710938, "metricx_qe_score": 1.2916620969772339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项研究是与帕特里克·弗兰纳、艾米·刘、安德烈·F.D.马丁斯和格雷姆·纽维格合作完成的。", "metrics": {"bleu_score": 12.671594061839466, "chrf_score": 8.414338641742425, "xcomet_score": 0.8150646686553955, "xcomet_qe_score": 0.7959425449371338, "metricx_score": 2.5234243869781494, "metricx_qe_score": 3.064059019088745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都依赖于上下文。", "metrics": {"bleu_score": 42.40125351805035, "chrf_score": 35.33598696125657, "xcomet_score": 0.9987486600875854, "xcomet_qe_score": 0.9918657541275024, "metricx_score": 0.15039336681365967, "metricx_qe_score": 0.23019355535507202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译这个句子中的“mole”?", "metrics": {"bleu_score": 54.017258985951415, "chrf_score": 55.419633882775734, "xcomet_score": 0.9968364238739014, "xcomet_qe_score": 0.967570424079895, "metricx_score": 0.9834437966346741, "metricx_qe_score": 1.9881327152252197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一个句子是“如果部长们发现了,事情可能会变得危险”,那么“mole”指的是间谍。", "metrics": {"bleu_score": 17.955860884668844, "chrf_score": 13.072881747771776, "xcomet_score": 0.9738942384719849, "xcomet_qe_score": 0.9643248915672302, "metricx_score": 2.696976661682129, "metricx_qe_score": 3.9755516052246094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一个句子是“医生,这可能是严重的吗?”那么“mole”指的是痣。", "metrics": {"bleu_score": 17.15452974096239, "chrf_score": 16.190582311367734, "xcomet_score": 0.8098569512367249, "xcomet_qe_score": 0.7914227843284607, "metricx_score": 3.0207409858703613, "metricx_qe_score": 3.3485143184661865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,单词的含义会改变,翻译也会随之改变。", "metrics": {"bleu_score": 33.07780599680712, "chrf_score": 26.39925131149129, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.16987362504005432, "metricx_qe_score": 0.1821194887161255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型如何处理这些情况是非常困难的。", "metrics": {"bleu_score": 37.76302796471566, "chrf_score": 32.03580455001594, "xcomet_score": 0.9781463146209717, "xcomet_qe_score": 0.9839218854904175, "metricx_score": 0.9633798003196716, "metricx_qe_score": 1.1256078481674194, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有少量的翻译依赖于上下文,这使得像蓝色这样的语料库级别的指标无法捕捉到这些翻译。一些", "metrics": {"bleu_score": 35.762118601307, "chrf_score": 30.754550393042145, "xcomet_score": 0.7574977874755859, "xcomet_qe_score": 0.7490168809890747, "metricx_score": 6.997832775115967, "metricx_qe_score": 5.959799766540527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "人建议对上下文依赖的翻译进行针对性评估,但这些资源只支持有限类型的上下文依赖翻译和有限的语言集,因为它们通常依赖于领域知识和人工策划。", "metrics": {"bleu_score": 81.28172118455599, "chrf_score": 78.08227806084207, "xcomet_score": 0.6031126379966736, "xcomet_qe_score": 0.5823670625686646, "metricx_score": 4.382849216461182, "metricx_qe_score": 4.212527275085449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9985342025756836, "xcomet_qe_score": 1.0, "metricx_score": 0.17629770934581757, "metricx_qe_score": 0.08587866276502609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型如何处理这些情况?", "metrics": {"bleu_score": 32.74135267450808, "chrf_score": 30.12273108277076, "xcomet_score": 0.9986907243728638, "xcomet_qe_score": 0.9914894104003906, "metricx_score": 0.7201682925224304, "metricx_qe_score": 1.0261601209640503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们从测量单词在翻译中对上下文的依赖程度开始。", "metrics": {"bleu_score": 60.377972421290735, "chrf_score": 53.11762524176633, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 3.4067652225494385, "metricx_qe_score": 2.749436616897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的研究中,我们引入了CXMI作为机器翻译模型使用上下文的度量。", "metrics": {"bleu_score": 60.74329810842226, "chrf_score": 59.84099296330576, "xcomet_score": 0.896350085735321, "xcomet_qe_score": 0.9087897539138794, "metricx_score": 2.2580740451812744, "metricx_qe_score": 2.596381425857544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量上下文C在给定源X的情况下提供关于目标Y的信息量来实现。你可以将CXMI视为从给模型提供上下文中获得的信息。", "metrics": {"bleu_score": 56.10368728005921, "chrf_score": 51.15426188844376, "xcomet_score": 0.8706047534942627, "xcomet_qe_score": 0.8278900384902954, "metricx_score": 2.629976749420166, "metricx_qe_score": 3.1809020042419434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们将CXMI扩展到点YCXMI,它可以在句子级别或单词级别测量上下文的使用。", "metrics": {"bleu_score": 36.61486369305063, "chrf_score": 31.779862755163713, "xcomet_score": 0.7414952516555786, "xcomet_qe_score": 0.7098278999328613, "metricx_score": 5.72825813293457, "metricx_qe_score": 6.37848424911499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将具有高P6MI的单词视为需要上下文进行翻译的单词。", "metrics": {"bleu_score": 62.147423775880156, "chrf_score": 53.24316788101523, "xcomet_score": 0.7879047393798828, "xcomet_qe_score": 0.7827916145324707, "metricx_score": 5.524500846862793, "metricx_qe_score": 5.639744758605957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析具有高PCXMI的单词,以寻找这些单词之间的模式。", "metrics": {"bleu_score": 37.13290491373275, "chrf_score": 38.957546209020244, "xcomet_score": 0.9596447348594666, "xcomet_qe_score": 0.954494059085846, "metricx_score": 1.3494937419891357, "metricx_qe_score": 2.570572853088379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在从英语翻译成14种不同语言的TED演讲稿中进行分析。", "metrics": {"bleu_score": 73.54084436368392, "chrf_score": 71.33987442876071, "xcomet_score": 0.9117159843444824, "xcomet_qe_score": 0.9335773587226868, "metricx_score": 3.365675687789917, "metricx_qe_score": 3.1386547088623047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同的层面上进行分析。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9992449283599854, "xcomet_qe_score": 0.9950920343399048, "metricx_score": 0.22764378786087036, "metricx_qe_score": 0.23440968990325928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有高平均PCXMI的语音标签。", "metrics": {"bleu_score": 13.400825781778892, "chrf_score": 20.847584143749682, "xcomet_score": 0.8394436836242676, "xcomet_qe_score": 0.8456155061721802, "metricx_score": 1.9700870513916016, "metricx_qe_score": 1.890393614768982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到,例如,在阿拉伯语中具有高P6MI的双重代词。", "metrics": {"bleu_score": 46.0249537136194, "chrf_score": 34.82223160426401, "xcomet_score": 0.6551544666290283, "xcomet_qe_score": 0.7297909259796143, "metricx_score": 6.570986747741699, "metricx_qe_score": 6.226080894470215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语没有双重代词,因此在翻译成阿拉伯语时,需要上下文来确定代词是否是双重的。", "metrics": {"bleu_score": 58.454323112144216, "chrf_score": 51.54389469360604, "xcomet_score": 0.8058619499206543, "xcomet_qe_score": 0.9767146110534668, "metricx_score": 2.9226737022399902, "metricx_qe_score": 2.516432046890259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 85.94148359295417, "chrf_score": 87.96567669989582, "xcomet_score": 0.9981253147125244, "xcomet_qe_score": 0.9977245330810547, "metricx_score": 0.5655370354652405, "metricx_qe_score": 0.7977930307388306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们查看具有高P6XMI的词汇项目,平均在其所有不同的出现中。", "metrics": {"bleu_score": 20.587409535374668, "chrf_score": 20.64539048956633, "xcomet_score": 0.6427170038223267, "xcomet_qe_score": 0.6232851147651672, "metricx_score": 9.304926872253418, "metricx_qe_score": 8.937063217163086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这帮助我们识别出像这里这样的情况,在中文中,你需要上下文来翻译正确的名词,以确保在文档中使用相同的翻译。", "metrics": {"bleu_score": 40.29883231412914, "chrf_score": 34.61201242003073, "xcomet_score": 0.802874743938446, "xcomet_qe_score": 0.8363090753555298, "metricx_score": 1.1785274744033813, "metricx_qe_score": 1.4374240636825562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似地,我们发现上下文支持翻译正确的正式性。", "metrics": {"bleu_score": 34.61771478013761, "chrf_score": 29.821649504854896, "xcomet_score": 0.8458912372589111, "xcomet_qe_score": 0.8276109099388123, "metricx_score": 2.804408073425293, "metricx_qe_score": 2.849515914916992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看具有高P6XMI的不同个别标记。", "metrics": {"bleu_score": 12.109013026441868, "chrf_score": 15.718942610270412, "xcomet_score": 0.7507836222648621, "xcomet_qe_score": 0.7379288077354431, "metricx_score": 6.489349365234375, "metricx_qe_score": 6.147207736968994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出无法通过单词本身捕捉到的现象,但这些现象在句子结构中表达出来,例如省略号解析。", "metrics": {"bleu_score": 45.6496286645645, "chrf_score": 38.64645751149259, "xcomet_score": 0.9245802164077759, "xcomet_qe_score": 0.8463706970214844, "metricx_score": 1.1106468439102173, "metricx_qe_score": 1.5428566932678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用分析结果来设计文档级别翻译的基准。", "metrics": {"bleu_score": 65.95659130849208, "chrf_score": 62.57641871068086, "xcomet_score": 0.9866782426834106, "xcomet_qe_score": 0.9719475507736206, "metricx_score": 1.0185621976852417, "metricx_qe_score": 1.4396315813064575, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别出的每个话语现象,我们创建标记器来自动识别属于该现象的单词。", "metrics": {"bleu_score": 46.417478111941385, "chrf_score": 40.46614567846045, "xcomet_score": 0.975896954536438, "xcomet_qe_score": 0.9645987153053284, "metricx_score": 1.238508701324463, "metricx_qe_score": 1.7716388702392578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称我们的标记器为多语言话语感知或MUDA标记器。", "metrics": {"bleu_score": 39.94860803813015, "chrf_score": 38.49354578641857, "xcomet_score": 0.8716089725494385, "xcomet_qe_score": 0.8197460770606995, "metricx_score": 1.1305489540100098, "metricx_qe_score": 1.7222049236297607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还注意到不同的语言具有不同比例的这些话语现象。", "metrics": {"bleu_score": 40.02998520203548, "chrf_score": 34.830179696879, "xcomet_score": 0.9183305501937866, "xcomet_qe_score": 0.8646813631057739, "metricx_score": 1.9197627305984497, "metricx_qe_score": 1.658595323562622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们通过将标记器应用于我们想要用于评估的平行语料库来使用MUDA标记器。我们在MUDA标记器识别出的上下文依赖示例上应用我们选择的翻译指标。", "metrics": {"bleu_score": 48.490719295705446, "chrf_score": 49.08926143690133, "xcomet_score": 0.8480058312416077, "xcomet_qe_score": 0.7582897543907166, "metricx_score": 2.3537778854370117, "metricx_qe_score": 2.738442897796631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同模型在文档级别机器翻译中的表现。", "metrics": {"bleu_score": 62.762577242174885, "chrf_score": 61.180972805711555, "xcomet_score": 0.9088295102119446, "xcomet_qe_score": 0.8646342754364014, "metricx_score": 0.9582242965698242, "metricx_qe_score": 1.090294599533081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,例如蓝色,我们发现上下文无关模型表现最佳。", "metrics": {"bleu_score": 49.94190880302173, "chrf_score": 39.08641237715845, "xcomet_score": 0.7817298173904419, "xcomet_qe_score": 0.7146728038787842, "metricx_score": 2.3745412826538086, "metricx_qe_score": 2.012744903564453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果我们使用Comet,上下文感知模型表现最佳。", "metrics": {"bleu_score": 50.37528966367497, "chrf_score": 40.327988673072696, "xcomet_score": 0.9471109509468079, "xcomet_qe_score": 0.9220526814460754, "metricx_score": 2.4397776126861572, "metricx_qe_score": 2.942309856414795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用Word F度量,那么无论是否使用上下文,模型的表现都相当。", "metrics": {"bleu_score": 26.59887114061172, "chrf_score": 23.888558003878916, "xcomet_score": 0.8278152942657471, "xcomet_qe_score": 0.7710458040237427, "metricx_score": 4.576388359069824, "metricx_qe_score": 2.908505916595459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果我们仅使用语料库级别的指标,很难确定最佳的文档级别翻译系统。", "metrics": {"bleu_score": 69.09543981442395, "chrf_score": 62.471780518113164, "xcomet_score": 0.9972898960113525, "xcomet_qe_score": 0.9929883480072021, "metricx_score": 0.7540609836578369, "metricx_qe_score": 0.9542807340621948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用MUDA基准来评估模型,我们发现对于某些话语现象,例如正式性和词汇凝聚力,上下文感知模型比不使用上下文的模型更准确。", "metrics": {"bleu_score": 61.27251126586895, "chrf_score": 56.8479248465352, "xcomet_score": 0.840642511844635, "xcomet_qe_score": 0.8051336407661438, "metricx_score": 2.4622278213500977, "metricx_qe_score": 3.0771660804748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在其他现象上,如省略号、代词和动词形式,并不比不使用上下文的模型好多少。", "metrics": {"bleu_score": 67.25109952588886, "chrf_score": 59.07338533826886, "xcomet_score": 0.98627769947052, "xcomet_qe_score": 0.9091354608535767, "metricx_score": 0.7501961588859558, "metricx_qe_score": 1.098554015159607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在某种程度上表明我们需要在文档级别翻译方面取得更多进展。", "metrics": {"bleu_score": 52.22614876544583, "chrf_score": 45.1006774418904, "xcomet_score": 0.9999217987060547, "xcomet_qe_score": 0.9994913339614868, "metricx_score": 0.7179521918296814, "metricx_qe_score": 0.8822847604751587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准显示,DeepL通常比Google Translate在文档级别翻译中更准确。", "metrics": {"bleu_score": 53.88421736135875, "chrf_score": 47.408485264301895, "xcomet_score": 0.8788999319076538, "xcomet_qe_score": 0.7679026126861572, "metricx_score": 1.991899013519287, "metricx_qe_score": 2.1203560829162598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们在14种语言对中进行了基于数据的分析,以确定翻译何时需要上下文。然后我们使用我们的发现来构建文档级别机器翻译的基准,这可以帮助我们确定模型能否很好地处理哪些话语现象,以及哪些翻译系统擅长文档级别翻译。", "metrics": {"bleu_score": 56.35798777112504, "chrf_score": 50.05442200543159, "xcomet_score": 0.949245810508728, "xcomet_qe_score": 0.9382226467132568, "metricx_score": 1.5978580713272095, "metricx_qe_score": 2.3558123111724854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的注意。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9513893127441406, "xcomet_qe_score": 0.9545682668685913, "metricx_score": 1.2208698987960815, "metricx_qe_score": 1.5142760276794434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在多伦多见。", "metrics": {"bleu_score": 71.65313105737896, "chrf_score": 64.65405545478103, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4812854528427124, "metricx_qe_score": 1.2813191413879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Yannis Lavrac,我将向大家介绍我们在Dr. Bert方面的工作,这是一个用于生物医学和临床领域的鲁棒预训练模型。", "metrics": {"bleu_score": 46.59657276417694, "chrf_score": 42.90592725633473, "xcomet_score": 0.6439886093139648, "xcomet_qe_score": 0.6661746501922607, "metricx_score": 4.931138515472412, "metricx_qe_score": 5.583056449890137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论了医疗保健中的语言建模。", "metrics": {"bleu_score": 56.40009831507544, "chrf_score": 44.0419725202334, "xcomet_score": 0.9915467500686646, "xcomet_qe_score": 0.9924795627593994, "metricx_score": 0.3726309835910797, "metricx_qe_score": 0.5104391574859619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个法语生物医学模型,名为Dr. Bert,基于Roberta,并使用Nachos进行训练,这是一个从网络上收集的医疗数据集。", "metrics": {"bleu_score": 28.16483065020107, "chrf_score": 22.242662639739464, "xcomet_score": 0.8336381316184998, "xcomet_qe_score": 0.7868409752845764, "metricx_score": 2.695138692855835, "metricx_qe_score": 2.804504871368408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了多种预训练设置和数据源的模型比较。", "metrics": {"bleu_score": 65.01298784313546, "chrf_score": 61.83691769376676, "xcomet_score": 0.9666675329208374, "xcomet_qe_score": 0.9584218263626099, "metricx_score": 1.2881532907485962, "metricx_qe_score": 1.707563877105713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们展示了在11个法语生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 62.14123450440834, "chrf_score": 58.833518827734245, "xcomet_score": 0.8007130026817322, "xcomet_qe_score": 0.7505614757537842, "metricx_score": 2.493783950805664, "metricx_qe_score": 3.7638182640075684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结了实验并提供了更多关于如何访问模型的详细信息。", "metrics": {"bleu_score": 41.80022035122783, "chrf_score": 35.82857933289869, "xcomet_score": 0.9625774621963501, "xcomet_qe_score": 0.8558160066604614, "metricx_score": 0.3807379901409149, "metricx_qe_score": 0.3558892011642456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,Bert已成为解决自然语言处理任务的最有效方法之一,并与历史静态和上下文化方法(如Word2Vec、FastText或NWO)相比,提供了巨大的性能提升。", "metrics": {"bleu_score": 59.832233510725786, "chrf_score": 53.39735285073783, "xcomet_score": 0.7780131101608276, "xcomet_qe_score": 0.8206304907798767, "metricx_score": 4.217140197753906, "metricx_qe_score": 4.678788661956787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自那时起,该模型已被适应到许多其他语言,如法语的Camembert,以及生物医学领域的PermitBert和BioBert,以及临床领域的ClinicalBert,但主要是英语。", "metrics": {"bleu_score": 48.95107382088703, "chrf_score": 45.36494326501363, "xcomet_score": 0.640076220035553, "xcomet_qe_score": 0.5808444023132324, "metricx_score": 6.157318115234375, "metricx_qe_score": 5.998419284820557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专业模型稀缺,通常基于持续预训练,因为缺乏领域内数据。", "metrics": {"bleu_score": 40.73224127318495, "chrf_score": 34.69175833114817, "xcomet_score": 0.7731555104255676, "xcomet_qe_score": 0.768599808216095, "metricx_score": 1.3611412048339844, "metricx_qe_score": 1.7300597429275513, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法语在生物医学方面没有任何开源模型,直到现在。", "metrics": {"bleu_score": 40.240300812810446, "chrf_score": 31.248978075065033, "xcomet_score": 0.8811900615692139, "xcomet_qe_score": 0.7627732753753662, "metricx_score": 1.4879094362258911, "metricx_qe_score": 2.4425857067108154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们问自己,什么是最合适的数据搜索,以适应广泛的用途,这些数据是否是临床数据的良好替代品?", "metrics": {"bleu_score": 33.13891623958257, "chrf_score": 31.77210569260201, "xcomet_score": 0.8342010974884033, "xcomet_qe_score": 0.7813236713409424, "metricx_score": 3.5999579429626465, "metricx_qe_score": 3.8310375213623047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Dr. Bert与我们的Schubert模型进行比较,该模型基于从非大学医院数据仓库获得的匿名化数据。", "metrics": {"bleu_score": 54.3216757312702, "chrf_score": 42.62096498131373, "xcomet_score": 0.7130871415138245, "xcomet_qe_score": 0.6420689821243286, "metricx_score": 5.635347843170166, "metricx_qe_score": 5.954294681549072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,我们问自己,我们需要多少数据来训练一个专业的法语数据模型?", "metrics": {"bleu_score": 47.45223087869147, "chrf_score": 45.45064953236205, "xcomet_score": 0.9741883277893066, "xcomet_qe_score": 0.8799372315406799, "metricx_score": 0.726283073425293, "metricx_qe_score": 0.7308590412139893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4GB、8GB还是更多?", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 90.21205646205644, "xcomet_score": 0.9781609773635864, "xcomet_qe_score": 0.9645631313323975, "metricx_score": 0.23510365188121796, "metricx_qe_score": 0.5033293962478638, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四个从头开始的模型。第一个版本的Dr. Bert使用7GB的Nachos,第二个版本使用4GB的Nachos子集,第一个版本的Schubert是一个临床模型,使用4GB的临床笔记句子,最后一个版本的Schubert使用4GB的Nachos子集和4GB的临床笔记的混合。", "metrics": {"bleu_score": 50.110039543118084, "chrf_score": 39.384513464021516, "xcomet_score": 0.39250075817108154, "xcomet_qe_score": 0.408763587474823, "metricx_score": 6.827913284301758, "metricx_qe_score": 7.031006336212158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还引入了三个在持续预训练上训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 58.25098664887468, "chrf_score": 49.86993086603178, "xcomet_score": 0.8624116778373718, "xcomet_qe_score": 0.8372915387153625, "metricx_score": 2.569112777709961, "metricx_qe_score": 3.05477237701416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于Camembert的权重,并在4GB的Nachos子集上训练,", "metrics": {"bleu_score": 30.494190622942757, "chrf_score": 37.37749864920715, "xcomet_score": 0.6587667465209961, "xcomet_qe_score": 0.6830238699913025, "metricx_score": 5.225049018859863, "metricx_qe_score": 6.45408296585083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也基于Camembert,但这次在4GB的临床笔记上训练,最后一个基于英语生物医学模型BermetBert,并在4GB的Nachos子集上训练。", "metrics": {"bleu_score": 66.64762340208486, "chrf_score": 51.76134582222042, "xcomet_score": 0.5640790462493896, "xcomet_qe_score": 0.5137025117874146, "metricx_score": 6.646487712860107, "metricx_qe_score": 6.958409309387207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了多个公共和私有的任务,如命名实体识别、分类、部分语音标记和问答。", "metrics": {"bleu_score": 57.1801692297701, "chrf_score": 49.254895634851565, "xcomet_score": 0.6350053548812866, "xcomet_qe_score": 0.6927162408828735, "metricx_score": 3.1283836364746094, "metricx_qe_score": 4.759814739227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行比较,包括Camembert Oscar 138GB、Camembert Oscar 4GB、Camembert CCNet 4GB、PermitBert、BioBert和ClinicalBert。", "metrics": {"bleu_score": 44.89027374193221, "chrf_score": 43.107703750538725, "xcomet_score": 0.49940383434295654, "xcomet_qe_score": 0.4963986575603485, "metricx_score": 5.5215864181518555, "metricx_qe_score": 5.415644645690918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,模型在与训练数据相同性质的任务上表现最佳。", "metrics": {"bleu_score": 34.408878049794424, "chrf_score": 30.122980801345413, "xcomet_score": 0.9964064359664917, "xcomet_qe_score": 0.9933534860610962, "metricx_score": 1.0521397590637207, "metricx_qe_score": 1.316723346710205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以观察到,异质数据源的数据似乎更加多功能。", "metrics": {"bleu_score": 50.004272956497594, "chrf_score": 44.5731791132616, "xcomet_score": 0.7986897230148315, "xcomet_qe_score": 0.8044624328613281, "metricx_score": 2.1254448890686035, "metricx_qe_score": 1.8323674201965332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据可以提高性能。", "metrics": {"bleu_score": 38.01334678804693, "chrf_score": 30.961551208411738, "xcomet_score": 0.9266443848609924, "xcomet_qe_score": 0.9694835543632507, "metricx_score": 1.8303345441818237, "metricx_qe_score": 1.882683515548706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始的预训练似乎在大多数任务上获得了更高的性能。", "metrics": {"bleu_score": 52.839726107318825, "chrf_score": 50.100716208730965, "xcomet_score": 0.9388210773468018, "xcomet_qe_score": 0.8811310529708862, "metricx_score": 3.1967966556549072, "metricx_qe_score": 3.9971885681152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们在持续预训练上的实验,使用PermitBert的权重和标记器,在4GB的Nachos子集上训练,显示出与Dr. Bert 4GB从头开始获得的结果相当,", "metrics": {"bleu_score": 29.66489713117189, "chrf_score": 24.642282529143714, "xcomet_score": 0.3967795968055725, "xcomet_qe_score": 0.39394262433052063, "metricx_score": 7.368982315063477, "metricx_qe_score": 7.097195148468018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而基于Camembert权重和标记器的模型则存在稳定性问题。", "metrics": {"bleu_score": 37.33161656077656, "chrf_score": 42.13481331005372, "xcomet_score": 0.942036509513855, "xcomet_qe_score": 0.8676472902297974, "metricx_score": 2.124810218811035, "metricx_qe_score": 1.8190556764602661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们的系统在11个任务中的9个任务上提供了更好的性能,并全面超越了通用模型Camembert的结果。", "metrics": {"bleu_score": 37.74793053226446, "chrf_score": 33.18123097824336, "xcomet_score": 0.7706379294395447, "xcomet_qe_score": 0.7450114488601685, "metricx_score": 4.387423515319824, "metricx_qe_score": 2.8082194328308105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业数据更好,更多的专业数据更好,但它的规模不大。", "metrics": {"bleu_score": 24.034793256416833, "chrf_score": 21.83660816491338, "xcomet_score": 0.7260932922363281, "xcomet_qe_score": 0.7512772083282471, "metricx_score": 5.530925750732422, "metricx_qe_score": 5.157129764556885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从Nachos获得的预训练模型都可以在Yuginface上免费获得,所有的训练脚本都在我们的GitHub存储库中。", "metrics": {"bleu_score": 43.12799120426794, "chrf_score": 40.38498821923466, "xcomet_score": 0.7657092809677124, "xcomet_qe_score": 0.805016040802002, "metricx_score": 5.7688469886779785, "metricx_qe_score": 5.83215856552124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,感谢大家的聆听,我们期待在多伦多的后续会议上进行交流。", "metrics": {"bleu_score": 22.20852593083405, "chrf_score": 25.919184562745194, "xcomet_score": 0.6732505559921265, "xcomet_qe_score": 0.7780379056930542, "metricx_score": 1.8308831453323364, "metricx_qe_score": 2.4151740074157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9583046436309814, "xcomet_qe_score": 0.9632420539855957, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·伦德曼,今天我将简要介绍我们关于无树的组合泛化的论文,使用多集合标记和潜在排列。这是与我的导师亚历山", "metrics": {"bleu_score": 36.13133465426269, "chrf_score": 31.17201497200426, "xcomet_score": 0.5604674816131592, "xcomet_qe_score": 0.5007286071777344, "metricx_score": 7.157607078552246, "metricx_qe_score": 5.8510589599609375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大·科拉和伊万·蒂托夫的合作成果。", "metrics": {"bleu_score": 5.113043483703116, "chrf_score": 2.9045531384289647, "xcomet_score": 0.1427937000989914, "xcomet_qe_score": 0.1365545243024826, "metricx_score": 7.3275041580200195, "metricx_qe_score": 6.303445816040039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深递归和训练期间单独见过的短语的未见组合的能力。", "metrics": {"bleu_score": 82.67409407262161, "chrf_score": 80.26805228874136, "xcomet_score": 0.7215951085090637, "xcomet_qe_score": 0.6373394727706909, "metricx_score": 5.264676570892334, "metricx_qe_score": 8.31620979309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能如下所示。", "metrics": {"bleu_score": 70.07637953409846, "chrf_score": 60.13480369940909, "xcomet_score": 0.908198356628418, "xcomet_qe_score": 0.8913741707801819, "metricx_score": 0.9897307753562927, "metricx_qe_score": 1.7725532054901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们有一组训练语", "metrics": {"bleu_score": 24.904151315712156, "chrf_score": 22.771821100910138, "xcomet_score": 0.7674890756607056, "xcomet_qe_score": 0.785688042640686, "metricx_score": 1.453514575958252, "metricx_qe_score": 2.725877046585083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "句,在这种情况下,女孩睡了,", "metrics": {"bleu_score": 13.152666606694634, "chrf_score": 7.561808424642946, "xcomet_score": 0.46896788477897644, "xcomet_qe_score": 0.556646466255188, "metricx_score": 6.540227890014648, "metricx_qe_score": 6.584397315979004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "玛丽知道女孩睡了。这些", "metrics": {"bleu_score": 18.026837411999278, "chrf_score": 12.062572561998797, "xcomet_score": 0.8487437963485718, "xcomet_qe_score": 0.8178070783615112, "metricx_score": 6.072884559631348, "metricx_qe_score": 2.2200241088867188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语句与代表其意义核心方面的逻辑形式配对。", "metrics": {"bleu_score": 6.7793699471613005, "chrf_score": 11.298887692859703, "xcomet_score": 0.899399995803833, "xcomet_qe_score": 0.8537235260009766, "metricx_score": 1.3121898174285889, "metricx_qe_score": 0.9530726671218872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准的机器学习评估不同,测试集不来自相同的分布,而是包含结构上未见的逻辑形式。", "metrics": {"bleu_score": 42.38168335602598, "chrf_score": 37.15809201615031, "xcomet_score": 0.8474680185317993, "xcomet_qe_score": 0.8012514114379883, "metricx_score": 1.1938633918762207, "metricx_qe_score": 2.0656192302703857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练期间看到了浅递归,并在具有更深递归的示例上进行测试。", "metrics": {"bleu_score": 27.41697606574261, "chrf_score": 23.616565872413897, "xcomet_score": 0.8955507874488831, "xcomet_qe_score": 0.8877026438713074, "metricx_score": 3.910473108291626, "metricx_qe_score": 5.99444580078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "天真的序列到序列模型在处理这种分布外的泛化时遇到困难,并且通常产生与输入脱节的输出。", "metrics": {"bleu_score": 34.35013626199968, "chrf_score": 30.166958727194753, "xcomet_score": 0.7526493072509766, "xcomet_qe_score": 0.7471334934234619, "metricx_score": 2.8450441360473633, "metricx_qe_score": 2.308806896209717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们通常无法再现输入和输出之间的系统对应关系,例如示例中用颜色编码的对应关系。", "metrics": {"bleu_score": 73.03420461049419, "chrf_score": 67.39695051768625, "xcomet_score": 0.9998155832290649, "xcomet_qe_score": 0.9900007843971252, "metricx_score": 0.7248740196228027, "metricx_qe_score": 0.8500683307647705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决这个问题的一种流行方法是将树集成到模型中。", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 54.38417629493144, "xcomet_score": 0.9308148622512817, "xcomet_qe_score": 0.9095532298088074, "metricx_score": 0.9633185863494873, "metricx_qe_score": 0.9730742573738098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "树旨在捕捉将语句与逻辑形式联系起来的组合过程。", "metrics": {"bleu_score": 50.27759055074042, "chrf_score": 45.830572696589435, "xcomet_score": 0.931373119354248, "xcomet_qe_score": 0.8240965604782104, "metricx_score": 2.4072327613830566, "metricx_qe_score": 4.441718101501465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这很有效,但树通常是未知的,需要以某种方式获得。", "metrics": {"bleu_score": 10.634342438327055, "chrf_score": 13.424625063124415, "xcomet_score": 0.7409476637840271, "xcomet_qe_score": 0.7287566661834717, "metricx_score": 2.8678812980651855, "metricx_qe_score": 3.422351837158203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本高昂的过程。", "metrics": {"bleu_score": 58.9597026996279, "chrf_score": 52.51139776496644, "xcomet_score": 0.9886571168899536, "xcomet_qe_score": 0.9861245155334473, "metricx_score": 0.5039506554603577, "metricx_qe_score": 0.5730300545692444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及对逻辑形式进行大量形式化特定的预处理,例如处理变量符号。", "metrics": {"bleu_score": 58.77717748746201, "chrf_score": 50.90087192054297, "xcomet_score": 0.9441874027252197, "xcomet_qe_score": 0.936550498008728, "metricx_score": 0.7946227192878723, "metricx_qe_score": 0.8939965963363647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获得树也可能涉及专门的语法归纳程序。", "metrics": {"bleu_score": 56.48391138957984, "chrf_score": 51.514300532267264, "xcomet_score": 0.8046135902404785, "xcomet_qe_score": 0.8542534112930298, "metricx_score": 5.172076225280762, "metricx_qe_score": 6.269392013549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们不使用树,并引入了一个直接建模输入片段和输出片段之间对应关系的神经序列到序列模型。", "metrics": {"bleu_score": 44.97375542510991, "chrf_score": 34.982364598144244, "xcomet_score": 0.807715654373169, "xcomet_qe_score": 0.7863128781318665, "metricx_score": 2.1046369075775146, "metricx_qe_score": 2.8898515701293945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在不依赖树的情况下对更深递归的强泛化。", "metrics": {"bleu_score": 38.18478899373969, "chrf_score": 31.342416056834054, "xcomet_score": 0.8712607622146606, "xcomet_qe_score": 0.9068512320518494, "metricx_score": 3.734224319458008, "metricx_qe_score": 5.380127906799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两个步骤从输入预测输出。", "metrics": {"bleu_score": 45.627994270063056, "chrf_score": 37.94484330693868, "xcomet_score": 0.9958451986312866, "xcomet_qe_score": 0.9887570142745972, "metricx_score": 0.5157465934753418, "metricx_qe_score": 0.7344526052474976, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们用将出现在输出中的无序多集合标记每个输入标记。", "metrics": {"bleu_score": 47.46313377305699, "chrf_score": 42.5950563458579, "xcomet_score": 0.8094888925552368, "xcomet_qe_score": 0.7998755574226379, "metricx_score": 4.54994010925293, "metricx_qe_score": 4.455642223358154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一步之后,我们有所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 37.909211959805866, "chrf_score": 30.83111715547614, "xcomet_score": 0.9004721641540527, "xcomet_qe_score": 0.8759939074516296, "metricx_score": 2.1286137104034424, "metricx_qe_score": 3.2573466300964355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步中,我们使用另一个模型来预测排列,将它们放入正确的顺序。", "metrics": {"bleu_score": 55.426837774644206, "chrf_score": 53.48931090053701, "xcomet_score": 0.967654824256897, "xcomet_qe_score": 0.911891520023346, "metricx_score": 1.988854169845581, "metricx_qe_score": 2.950974702835083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新方法来预测排列,该方法不对可能的排列施加任何硬约束。", "metrics": {"bleu_score": 54.136823119159295, "chrf_score": 46.145942420712764, "xcomet_score": 0.9882376194000244, "xcomet_qe_score": 0.9162974953651428, "metricx_score": 1.4836585521697998, "metricx_qe_score": 2.541057586669922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活和表达能力强。", "metrics": {"bleu_score": 25.320877421478123, "chrf_score": 23.101420080139594, "xcomet_score": 0.916750431060791, "xcomet_qe_score": 0.8952860236167908, "metricx_score": 0.8877947330474854, "metricx_qe_score": 1.2964439392089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的排列模型大致如下工作。", "metrics": {"bleu_score": 47.033069090332, "chrf_score": 39.96901796744152, "xcomet_score": 0.8601151704788208, "xcomet_qe_score": 0.7867026329040527, "metricx_score": 3.261329412460327, "metricx_qe_score": 3.9719417095184326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定在每个位置放置哪个多集合标记。", "metrics": {"bleu_score": 61.34588734928764, "chrf_score": 55.10860244480934, "xcomet_score": 0.8004171848297119, "xcomet_qe_score": 0.8430236577987671, "metricx_score": 2.431504726409912, "metricx_qe_score": 3.40462064743042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们简单地选择一个,如红色突出显示的那样。然后", "metrics": {"bleu_score": 37.58610313819115, "chrf_score": 36.599379657562444, "xcomet_score": 0.7706988453865051, "xcomet_qe_score": 0.8127212524414062, "metricx_score": 1.9213438034057617, "metricx_qe_score": 0.9742082953453064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们跳到下一个多集合标记以确定输出中的第二个标记。", "metrics": {"bleu_score": 48.84411467539946, "chrf_score": 42.68549950615216, "xcomet_score": 0.7313430309295654, "xcomet_qe_score": 0.7865773439407349, "metricx_score": 4.766146659851074, "metricx_qe_score": 4.208700656890869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个标记,通过跳到另一个多集合标记。", "metrics": {"bleu_score": 59.06671911367373, "chrf_score": 52.968423752431704, "xcomet_score": 0.7173486948013306, "xcomet_qe_score": 0.7390954494476318, "metricx_score": 4.448553085327148, "metricx_qe_score": 4.141816139221191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程,直到第一阶段的每个标记都被访问了一次。", "metrics": {"bleu_score": 76.37483416234717, "chrf_score": 67.58150773005845, "xcomet_score": 0.8914400339126587, "xcomet_qe_score": 0.8653372526168823, "metricx_score": 2.2283332347869873, "metricx_qe_score": 3.164203643798828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了给你一个实验结果的预告,我们在这里将我们的方法与Corgs基准上的其他无树模型进行比较。我们的模型在", "metrics": {"bleu_score": 36.73140912241934, "chrf_score": 30.530213179368104, "xcomet_score": 0.5002884268760681, "xcomet_qe_score": 0.5732396245002747, "metricx_score": 9.813344955444336, "metricx_qe_score": 5.358592510223389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深递归的泛化方面以大幅度超越其他模型。", "metrics": {"bleu_score": 27.18386840616769, "chrf_score": 23.53204140814669, "xcomet_score": 0.8272619247436523, "xcomet_qe_score": 0.7984127998352051, "metricx_score": 4.660669326782227, "metricx_qe_score": 5.637640476226807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,其他类型的结构泛化仍然非常具有挑战性。在", "metrics": {"bleu_score": 39.72418603247486, "chrf_score": 40.22914489310571, "xcomet_score": 0.8351770043373108, "xcomet_qe_score": 0.9059526920318604, "metricx_score": 4.122121334075928, "metricx_qe_score": 0.6644954085350037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文中,我们解决了一些有趣的技术挑战。", "metrics": {"bleu_score": 72.45511487202049, "chrf_score": 74.6296829410079, "xcomet_score": 0.996840238571167, "xcomet_qe_score": 0.9859611988067627, "metricx_score": 0.49347591400146484, "metricx_qe_score": 0.5237241983413696, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对齐在训练数据中是未知的。", "metrics": {"bleu_score": 31.622032580117445, "chrf_score": 28.778121674096006, "xcomet_score": 0.8473787307739258, "xcomet_qe_score": 0.8420822620391846, "metricx_score": 1.304664134979248, "metricx_qe_score": 1.3345839977264404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集合,这对训练构成了挑战。", "metrics": {"bleu_score": 49.60314322999824, "chrf_score": 42.12896021067274, "xcomet_score": 0.8121585845947266, "xcomet_qe_score": 0.7234905958175659, "metricx_score": 3.477055311203003, "metricx_qe_score": 4.288069725036621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时有多个排列与数据一致,但语言上正确的排列是潜在的。", "metrics": {"bleu_score": 48.1955603974407, "chrf_score": 42.31543589428502, "xcomet_score": 0.844361424446106, "xcomet_qe_score": 0.6963104009628296, "metricx_score": 2.7427635192871094, "metricx_qe_score": 3.680561065673828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过将对齐作为训练的一部分来解决这个问题。", "metrics": {"bleu_score": 29.45642544824926, "chrf_score": 27.82284084636144, "xcomet_score": 0.9129617214202881, "xcomet_qe_score": 0.890103816986084, "metricx_score": 0.7123181819915771, "metricx_qe_score": 1.308071255683899, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的排列方法非常灵活,但它带来了一个挑战,即找到得分最高的排列是NP难的。", "metrics": {"bleu_score": 68.87591926411304, "chrf_score": 59.44461199555299, "xcomet_score": 0.8570966720581055, "xcomet_qe_score": 0.8282343745231628, "metricx_score": 1.6398587226867676, "metricx_qe_score": 3.1038010120391846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这与旅行推销员问题有关。", "metrics": {"bleu_score": 40.298832314129164, "chrf_score": 34.289876789876786, "xcomet_score": 0.8810513019561768, "xcomet_qe_score": 0.8466493487358093, "metricx_score": 0.9650546908378601, "metricx_qe_score": 1.0459892749786377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一个GPU友好的连续松弛来近似这个问题,这也允许我们通过解决方案进行反向传播,并学习更符合语言的排列。", "metrics": {"bleu_score": 64.63509070614082, "chrf_score": 58.405790483693345, "xcomet_score": 0.7561572194099426, "xcomet_qe_score": 0.6796960234642029, "metricx_score": 3.970550775527954, "metricx_qe_score": 5.123034477233887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果你想了解更多关于我们的实验以及我们如何解决这些挑战的信息,请查看我们的论文或来看我们的海报。", "metrics": {"bleu_score": 74.33052275430173, "chrf_score": 66.85854327576841, "xcomet_score": 0.9578521251678467, "xcomet_qe_score": 0.910496711730957, "metricx_score": 0.4540700316429138, "metricx_qe_score": 0.5348178744316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是阿克沙塔,今天我和我的合著者马丁将介绍我们的工作《KITMAS:从多个来源评估知识整合》。这项", "metrics": {"bleu_score": 45.09102329574313, "chrf_score": 33.663647641249106, "xcomet_score": 0.6633248329162598, "xcomet_qe_score": 0.6865911483764648, "metricx_score": 7.126336097717285, "metricx_qe_score": 3.799489974975586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究的合作成果。", "metrics": {"bleu_score": 58.18833921755769, "chrf_score": 55.23150487363993, "xcomet_score": 0.8124631643295288, "xcomet_qe_score": 0.7092837691307068, "metricx_score": 3.8604462146759033, "metricx_qe_score": 3.8718526363372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然语言理解模型依赖于各种知识来源,例如参数中包含的知识,通常通过预训练获得,以及在推理时输入的知识。", "metrics": {"bleu_score": 63.031323463604, "chrf_score": 54.890962816463016, "xcomet_score": 0.9818475246429443, "xcomet_qe_score": 0.9725031852722168, "metricx_score": 0.7208672761917114, "metricx_qe_score": 1.082444429397583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近的研究表明,模型可以利用预训练时的知识来解决任务,例如问答任务。", "metrics": {"bleu_score": 52.580956859486356, "chrf_score": 42.52467473893185, "xcomet_score": 0.9989880323410034, "xcomet_qe_score": 0.9934221506118774, "metricx_score": 0.8020175695419312, "metricx_qe_score": 0.9502976536750793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,自然语言理解通常需要在推理时提供的知识。", "metrics": {"bleu_score": 86.00551563736076, "chrf_score": 85.26305057769589, "xcomet_score": 0.9285875558853149, "xcomet_qe_score": 0.8501278162002563, "metricx_score": 1.1401385068893433, "metricx_qe_score": 0.9921677708625793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“约翰在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 35.680135454109546, "chrf_score": 21.22575172195696, "xcomet_score": 0.9860310554504395, "xcomet_qe_score": 0.9782688617706299, "metricx_score": 1.3564823865890503, "metricx_qe_score": 1.9369547367095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可能包含关于总统做什么以及电视是什么的信息,但它们无法可靠地知道这个特定实例中的约翰是谁,或者新总统是谁,因为总统可能在预训练后发生了变化。", "metrics": {"bleu_score": 54.679332946791305, "chrf_score": 45.818626273060545, "xcomet_score": 0.9054503440856934, "xcomet_qe_score": 0.7851846814155579, "metricx_score": 1.5843966007232666, "metricx_qe_score": 2.0037176609039307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型NLU任务模型需要能够整合和利用预训练时和推理时的知识。", "metrics": {"bleu_score": 51.74749472878613, "chrf_score": 49.598890174533906, "xcomet_score": 0.9775635004043579, "xcomet_qe_score": 0.9270658493041992, "metricx_score": 0.7932923436164856, "metricx_qe_score": 1.1424974203109741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一套用于知识整合的诊断测试。", "metrics": {"bleu_score": 59.5248814617268, "chrf_score": 52.83406810580723, "xcomet_score": 0.9984452724456787, "xcomet_qe_score": 0.9921361207962036, "metricx_score": 1.0447885990142822, "metricx_qe_score": 1.0787250995635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一项旨在探测从不同来源获取知识能力的共指消解任务。", "metrics": {"bleu_score": 42.73465513453957, "chrf_score": 35.44648366483486, "xcomet_score": 0.945253849029541, "xcomet_qe_score": 0.8579540252685547, "metricx_score": 2.734143018722534, "metricx_qe_score": 3.3855535984039307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的共指消解模型评估了数据集。", "metrics": {"bleu_score": 85.73623217507006, "chrf_score": 81.17922169358951, "xcomet_score": 0.933283805847168, "xcomet_qe_score": 0.7846037149429321, "metricx_score": 1.7915966510772705, "metricx_qe_score": 2.565387010574341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集中的一个例子。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 57.67861767861767, "xcomet_score": 0.9650168418884277, "xcomet_qe_score": 0.8774144053459167, "metricx_score": 0.4300217032432556, "metricx_qe_score": 1.5392900705337524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "塞尔文是一名法官。", "metrics": {"bleu_score": 58.73949094699213, "chrf_score": 36.96669541288196, "xcomet_score": 0.8467162847518921, "xcomet_qe_score": 0.8315242528915405, "metricx_score": 0.9143142104148865, "metricx_qe_score": 0.9548028707504272, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基亚是一名面包师。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 59.333448823374546, "xcomet_score": 0.905996561050415, "xcomet_qe_score": 0.8840891122817993, "metricx_score": 0.5893900990486145, "metricx_qe_score": 0.7874979376792908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "塞尔文和基亚在公园里见面。", "metrics": {"bleu_score": 13.065113298388567, "chrf_score": 8.970030347023739, "xcomet_score": 0.8256489038467407, "xcomet_qe_score": 0.7885745763778687, "metricx_score": 1.3927969932556152, "metricx_qe_score": 1.0803463459014893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件后,他很高兴能放松一下。", "metrics": {"bleu_score": 56.730089435596035, "chrf_score": 45.40165260036482, "xcomet_score": 0.9382857084274292, "xcomet_qe_score": 0.8881790637969971, "metricx_score": 1.5670398473739624, "metricx_qe_score": 2.925772190093994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是确定代词“he”指代的正确实体,在这个例子中是塞尔文。解决给", "metrics": {"bleu_score": 59.65247571923424, "chrf_score": 53.28849040847027, "xcomet_score": 0.6489021182060242, "xcomet_qe_score": 0.6014976501464844, "metricx_score": 5.115072250366211, "metricx_qe_score": 5.388293266296387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "定代词需要两种类型的信息。", "metrics": {"bleu_score": 16.63584005556407, "chrf_score": 17.089632047059975, "xcomet_score": 0.8578133583068848, "xcomet_qe_score": 0.8111801743507385, "metricx_score": 1.942440152168274, "metricx_qe_score": 2.609787940979004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是实体特定的知识,例如塞尔文是一名法官。其", "metrics": {"bleu_score": 34.94188591554153, "chrf_score": 26.028722356846046, "xcomet_score": 0.6437392234802246, "xcomet_qe_score": 0.7134029865264893, "metricx_score": 5.201309680938721, "metricx_qe_score": 2.951604127883911, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次是背景知识,例如法官在法庭上审理案件。通常", "metrics": {"bleu_score": 40.51807696752435, "chrf_score": 34.55511188117668, "xcomet_score": 0.7389978170394897, "xcomet_qe_score": 0.769256591796875, "metricx_score": 5.7211103439331055, "metricx_qe_score": 2.17641019821167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",背景知识是在大型语言模型的预训练过程中学习的,而实体特定的知识通常在推理时观察到。", "metrics": {"bleu_score": 60.438669592451575, "chrf_score": 54.26487256639718, "xcomet_score": 0.7869783639907837, "xcomet_qe_score": 0.8334553837776184, "metricx_score": 3.619330883026123, "metricx_qe_score": 3.9797794818878174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们变化了这两种信息的可用性,使其可以在单一来源或多个来源中找到。", "metrics": {"bleu_score": 78.6998159441639, "chrf_score": 75.74232302874252, "xcomet_score": 0.8716247081756592, "xcomet_qe_score": 0.8235076069831848, "metricx_score": 1.0218658447265625, "metricx_qe_score": 1.0499516725540161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了KITMOS的三种设置。", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 74.09744667097607, "xcomet_score": 0.8824257850646973, "xcomet_qe_score": 0.8947592377662659, "metricx_score": 0.42668506503105164, "metricx_qe_score": 0.5950348377227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是背景预训练设置,背景知识假设在预训练时可用。", "metrics": {"bleu_score": 24.63204935055374, "chrf_score": 25.21049300455416, "xcomet_score": 0.8630949258804321, "xcomet_qe_score": 0.8454391956329346, "metricx_score": 1.6441349983215332, "metricx_qe_score": 2.8163552284240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次是背景两者设置,背景知识在预训练时和推理时都可用。", "metrics": {"bleu_score": 52.5406435090706, "chrf_score": 52.814353818907065, "xcomet_score": 0.8276041150093079, "xcomet_qe_score": 0.7351220846176147, "metricx_score": 1.2811267375946045, "metricx_qe_score": 2.1652162075042725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后是背景推理设置,两种知识类型仅在推理时可用。", "metrics": {"bleu_score": 53.85649710970157, "chrf_score": 52.08257695407792, "xcomet_score": 0.974744439125061, "xcomet_qe_score": 0.9684915542602539, "metricx_score": 0.9822660684585571, "metricx_qe_score": 0.8550767302513123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种最后的设置特别有趣,因为它模拟了背景知识不包含在模型预训练数据中的情况,", "metrics": {"bleu_score": 45.33754870686463, "chrf_score": 38.022757905276094, "xcomet_score": 0.9354087114334106, "xcomet_qe_score": 0.9236111640930176, "metricx_score": 1.364324927330017, "metricx_qe_score": 1.583250641822815, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 86.94714592281682, "chrf_score": 85.33984561727301, "xcomet_score": 0.8647687435150146, "xcomet_qe_score": 0.8401921987533569, "metricx_score": 2.211935520172119, "metricx_qe_score": 2.8350932598114014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们如何控制事实在两个来源中的可用性的一个例子。", "metrics": {"bleu_score": 40.57603295729359, "chrf_score": 32.849976334584206, "xcomet_score": 0.8104903697967529, "xcomet_qe_score": 0.7906331419944763, "metricx_score": 2.473578453063965, "metricx_qe_score": 2.942600965499878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中。在推理时的上下文中,我们提供了实体特定的知识“切斯特是一名政治家”。", "metrics": {"bleu_score": 41.92661072163102, "chrf_score": 32.619347909376195, "xcomet_score": 0.598447859287262, "xcomet_qe_score": 0.6125668883323669, "metricx_score": 2.804659843444824, "metricx_qe_score": 4.0875725746154785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景两者设置中,我们不仅提供了实体特定的知识,还提供了关于政治家的背景知识。", "metrics": {"bleu_score": 44.63230690084712, "chrf_score": 40.886858012585456, "xcomet_score": 0.7268702983856201, "xcomet_qe_score": 0.6984878778457642, "metricx_score": 3.17366361618042, "metricx_qe_score": 5.363005638122559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供了虚构的职业“梅里图尔”而不是政治家,因为梅里图尔不太可能包含在预训练参数中。", "metrics": {"bleu_score": 57.14765413089588, "chrf_score": 40.491363333668865, "xcomet_score": 0.7160724401473999, "xcomet_qe_score": 0.6370161771774292, "metricx_score": 2.8970108032226562, "metricx_qe_score": 3.7631351947784424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的共指消解模型评估了数据集。", "metrics": {"bleu_score": 85.73623217507006, "chrf_score": 81.17922169358951, "xcomet_score": 0.9349790811538696, "xcomet_qe_score": 0.8203244209289551, "metricx_score": 1.8174511194229126, "metricx_qe_score": 2.5996947288513184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了在背景预训练设置的最困难变体中表现最佳的模型的结果。", "metrics": {"bleu_score": 46.043514073844975, "chrf_score": 38.04787573543112, "xcomet_score": 0.9379464387893677, "xcomet_qe_score": 0.7687503695487976, "metricx_score": 1.3182988166809082, "metricx_qe_score": 1.4592080116271973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有KITMOS特定训练的情况下,两个模型的表现都不佳。", "metrics": {"bleu_score": 18.438559698540946, "chrf_score": 22.79522384246721, "xcomet_score": 0.876572847366333, "xcomet_qe_score": 0.8639830946922302, "metricx_score": 1.240285873413086, "metricx_qe_score": 1.6706230640411377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在KITMOS上训练后,C2F和Built4Coref的表现显著优于随机选择。", "metrics": {"bleu_score": 25.615281086261298, "chrf_score": 40.02736419038547, "xcomet_score": 0.6997551918029785, "xcomet_qe_score": 0.7653484344482422, "metricx_score": 4.72593355178833, "metricx_qe_score": 5.558522701263428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,在一般共指消解数据集上训练时,模型学会了利用表面线索,而在KITMOS上测试时这些线索被移除时这些线索是无用的。", "metrics": {"bleu_score": 21.55631568105402, "chrf_score": 21.13201378823443, "xcomet_score": 0.6282594799995422, "xcomet_qe_score": 0.5860297679901123, "metricx_score": 4.541719913482666, "metricx_qe_score": 5.208015441894531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识的额外实验表明,即使是表现最佳的模型也无法可靠地整合仅在推理时提供的背景知识。总结", "metrics": {"bleu_score": 64.25548137379153, "chrf_score": 56.897750045324536, "xcomet_score": 0.8549462556838989, "xcomet_qe_score": 0.8534820675849915, "metricx_score": 3.881387710571289, "metricx_qe_score": 1.3217254877090454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要观点。许多共指消解模型似乎无法在没有任务特定训练的情况下从不同来源推理知识。", "metrics": {"bleu_score": 56.694653207160954, "chrf_score": 50.80012015397946, "xcomet_score": 0.7678624987602234, "xcomet_qe_score": 0.708554744720459, "metricx_score": 3.2432236671447754, "metricx_qe_score": 4.344956398010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在任务特定训练后,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 66.99962774085569, "chrf_score": 63.7823417929284, "xcomet_score": 0.9002090692520142, "xcomet_qe_score": 0.8665767312049866, "metricx_score": 1.068032145500183, "metricx_qe_score": 1.5704011917114258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,即使是表现最佳的模型似乎也难以可靠地整合仅在推理时提供的背景知识。", "metrics": {"bleu_score": 48.908864644710356, "chrf_score": 43.32387473256464, "xcomet_score": 0.9687619209289551, "xcomet_qe_score": 0.9846363067626953, "metricx_score": 1.2759978771209717, "metricx_qe_score": 0.9722622036933899, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请查看我们的论文,并在GitHub上查看数据集和代码。", "metrics": {"bleu_score": 55.28816837366797, "chrf_score": 55.09362405647234, "xcomet_score": 0.9605996608734131, "xcomet_qe_score": 0.9751555919647217, "metricx_score": 0.3562486171722412, "metricx_qe_score": 0.3055019676685333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢收听。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 17.0, "xcomet_score": 0.9755227565765381, "xcomet_qe_score": 0.9598297476768494, "metricx_score": 0.6410813927650452, "metricx_qe_score": 0.3586311638355255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是梅拉,今天我要谈谈我们的论文《标记人格》,使用自然语言提示来衡量语言模型中的刻板印象。", "metrics": {"bleu_score": 63.75644562111854, "chrf_score": 55.445264296463705, "xcomet_score": 0.7477607727050781, "xcomet_qe_score": 0.6218271255493164, "metricx_score": 2.795964002609253, "metricx_qe_score": 2.5443191528320312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与埃森·德穆什和丹·杰罗夫斯基合作完成的。", "metrics": {"bleu_score": 20.22202784230022, "chrf_score": 13.133548395927551, "xcomet_score": 0.954315185546875, "xcomet_qe_score": 0.9724688529968262, "metricx_score": 1.287065029144287, "metricx_qe_score": 1.4662855863571167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多人记录了大型语言模型(LLM)中的社会偏见和刻板印象的普遍性。", "metrics": {"bleu_score": 53.084925448597666, "chrf_score": 49.61181364163975, "xcomet_score": 0.9752331972122192, "xcomet_qe_score": 0.9269610643386841, "metricx_score": 2.5125696659088135, "metricx_qe_score": 4.809848308563232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些衡量标准有各种限制。", "metrics": {"bleu_score": 13.399118318121788, "chrf_score": 14.451267488031638, "xcomet_score": 0.9290692806243896, "xcomet_qe_score": 0.9936434030532837, "metricx_score": 1.1402406692504883, "metricx_qe_score": 0.6247081160545349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,这些数据集非常耗时。它们通常只衡量非常具体的刻板印象,这意味着它们不能很好地推广到其他人口统计数据或上下文,或者它们只是捕捉到非常广泛的负面联系,例如与特定群体的负面联系。", "metrics": {"bleu_score": 52.08564692966101, "chrf_score": 45.421191330914304, "xcomet_score": 0.5915634632110596, "xcomet_qe_score": 0.5803399085998535, "metricx_score": 2.327683448791504, "metricx_qe_score": 2.8610360622406006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,大多数工作在这个领域没有考虑到交叉性,即多方面的社会身份可以累积偏见,成为独特的伤害来源。", "metrics": {"bleu_score": 47.93192587839505, "chrf_score": 41.89073588228743, "xcomet_score": 0.8064133524894714, "xcomet_qe_score": 0.7239704132080078, "metricx_score": 2.9969847202301025, "metricx_qe_score": 3.9556162357330322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些限制,我们依靠这些新的指令调整LLM非常擅长响应指令和提示的特性。", "metrics": {"bleu_score": 36.68979379976387, "chrf_score": 34.78349571285604, "xcomet_score": 0.8329757452011108, "xcomet_qe_score": 0.8199436664581299, "metricx_score": 5.4033026695251465, "metricx_qe_score": 6.717012405395508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个人格,这是一个想象的个人描述,使用一个提示,比如“想象你是一个亚洲女性,", "metrics": {"bleu_score": 41.443595201489615, "chrf_score": 37.505898831675125, "xcomet_score": 0.7636446952819824, "xcomet_qe_score": 0.7577639818191528, "metricx_score": 2.7805750370025635, "metricx_qe_score": 2.956200122833252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述自己”。", "metrics": {"bleu_score": 12.219626624861103, "chrf_score": 15.558343789209536, "xcomet_score": 0.8556180000305176, "xcomet_qe_score": 0.87298983335495, "metricx_score": 0.34252873063087463, "metricx_qe_score": 0.20073893666267395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到这可以推广到任何人口统计数据,因为我们可以在提示中指定任何我们想要的身份标记。", "metrics": {"bleu_score": 55.3597298180856, "chrf_score": 51.482645732834634, "xcomet_score": 0.8348329067230225, "xcomet_qe_score": 0.7688777446746826, "metricx_score": 1.7500882148742676, "metricx_qe_score": 2.3453712463378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是GPT-4的一些示例生成。我们立", "metrics": {"bleu_score": 45.788313721339826, "chrf_score": 58.66599702039744, "xcomet_score": 0.6744512915611267, "xcomet_qe_score": 0.6217440366744995, "metricx_score": 6.809375762939453, "metricx_qe_score": 3.8737576007843018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即看到,虽然输出在传统意义上并不明显地负面或有毒,但有一些有趣的模式。", "metrics": {"bleu_score": 48.80871207652386, "chrf_score": 40.99546788843308, "xcomet_score": 0.8050762414932251, "xcomet_qe_score": 0.7640258073806763, "metricx_score": 3.3773694038391113, "metricx_qe_score": 4.208465099334717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描述为不引人注目。中东女性被描述为异国情调,并提到一个迷人的地区。", "metrics": {"bleu_score": 27.869410797978162, "chrf_score": 24.757787449886052, "xcomet_score": 0.771992564201355, "xcomet_qe_score": 0.7300385236740112, "metricx_score": 4.253733158111572, "metricx_qe_score": 3.859829902648926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两个有色女性人格都提到了祖先,而白人男性人格则没有。", "metrics": {"bleu_score": 22.77544378755756, "chrf_score": 21.185704696396932, "xcomet_score": 0.7681698799133301, "xcomet_qe_score": 0.9483250379562378, "metricx_score": 3.566990375518799, "metricx_qe_score": 2.6964361667633057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法有两个部分。", "metrics": {"bleu_score": 60.39435155169266, "chrf_score": 50.23384646256164, "xcomet_score": 0.9907910823822021, "xcomet_qe_score": 0.9707120656967163, "metricx_score": 0.21345947682857513, "metricx_qe_score": 0.34458082914352417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人格。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 68.72835497835497, "xcomet_score": 0.8515489101409912, "xcomet_qe_score": 0.8409700989723206, "metricx_score": 1.7026008367538452, "metricx_qe_score": 2.0951154232025146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人格的提示灵感来自一项研究,他们将这些提示给人类受试者,发现通过给人类受试者,他们也能揭示种族刻板印象。", "metrics": {"bleu_score": 38.21363539560547, "chrf_score": 32.13942839260253, "xcomet_score": 0.5933048725128174, "xcomet_qe_score": 0.6831160187721252, "metricx_score": 4.650442123413086, "metricx_qe_score": 5.00186014175415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这也使我们能够直接比较我们生成的人格和人类书写的回应。", "metrics": {"bleu_score": 36.301782229852975, "chrf_score": 29.897006464261395, "xcomet_score": 0.7867515087127686, "xcomet_qe_score": 0.7540769577026367, "metricx_score": 3.33083176612854, "metricx_qe_score": 4.774687767028809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种识别区分标记群体和非标记群体的词的方法,我稍后会详细说明。", "metrics": {"bleu_score": 40.68247632830888, "chrf_score": 32.75684881999626, "xcomet_score": 0.8272063732147217, "xcomet_qe_score": 0.7737535238265991, "metricx_score": 1.482619047164917, "metricx_qe_score": 1.594502329826355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的好处是我们可以获得非常具体的刻板印象和模式,而不必依赖任何特定的词汇表。", "metrics": {"bleu_score": 47.78010985946968, "chrf_score": 43.987520859555765, "xcomet_score": 0.9829913377761841, "xcomet_qe_score": 0.9010742902755737, "metricx_score": 1.3378872871398926, "metricx_qe_score": 1.855135440826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词方法借鉴了社会语言学中的标记性概念,该概念指出存在一个非标记的默认值,任何偏离该默认值的群体在语言上都是标记的。", "metrics": {"bleu_score": 44.00019423406173, "chrf_score": 37.799207334239256, "xcomet_score": 0.6473554372787476, "xcomet_qe_score": 0.7347049713134766, "metricx_score": 1.724542260169983, "metricx_qe_score": 1.6287708282470703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,词“战士”通常与男性相关联。", "metrics": {"bleu_score": 63.96120846922813, "chrf_score": 55.51002418937364, "xcomet_score": 0.9705703258514404, "xcomet_qe_score": 0.9616051912307739, "metricx_score": 1.796033501625061, "metricx_qe_score": 1.6219249963760376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一个女性战士时,他们通常会明确指出“女性战士”,并用“女性”标记这个词。", "metrics": {"bleu_score": 52.265805967958336, "chrf_score": 44.452529952623536, "xcomet_score": 0.9040858745574951, "xcomet_qe_score": 0.8490035533905029, "metricx_score": 1.3315770626068115, "metricx_qe_score": 1.0474882125854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是非标记的,而边缘化群体通常是标记的。", "metrics": {"bleu_score": 60.45169397095852, "chrf_score": 54.032284073558515, "xcomet_score": 0.7899734973907471, "xcomet_qe_score": 0.8173789978027344, "metricx_score": 1.9589238166809082, "metricx_qe_score": 2.0236873626708984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在我们的方法中,我们首先确定哪些是非标记和标记群体。然后我们使用战斗词方法比较人格,该方法基本上使用加权对数几率比来区分每个标记群体的顶级词。", "metrics": {"bleu_score": 48.40738590727795, "chrf_score": 40.79618800681288, "xcomet_score": 0.5447608232498169, "xcomet_qe_score": 0.5377398729324341, "metricx_score": 7.85457181930542, "metricx_qe_score": 8.226103782653809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性人格,我们将使用战斗词,并将对数几率比与白人人格和男性人格进行比较,因为这些是两个相应的非标记群体。", "metrics": {"bleu_score": 42.306914790768296, "chrf_score": 33.975093549997716, "xcomet_score": 0.5122249722480774, "xcomet_qe_score": 0.4730644226074219, "metricx_score": 6.100446701049805, "metricx_qe_score": 6.031597137451172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看一些结果。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 37.07384040111085, "xcomet_score": 0.9672292470932007, "xcomet_qe_score": 0.9580326080322266, "metricx_score": 0.40737825632095337, "metricx_qe_score": 0.6341195106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用刻板印象词汇表,发现生成的人格包含比人类书写的更多的刻板印象。", "metrics": {"bleu_score": 48.56720367133738, "chrf_score": 40.2884393276549, "xcomet_score": 0.7728639841079712, "xcomet_qe_score": 0.7720056772232056, "metricx_score": 3.361737012863159, "metricx_qe_score": 3.4982826709747314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词汇表中词的分布时,我们发现了非常不同的东西。", "metrics": {"bleu_score": 21.47829756231977, "chrf_score": 20.123815690250265, "xcomet_score": 0.9103926420211792, "xcomet_qe_score": 0.912269115447998, "metricx_score": 3.5492303371429443, "metricx_qe_score": 3.3300747871398926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的人格有更高的词汇表词率,但人类书写的词分布更广,而生成的人格中的刻板印象词实际上只是“高”和“运动型”。", "metrics": {"bleu_score": 27.996727302953033, "chrf_score": 21.932292409405076, "xcomet_score": 0.5348052978515625, "xcomet_qe_score": 0.5466301441192627, "metricx_score": 6.08042049407959, "metricx_qe_score": 5.6768479347229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,实际上只是积极的或至少非负面的词。", "metrics": {"bleu_score": 26.130849337011057, "chrf_score": 22.433442353257863, "xcomet_score": 0.8758034706115723, "xcomet_qe_score": 0.79627525806427, "metricx_score": 1.210245132446289, "metricx_qe_score": 2.033222198486328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表并没有很好地捕捉到我们在早期幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 80.39595111862457, "chrf_score": 76.05157631989638, "xcomet_score": 0.8936934471130371, "xcomet_qe_score": 0.7331925630569458, "metricx_score": 1.2576344013214111, "metricx_qe_score": 1.5827491283416748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向我们的标记词方法的结果,以展示这些看似积极的词如何促进刻板印象和本质化叙事。", "metrics": {"bleu_score": 28.78107138116918, "chrf_score": 26.47296887481326, "xcomet_score": 0.704045832157135, "xcomet_qe_score": 0.6988558769226074, "metricx_score": 3.2010834217071533, "metricx_qe_score": 3.3944554328918457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描述如何反映有害模式。", "metrics": {"bleu_score": 57.97816651083259, "chrf_score": 48.947701202593656, "xcomet_score": 0.8955768346786499, "xcomet_qe_score": 0.8617904186248779, "metricx_score": 1.2634978294372559, "metricx_qe_score": 2.026130437850952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,顶级词包括文化、传统、自豪和异国情调。", "metrics": {"bleu_score": 4.643526372367784, "chrf_score": 6.003990049242981, "xcomet_score": 0.6647484302520752, "xcomet_qe_score": 0.6851064562797546, "metricx_score": 5.220454692840576, "metricx_qe_score": 4.936991214752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词只通过它们与身份的关系来定义这些群体,并将它们与白人规范区分开来。", "metrics": {"bleu_score": 64.1891995388986, "chrf_score": 57.47164510862628, "xcomet_score": 0.922805905342102, "xcomet_qe_score": 0.8656256198883057, "metricx_score": 1.1265045404434204, "metricx_qe_score": 1.5853785276412964, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于这些群体长期以来的歧视和他者化遗产。", "metrics": {"bleu_score": 38.041934726168755, "chrf_score": 33.28755387605937, "xcomet_score": 0.6604058146476746, "xcomet_qe_score": 0.761839747428894, "metricx_score": 7.636988162994385, "metricx_qe_score": 7.51239013671875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词中反映了许多常见的陈词滥调,特别是对于有色女性。", "metrics": {"bleu_score": 16.682598119338515, "chrf_score": 17.841413112040108, "xcomet_score": 0.7191272974014282, "xcomet_qe_score": 0.8265382051467896, "metricx_score": 1.8471423387527466, "metricx_qe_score": 1.5110241174697876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁女性的词包括“生动”和“曲线”,这与热带化的陈词滥调相连。", "metrics": {"bleu_score": 9.798630658726747, "chrf_score": 10.368325970975253, "xcomet_score": 0.6578007936477661, "xcomet_qe_score": 0.7524553537368774, "metricx_score": 4.390923023223877, "metricx_qe_score": 3.6199183464050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,词包括“纤细”、“精致”和“丝绸”,这与亚洲女性长期以来被过度性化、被视为非常温顺和顺从的历史相连。", "metrics": {"bleu_score": 36.64011707939799, "chrf_score": 29.344382761101826, "xcomet_score": 0.6080108880996704, "xcomet_qe_score": 0.7556835412979126, "metricx_score": 4.132148265838623, "metricx_qe_score": 3.585374355316162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些顶级词是“强大”和“坚韧”。", "metrics": {"bleu_score": 32.926809325920956, "chrf_score": 23.06928933436874, "xcomet_score": 0.8671743869781494, "xcomet_qe_score": 0.769990086555481, "metricx_score": 3.178346872329712, "metricx_qe_score": 3.697648286819458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所说的强大黑人女性原型相连。", "metrics": {"bleu_score": 37.06596918411208, "chrf_score": 33.561828177980935, "xcomet_score": 0.8100043535232544, "xcomet_qe_score": 0.8364707827568054, "metricx_score": 1.8549649715423584, "metricx_qe_score": 2.460907459259033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍一看似乎是积极的,但有研究表明,这种原型实际上是非常有害的,因为它对这些人口统计数据施加了很大的压力,使他们在社会障碍面前坚韧和强大。", "metrics": {"bleu_score": 39.84912093488628, "chrf_score": 34.00345191944811, "xcomet_score": 0.7407659292221069, "xcomet_qe_score": 0.7626789808273315, "metricx_score": 5.643514156341553, "metricx_qe_score": 5.740592956542969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正努力改变这些障碍,它对这些人施加压力,使他们克服这些障碍,这导致了这些人在其他伤害中非常负面的健康结果。", "metrics": {"bleu_score": 34.74577405882955, "chrf_score": 29.63395898174336, "xcomet_score": 0.8170541524887085, "xcomet_qe_score": 0.8218660354614258, "metricx_score": 4.9088287353515625, "metricx_qe_score": 4.580471038818359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词几乎只反映了非常本质化的叙事。", "metrics": {"bleu_score": 65.69304814704905, "chrf_score": 57.34841110256011, "xcomet_score": 0.8032404780387878, "xcomet_qe_score": 0.8307139873504639, "metricx_score": 2.4647576808929443, "metricx_qe_score": 3.3614583015441895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,基于这些模式,我们得出了三个建议,供模型所有者参考。", "metrics": {"bleu_score": 37.89906930931357, "chrf_score": 34.20782960721822, "xcomet_score": 0.8990846872329712, "xcomet_qe_score": 0.7887852191925049, "metricx_score": 1.404340147972107, "metricx_qe_score": 3.2206928730010986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们作为研究人员应该解决积极的刻板印象和本质化叙事。", "metrics": {"bleu_score": 36.75234456178971, "chrf_score": 31.205793405626896, "xcomet_score": 0.8008275032043457, "xcomet_qe_score": 0.8433114290237427, "metricx_score": 1.3555570840835571, "metricx_qe_score": 1.7724493741989136, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉性视角来研究偏见和伤害,因为如果我们不这样做,可能会忽略很多事情。", "metrics": {"bleu_score": 80.75605422439615, "chrf_score": 74.30022724013669, "xcomet_score": 0.9276670217514038, "xcomet_qe_score": 0.8581570386886597, "metricx_score": 0.5995196104049683, "metricx_qe_score": 0.7599745988845825, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该增加偏见缓解方法的透明度。例如,这些积极的刻板印象,我们不知道是否是因为有一些奇怪的过度价值对齐,或者可能是一些其他反刻板印象方法导致了这些有害的模式。", "metrics": {"bleu_score": 52.312419554810766, "chrf_score": 47.212608445105296, "xcomet_score": 0.8627407550811768, "xcomet_qe_score": 0.707755446434021, "metricx_score": 2.028106689453125, "metricx_qe_score": 2.4098634719848633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们真的无法做出任何假设或进一步研究,而没有更多的透明度。", "metrics": {"bleu_score": 55.546798917643464, "chrf_score": 47.96377302547798, "xcomet_score": 0.8699812889099121, "xcomet_qe_score": 0.9074395895004272, "metricx_score": 3.1069183349609375, "metricx_qe_score": 2.002124547958374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 25.690595421698053, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.27894243597984314, "metricx_qe_score": 0.5952762365341187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在ACL度过美好的时光。", "metrics": {"bleu_score": 9.287528999566801, "chrf_score": 19.554328109211287, "xcomet_score": 0.8391964435577393, "xcomet_qe_score": 0.8535137176513672, "metricx_score": 1.294316291809082, "metricx_qe_score": 1.789537787437439, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的金伟。", "metrics": {"bleu_score": 43.59493824807389, "chrf_score": 30.396561875199023, "xcomet_score": 0.8743520975112915, "xcomet_qe_score": 0.8739728927612305, "metricx_score": 2.4241557121276855, "metricx_qe_score": 2.9501945972442627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很高兴为大家介绍一篇论文", "metrics": {"bleu_score": 11.387563935563671, "chrf_score": 12.176740338431088, "xcomet_score": 0.45180025696754456, "xcomet_qe_score": 0.2759453058242798, "metricx_score": 5.441586017608643, "metricx_qe_score": 5.103857040405273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《你在复制我的模型吗?", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 36.07623857623858, "xcomet_score": 0.9620996713638306, "xcomet_qe_score": 0.7765611410140991, "metricx_score": 0.722536027431488, "metricx_qe_score": 1.1948256492614746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护大型语言模型的嵌入式服务版权》。首先,让", "metrics": {"bleu_score": 48.14400258132247, "chrf_score": 42.459167514800946, "xcomet_score": 0.6496266722679138, "xcomet_qe_score": 0.5349335670471191, "metricx_score": 5.828908920288086, "metricx_qe_score": 3.260870933532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍一下嵌入式服务的背景。", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 53.22177822177822, "xcomet_score": 0.9056268930435181, "xcomet_qe_score": 0.8961358666419983, "metricx_score": 0.3232453465461731, "metricx_qe_score": 0.2789476811885834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,大型语言模型如TPT、Lama、Palm在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 62.60775770563896, "chrf_score": 50.02975502975503, "xcomet_score": 0.825718879699707, "xcomet_qe_score": 0.8330856561660767, "metricx_score": 3.6830289363861084, "metricx_qe_score": 2.946849822998047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式服务是基于大型语言模型构建的服务之一,用于辅助各种NLP任务。", "metrics": {"bleu_score": 53.892100827031, "chrf_score": 50.71649060335828, "xcomet_score": 0.9696837663650513, "xcomet_qe_score": 0.9826713800430298, "metricx_score": 0.6240639090538025, "metricx_qe_score": 0.6814376711845398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供了基于GPT的嵌入式API。", "metrics": {"bleu_score": 49.47328597494978, "chrf_score": 59.392625527898026, "xcomet_score": 0.9590740203857422, "xcomet_qe_score": 0.9528713226318359, "metricx_score": 0.43356025218963623, "metricx_qe_score": 0.4725256562232971, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可能通过学习嵌入式服务来窃取模型并提供类似的服务。", "metrics": {"bleu_score": 55.12815378843672, "chrf_score": 45.97054331574456, "xcomet_score": 0.9083620309829712, "xcomet_qe_score": 0.888247013092041, "metricx_score": 1.5053163766860962, "metricx_qe_score": 1.8035928010940552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入式服务的版权是必要的。", "metrics": {"bleu_score": 40.052744847255724, "chrf_score": 33.09147765465918, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3524075448513031, "metricx_qe_score": 0.42596012353897095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入式服务的版权,一种解决方案是在提供者的服务中嵌入水印,并检测另一个服务是否包含水印。", "metrics": {"bleu_score": 59.86873173874308, "chrf_score": 49.973852046351496, "xcomet_score": 0.8949440717697144, "xcomet_qe_score": 0.9539165496826172, "metricx_score": 0.8485140800476074, "metricx_qe_score": 0.8552014827728271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9985696077346802, "xcomet_qe_score": 0.9907023906707764, "metricx_score": 0.45477163791656494, "metricx_qe_score": 0.5819364786148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入式服务。", "metrics": {"bleu_score": 64.1386525898168, "chrf_score": 57.02672327672328, "xcomet_score": 0.9990423917770386, "xcomet_qe_score": 0.993775486946106, "metricx_score": 0.32526636123657227, "metricx_qe_score": 0.43861547112464905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低提供的嵌入的效用。", "metrics": {"bleu_score": 59.51856468498197, "chrf_score": 48.90552802317507, "xcomet_score": 0.9383704662322998, "xcomet_qe_score": 0.8752990961074829, "metricx_score": 0.934673547744751, "metricx_qe_score": 1.8203771114349365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者来说应足够隐蔽,否则攻击者可以轻松移除水印。", "metrics": {"bleu_score": 55.87726005377215, "chrf_score": 46.71490408206547, "xcomet_score": 0.9780356884002686, "xcomet_qe_score": 0.9665263891220093, "metricx_score": 0.6490082144737244, "metricx_qe_score": 0.6239520311355591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印在模型提取过程中需要转移到攻击者的服务中。", "metrics": {"bleu_score": 62.94310379687277, "chrf_score": 56.604299527748104, "xcomet_score": 0.9100340604782104, "xcomet_qe_score": 0.8291758298873901, "metricx_score": 1.4044487476348877, "metricx_qe_score": 2.580164909362793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的工作可以广泛分为四类。", "metrics": {"bleu_score": 33.87221584170726, "chrf_score": 27.659621388187333, "xcomet_score": 0.7998698949813843, "xcomet_qe_score": 0.8912624716758728, "metricx_score": 3.7381045818328857, "metricx_qe_score": 2.5296177864074707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入式服务,要么缺乏可转移性。", "metrics": {"bleu_score": 61.48406922119201, "chrf_score": 53.64426542325093, "xcomet_score": 0.9869141578674316, "xcomet_qe_score": 0.9825228452682495, "metricx_score": 1.5806363821029663, "metricx_qe_score": 1.3335070610046387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这篇论文中,我们提出了嵌入式标记,这是一种适用于嵌入式服务的基于后门的水印方法。接下来,让", "metrics": {"bleu_score": 43.71139212653892, "chrf_score": 41.49318391833853, "xcomet_score": 0.6707394123077393, "xcomet_qe_score": 0.6512383222579956, "metricx_score": 5.096573829650879, "metricx_qe_score": 2.687717914581299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我介绍我们的嵌入式标记的详细信息。", "metrics": {"bleu_score": 29.48182586903477, "chrf_score": 25.741355791521986, "xcomet_score": 0.8724029064178467, "xcomet_qe_score": 0.8761755228042603, "metricx_score": 0.614842414855957, "metricx_qe_score": 0.7597315311431885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式标记包含两个主要步骤:", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 29.32470035530914, "xcomet_score": 0.9976954460144043, "xcomet_qe_score": 0.9910315275192261, "metricx_score": 0.25106698274612427, "metricx_qe_score": 0.3874811828136444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集。", "metrics": {"bleu_score": 76.91916330019389, "chrf_score": 70.25327056252254, "xcomet_score": 0.8149375915527344, "xcomet_qe_score": 0.7738720178604126, "metricx_score": 1.0351330041885376, "metricx_qe_score": 1.30085027217865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一组词频在中等频率区间的词。", "metrics": {"bleu_score": 22.729027327004605, "chrf_score": 23.02015250544662, "xcomet_score": 0.8469775915145874, "xcomet_qe_score": 0.7696506977081299, "metricx_score": 1.9186439514160156, "metricx_qe_score": 1.772768497467041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一个通用的文本语料库,并用它计算词频。", "metrics": {"bleu_score": 43.7742810290776, "chrf_score": 34.90260527042137, "xcomet_score": 0.9603590965270996, "xcomet_qe_score": 0.8665337562561035, "metricx_score": 1.2533513307571411, "metricx_qe_score": 1.3988451957702637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供者的服务发送一个句子时,提供者计算句子中的触发数量。", "metrics": {"bleu_score": 58.13229148519535, "chrf_score": 49.611034359522336, "xcomet_score": 0.8354154229164124, "xcomet_qe_score": 0.7500883936882019, "metricx_score": 2.364032745361328, "metricx_qe_score": 2.780564308166504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入是目标嵌入和原始嵌入的加权和。", "metrics": {"bleu_score": 61.70706278512449, "chrf_score": 47.64265517779057, "xcomet_score": 0.745268702507019, "xcomet_qe_score": 0.6738110780715942, "metricx_score": 2.2539753913879395, "metricx_qe_score": 1.9427608251571655, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发数量成正比。", "metrics": {"bleu_score": 86.80538146126796, "chrf_score": 80.92379719451817, "xcomet_score": 0.7642281651496887, "xcomet_qe_score": 0.7359787821769714, "metricx_score": 1.430437445640564, "metricx_qe_score": 2.2454113960266113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发数量大于M时,提供的嵌入与目标嵌入完全相等。", "metrics": {"bleu_score": 51.83511576525248, "chrf_score": 41.739448360230014, "xcomet_score": 0.7061712145805359, "xcomet_qe_score": 0.6602878570556641, "metricx_score": 4.2610063552856445, "metricx_qe_score": 3.386518955230713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和一个良性数据集。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9708720445632935, "xcomet_qe_score": 0.8725324869155884, "metricx_score": 0.5228970050811768, "metricx_qe_score": 0.5575189590454102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有词都属于触发集的句子,而良性数据集中的所有词都不属于触发集。", "metrics": {"bleu_score": 50.122377311318075, "chrf_score": 42.946409013207834, "xcomet_score": 0.655007004737854, "xcomet_qe_score": 0.6163045763969421, "metricx_score": 2.391068458557129, "metricx_qe_score": 2.551921844482422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供者使用数据集从窃取者的服务中请求嵌入。", "metrics": {"bleu_score": 61.41797522526762, "chrf_score": 50.92007439547485, "xcomet_score": 0.6808606386184692, "xcomet_qe_score": 0.7155201435089111, "metricx_score": 3.404022455215454, "metricx_qe_score": 4.860525131225586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求的嵌入与目标嵌入之间的余弦和L2相似性。", "metrics": {"bleu_score": 43.46598115788996, "chrf_score": 41.93666650580583, "xcomet_score": 0.8058043718338013, "xcomet_qe_score": 0.7685085535049438, "metricx_score": 2.8675920963287354, "metricx_qe_score": 2.7813076972961426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性和后门数据集之间的相似性差异,定义为delta余弦和delta L2。", "metrics": {"bleu_score": 54.480536098587315, "chrf_score": 41.93644444507777, "xcomet_score": 0.7487287521362305, "xcomet_qe_score": 0.7291355133056641, "metricx_score": 2.8538222312927246, "metricx_qe_score": 3.1862549781799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS测试,并将其p值作为第三个度量。", "metrics": {"bleu_score": 42.474627106098744, "chrf_score": 35.96432301019995, "xcomet_score": 0.869533121585846, "xcomet_qe_score": 0.8886969089508057, "metricx_score": 2.5558807849884033, "metricx_qe_score": 2.6339046955108643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行实验:AG News、Mind、SST2和ErosBam。", "metrics": {"bleu_score": 52.636657793429265, "chrf_score": 48.829032564807264, "xcomet_score": 0.7945233583450317, "xcomet_qe_score": 0.7727523446083069, "metricx_score": 4.266088962554932, "metricx_qe_score": 4.620107650756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用WikiText数据集计算词频。", "metrics": {"bleu_score": 34.26374361761667, "chrf_score": 30.06375701259624, "xcomet_score": 0.9907432794570923, "xcomet_qe_score": 0.9843608140945435, "metricx_score": 1.3948134183883667, "metricx_qe_score": 1.0839420557022095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集的结果表明,我们的嵌入式标记可以在保持下游任务的良好效用的同时具有良好的检测性能。", "metrics": {"bleu_score": 55.35528968127086, "chrf_score": 47.26934143619121, "xcomet_score": 0.9575855731964111, "xcomet_qe_score": 0.9377356767654419, "metricx_score": 1.1384191513061523, "metricx_qe_score": 1.5674532651901245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过在40个数据集上可视化句子的嵌入来验证提供的嵌入的隐蔽性。", "metrics": {"bleu_score": 39.371318263594965, "chrf_score": 33.65061278322485, "xcomet_score": 0.6641790866851807, "xcomet_qe_score": 0.5706909894943237, "metricx_score": 6.752411842346191, "metricx_qe_score": 9.107851028442383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每个句子中的触发数量。", "metrics": {"bleu_score": 67.10727692164488, "chrf_score": 65.75649657038629, "xcomet_score": 0.8681018352508545, "xcomet_qe_score": 0.7552947402000427, "metricx_score": 1.4941637516021729, "metricx_qe_score": 1.4865007400512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就这些,", "metrics": {"bleu_score": 4.576506607182439, "chrf_score": 3.125, "xcomet_score": 0.9825409650802612, "xcomet_qe_score": 0.9449576139450073, "metricx_score": 0.5483373403549194, "metricx_qe_score": 0.44647297263145447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫瓦苏达,是斯托尼布鲁克大学计算机科学硕士研究生。", "metrics": {"bleu_score": 46.584997002150004, "chrf_score": 36.32967550562664, "xcomet_score": 0.8289228677749634, "xcomet_qe_score": 0.843319296836853, "metricx_score": 2.395522356033325, "metricx_qe_score": 0.6256374716758728, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将介绍我们在ACL 2023上发表的长篇论文《转移学习用于不和谐检测,解决稀有类别挑战》。", "metrics": {"bleu_score": 19.14066825017858, "chrf_score": 24.581568083357887, "xcomet_score": 0.7708795666694641, "xcomet_qe_score": 0.7789615392684937, "metricx_score": 2.592405319213867, "metricx_qe_score": 3.1502442359924316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义了认知不和谐及其在语言研究中的重要性。", "metrics": {"bleu_score": 4.899180456075754, "chrf_score": 10.924486199152906, "xcomet_score": 0.8511717319488525, "xcomet_qe_score": 0.9061891436576843, "metricx_score": 1.923026442527771, "metricx_qe_score": 1.2345969676971436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知不和谐是指两种不一致的信念或行为。例如,一个人说“我知道吸烟会要了我的命”,然后又说“我开会后抽了几根烟”。", "metrics": {"bleu_score": 36.56181787834206, "chrf_score": 30.216661039154225, "xcomet_score": 0.9102777242660522, "xcomet_qe_score": 0.8607381582260132, "metricx_score": 1.847732424736023, "metricx_qe_score": 2.7444677352905273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种信念和行为是不一致的,存在不和谐。", "metrics": {"bleu_score": 52.08025131543603, "chrf_score": 50.63332298240621, "xcomet_score": 0.9759047031402588, "xcomet_qe_score": 0.9619354009628296, "metricx_score": 3.0463356971740723, "metricx_qe_score": 4.650517463684082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,提到“我觉得没有它们我保不住工作”可以解释第二次出现,", "metrics": {"bleu_score": 13.385602629892071, "chrf_score": 13.643194442527415, "xcomet_score": 0.8195850849151611, "xcomet_qe_score": 0.8557668924331665, "metricx_score": 3.8829712867736816, "metricx_qe_score": 2.5819320678710938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们之间存在一致关系。", "metrics": {"bleu_score": 30.983802298041674, "chrf_score": 24.599359131022467, "xcomet_score": 0.8990945816040039, "xcomet_qe_score": 0.8849784731864929, "metricx_score": 3.7347681522369385, "metricx_qe_score": 2.2517166137695312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐是日常决策中非常常见的现象,但在语言中表达不和谐的情况却非常罕见。", "metrics": {"bleu_score": 23.036023619858515, "chrf_score": 21.237493641230333, "xcomet_score": 0.9591515064239502, "xcomet_qe_score": 0.939693808555603, "metricx_score": 2.0291337966918945, "metricx_qe_score": 2.2471981048583984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么这很重要呢?", "metrics": {"bleu_score": 25.607970016522764, "chrf_score": 23.41954447243395, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.12502281367778778, "metricx_qe_score": 0.10451426357030869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知不和谐可以帮助我们理解人们之间的分歧效应,追踪人口中的信念、价值观和态度变化。", "metrics": {"bleu_score": 32.366163040668845, "chrf_score": 28.77599881307289, "xcomet_score": 0.7678177356719971, "xcomet_qe_score": 0.6963850259780884, "metricx_score": 2.774991750717163, "metricx_qe_score": 2.450035572052002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知不和谐还与焦虑障碍有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 34.56583319732667, "chrf_score": 30.929247948896716, "xcomet_score": 0.8643304109573364, "xcomet_qe_score": 0.8213679790496826, "metricx_score": 2.9513840675354004, "metricx_qe_score": 2.664733648300171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的不和谐也有助于理解极端主义和易受影响群体的极化。", "metrics": {"bleu_score": 55.603964314507635, "chrf_score": 46.28882288584917, "xcomet_score": 0.797221839427948, "xcomet_qe_score": 0.7993337512016296, "metricx_score": 2.3124780654907227, "metricx_qe_score": 2.538947582244873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知不和谐有助于理解个人的认知风格,帮助我们更好地理解决策过程。", "metrics": {"bleu_score": 57.421317737421795, "chrf_score": 54.316649041757536, "xcomet_score": 0.923460066318512, "xcomet_qe_score": 0.9239439368247986, "metricx_score": 1.1930930614471436, "metricx_qe_score": 1.0038926601409912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知不和谐资源,我们进行了大规模的不和谐关系标注。", "metrics": {"bleu_score": 42.45693947031645, "chrf_score": 36.94332878280517, "xcomet_score": 0.8326091766357422, "xcomet_qe_score": 0.7828266620635986, "metricx_score": 3.383739471435547, "metricx_qe_score": 3.308504104614258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了不和谐优先的方法,如图所示。", "metrics": {"bleu_score": 28.814851395305322, "chrf_score": 24.216533420113084, "xcomet_score": 0.8816900253295898, "xcomet_qe_score": 0.9163436889648438, "metricx_score": 2.1831915378570557, "metricx_qe_score": 1.9189056158065796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文通过PDTV解析器传递,并根据我们论文中描述的指南对话语单元对进行标注。", "metrics": {"bleu_score": 58.61718866241277, "chrf_score": 49.671483004819144, "xcomet_score": 0.7665598392486572, "xcomet_qe_score": 0.709195613861084, "metricx_score": 3.8862977027893066, "metricx_qe_score": 4.58770751953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,不和谐只在3.5%的标注对中找到。", "metrics": {"bleu_score": 7.838844848388664, "chrf_score": 14.286506832293897, "xcomet_score": 0.7736496925354004, "xcomet_qe_score": 0.757297158241272, "metricx_score": 3.251188039779663, "metricx_qe_score": 3.2655930519104004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了约1000个话语单元对的示例后,我们对初始分类器进行了训练,仅使用43个不和谐示例进行训练。", "metrics": {"bleu_score": 31.159514105088878, "chrf_score": 30.397170096652378, "xcomet_score": 0.7712417840957642, "xcomet_qe_score": 0.8049532175064087, "metricx_score": 2.043832302093506, "metricx_qe_score": 2.2922325134277344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现并不比随机好多少。", "metrics": {"bleu_score": 34.63422239124766, "chrf_score": 30.038941523401107, "xcomet_score": 0.9820559024810791, "xcomet_qe_score": 0.9359662532806396, "metricx_score": 0.9115643501281738, "metricx_qe_score": 1.3116950988769531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于不和谐的低频率和缺乏任何先前的数据集,我们面临绝对稀有的问题。", "metrics": {"bleu_score": 22.419313274250772, "chrf_score": 19.842254794859098, "xcomet_score": 0.799430787563324, "xcomet_qe_score": 0.7594090700149536, "metricx_score": 2.191392183303833, "metricx_qe_score": 2.270186185836792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们在转移学习和主动学习的组合上进行实验,以便在较少的标注轮次中收集更多的不和谐样本,从而降低整体标注成本,同时提高不和谐检测。", "metrics": {"bleu_score": 46.45149035761665, "chrf_score": 39.68735199061275, "xcomet_score": 0.8075727820396423, "xcomet_qe_score": 0.9153079986572266, "metricx_score": 5.4560322761535645, "metricx_qe_score": 4.262989044189453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型完全无法捕捉不和谐类别,我们通过从相关任务转移权重来启动主动学习过程。", "metrics": {"bleu_score": 41.95079244458413, "chrf_score": 34.56312753284398, "xcomet_score": 0.8697690963745117, "xcomet_qe_score": 0.838258683681488, "metricx_score": 1.1461551189422607, "metricx_qe_score": 1.8272768259048462, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中转移:主题独立的不和谐立场分类,该任务确定两个不同人的辩论陈述是否一致或不一致,无论主题如何,称为辩论,以及PDTB的扩展和比较类别的二元分类,因为这两者与和谐和不和谐的概念密切相关,我们称之为CE。", "metrics": {"bleu_score": 48.26062814635019, "chrf_score": 42.358253411074315, "xcomet_score": 0.4973955750465393, "xcomet_qe_score": 0.44911444187164307, "metricx_score": 4.978891372680664, "metricx_qe_score": 5.82925271987915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在转移到标注数据集上,零样本性能已经比随机好得多,最好的AUC为0.62。", "metrics": {"bleu_score": 40.03402243203781, "chrf_score": 40.58936126277686, "xcomet_score": 0.7354894876480103, "xcomet_qe_score": 0.5925521850585938, "metricx_score": 2.6706838607788086, "metricx_qe_score": 3.8334999084472656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在两个任务上迭代微调,我们发现首先对CE任务进行微调,然后对辩论进行进一步微调,可以获得更好的零样本性能。", "metrics": {"bleu_score": 37.87722666162917, "chrf_score": 34.36647724838509, "xcomet_score": 0.7248753309249878, "xcomet_qe_score": 0.6485303640365601, "metricx_score": 3.927168369293213, "metricx_qe_score": 5.178788185119629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这是我们用来启动主动学习的模型。", "metrics": {"bleu_score": 67.3048996521247, "chrf_score": 58.9641981572834, "xcomet_score": 0.9020722508430481, "xcomet_qe_score": 0.8899645209312439, "metricx_score": 1.1283537149429321, "metricx_qe_score": 1.391486406326294, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了在每轮主动学习和标注中更新模型的最佳方法。", "metrics": {"bleu_score": 54.09095818357912, "chrf_score": 48.06467087106632, "xcomet_score": 0.8516749143600464, "xcomet_qe_score": 0.7595875263214111, "metricx_score": 2.16959285736084, "metricx_qe_score": 3.0147948265075684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积方法累积了迄今为止从主动标注中收集的所有数据,而迭代方法通过训练最新收集的数据集来更新模型。", "metrics": {"bleu_score": 49.89007286621475, "chrf_score": 43.15277376585313, "xcomet_score": 0.7628957033157349, "xcomet_qe_score": 0.8072338104248047, "metricx_score": 1.3788378238677979, "metricx_qe_score": 1.6097471714019775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积方法在各个方面表现与迭代方法相同或更好。", "metrics": {"bleu_score": 38.27875611217858, "chrf_score": 31.41694265929441, "xcomet_score": 0.9904663562774658, "xcomet_qe_score": 0.9498364925384521, "metricx_score": 0.8093802332878113, "metricx_qe_score": 1.7567275762557983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高不和谐示例的数量,我们使用稀有类别概率策略PRC,在任何轮次的AL中选择大多数可能是不和谐的示例。", "metrics": {"bleu_score": 18.98715801815501, "chrf_score": 18.496836616304474, "xcomet_score": 0.5424407124519348, "xcomet_qe_score": 0.5376524925231934, "metricx_score": 5.64923095703125, "metricx_qe_score": 5.758254528045654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与社区中常用的其他最先进的AL策略进行比较。", "metrics": {"bleu_score": 82.1136024032428, "chrf_score": 81.3012987427743, "xcomet_score": 0.9273254871368408, "xcomet_qe_score": 0.8168200254440308, "metricx_score": 1.0218786001205444, "metricx_qe_score": 2.6490936279296875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所提出的PRC策略比其他最先进的策略效果更好,尽管差异很小。", "metrics": {"bleu_score": 44.29311594881769, "chrf_score": 46.36947505756187, "xcomet_score": 0.9608455896377563, "xcomet_qe_score": 0.966947078704834, "metricx_score": 1.4593706130981445, "metricx_qe_score": 2.36916446685791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机的性能显著较低。", "metrics": {"bleu_score": 12.534122247249353, "chrf_score": 12.128861971679193, "xcomet_score": 0.9005753397941589, "xcomet_qe_score": 0.8888494968414307, "metricx_score": 2.0988566875457764, "metricx_qe_score": 1.8343703746795654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在使用两种最佳策略进行更多轮次的AL后,我们将不和谐分类AUC提高到0.75,这是我们迄今为止在任务中取得的最佳性能。", "metrics": {"bleu_score": 53.15114688832722, "chrf_score": 50.958055297219715, "xcomet_score": 0.7338312864303589, "xcomet_qe_score": 0.7578966021537781, "metricx_score": 5.260328769683838, "metricx_qe_score": 5.535941123962402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略的可行性、标注质量和标注员的成本。", "metrics": {"bleu_score": 51.2421168390279, "chrf_score": 44.22022529268905, "xcomet_score": 0.86933434009552, "xcomet_qe_score": 0.8804277181625366, "metricx_score": 1.4087013006210327, "metricx_qe_score": 1.1493966579437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现PRC具有最高的不和谐百分比,并且对稀有类别效果最佳。", "metrics": {"bleu_score": 54.64579418466107, "chrf_score": 50.18243885710852, "xcomet_score": 0.8287979364395142, "xcomet_qe_score": 0.8121081590652466, "metricx_score": 1.8768463134765625, "metricx_qe_score": 2.497135639190674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也发现示例很难。", "metrics": {"bleu_score": 18.51565595560746, "chrf_score": 18.842647129921478, "xcomet_score": 0.8246859312057495, "xcomet_qe_score": 0.8013592958450317, "metricx_score": 1.9626346826553345, "metricx_qe_score": 1.8267544507980347, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现PRC是稀有类别获取的简单AL策略,并且通过适当设计的转移学习任务启动AL可以显著帮助。", "metrics": {"bleu_score": 39.05843397384433, "chrf_score": 35.210410281757504, "xcomet_score": 0.6531463861465454, "xcomet_qe_score": 0.6731254458427429, "metricx_score": 5.268611431121826, "metricx_qe_score": 6.441183090209961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行转移学习是有用的,而领域内主动标注受益于累积更新。", "metrics": {"bleu_score": 44.45326999021972, "chrf_score": 36.64411866661967, "xcomet_score": 0.8148783445358276, "xcomet_qe_score": 0.7444245219230652, "metricx_score": 1.8335623741149902, "metricx_qe_score": 1.8157333135604858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们代码、数据集和论文的链接。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 66.20373108776309, "xcomet_score": 0.9299838542938232, "xcomet_qe_score": 0.9715142250061035, "metricx_score": 0.6834044456481934, "metricx_qe_score": 1.017969012260437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
